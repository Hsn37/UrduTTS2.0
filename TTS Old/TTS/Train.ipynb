{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "##### Run Pronouncer Before Preprocessing and Eval/Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aleena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xvzf Urdu/aleena.haider.tar.gz -C Urdu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python preprocess.py --base_dir . --dataset urdu --output training-aleena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aneesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wavs/SP004aneesa.abbasiF0446.wav\n",
      "wavs/SP004aneesa.abbasiF0240.wav\n",
      "wavs/SP004aneesa.abbasiF0162.wav\n",
      "wavs/SP004aneesa.abbasiF0524.wav\n",
      "wavs/SP004aneesa.abbasiF0602.wav\n",
      "wavs/SP004aneesa.abbasiF0411.wav\n",
      "wavs/SP004aneesa.abbasiF0399.wav\n",
      "wavs/SP004aneesa.abbasiF0080.wav\n",
      "wavs/SP004aneesa.abbasiF0016.wav\n",
      "wavs/SP004aneesa.abbasiF0286.wav\n",
      "wavs/SP004aneesa.abbasiF0364.wav\n",
      "wavs/SP004aneesa.abbasiF0455.wav\n",
      "wavs/SP004aneesa.abbasiF0457.wav\n",
      "wavs/SP004aneesa.abbasiF0648.wav\n",
      "wavs/SP004aneesa.abbasiF0173.wav\n",
      "wavs/SP004aneesa.abbasiF0251.wav\n",
      "wavs/SP004aneesa.abbasiF0109.wav\n",
      "wavs/SP004aneesa.abbasiF0535.wav\n",
      "wavs/SP004aneesa.abbasiF0613.wav\n",
      "wavs/SP004aneesa.abbasiF0546.wav\n",
      "wavs/SP004aneesa.abbasiF0422.wav\n",
      "wavs/SP004aneesa.abbasiF0500.wav\n",
      "wavs/SP004aneesa.abbasiF0091.wav\n",
      "wavs/SP004aneesa.abbasiF0027.wav\n",
      "wavs/SP004aneesa.abbasiF0297.wav\n",
      "wavs/SP004aneesa.abbasiF0375.wav\n",
      "wavs/SP004aneesa.abbasiF0700.wav\n",
      "wavs/SP004aneesa.abbasiF0702.wav\n",
      "wavs/SP004aneesa.abbasiF0659.wav\n",
      "wavs/SP004aneesa.abbasiF0340.wav\n",
      "wavs/SP004aneesa.abbasiF0262.wav\n",
      "wavs/SP004aneesa.abbasiF0624.wav\n",
      "wavs/SP004aneesa.abbasiF0468.wav\n",
      "wavs/SP004aneesa.abbasiF0635.wav\n",
      "wavs/SP004aneesa.abbasiF0433.wav\n",
      "wavs/SP004aneesa.abbasiF0511.wav\n",
      "wavs/SP004aneesa.abbasiF0003.wav\n",
      "wavs/SP004aneesa.abbasiF0038.wav\n",
      "wavs/SP004aneesa.abbasiF0386.wav\n",
      "wavs/SP004aneesa.abbasiF0351.wav\n",
      "wavs/SP004aneesa.abbasiF0600.wav\n",
      "wavs/SP004aneesa.abbasiF0195.wav\n",
      "wavs/SP004aneesa.abbasiF0273.wav\n",
      "wavs/SP004aneesa.abbasiF0209.wav\n",
      "wavs/SP004aneesa.abbasiF0557.wav\n",
      "wavs/SP004aneesa.abbasiF0479.wav\n",
      "wavs/SP004aneesa.abbasiF0160.wav\n",
      "wavs/SP004aneesa.abbasiF0522.wav\n",
      "wavs/SP004aneesa.abbasiF0444.wav\n",
      "wavs/SP004aneesa.abbasiF0049.wav\n",
      "wavs/SP004aneesa.abbasiF0397.wav\n",
      "wavs/SP004aneesa.abbasiF0014.wav\n",
      "wavs/SP004aneesa.abbasiF0362.wav\n",
      "wavs/SP004aneesa.abbasiF0284.wav\n",
      "wavs/SP004aneesa.abbasiF0568.wav\n",
      "wavs/SP004aneesa.abbasiF0646.wav\n",
      "wavs/SP004aneesa.abbasiF0171.wav\n",
      "wavs/SP004aneesa.abbasiF0107.wav\n",
      "wavs/SP004aneesa.abbasiF0533.wav\n",
      "wavs/SP004aneesa.abbasiF0611.wav\n",
      "wavs/SP004aneesa.abbasiF0466.wav\n",
      "wavs/SP004aneesa.abbasiF0420.wav\n",
      "wavs/SP004aneesa.abbasiF0025.wav\n",
      "wavs/SP004aneesa.abbasiF0373.wav\n",
      "wavs/SP004aneesa.abbasiF0295.wav\n",
      "wavs/SP004aneesa.abbasiF0309.wav\n",
      "wavs/SP004aneesa.abbasiF0657.wav\n",
      "wavs/SP004aneesa.abbasiF0579.wav\n",
      "wavs/SP004aneesa.abbasiF0182.wav\n",
      "wavs/SP004aneesa.abbasiF0260.wav\n",
      "wavs/SP004aneesa.abbasiF0118.wav\n",
      "wavs/SP004aneesa.abbasiF0622.wav\n",
      "wavs/SP004aneesa.abbasiF0544.wav\n",
      "wavs/SP004aneesa.abbasiF0431.wav\n",
      "wavs/SP004aneesa.abbasiF0001.wav\n",
      "wavs/SP004aneesa.abbasiF0036.wav\n",
      "wavs/SP004aneesa.abbasiF0384.wav\n",
      "wavs/SP004aneesa.abbasiF0207.wav\n",
      "wavs/SP004aneesa.abbasiF0553.wav\n",
      "wavs/SP004aneesa.abbasiF0633.wav\n",
      "wavs/SP004aneesa.abbasiF0668.wav\n",
      "wavs/SP004aneesa.abbasiF0193.wav\n",
      "wavs/SP004aneesa.abbasiF0271.wav\n",
      "wavs/SP004aneesa.abbasiF0129.wav\n",
      "wavs/SP004aneesa.abbasiF0477.wav\n",
      "wavs/SP004aneesa.abbasiF0555.wav\n",
      "wavs/SP004aneesa.abbasiF0566.wav\n",
      "wavs/SP004aneesa.abbasiF0520.wav\n",
      "wavs/SP004aneesa.abbasiF0442.wav\n",
      "wavs/SP004aneesa.abbasiF0047.wav\n",
      "wavs/SP004aneesa.abbasiF0395.wav\n",
      "wavs/SP004aneesa.abbasiF0012.wav\n",
      "wavs/SP004aneesa.abbasiF0679.wav\n",
      "wavs/SP004aneesa.abbasiF0282.wav\n",
      "wavs/SP004aneesa.abbasiF0360.wav\n",
      "wavs/SP004aneesa.abbasiF0218.wav\n",
      "wavs/SP004aneesa.abbasiF0644.wav\n",
      "wavs/SP004aneesa.abbasiF0488.wav\n",
      "wavs/SP004aneesa.abbasiF0464.wav\n",
      "wavs/SP004aneesa.abbasiF0105.wav\n",
      "wavs/SP004aneesa.abbasiF0531.wav\n",
      "wavs/SP004aneesa.abbasiF0453.wav\n",
      "wavs/SP004aneesa.abbasiF0058.wav\n",
      "wavs/SP004aneesa.abbasiF0307.wav\n",
      "wavs/SP004aneesa.abbasiF0499.wav\n",
      "wavs/SP004aneesa.abbasiF0023.wav\n",
      "wavs/SP004aneesa.abbasiF0293.wav\n",
      "wavs/SP004aneesa.abbasiF0371.wav\n",
      "wavs/SP004aneesa.abbasiF0229.wav\n",
      "wavs/SP004aneesa.abbasiF0655.wav\n",
      "wavs/SP004aneesa.abbasiF0577.wav\n",
      "wavs/SP004aneesa.abbasiF0180.wav\n",
      "wavs/SP004aneesa.abbasiF0116.wav\n",
      "wavs/SP004aneesa.abbasiF0542.wav\n",
      "wavs/SP004aneesa.abbasiF0620.wav\n",
      "wavs/SP004aneesa.abbasiF0069.wav\n",
      "metadata.csv\n",
      "wavs/SP004aneesa.abbasiF0034.wav\n",
      "wavs/SP004aneesa.abbasiF0382.wav\n",
      "wavs/SP004aneesa.abbasiF0318.wav\n",
      "wavs/SP004aneesa.abbasiF0666.wav\n",
      "wavs/SP004aneesa.abbasiF0588.wav\n",
      "wavs/SP004aneesa.abbasiF0191.wav\n",
      "wavs/SP004aneesa.abbasiF0127.wav\n",
      "wavs/SP004aneesa.abbasiF0205.wav\n",
      "wavs/SP004aneesa.abbasiF0475.wav\n",
      "wavs/SP004aneesa.abbasiF0631.wav\n",
      "wavs/SP004aneesa.abbasiF0486.wav\n",
      "wavs/SP004aneesa.abbasiF0440.wav\n",
      "wavs/SP004aneesa.abbasiF0677.wav\n",
      "wavs/SP004aneesa.abbasiF0045.wav\n",
      "wavs/SP004aneesa.abbasiF0393.wav\n",
      "wavs/SP004aneesa.abbasiF0329.wav\n",
      "wavs/SP004aneesa.abbasiF0010.wav\n",
      "wavs/SP004aneesa.abbasiF0599.wav\n",
      "wavs/SP004aneesa.abbasiF0280.wav\n",
      "wavs/SP004aneesa.abbasiF0216.wav\n",
      "wavs/SP004aneesa.abbasiF0138.wav\n",
      "wavs/SP004aneesa.abbasiF0564.wav\n",
      "wavs/SP004aneesa.abbasiF0642.wav\n",
      "wavs/SP004aneesa.abbasiF0103.wav\n",
      "wavs/SP004aneesa.abbasiF0451.wav\n",
      "wavs/SP004aneesa.abbasiF0688.wav\n",
      "wavs/SP004aneesa.abbasiF0056.wav\n",
      "wavs/SP004aneesa.abbasiF0305.wav\n",
      "wavs/\n",
      "wavs/SP004aneesa.abbasiF0575.wav\n",
      "wavs/SP004aneesa.abbasiF0021.wav\n",
      "wavs/SP004aneesa.abbasiF0291.wav\n",
      "wavs/SP004aneesa.abbasiF0149.wav\n",
      "wavs/SP004aneesa.abbasiF0227.wav\n",
      "wavs/SP004aneesa.abbasiF0653.wav\n",
      "wavs/SP004aneesa.abbasiF0497.wav\n",
      "wavs/SP004aneesa.abbasiF0114.wav\n",
      "wavs/SP004aneesa.abbasiF0462.wav\n",
      "wavs/SP004aneesa.abbasiF0540.wav\n",
      "wavs/SP004aneesa.abbasiF0067.wav\n",
      "wavs/SP004aneesa.abbasiF0699.wav\n",
      "wavs/SP004aneesa.abbasiF0032.wav\n",
      "wavs/SP004aneesa.abbasiF0380.wav\n",
      "wavs/SP004aneesa.abbasiF0316.wav\n",
      "wavs/SP004aneesa.abbasiF0238.wav\n",
      "wavs/SP004aneesa.abbasiF0664.wav\n",
      "wavs/SP004aneesa.abbasiF0586.wav\n",
      "wavs/SP004aneesa.abbasiF0125.wav\n",
      "wavs/SP004aneesa.abbasiF0203.wav\n",
      "wavs/SP004aneesa.abbasiF0473.wav\n",
      "wavs/SP004aneesa.abbasiF0551.wav\n",
      "wavs/SP004aneesa.abbasiF0409.wav\n",
      "wavs/SP004aneesa.abbasiF0078.wav\n",
      "wavs/SP004aneesa.abbasiF0640.wav\n",
      "wavs/SP004aneesa.abbasiF0043.wav\n",
      "wavs/SP004aneesa.abbasiF0327.wav\n",
      "wavs/SP004aneesa.abbasiF0391.wav\n",
      "wavs/SP004aneesa.abbasiF0249.wav\n",
      "wavs/SP004aneesa.abbasiF0675.wav\n",
      "wavs/SP004aneesa.abbasiF0597.wav\n",
      "wavs/SP004aneesa.abbasiF0136.wav\n",
      "wavs/SP004aneesa.abbasiF0214.wav\n",
      "wavs/SP004aneesa.abbasiF0562.wav\n",
      "wavs/SP004aneesa.abbasiF0484.wav\n",
      "wavs/SP004aneesa.abbasiF0101.wav\n",
      "wavs/SP004aneesa.abbasiF0089.wav\n",
      "wavs/SP004aneesa.abbasiF0054.wav\n",
      "wavs/SP004aneesa.abbasiF0225.wav\n",
      "wavs/SP004aneesa.abbasiF0338.wav\n",
      "wavs/SP004aneesa.abbasiF0573.wav\n",
      "wavs/SP004aneesa.abbasiF0686.wav\n",
      "wavs/SP004aneesa.abbasiF0303.wav\n",
      "wavs/SP004aneesa.abbasiF0147.wav\n",
      "wavs/SP004aneesa.abbasiF0651.wav\n",
      "wavs/SP004aneesa.abbasiF0495.wav\n",
      "wavs/SP004aneesa.abbasiF0509.wav\n",
      "wavs/SP004aneesa.abbasiF0112.wav\n",
      "wavs/SP004aneesa.abbasiF0460.wav\n",
      "wavs/SP004aneesa.abbasiF0065.wav\n",
      "wavs/SP004aneesa.abbasiF0158.wav\n",
      "wavs/SP004aneesa.abbasiF0349.wav\n",
      "wavs/SP004aneesa.abbasiF0697.wav\n",
      "wavs/SP004aneesa.abbasiF0030.wav\n",
      "wavs/SP004aneesa.abbasiF0236.wav\n",
      "wavs/SP004aneesa.abbasiF0314.wav\n",
      "wavs/SP004aneesa.abbasiF0662.wav\n",
      "wavs/SP004aneesa.abbasiF0584.wav\n",
      "wavs/SP004aneesa.abbasiF0201.wav\n",
      "wavs/SP004aneesa.abbasiF0123.wav\n",
      "wavs/SP004aneesa.abbasiF0471.wav\n",
      "wavs/SP004aneesa.abbasiF0407.wav\n",
      "wavs/SP004aneesa.abbasiF0076.wav\n",
      "wavs/SP004aneesa.abbasiF0247.wav\n",
      "wavs/SP004aneesa.abbasiF0041.wav\n",
      "wavs/SP004aneesa.abbasiF0325.wav\n",
      "wavs/SP004aneesa.abbasiF0169.wav\n",
      "wavs/SP004aneesa.abbasiF0595.wav\n",
      "wavs/SP004aneesa.abbasiF0673.wav\n",
      "wavs/SP004aneesa.abbasiF0609.wav\n",
      "wavs/SP004aneesa.abbasiF0134.wav\n",
      "wavs/SP004aneesa.abbasiF0212.wav\n",
      "wavs/SP004aneesa.abbasiF0482.wav\n",
      "wavs/SP004aneesa.abbasiF0560.wav\n",
      "wavs/SP004aneesa.abbasiF0418.wav\n",
      "wavs/SP004aneesa.abbasiF0087.wav\n",
      "wavs/SP004aneesa.abbasiF0223.wav\n",
      "wavs/SP004aneesa.abbasiF0052.wav\n",
      "wavs/SP004aneesa.abbasiF0429.wav\n",
      "wavs/SP004aneesa.abbasiF0258.wav\n",
      "wavs/SP004aneesa.abbasiF0336.wav\n",
      "wavs/SP004aneesa.abbasiF0684.wav\n",
      "wavs/SP004aneesa.abbasiF0301.wav\n",
      "wavs/SP004aneesa.abbasiF0145.wav\n",
      "wavs/SP004aneesa.abbasiF0493.wav\n",
      "wavs/SP004aneesa.abbasiF0571.wav\n",
      "wavs/SP004aneesa.abbasiF0507.wav\n",
      "wavs/SP004aneesa.abbasiF0110.wav\n",
      "wavs/SP004aneesa.abbasiF0098.wav\n",
      "wavs/SP004aneesa.abbasiF0156.wav\n",
      "wavs/SP004aneesa.abbasiF0063.wav\n",
      "wavs/SP004aneesa.abbasiF0269.wav\n",
      "wavs/SP004aneesa.abbasiF0347.wav\n",
      "wavs/SP004aneesa.abbasiF0695.wav\n",
      "wavs/SP004aneesa.abbasiF0234.wav\n",
      "wavs/SP004aneesa.abbasiF0312.wav\n",
      "wavs/SP004aneesa.abbasiF0582.wav\n",
      "wavs/SP004aneesa.abbasiF0660.wav\n",
      "wavs/SP004aneesa.abbasiF0518.wav\n",
      "wavs/SP004aneesa.abbasiF0121.wav\n",
      "wavs/SP004aneesa.abbasiF0405.wav\n",
      "wavs/SP004aneesa.abbasiF0074.wav\n",
      "wavs/SP004aneesa.abbasiF0607.wav\n",
      "wavs/SP004aneesa.abbasiF0358.wav\n",
      "wavs/SP004aneesa.abbasiF0167.wav\n",
      "wavs/SP004aneesa.abbasiF0245.wav\n",
      "wavs/SP004aneesa.abbasiF0323.wav\n",
      "wavs/SP004aneesa.abbasiF0593.wav\n",
      "wavs/SP004aneesa.abbasiF0671.wav\n",
      "wavs/SP004aneesa.abbasiF0529.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wavs/SP004aneesa.abbasiF0132.wav\n",
      "wavs/SP004aneesa.abbasiF0210.wav\n",
      "wavs/SP004aneesa.abbasiF0480.wav\n",
      "wavs/SP004aneesa.abbasiF0416.wav\n",
      "wavs/SP004aneesa.abbasiF0085.wav\n",
      "wavs/SP004aneesa.abbasiF0334.wav\n",
      "wavs/SP004aneesa.abbasiF0369.wav\n",
      "wavs/SP004aneesa.abbasiF0050.wav\n",
      "wavs/SP004aneesa.abbasiF0178.wav\n",
      "wavs/SP004aneesa.abbasiF0256.wav\n",
      "wavs/SP004aneesa.abbasiF0682.wav\n",
      "wavs/SP004aneesa.abbasiF0505.wav\n",
      "wavs/SP004aneesa.abbasiF0618.wav\n",
      "wavs/SP004aneesa.abbasiF0221.wav\n",
      "wavs/SP004aneesa.abbasiF0143.wav\n",
      "wavs/SP004aneesa.abbasiF0491.wav\n",
      "wavs/SP004aneesa.abbasiF0427.wav\n",
      "wavs/SP004aneesa.abbasiF0096.wav\n",
      "wavs/SP004aneesa.abbasiF0345.wav\n",
      "wavs/SP004aneesa.abbasiF0154.wav\n",
      "wavs/SP004aneesa.abbasiF0061.wav\n",
      "wavs/SP004aneesa.abbasiF0527.wav\n",
      "wavs/SP004aneesa.abbasiF0267.wav\n",
      "wavs/SP004aneesa.abbasiF0189.wav\n",
      "wavs/SP004aneesa.abbasiF0693.wav\n",
      "wavs/SP004aneesa.abbasiF0629.wav\n",
      "wavs/SP004aneesa.abbasiF0707.wav\n",
      "wavs/SP004aneesa.abbasiF0232.wav\n",
      "wavs/SP004aneesa.abbasiF0310.wav\n",
      "wavs/SP004aneesa.abbasiF0580.wav\n",
      "wavs/SP004aneesa.abbasiF0438.wav\n",
      "wavs/SP004aneesa.abbasiF0516.wav\n",
      "wavs/SP004aneesa.abbasiF0449.wav\n",
      "wavs/SP004aneesa.abbasiF0403.wav\n",
      "wavs/SP004aneesa.abbasiF0072.wav\n",
      "wavs/SP004aneesa.abbasiF0008.wav\n",
      "wavs/SP004aneesa.abbasiF0356.wav\n",
      "wavs/SP004aneesa.abbasiF0278.wav\n",
      "wavs/SP004aneesa.abbasiF0321.wav\n",
      "wavs/SP004aneesa.abbasiF0165.wav\n",
      "wavs/SP004aneesa.abbasiF0243.wav\n",
      "wavs/SP004aneesa.abbasiF0591.wav\n",
      "wavs/SP004aneesa.abbasiF0605.wav\n",
      "wavs/SP004aneesa.abbasiF0130.wav\n",
      "wavs/SP004aneesa.abbasiF0616.wav\n",
      "wavs/SP004aneesa.abbasiF0414.wav\n",
      "wavs/SP004aneesa.abbasiF0083.wav\n",
      "wavs/SP004aneesa.abbasiF0019.wav\n",
      "wavs/SP004aneesa.abbasiF0289.wav\n",
      "wavs/SP004aneesa.abbasiF0367.wav\n",
      "wavs/SP004aneesa.abbasiF0176.wav\n",
      "wavs/SP004aneesa.abbasiF0254.wav\n",
      "wavs/SP004aneesa.abbasiF0332.wav\n",
      "wavs/SP004aneesa.abbasiF0680.wav\n",
      "wavs/SP004aneesa.abbasiF0538.wav\n",
      "wavs/SP004aneesa.abbasiF0141.wav\n",
      "wavs/SP004aneesa.abbasiF0425.wav\n",
      "wavs/SP004aneesa.abbasiF0503.wav\n",
      "wavs/SP004aneesa.abbasiF0094.wav\n",
      "wavs/SP004aneesa.abbasiF0549.wav\n",
      "wavs/SP004aneesa.abbasiF0187.wav\n",
      "wavs/SP004aneesa.abbasiF0378.wav\n",
      "wavs/SP004aneesa.abbasiF0354.wav\n",
      "wavs/SP004aneesa.abbasiF0343.wav\n",
      "wavs/SP004aneesa.abbasiF0265.wav\n",
      "wavs/SP004aneesa.abbasiF0691.wav\n",
      "wavs/SP004aneesa.abbasiF0627.wav\n",
      "wavs/SP004aneesa.abbasiF0705.wav\n",
      "wavs/SP004aneesa.abbasiF0152.wav\n",
      "wavs/SP004aneesa.abbasiF0230.wav\n",
      "wavs/SP004aneesa.abbasiF0514.wav\n",
      "wavs/SP004aneesa.abbasiF0436.wav\n",
      "wavs/SP004aneesa.abbasiF0401.wav\n",
      "wavs/SP004aneesa.abbasiF0389.wav\n",
      "wavs/SP004aneesa.abbasiF0263.wav\n",
      "wavs/SP004aneesa.abbasiF0070.wav\n",
      "wavs/SP004aneesa.abbasiF0006.wav\n",
      "wavs/SP004aneesa.abbasiF0276.wav\n",
      "wavs/SP004aneesa.abbasiF0198.wav\n",
      "wavs/SP004aneesa.abbasiF0447.wav\n",
      "wavs/SP004aneesa.abbasiF0638.wav\n",
      "wavs/SP004aneesa.abbasiF0241.wav\n",
      "wavs/SP004aneesa.abbasiF0163.wav\n",
      "wavs/SP004aneesa.abbasiF0525.wav\n",
      "wavs/SP004aneesa.abbasiF0603.wav\n",
      "wavs/SP004aneesa.abbasiF0458.wav\n",
      "wavs/SP004aneesa.abbasiF0412.wav\n",
      "wavs/SP004aneesa.abbasiF0081.wav\n",
      "wavs/SP004aneesa.abbasiF0017.wav\n",
      "wavs/SP004aneesa.abbasiF0365.wav\n",
      "wavs/SP004aneesa.abbasiF0287.wav\n",
      "wavs/SP004aneesa.abbasiF0252.wav\n",
      "wavs/SP004aneesa.abbasiF0649.wav\n",
      "wavs/SP004aneesa.abbasiF0174.wav\n",
      "wavs/SP004aneesa.abbasiF0330.wav\n",
      "wavs/SP004aneesa.abbasiF0536.wav\n",
      "wavs/SP004aneesa.abbasiF0614.wav\n",
      "wavs/SP004aneesa.abbasiF0547.wav\n",
      "wavs/SP004aneesa.abbasiF0501.wav\n",
      "wavs/SP004aneesa.abbasiF0423.wav\n",
      "wavs/SP004aneesa.abbasiF0092.wav\n",
      "wavs/SP004aneesa.abbasiF0028.wav\n",
      "wavs/SP004aneesa.abbasiF0376.wav\n",
      "wavs/SP004aneesa.abbasiF0298.wav\n",
      "wavs/SP004aneesa.abbasiF0469.wav\n",
      "wavs/SP004aneesa.abbasiF0445.wav\n",
      "wavs/SP004aneesa.abbasiF0341.wav\n",
      "wavs/SP004aneesa.abbasiF0185.wav\n",
      "wavs/SP004aneesa.abbasiF0625.wav\n",
      "wavs/SP004aneesa.abbasiF0703.wav\n",
      "wavs/SP004aneesa.abbasiF0150.wav\n",
      "wavs/SP004aneesa.abbasiF0434.wav\n",
      "wavs/SP004aneesa.abbasiF0512.wav\n",
      "wavs/SP004aneesa.abbasiF0039.wav\n",
      "wavs/SP004aneesa.abbasiF0387.wav\n",
      "wavs/SP004aneesa.abbasiF0352.wav\n",
      "wavs/SP004aneesa.abbasiF0004.wav\n",
      "wavs/SP004aneesa.abbasiF0274.wav\n",
      "wavs/SP004aneesa.abbasiF0196.wav\n",
      "wavs/SP004aneesa.abbasiF0558.wav\n",
      "wavs/SP004aneesa.abbasiF0636.wav\n",
      "wavs/SP004aneesa.abbasiF0161.wav\n",
      "wavs/SP004aneesa.abbasiF0523.wav\n",
      "wavs/SP004aneesa.abbasiF0601.wav\n",
      "wavs/SP004aneesa.abbasiF0456.wav\n",
      "wavs/SP004aneesa.abbasiF0398.wav\n",
      "wavs/SP004aneesa.abbasiF0410.wav\n",
      "wavs/SP004aneesa.abbasiF0015.wav\n",
      "wavs/SP004aneesa.abbasiF0285.wav\n",
      "wavs/SP004aneesa.abbasiF0363.wav\n",
      "wavs/SP004aneesa.abbasiF0569.wav\n",
      "wavs/SP004aneesa.abbasiF0647.wav\n",
      "wavs/SP004aneesa.abbasiF0250.wav\n",
      "wavs/SP004aneesa.abbasiF0172.wav\n",
      "wavs/SP004aneesa.abbasiF0108.wav\n",
      "wavs/SP004aneesa.abbasiF0534.wav\n",
      "wavs/SP004aneesa.abbasiF0612.wav\n",
      "wavs/SP004aneesa.abbasiF0467.wav\n",
      "wavs/SP004aneesa.abbasiF0421.wav\n",
      "wavs/SP004aneesa.abbasiF0090.wav\n",
      "wavs/SP004aneesa.abbasiF0026.wav\n",
      "wavs/SP004aneesa.abbasiF0296.wav\n",
      "wavs/SP004aneesa.abbasiF0374.wav\n",
      "wavs/SP004aneesa.abbasiF0623.wav\n",
      "wavs/SP004aneesa.abbasiF0658.wav\n",
      "wavs/SP004aneesa.abbasiF0183.wav\n",
      "wavs/SP004aneesa.abbasiF0261.wav\n",
      "wavs/SP004aneesa.abbasiF0119.wav\n",
      "wavs/SP004aneesa.abbasiF0701.wav\n",
      "wavs/SP004aneesa.abbasiF0545.wav\n",
      "wavs/SP004aneesa.abbasiF0432.wav\n",
      "wavs/SP004aneesa.abbasiF0510.wav\n",
      "wavs/SP004aneesa.abbasiF0669.wav\n",
      "wavs/SP004aneesa.abbasiF0037.wav\n",
      "wavs/SP004aneesa.abbasiF0350.wav\n",
      "wavs/SP004aneesa.abbasiF0385.wav\n",
      "wavs/SP004aneesa.abbasiF0556.wav\n",
      "wavs/SP004aneesa.abbasiF0002.wav\n",
      "wavs/SP004aneesa.abbasiF0272.wav\n",
      "wavs/SP004aneesa.abbasiF0194.wav\n",
      "wavs/SP004aneesa.abbasiF0208.wav\n",
      "wavs/SP004aneesa.abbasiF0634.wav\n",
      "wavs/SP004aneesa.abbasiF0478.wav\n",
      "wavs/SP004aneesa.abbasiF0454.wav\n",
      "wavs/SP004aneesa.abbasiF0521.wav\n",
      "wavs/SP004aneesa.abbasiF0443.wav\n",
      "wavs/SP004aneesa.abbasiF0048.wav\n",
      "wavs/SP004aneesa.abbasiF0396.wav\n",
      "wavs/SP004aneesa.abbasiF0567.wav\n",
      "wavs/SP004aneesa.abbasiF0013.wav\n",
      "wavs/SP004aneesa.abbasiF0283.wav\n",
      "wavs/SP004aneesa.abbasiF0361.wav\n",
      "wavs/SP004aneesa.abbasiF0219.wav\n",
      "wavs/SP004aneesa.abbasiF0489.wav\n",
      "wavs/SP004aneesa.abbasiF0645.wav\n",
      "wavs/SP004aneesa.abbasiF0170.wav\n",
      "wavs/SP004aneesa.abbasiF0106.wav\n",
      "wavs/SP004aneesa.abbasiF0532.wav\n",
      "wavs/SP004aneesa.abbasiF0610.wav\n",
      "wavs/SP004aneesa.abbasiF0059.wav\n",
      "wavs/SP004aneesa.abbasiF0024.wav\n",
      "wavs/SP004aneesa.abbasiF0372.wav\n",
      "wavs/SP004aneesa.abbasiF0294.wav\n",
      "wavs/SP004aneesa.abbasiF0308.wav\n",
      "wavs/SP004aneesa.abbasiF0578.wav\n",
      "wavs/SP004aneesa.abbasiF0656.wav\n",
      "wavs/SP004aneesa.abbasiF0181.wav\n",
      "wavs/SP004aneesa.abbasiF0465.wav\n",
      "wavs/SP004aneesa.abbasiF0117.wav\n",
      "wavs/SP004aneesa.abbasiF0543.wav\n",
      "wavs/SP004aneesa.abbasiF0621.wav\n",
      "wavs/SP004aneesa.abbasiF0554.wav\n",
      "wavs/SP004aneesa.abbasiF0430.wav\n",
      "wavs/SP004aneesa.abbasiF0206.wav\n",
      "wavs/SP004aneesa.abbasiF0000.wav\n",
      "wavs/SP004aneesa.abbasiF0035.wav\n",
      "wavs/SP004aneesa.abbasiF0383.wav\n",
      "wavs/SP004aneesa.abbasiF0319.wav\n",
      "wavs/SP004aneesa.abbasiF0667.wav\n",
      "wavs/SP004aneesa.abbasiF0589.wav\n",
      "wavs/SP004aneesa.abbasiF0192.wav\n",
      "wavs/SP004aneesa.abbasiF0270.wav\n",
      "wavs/SP004aneesa.abbasiF0128.wav\n",
      "wavs/SP004aneesa.abbasiF0632.wav\n",
      "wavs/SP004aneesa.abbasiF0476.wav\n",
      "wavs/SP004aneesa.abbasiF0441.wav\n",
      "wavs/SP004aneesa.abbasiF0139.wav\n",
      "wavs/SP004aneesa.abbasiF0046.wav\n",
      "wavs/SP004aneesa.abbasiF0394.wav\n",
      "wavs/SP004aneesa.abbasiF0563.wav\n",
      "wavs/SP004aneesa.abbasiF0011.wav\n",
      "wavs/SP004aneesa.abbasiF0678.wav\n",
      "wavs/SP004aneesa.abbasiF0281.wav\n",
      "wavs/SP004aneesa.abbasiF0565.wav\n",
      "wavs/SP004aneesa.abbasiF0217.wav\n",
      "wavs/SP004aneesa.abbasiF0643.wav\n",
      "wavs/SP004aneesa.abbasiF0487.wav\n",
      "wavs/SP004aneesa.abbasiF0104.wav\n",
      "wavs/SP004aneesa.abbasiF0654.wav\n",
      "wavs/SP004aneesa.abbasiF0530.wav\n",
      "wavs/SP004aneesa.abbasiF0452.wav\n",
      "wavs/SP004aneesa.abbasiF0057.wav\n",
      "wavs/SP004aneesa.abbasiF0228.wav\n",
      "wavs/SP004aneesa.abbasiF0022.wav\n",
      "wavs/SP004aneesa.abbasiF0689.wav\n",
      "wavs/SP004aneesa.abbasiF0370.wav\n",
      "wavs/SP004aneesa.abbasiF0292.wav\n",
      "wavs/SP004aneesa.abbasiF0306.wav\n",
      "wavs/SP004aneesa.abbasiF0498.wav\n",
      "wavs/SP004aneesa.abbasiF0576.wav\n",
      "wavs/SP004aneesa.abbasiF0115.wav\n",
      "wavs/SP004aneesa.abbasiF0463.wav\n",
      "wavs/SP004aneesa.abbasiF0541.wav\n",
      "wavs/SP004aneesa.abbasiF0068.wav\n",
      "wavs/SP004aneesa.abbasiF0317.wav\n",
      "wavs/SP004aneesa.abbasiF0033.wav\n",
      "wavs/SP004aneesa.abbasiF0474.wav\n",
      "wavs/SP004aneesa.abbasiF0381.wav\n",
      "wavs/SP004aneesa.abbasiF0239.wav\n",
      "wavs/SP004aneesa.abbasiF0587.wav\n",
      "wavs/SP004aneesa.abbasiF0665.wav\n",
      "wavs/SP004aneesa.abbasiF0190.wav\n",
      "wavs/SP004aneesa.abbasiF0126.wav\n",
      "wavs/SP004aneesa.abbasiF0204.wav\n",
      "wavs/SP004aneesa.abbasiF0552.wav\n",
      "wavs/SP004aneesa.abbasiF0630.wav\n",
      "wavs/SP004aneesa.abbasiF0079.wav\n",
      "wavs/SP004aneesa.abbasiF0044.wav\n",
      "wavs/SP004aneesa.abbasiF0392.wav\n",
      "wavs/SP004aneesa.abbasiF0328.wav\n",
      "wavs/SP004aneesa.abbasiF0676.wav\n",
      "wavs/SP004aneesa.abbasiF0598.wav\n",
      "wavs/SP004aneesa.abbasiF0215.wav\n",
      "wavs/SP004aneesa.abbasiF0137.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wavs/SP004aneesa.abbasiF0641.wav\n",
      "wavs/SP004aneesa.abbasiF0485.wav\n",
      "wavs/SP004aneesa.abbasiF0102.wav\n",
      "wavs/SP004aneesa.abbasiF0450.wav\n",
      "wavs/SP004aneesa.abbasiF0226.wav\n",
      "wavs/SP004aneesa.abbasiF0055.wav\n",
      "wavs/SP004aneesa.abbasiF0148.wav\n",
      "wavs/SP004aneesa.abbasiF0339.wav\n",
      "wavs/SP004aneesa.abbasiF0687.wav\n",
      "wavs/SP004aneesa.abbasiF0020.wav\n",
      "wavs/SP004aneesa.abbasiF0290.wav\n",
      "wavs/SP004aneesa.abbasiF0574.wav\n",
      "wavs/SP004aneesa.abbasiF0304.wav\n",
      "wavs/SP004aneesa.abbasiF0652.wav\n",
      "wavs/SP004aneesa.abbasiF0496.wav\n",
      "wavs/SP004aneesa.abbasiF0113.wav\n",
      "wavs/SP004aneesa.abbasiF0461.wav\n",
      "wavs/SP004aneesa.abbasiF0066.wav\n",
      "wavs/SP004aneesa.abbasiF0159.wav\n",
      "wavs/SP004aneesa.abbasiF0031.wav\n",
      "wavs/SP004aneesa.abbasiF0698.wav\n",
      "wavs/SP004aneesa.abbasiF0237.wav\n",
      "wavs/SP004aneesa.abbasiF0315.wav\n",
      "wavs/SP004aneesa.abbasiF0585.wav\n",
      "wavs/SP004aneesa.abbasiF0663.wav\n",
      "wavs/SP004aneesa.abbasiF0124.wav\n",
      "wavs/SP004aneesa.abbasiF0202.wav\n",
      "wavs/SP004aneesa.abbasiF0550.wav\n",
      "wavs/SP004aneesa.abbasiF0472.wav\n",
      "wavs/SP004aneesa.abbasiF0408.wav\n",
      "wavs/SP004aneesa.abbasiF0077.wav\n",
      "wavs/SP004aneesa.abbasiF0042.wav\n",
      "wavs/SP004aneesa.abbasiF0326.wav\n",
      "wavs/SP004aneesa.abbasiF0390.wav\n",
      "wavs/SP004aneesa.abbasiF0248.wav\n",
      "wavs/SP004aneesa.abbasiF0674.wav\n",
      "wavs/SP004aneesa.abbasiF0596.wav\n",
      "wavs/SP004aneesa.abbasiF0213.wav\n",
      "wavs/SP004aneesa.abbasiF0135.wav\n",
      "wavs/SP004aneesa.abbasiF0561.wav\n",
      "wavs/SP004aneesa.abbasiF0483.wav\n",
      "wavs/SP004aneesa.abbasiF0419.wav\n",
      "wavs/SP004aneesa.abbasiF0088.wav\n",
      "wavs/SP004aneesa.abbasiF0100.wav\n",
      "wavs/SP004aneesa.abbasiF0302.wav\n",
      "wavs/SP004aneesa.abbasiF0053.wav\n",
      "wavs/SP004aneesa.abbasiF0259.wav\n",
      "wavs/SP004aneesa.abbasiF0337.wav\n",
      "wavs/SP004aneesa.abbasiF0572.wav\n",
      "wavs/SP004aneesa.abbasiF0685.wav\n",
      "wavs/SP004aneesa.abbasiF0224.wav\n",
      "wavs/SP004aneesa.abbasiF0146.wav\n",
      "wavs/SP004aneesa.abbasiF0650.wav\n",
      "wavs/SP004aneesa.abbasiF0494.wav\n",
      "wavs/SP004aneesa.abbasiF0508.wav\n",
      "wavs/SP004aneesa.abbasiF0099.wav\n",
      "wavs/SP004aneesa.abbasiF0111.wav\n",
      "wavs/SP004aneesa.abbasiF0064.wav\n",
      "wavs/SP004aneesa.abbasiF0157.wav\n",
      "wavs/SP004aneesa.abbasiF0348.wav\n",
      "wavs/SP004aneesa.abbasiF0696.wav\n",
      "wavs/SP004aneesa.abbasiF0235.wav\n",
      "wavs/SP004aneesa.abbasiF0313.wav\n",
      "wavs/SP004aneesa.abbasiF0661.wav\n",
      "wavs/SP004aneesa.abbasiF0583.wav\n",
      "wavs/SP004aneesa.abbasiF0519.wav\n",
      "wavs/SP004aneesa.abbasiF0122.wav\n",
      "wavs/SP004aneesa.abbasiF0200.wav\n",
      "wavs/SP004aneesa.abbasiF0470.wav\n",
      "wavs/SP004aneesa.abbasiF0406.wav\n",
      "wavs/SP004aneesa.abbasiF0075.wav\n",
      "wavs/SP004aneesa.abbasiF0168.wav\n",
      "wavs/SP004aneesa.abbasiF0359.wav\n",
      "wavs/SP004aneesa.abbasiF0040.wav\n",
      "wavs/SP004aneesa.abbasiF0246.wav\n",
      "wavs/SP004aneesa.abbasiF0324.wav\n",
      "wavs/SP004aneesa.abbasiF0594.wav\n",
      "wavs/SP004aneesa.abbasiF0672.wav\n",
      "wavs/SP004aneesa.abbasiF0608.wav\n",
      "wavs/SP004aneesa.abbasiF0133.wav\n",
      "wavs/SP004aneesa.abbasiF0211.wav\n",
      "wavs/SP004aneesa.abbasiF0481.wav\n",
      "wavs/SP004aneesa.abbasiF0417.wav\n",
      "wavs/SP004aneesa.abbasiF0086.wav\n",
      "wavs/SP004aneesa.abbasiF0051.wav\n",
      "wavs/SP004aneesa.abbasiF0335.wav\n",
      "wavs/SP004aneesa.abbasiF0506.wav\n",
      "wavs/SP004aneesa.abbasiF0179.wav\n",
      "wavs/SP004aneesa.abbasiF0257.wav\n",
      "wavs/SP004aneesa.abbasiF0683.wav\n",
      "wavs/SP004aneesa.abbasiF0300.wav\n",
      "wavs/SP004aneesa.abbasiF0619.wav\n",
      "wavs/SP004aneesa.abbasiF0144.wav\n",
      "wavs/SP004aneesa.abbasiF0222.wav\n",
      "wavs/SP004aneesa.abbasiF0570.wav\n",
      "wavs/SP004aneesa.abbasiF0492.wav\n",
      "wavs/SP004aneesa.abbasiF0428.wav\n",
      "wavs/SP004aneesa.abbasiF0097.wav\n",
      "wavs/SP004aneesa.abbasiF0268.wav\n",
      "wavs/SP004aneesa.abbasiF0062.wav\n",
      "wavs/SP004aneesa.abbasiF0517.wav\n",
      "wavs/SP004aneesa.abbasiF0233.wav\n",
      "wavs/SP004aneesa.abbasiF0346.wav\n",
      "wavs/SP004aneesa.abbasiF0694.wav\n",
      "wavs/SP004aneesa.abbasiF0311.wav\n",
      "wavs/SP004aneesa.abbasiF0155.wav\n",
      "wavs/SP004aneesa.abbasiF0581.wav\n",
      "wavs/SP004aneesa.abbasiF0439.wav\n",
      "wavs/SP004aneesa.abbasiF0120.wav\n",
      "wavs/SP004aneesa.abbasiF0606.wav\n",
      "wavs/SP004aneesa.abbasiF0404.wav\n",
      "wavs/SP004aneesa.abbasiF0073.wav\n",
      "wavs/SP004aneesa.abbasiF0009.wav\n",
      "wavs/SP004aneesa.abbasiF0279.wav\n",
      "wavs/SP004aneesa.abbasiF0357.wav\n",
      "wavs/SP004aneesa.abbasiF0166.wav\n",
      "wavs/SP004aneesa.abbasiF0244.wav\n",
      "wavs/SP004aneesa.abbasiF0322.wav\n",
      "wavs/SP004aneesa.abbasiF0670.wav\n",
      "wavs/SP004aneesa.abbasiF0592.wav\n",
      "wavs/SP004aneesa.abbasiF0528.wav\n",
      "wavs/SP004aneesa.abbasiF0131.wav\n",
      "wavs/SP004aneesa.abbasiF0415.wav\n",
      "wavs/SP004aneesa.abbasiF0084.wav\n",
      "wavs/SP004aneesa.abbasiF0333.wav\n",
      "wavs/SP004aneesa.abbasiF0368.wav\n",
      "wavs/SP004aneesa.abbasiF0539.wav\n",
      "wavs/SP004aneesa.abbasiF0504.wav\n",
      "wavs/SP004aneesa.abbasiF0177.wav\n",
      "wavs/SP004aneesa.abbasiF0255.wav\n",
      "wavs/SP004aneesa.abbasiF0681.wav\n",
      "wavs/SP004aneesa.abbasiF0617.wav\n",
      "wavs/SP004aneesa.abbasiF0220.wav\n",
      "wavs/SP004aneesa.abbasiF0142.wav\n",
      "wavs/SP004aneesa.abbasiF0490.wav\n",
      "wavs/SP004aneesa.abbasiF0426.wav\n",
      "wavs/SP004aneesa.abbasiF0095.wav\n",
      "wavs/SP004aneesa.abbasiF0628.wav\n",
      "wavs/SP004aneesa.abbasiF0266.wav\n",
      "wavs/SP004aneesa.abbasiF0379.wav\n",
      "wavs/SP004aneesa.abbasiF0175.wav\n",
      "wavs/SP004aneesa.abbasiF0060.wav\n",
      "wavs/SP004aneesa.abbasiF0344.wav\n",
      "wavs/SP004aneesa.abbasiF0188.wav\n",
      "wavs/SP004aneesa.abbasiF0692.wav\n",
      "wavs/SP004aneesa.abbasiF0706.wav\n",
      "wavs/SP004aneesa.abbasiF0231.wav\n",
      "wavs/SP004aneesa.abbasiF0153.wav\n",
      "wavs/SP004aneesa.abbasiF0515.wav\n",
      "wavs/SP004aneesa.abbasiF0437.wav\n",
      "wavs/SP004aneesa.abbasiF0355.wav\n",
      "wavs/SP004aneesa.abbasiF0402.wav\n",
      "wavs/SP004aneesa.abbasiF0071.wav\n",
      "wavs/SP004aneesa.abbasiF0199.wav\n",
      "wavs/SP004aneesa.abbasiF0007.wav\n",
      "wavs/SP004aneesa.abbasiF0242.wav\n",
      "wavs/SP004aneesa.abbasiF0277.wav\n",
      "wavs/SP004aneesa.abbasiF0448.wav\n",
      "wavs/SP004aneesa.abbasiF0639.wav\n",
      "wavs/SP004aneesa.abbasiF0164.wav\n",
      "wavs/SP004aneesa.abbasiF0320.wav\n",
      "wavs/SP004aneesa.abbasiF0590.wav\n",
      "wavs/SP004aneesa.abbasiF0526.wav\n",
      "wavs/SP004aneesa.abbasiF0604.wav\n",
      "wavs/SP004aneesa.abbasiF0413.wav\n",
      "wavs/SP004aneesa.abbasiF0342.wav\n",
      "wavs/SP004aneesa.abbasiF0082.wav\n",
      "wavs/SP004aneesa.abbasiF0018.wav\n",
      "wavs/SP004aneesa.abbasiF0288.wav\n",
      "wavs/SP004aneesa.abbasiF0366.wav\n",
      "wavs/SP004aneesa.abbasiF0615.wav\n",
      "wavs/SP004aneesa.abbasiF0253.wav\n",
      "wavs/SP004aneesa.abbasiF0331.wav\n",
      "wavs/SP004aneesa.abbasiF0459.wav\n",
      "wavs/SP004aneesa.abbasiF0537.wav\n",
      "wavs/SP004aneesa.abbasiF0140.wav\n",
      "wavs/SP004aneesa.abbasiF0704.wav\n",
      "wavs/SP004aneesa.abbasiF0424.wav\n",
      "wavs/SP004aneesa.abbasiF0502.wav\n",
      "wavs/SP004aneesa.abbasiF0093.wav\n",
      "wavs/SP004aneesa.abbasiF0029.wav\n",
      "wavs/SP004aneesa.abbasiF0377.wav\n",
      "wavs/SP004aneesa.abbasiF0299.wav\n",
      "wavs/SP004aneesa.abbasiF0184.wav\n",
      "wavs/SP004aneesa.abbasiF0548.wav\n",
      "wavs/SP004aneesa.abbasiF0186.wav\n",
      "wavs/SP004aneesa.abbasiF0264.wav\n",
      "wavs/SP004aneesa.abbasiF0690.wav\n",
      "wavs/SP004aneesa.abbasiF0626.wav\n",
      "wavs/SP004aneesa.abbasiF0151.wav\n",
      "wavs/SP004aneesa.abbasiF0435.wav\n",
      "wavs/SP004aneesa.abbasiF0513.wav\n",
      "wavs/SP004aneesa.abbasiF0388.wav\n",
      "wavs/SP004aneesa.abbasiF0400.wav\n",
      "wavs/SP004aneesa.abbasiF0197.wav\n",
      "wavs/SP004aneesa.abbasiF0005.wav\n",
      "wavs/SP004aneesa.abbasiF0275.wav\n",
      "wavs/SP004aneesa.abbasiF0353.wav\n",
      "wavs/SP004aneesa.abbasiF0559.wav\n",
      "wavs/SP004aneesa.abbasiF0637.wav\n"
     ]
    }
   ],
   "source": [
    "!tar xvzf Urdu/aneesa.abbasi.tar.gz -C Urdu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "100%|█████████████████████████████████████████| 708/708 [04:05<00:00,  4.60it/s]\n",
      "Wrote 708 utterances, 398371 frames (1.38 hours)\n",
      "Max input length:  271\n",
      "Max output length: 1044\n"
     ]
    }
   ],
   "source": [
    "!python preprocess.py --base_dir . --dataset urdu --output training-aneesa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mukhtar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xvzf Urdu/mukhtar.ahmad.tar.gz -C Urdu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python preprocess.py --base_dir . --dataset urdu --output training-mukhtar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aleena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --base_dir . --input training-aleena/train.txt --name aleena --restore_step 441000 --hparams \"max_iters=300\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aneesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Checkpoint path: ./logs-aneesa/model.ckpt\n",
      "Loading training data from: ./training-aneesa/train.txt\n",
      "Using model: tacotron\n",
      "Hyperparameters:\n",
      "  adam_beta1: 0.9\n",
      "  adam_beta2: 0.999\n",
      "  attention_depth: 256\n",
      "  batch_size: 32\n",
      "  cleaners: basic_cleaners\n",
      "  decay_learning_rate: True\n",
      "  decoder_depth: 256\n",
      "  embed_depth: 256\n",
      "  encoder_depth: 256\n",
      "  frame_length_ms: 50\n",
      "  frame_shift_ms: 12.5\n",
      "  griffin_lim_iters: 60\n",
      "  initial_learning_rate: 0.002\n",
      "  max_iters: 300\n",
      "  min_level_db: -100\n",
      "  num_freq: 1025\n",
      "  num_mels: 80\n",
      "  outputs_per_step: 5\n",
      "  postnet_depth: 256\n",
      "  power: 1.5\n",
      "  preemphasis: 0.97\n",
      "  prenet_depths: [256, 128]\n",
      "  ref_level_db: 20\n",
      "  sample_rate: 20000\n",
      "  use_cmudict: False\n",
      "Loaded metadata for 708 examples (1.38 hours)\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tvenv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tacotron/models/modules.py:10: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tacotron/models/modules.py:11: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tvenv/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tacotron/models/modules.py:106: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv1d instead.\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tacotron/models/modules.py:107: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tacotron/models/modules.py:52: max_pooling1d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling1d instead.\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tacotron/models/modules.py:75: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tacotron/models/modules.py:79: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tvenv/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tvenv/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tacotron/models/tacotron.py:68: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "Initialized Tacotron model. Dimensions: \n",
      "  embedding:               256\n",
      "  prenet out:              128\n",
      "  encoder out:             256\n",
      "  attention out:           256\n",
      "  concat attn & out:       512\n",
      "  decoder cell out:        256\n",
      "  decoder out (5 frames):  400\n",
      "  decoder out (1 frame):   80\n",
      "  postnet out:             256\n",
      "  linear out:              1025\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tvenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "Resuming from checkpoint: ./logs-aneesa/model.ckpt-500000 at commit: None\n",
      "Generated 32 batches of size 32 in 1.468 sec\n",
      "Step 500001  [9.765 sec/step, loss=0.07334, avg_loss=0.07334]\n",
      "Step 500002  [7.479 sec/step, loss=0.07295, avg_loss=0.07314]\n",
      "Step 500003  [6.502 sec/step, loss=0.07550, avg_loss=0.07393]\n",
      "Step 500004  [6.445 sec/step, loss=0.07594, avg_loss=0.07443]\n",
      "Step 500005  [6.393 sec/step, loss=0.07803, avg_loss=0.07515]\n",
      "Step 500006  [6.531 sec/step, loss=0.07731, avg_loss=0.07551]\n",
      "Step 500007  [6.242 sec/step, loss=0.07533, avg_loss=0.07549]\n",
      "Step 500008  [6.109 sec/step, loss=0.07536, avg_loss=0.07547]\n",
      "Step 500009  [6.032 sec/step, loss=0.07680, avg_loss=0.07562]\n",
      "Step 500010  [5.846 sec/step, loss=0.07607, avg_loss=0.07566]\n",
      "Step 500011  [5.913 sec/step, loss=0.07843, avg_loss=0.07591]\n",
      "Step 500012  [5.945 sec/step, loss=0.07852, avg_loss=0.07613]\n",
      "Step 500013  [5.871 sec/step, loss=0.07748, avg_loss=0.07623]\n",
      "Step 500014  [6.013 sec/step, loss=0.07506, avg_loss=0.07615]\n",
      "Step 500015  [6.031 sec/step, loss=0.07818, avg_loss=0.07629]\n",
      "Step 500016  [5.986 sec/step, loss=0.07821, avg_loss=0.07641]\n",
      "Step 500017  [5.943 sec/step, loss=0.07750, avg_loss=0.07647]\n",
      "Step 500018  [5.927 sec/step, loss=0.07741, avg_loss=0.07652]\n",
      "Step 500019  [6.158 sec/step, loss=0.06825, avg_loss=0.07609]\n",
      "Step 500020  [6.034 sec/step, loss=0.07360, avg_loss=0.07596]\n",
      "Step 500021  [6.010 sec/step, loss=0.07708, avg_loss=0.07602]\n",
      "Step 500022  [5.977 sec/step, loss=0.07756, avg_loss=0.07609]\n",
      "Step 500023  [5.959 sec/step, loss=0.07673, avg_loss=0.07612]\n",
      "Generated 32 batches of size 32 in 2.463 sec\n",
      "Step 500024  [5.970 sec/step, loss=0.07848, avg_loss=0.07621]\n",
      "Step 500025  [5.990 sec/step, loss=0.07827, avg_loss=0.07630]\n",
      "Step 500026  [5.943 sec/step, loss=0.07759, avg_loss=0.07635]\n",
      "Step 500027  [5.941 sec/step, loss=0.07677, avg_loss=0.07636]\n",
      "Step 500028  [5.912 sec/step, loss=0.07615, avg_loss=0.07635]\n",
      "Step 500029  [5.867 sec/step, loss=0.07473, avg_loss=0.07630]\n",
      "Step 500030  [5.847 sec/step, loss=0.07572, avg_loss=0.07628]\n",
      "Step 500031  [5.856 sec/step, loss=0.07521, avg_loss=0.07624]\n",
      "Step 500032  [5.781 sec/step, loss=0.06786, avg_loss=0.07598]\n",
      "Step 500033  [5.738 sec/step, loss=0.07636, avg_loss=0.07599]\n",
      "Step 500034  [5.752 sec/step, loss=0.07830, avg_loss=0.07606]\n",
      "Step 500035  [5.737 sec/step, loss=0.07797, avg_loss=0.07612]\n",
      "Step 500036  [5.712 sec/step, loss=0.07338, avg_loss=0.07604]\n",
      "Step 500037  [5.698 sec/step, loss=0.07438, avg_loss=0.07600]\n",
      "Step 500038  [5.685 sec/step, loss=0.07681, avg_loss=0.07602]\n",
      "Step 500039  [5.633 sec/step, loss=0.07395, avg_loss=0.07596]\n",
      "Step 500040  [5.746 sec/step, loss=0.06830, avg_loss=0.07577]\n",
      "Step 500041  [5.705 sec/step, loss=0.07296, avg_loss=0.07570]\n",
      "Step 500042  [5.701 sec/step, loss=0.07752, avg_loss=0.07575]\n",
      "Step 500043  [5.694 sec/step, loss=0.07355, avg_loss=0.07570]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500044  [5.643 sec/step, loss=0.06780, avg_loss=0.07552]\n",
      "Step 500045  [5.664 sec/step, loss=0.07703, avg_loss=0.07555]\n",
      "Step 500046  [5.670 sec/step, loss=0.07835, avg_loss=0.07561]\n",
      "Step 500047  [5.656 sec/step, loss=0.07682, avg_loss=0.07564]\n",
      "Step 500048  [5.663 sec/step, loss=0.07730, avg_loss=0.07567]\n",
      "Step 500049  [5.670 sec/step, loss=0.07839, avg_loss=0.07573]\n",
      "Step 500050  [5.693 sec/step, loss=0.07816, avg_loss=0.07578]\n",
      "Step 500051  [5.675 sec/step, loss=0.07530, avg_loss=0.07577]\n",
      "Step 500052  [5.680 sec/step, loss=0.07773, avg_loss=0.07580]\n",
      "Step 500053  [5.668 sec/step, loss=0.07631, avg_loss=0.07581]\n",
      "Step 500054  [5.652 sec/step, loss=0.07551, avg_loss=0.07581]\n",
      "Step 500055  [5.666 sec/step, loss=0.07608, avg_loss=0.07581]\n",
      "Generated 32 batches of size 32 in 2.615 sec\n",
      "Step 500056  [5.658 sec/step, loss=0.07707, avg_loss=0.07583]\n",
      "Step 500057  [5.670 sec/step, loss=0.07842, avg_loss=0.07588]\n",
      "Step 500058  [5.643 sec/step, loss=0.07649, avg_loss=0.07589]\n",
      "Step 500059  [5.630 sec/step, loss=0.07738, avg_loss=0.07592]\n",
      "Step 500060  [5.609 sec/step, loss=0.07543, avg_loss=0.07591]\n",
      "Step 500061  [5.620 sec/step, loss=0.07599, avg_loss=0.07591]\n",
      "Step 500062  [5.655 sec/step, loss=0.07584, avg_loss=0.07591]\n",
      "Step 500063  [5.655 sec/step, loss=0.07662, avg_loss=0.07592]\n",
      "Step 500064  [5.655 sec/step, loss=0.07689, avg_loss=0.07593]\n",
      "Step 500065  [5.660 sec/step, loss=0.07779, avg_loss=0.07596]\n",
      "Step 500066  [5.657 sec/step, loss=0.07282, avg_loss=0.07592]\n",
      "Step 500067  [5.655 sec/step, loss=0.07647, avg_loss=0.07592]\n",
      "Step 500068  [5.647 sec/step, loss=0.07621, avg_loss=0.07593]\n",
      "Step 500069  [5.635 sec/step, loss=0.07682, avg_loss=0.07594]\n",
      "Step 500070  [5.623 sec/step, loss=0.07405, avg_loss=0.07591]\n",
      "Step 500071  [5.604 sec/step, loss=0.07466, avg_loss=0.07590]\n",
      "Step 500072  [5.590 sec/step, loss=0.07469, avg_loss=0.07588]\n",
      "Step 500073  [5.560 sec/step, loss=0.06704, avg_loss=0.07576]\n",
      "Step 500074  [5.543 sec/step, loss=0.07551, avg_loss=0.07575]\n",
      "Step 500075  [5.624 sec/step, loss=0.06764, avg_loss=0.07565]\n",
      "Step 500076  [5.616 sec/step, loss=0.07603, avg_loss=0.07565]\n",
      "Step 500077  [5.624 sec/step, loss=0.07545, avg_loss=0.07565]\n",
      "Step 500078  [5.610 sec/step, loss=0.07515, avg_loss=0.07564]\n",
      "Step 500079  [5.619 sec/step, loss=0.07805, avg_loss=0.07567]\n",
      "Step 500080  [5.629 sec/step, loss=0.07818, avg_loss=0.07570]\n",
      "Step 500081  [5.653 sec/step, loss=0.07682, avg_loss=0.07572]\n",
      "Step 500082  [5.634 sec/step, loss=0.07586, avg_loss=0.07572]\n",
      "Step 500083  [5.629 sec/step, loss=0.07804, avg_loss=0.07575]\n",
      "Step 500084  [5.610 sec/step, loss=0.07160, avg_loss=0.07570]\n",
      "Step 500085  [5.615 sec/step, loss=0.07765, avg_loss=0.07572]\n",
      "Step 500086  [5.593 sec/step, loss=0.07311, avg_loss=0.07569]\n",
      "Step 500087  [5.608 sec/step, loss=0.07729, avg_loss=0.07571]\n",
      "Generated 32 batches of size 32 in 2.445 sec\n",
      "Step 500088  [5.617 sec/step, loss=0.07659, avg_loss=0.07572]\n",
      "Step 500089  [5.615 sec/step, loss=0.07670, avg_loss=0.07573]\n",
      "Step 500090  [5.622 sec/step, loss=0.07792, avg_loss=0.07575]\n",
      "Step 500091  [5.620 sec/step, loss=0.07388, avg_loss=0.07573]\n",
      "Step 500092  [5.612 sec/step, loss=0.07709, avg_loss=0.07575]\n",
      "Step 500093  [5.613 sec/step, loss=0.07771, avg_loss=0.07577]\n",
      "Step 500094  [5.614 sec/step, loss=0.07739, avg_loss=0.07579]\n",
      "Step 500095  [5.623 sec/step, loss=0.07575, avg_loss=0.07579]\n",
      "Step 500096  [5.620 sec/step, loss=0.07694, avg_loss=0.07580]\n",
      "Step 500097  [5.608 sec/step, loss=0.07586, avg_loss=0.07580]\n",
      "Step 500098  [5.595 sec/step, loss=0.07309, avg_loss=0.07577]\n",
      "Step 500099  [5.583 sec/step, loss=0.07560, avg_loss=0.07577]\n",
      "Step 500100  [5.575 sec/step, loss=0.07574, avg_loss=0.07577]\n",
      "Writing summary at step: 500100\n",
      "Step 500101  [5.536 sec/step, loss=0.07838, avg_loss=0.07582]\n",
      "Step 500102  [5.539 sec/step, loss=0.07673, avg_loss=0.07586]\n",
      "Step 500103  [5.544 sec/step, loss=0.07441, avg_loss=0.07585]\n",
      "Step 500104  [5.544 sec/step, loss=0.07696, avg_loss=0.07586]\n",
      "Step 500105  [5.544 sec/step, loss=0.07653, avg_loss=0.07584]\n",
      "Step 500106  [5.512 sec/step, loss=0.07515, avg_loss=0.07582]\n",
      "Step 500107  [5.519 sec/step, loss=0.07775, avg_loss=0.07584]\n",
      "Step 500108  [5.514 sec/step, loss=0.07281, avg_loss=0.07582]\n",
      "Step 500109  [5.520 sec/step, loss=0.07503, avg_loss=0.07580]\n",
      "Step 500110  [5.530 sec/step, loss=0.07568, avg_loss=0.07580]\n",
      "Step 500111  [5.514 sec/step, loss=0.07675, avg_loss=0.07578]\n",
      "Step 500112  [5.525 sec/step, loss=0.07581, avg_loss=0.07575]\n",
      "Step 500113  [5.578 sec/step, loss=0.06788, avg_loss=0.07566]\n",
      "Step 500114  [5.562 sec/step, loss=0.07818, avg_loss=0.07569]\n",
      "Step 500115  [5.570 sec/step, loss=0.07553, avg_loss=0.07566]\n",
      "Step 500116  [5.565 sec/step, loss=0.07766, avg_loss=0.07566]\n",
      "Step 500117  [5.545 sec/step, loss=0.06964, avg_loss=0.07558]\n",
      "Step 500118  [5.544 sec/step, loss=0.07507, avg_loss=0.07556]\n",
      "Generated 32 batches of size 32 in 2.424 sec\n",
      "Step 500119  [5.502 sec/step, loss=0.07641, avg_loss=0.07564]\n",
      "Step 500120  [5.514 sec/step, loss=0.07742, avg_loss=0.07567]\n",
      "Step 500121  [5.511 sec/step, loss=0.07693, avg_loss=0.07567]\n",
      "Step 500122  [5.506 sec/step, loss=0.07596, avg_loss=0.07566]\n",
      "Step 500123  [5.515 sec/step, loss=0.07865, avg_loss=0.07568]\n",
      "Step 500124  [5.510 sec/step, loss=0.07608, avg_loss=0.07565]\n",
      "Step 500125  [5.512 sec/step, loss=0.07721, avg_loss=0.07564]\n",
      "Step 500126  [5.515 sec/step, loss=0.07722, avg_loss=0.07564]\n",
      "Step 500127  [5.494 sec/step, loss=0.07464, avg_loss=0.07562]\n",
      "Step 500128  [5.503 sec/step, loss=0.07823, avg_loss=0.07564]\n",
      "Step 500129  [5.520 sec/step, loss=0.07596, avg_loss=0.07565]\n",
      "Step 500130  [5.514 sec/step, loss=0.07552, avg_loss=0.07565]\n",
      "Step 500131  [5.493 sec/step, loss=0.07308, avg_loss=0.07563]\n",
      "Step 500132  [5.517 sec/step, loss=0.07694, avg_loss=0.07572]\n",
      "Step 500133  [5.521 sec/step, loss=0.07408, avg_loss=0.07569]\n",
      "Step 500134  [5.531 sec/step, loss=0.07646, avg_loss=0.07568]\n",
      "Step 500135  [5.543 sec/step, loss=0.07709, avg_loss=0.07567]\n",
      "Step 500136  [5.554 sec/step, loss=0.07789, avg_loss=0.07571]\n",
      "Step 500137  [5.554 sec/step, loss=0.07706, avg_loss=0.07574]\n",
      "Step 500138  [5.553 sec/step, loss=0.07786, avg_loss=0.07575]\n",
      "Step 500139  [5.567 sec/step, loss=0.07714, avg_loss=0.07578]\n",
      "Step 500140  [5.526 sec/step, loss=0.07829, avg_loss=0.07588]\n",
      "Step 500141  [5.531 sec/step, loss=0.07395, avg_loss=0.07589]\n",
      "Step 500142  [5.515 sec/step, loss=0.07608, avg_loss=0.07588]\n",
      "Step 500143  [5.515 sec/step, loss=0.07698, avg_loss=0.07591]\n",
      "Step 500144  [5.530 sec/step, loss=0.07659, avg_loss=0.07600]\n",
      "Step 500145  [5.499 sec/step, loss=0.07334, avg_loss=0.07596]\n",
      "Step 500146  [5.493 sec/step, loss=0.07634, avg_loss=0.07594]\n",
      "Step 500147  [5.506 sec/step, loss=0.07758, avg_loss=0.07595]\n",
      "Step 500148  [5.492 sec/step, loss=0.07807, avg_loss=0.07596]\n",
      "Step 500149  [5.498 sec/step, loss=0.07595, avg_loss=0.07593]\n",
      "Step 500150  [5.485 sec/step, loss=0.07790, avg_loss=0.07593]\n",
      "Generated 32 batches of size 32 in 2.386 sec\n",
      "Step 500151  [5.504 sec/step, loss=0.07779, avg_loss=0.07596]\n",
      "Step 500152  [5.486 sec/step, loss=0.07594, avg_loss=0.07594]\n",
      "Step 500153  [5.496 sec/step, loss=0.07499, avg_loss=0.07592]\n",
      "Step 500154  [5.498 sec/step, loss=0.07392, avg_loss=0.07591]\n",
      "Step 500155  [5.466 sec/step, loss=0.06721, avg_loss=0.07582]\n",
      "Step 500156  [5.455 sec/step, loss=0.07516, avg_loss=0.07580]\n",
      "Step 500157  [5.466 sec/step, loss=0.07474, avg_loss=0.07576]\n",
      "Step 500158  [5.474 sec/step, loss=0.07650, avg_loss=0.07576]\n",
      "Step 500159  [5.524 sec/step, loss=0.06775, avg_loss=0.07567]\n",
      "Step 500160  [5.531 sec/step, loss=0.07774, avg_loss=0.07569]\n",
      "Step 500161  [5.568 sec/step, loss=0.06767, avg_loss=0.07561]\n",
      "Step 500162  [5.546 sec/step, loss=0.07704, avg_loss=0.07562]\n",
      "Step 500163  [5.547 sec/step, loss=0.07796, avg_loss=0.07563]\n",
      "Step 500164  [5.530 sec/step, loss=0.07593, avg_loss=0.07562]\n",
      "Step 500165  [5.544 sec/step, loss=0.07717, avg_loss=0.07562]\n",
      "Step 500166  [5.550 sec/step, loss=0.07469, avg_loss=0.07564]\n",
      "Step 500167  [5.538 sec/step, loss=0.07574, avg_loss=0.07563]\n",
      "Step 500168  [5.549 sec/step, loss=0.07833, avg_loss=0.07565]\n",
      "Step 500169  [5.555 sec/step, loss=0.07657, avg_loss=0.07565]\n",
      "Step 500170  [5.577 sec/step, loss=0.07495, avg_loss=0.07566]\n",
      "Step 500171  [5.587 sec/step, loss=0.07606, avg_loss=0.07567]\n",
      "Step 500172  [5.588 sec/step, loss=0.07540, avg_loss=0.07568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500173  [5.613 sec/step, loss=0.07799, avg_loss=0.07579]\n",
      "Step 500174  [5.617 sec/step, loss=0.07689, avg_loss=0.07580]\n",
      "Step 500175  [5.546 sec/step, loss=0.07737, avg_loss=0.07590]\n",
      "Step 500176  [5.539 sec/step, loss=0.07579, avg_loss=0.07590]\n",
      "Step 500177  [5.513 sec/step, loss=0.07293, avg_loss=0.07587]\n",
      "Step 500178  [5.526 sec/step, loss=0.07814, avg_loss=0.07590]\n",
      "Step 500179  [5.511 sec/step, loss=0.07683, avg_loss=0.07589]\n",
      "Step 500180  [5.507 sec/step, loss=0.07796, avg_loss=0.07589]\n",
      "Step 500181  [5.479 sec/step, loss=0.07377, avg_loss=0.07586]\n",
      "Step 500182  [5.500 sec/step, loss=0.07585, avg_loss=0.07586]\n",
      "Generated 32 batches of size 32 in 2.379 sec\n",
      "Step 500183  [5.508 sec/step, loss=0.07540, avg_loss=0.07583]\n",
      "Step 500184  [5.533 sec/step, loss=0.07853, avg_loss=0.07590]\n",
      "Step 500185  [5.527 sec/step, loss=0.07754, avg_loss=0.07590]\n",
      "Step 500186  [5.540 sec/step, loss=0.07455, avg_loss=0.07591]\n",
      "Step 500187  [5.522 sec/step, loss=0.07634, avg_loss=0.07590]\n",
      "Step 500188  [5.509 sec/step, loss=0.07743, avg_loss=0.07591]\n",
      "Step 500189  [5.500 sec/step, loss=0.07533, avg_loss=0.07590]\n",
      "Step 500190  [5.476 sec/step, loss=0.07222, avg_loss=0.07584]\n",
      "Step 500191  [5.456 sec/step, loss=0.06597, avg_loss=0.07576]\n",
      "Step 500192  [5.474 sec/step, loss=0.07738, avg_loss=0.07576]\n",
      "Step 500193  [5.460 sec/step, loss=0.07549, avg_loss=0.07574]\n",
      "Step 500194  [5.457 sec/step, loss=0.07507, avg_loss=0.07572]\n",
      "Step 500195  [5.448 sec/step, loss=0.07774, avg_loss=0.07574]\n",
      "Step 500196  [5.455 sec/step, loss=0.07790, avg_loss=0.07575]\n",
      "Step 500197  [5.472 sec/step, loss=0.07560, avg_loss=0.07575]\n",
      "Step 500198  [5.472 sec/step, loss=0.07551, avg_loss=0.07577]\n",
      "Step 500199  [5.474 sec/step, loss=0.07382, avg_loss=0.07575]\n",
      "Step 500200  [5.466 sec/step, loss=0.07382, avg_loss=0.07573]\n",
      "Writing summary at step: 500200\n",
      "Step 500201  [5.462 sec/step, loss=0.07653, avg_loss=0.07571]\n",
      "Step 500202  [5.464 sec/step, loss=0.07821, avg_loss=0.07573]\n",
      "Step 500203  [5.460 sec/step, loss=0.07500, avg_loss=0.07573]\n",
      "Step 500204  [5.458 sec/step, loss=0.07697, avg_loss=0.07573]\n",
      "Step 500205  [5.452 sec/step, loss=0.07487, avg_loss=0.07572]\n",
      "Step 500206  [5.511 sec/step, loss=0.06752, avg_loss=0.07564]\n",
      "Step 500207  [5.496 sec/step, loss=0.07404, avg_loss=0.07560]\n",
      "Step 500208  [5.480 sec/step, loss=0.06674, avg_loss=0.07554]\n",
      "Step 500209  [5.466 sec/step, loss=0.07714, avg_loss=0.07557]\n",
      "Step 500210  [5.469 sec/step, loss=0.07764, avg_loss=0.07558]\n",
      "Step 500211  [5.477 sec/step, loss=0.07812, avg_loss=0.07560]\n",
      "Step 500212  [5.454 sec/step, loss=0.07793, avg_loss=0.07562]\n",
      "Step 500213  [5.402 sec/step, loss=0.07697, avg_loss=0.07571]\n",
      "Generated 32 batches of size 32 in 2.391 sec\n",
      "Step 500214  [5.404 sec/step, loss=0.07813, avg_loss=0.07571]\n",
      "Step 500215  [5.379 sec/step, loss=0.07698, avg_loss=0.07572]\n",
      "Step 500216  [5.374 sec/step, loss=0.07605, avg_loss=0.07571]\n",
      "Step 500217  [5.380 sec/step, loss=0.07596, avg_loss=0.07577]\n",
      "Step 500218  [5.375 sec/step, loss=0.07588, avg_loss=0.07578]\n",
      "Step 500219  [5.378 sec/step, loss=0.07696, avg_loss=0.07579]\n",
      "Step 500220  [5.377 sec/step, loss=0.07636, avg_loss=0.07577]\n",
      "Step 500221  [5.373 sec/step, loss=0.07551, avg_loss=0.07576]\n",
      "Step 500222  [5.400 sec/step, loss=0.07492, avg_loss=0.07575]\n",
      "Step 500223  [5.399 sec/step, loss=0.07868, avg_loss=0.07575]\n",
      "Step 500224  [5.390 sec/step, loss=0.07644, avg_loss=0.07575]\n",
      "Step 500225  [5.374 sec/step, loss=0.07744, avg_loss=0.07576]\n",
      "Step 500226  [5.384 sec/step, loss=0.07831, avg_loss=0.07577]\n",
      "Step 500227  [5.407 sec/step, loss=0.07798, avg_loss=0.07580]\n",
      "Step 500228  [5.390 sec/step, loss=0.07193, avg_loss=0.07574]\n",
      "Step 500229  [5.368 sec/step, loss=0.07576, avg_loss=0.07574]\n",
      "Step 500230  [5.383 sec/step, loss=0.07563, avg_loss=0.07574]\n",
      "Step 500231  [5.443 sec/step, loss=0.06814, avg_loss=0.07569]\n",
      "Step 500232  [5.460 sec/step, loss=0.07501, avg_loss=0.07567]\n",
      "Step 500233  [5.466 sec/step, loss=0.07463, avg_loss=0.07567]\n",
      "Step 500234  [5.428 sec/step, loss=0.07044, avg_loss=0.07561]\n",
      "Step 500235  [5.423 sec/step, loss=0.07714, avg_loss=0.07561]\n",
      "Step 500236  [5.422 sec/step, loss=0.07826, avg_loss=0.07562]\n",
      "Step 500237  [5.416 sec/step, loss=0.07527, avg_loss=0.07560]\n",
      "Step 500238  [5.415 sec/step, loss=0.07508, avg_loss=0.07557]\n",
      "Step 500239  [5.411 sec/step, loss=0.07735, avg_loss=0.07557]\n",
      "Step 500240  [5.420 sec/step, loss=0.07497, avg_loss=0.07554]\n",
      "Step 500241  [5.429 sec/step, loss=0.07794, avg_loss=0.07558]\n",
      "Step 500242  [5.426 sec/step, loss=0.07377, avg_loss=0.07556]\n",
      "Step 500243  [5.424 sec/step, loss=0.07752, avg_loss=0.07556]\n",
      "Step 500244  [5.420 sec/step, loss=0.07562, avg_loss=0.07555]\n",
      "Step 500245  [5.433 sec/step, loss=0.07758, avg_loss=0.07560]\n",
      "Generated 32 batches of size 32 in 2.416 sec\n",
      "Step 500246  [5.443 sec/step, loss=0.07867, avg_loss=0.07562]\n",
      "Step 500247  [5.429 sec/step, loss=0.07292, avg_loss=0.07557]\n",
      "Step 500248  [5.431 sec/step, loss=0.07525, avg_loss=0.07554]\n",
      "Step 500249  [5.403 sec/step, loss=0.07339, avg_loss=0.07552]\n",
      "Step 500250  [5.403 sec/step, loss=0.07484, avg_loss=0.07549]\n",
      "Step 500251  [5.377 sec/step, loss=0.07568, avg_loss=0.07547]\n",
      "Step 500252  [5.389 sec/step, loss=0.07672, avg_loss=0.07547]\n",
      "Step 500253  [5.390 sec/step, loss=0.07558, avg_loss=0.07548]\n",
      "Step 500254  [5.397 sec/step, loss=0.07787, avg_loss=0.07552]\n",
      "Step 500255  [5.412 sec/step, loss=0.07698, avg_loss=0.07562]\n",
      "Step 500256  [5.420 sec/step, loss=0.07621, avg_loss=0.07563]\n",
      "Step 500257  [5.389 sec/step, loss=0.07558, avg_loss=0.07564]\n",
      "Step 500258  [5.394 sec/step, loss=0.07594, avg_loss=0.07563]\n",
      "Step 500259  [5.351 sec/step, loss=0.07793, avg_loss=0.07573]\n",
      "Step 500260  [5.358 sec/step, loss=0.07856, avg_loss=0.07574]\n",
      "Step 500261  [5.306 sec/step, loss=0.07715, avg_loss=0.07584]\n",
      "Step 500262  [5.349 sec/step, loss=0.06742, avg_loss=0.07574]\n",
      "Step 500263  [5.356 sec/step, loss=0.07727, avg_loss=0.07573]\n",
      "Step 500264  [5.371 sec/step, loss=0.07728, avg_loss=0.07575]\n",
      "Step 500265  [5.330 sec/step, loss=0.06649, avg_loss=0.07564]\n",
      "Step 500266  [5.310 sec/step, loss=0.07268, avg_loss=0.07562]\n",
      "Step 500267  [5.303 sec/step, loss=0.07353, avg_loss=0.07560]\n",
      "Step 500268  [5.296 sec/step, loss=0.07543, avg_loss=0.07557]\n",
      "Step 500269  [5.304 sec/step, loss=0.07767, avg_loss=0.07558]\n",
      "Step 500270  [5.274 sec/step, loss=0.07621, avg_loss=0.07559]\n",
      "Step 500271  [5.262 sec/step, loss=0.07578, avg_loss=0.07559]\n",
      "Step 500272  [5.265 sec/step, loss=0.07515, avg_loss=0.07559]\n",
      "Step 500273  [5.267 sec/step, loss=0.07556, avg_loss=0.07556]\n",
      "Step 500274  [5.266 sec/step, loss=0.07404, avg_loss=0.07553]\n",
      "Step 500275  [5.277 sec/step, loss=0.07777, avg_loss=0.07554]\n",
      "Step 500276  [5.282 sec/step, loss=0.07668, avg_loss=0.07555]\n",
      "Step 500277  [5.291 sec/step, loss=0.07564, avg_loss=0.07557]\n",
      "Generated 32 batches of size 32 in 2.397 sec\n",
      "Step 500278  [5.288 sec/step, loss=0.07709, avg_loss=0.07556]\n",
      "Step 500279  [5.302 sec/step, loss=0.07791, avg_loss=0.07557]\n",
      "Step 500280  [5.293 sec/step, loss=0.07767, avg_loss=0.07557]\n",
      "Step 500281  [5.321 sec/step, loss=0.07499, avg_loss=0.07558]\n",
      "Step 500282  [5.327 sec/step, loss=0.07718, avg_loss=0.07560]\n",
      "Step 500283  [5.310 sec/step, loss=0.07480, avg_loss=0.07559]\n",
      "Step 500284  [5.295 sec/step, loss=0.07695, avg_loss=0.07558]\n",
      "Step 500285  [5.299 sec/step, loss=0.07592, avg_loss=0.07556]\n",
      "Step 500286  [5.303 sec/step, loss=0.07646, avg_loss=0.07558]\n",
      "Step 500287  [5.300 sec/step, loss=0.07351, avg_loss=0.07555]\n",
      "Step 500288  [5.300 sec/step, loss=0.07749, avg_loss=0.07555]\n",
      "Step 500289  [5.288 sec/step, loss=0.06840, avg_loss=0.07548]\n",
      "Step 500290  [5.311 sec/step, loss=0.07761, avg_loss=0.07554]\n",
      "Step 500291  [5.331 sec/step, loss=0.07650, avg_loss=0.07564]\n",
      "Step 500292  [5.321 sec/step, loss=0.07827, avg_loss=0.07565]\n",
      "Step 500293  [5.335 sec/step, loss=0.07874, avg_loss=0.07568]\n",
      "Step 500294  [5.335 sec/step, loss=0.07719, avg_loss=0.07570]\n",
      "Step 500295  [5.325 sec/step, loss=0.07561, avg_loss=0.07568]\n",
      "Step 500296  [5.318 sec/step, loss=0.07656, avg_loss=0.07567]\n",
      "Step 500297  [5.318 sec/step, loss=0.07659, avg_loss=0.07568]\n",
      "Step 500298  [5.320 sec/step, loss=0.07698, avg_loss=0.07569]\n",
      "Step 500299  [5.335 sec/step, loss=0.07817, avg_loss=0.07574]\n",
      "Step 500300  [5.335 sec/step, loss=0.07304, avg_loss=0.07573]\n",
      "Writing summary at step: 500300\n",
      "Step 500301  [5.325 sec/step, loss=0.07380, avg_loss=0.07570]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500302  [5.320 sec/step, loss=0.07346, avg_loss=0.07565]\n",
      "Step 500303  [5.316 sec/step, loss=0.07634, avg_loss=0.07567]\n",
      "Step 500304  [5.329 sec/step, loss=0.07427, avg_loss=0.07564]\n",
      "Step 500305  [5.314 sec/step, loss=0.07624, avg_loss=0.07565]\n",
      "Step 500306  [5.264 sec/step, loss=0.07726, avg_loss=0.07575]\n",
      "Step 500307  [5.327 sec/step, loss=0.06792, avg_loss=0.07569]\n",
      "Step 500308  [5.362 sec/step, loss=0.07670, avg_loss=0.07579]\n",
      "Generated 32 batches of size 32 in 2.415 sec\n",
      "Step 500309  [5.370 sec/step, loss=0.07652, avg_loss=0.07578]\n",
      "Step 500310  [5.365 sec/step, loss=0.07679, avg_loss=0.07578]\n",
      "Step 500311  [5.354 sec/step, loss=0.07720, avg_loss=0.07577]\n",
      "Step 500312  [5.366 sec/step, loss=0.07745, avg_loss=0.07576]\n",
      "Step 500313  [5.358 sec/step, loss=0.07537, avg_loss=0.07575]\n",
      "Step 500314  [5.356 sec/step, loss=0.07787, avg_loss=0.07574]\n",
      "Step 500315  [5.358 sec/step, loss=0.07524, avg_loss=0.07572]\n",
      "Step 500316  [5.376 sec/step, loss=0.07497, avg_loss=0.07571]\n",
      "Step 500317  [5.395 sec/step, loss=0.07706, avg_loss=0.07573]\n",
      "Step 500318  [5.386 sec/step, loss=0.07596, avg_loss=0.07573]\n",
      "Step 500319  [5.371 sec/step, loss=0.07635, avg_loss=0.07572]\n",
      "Step 500320  [5.362 sec/step, loss=0.07608, avg_loss=0.07572]\n",
      "Step 500321  [5.381 sec/step, loss=0.07537, avg_loss=0.07572]\n",
      "Step 500322  [5.345 sec/step, loss=0.07245, avg_loss=0.07569]\n",
      "Step 500323  [5.342 sec/step, loss=0.07852, avg_loss=0.07569]\n",
      "Step 500324  [5.343 sec/step, loss=0.07763, avg_loss=0.07570]\n",
      "Step 500325  [5.393 sec/step, loss=0.06810, avg_loss=0.07561]\n",
      "Step 500326  [5.388 sec/step, loss=0.07772, avg_loss=0.07560]\n",
      "Step 500327  [5.375 sec/step, loss=0.07734, avg_loss=0.07560]\n",
      "Step 500328  [5.385 sec/step, loss=0.07669, avg_loss=0.07564]\n",
      "Step 500329  [5.407 sec/step, loss=0.07823, avg_loss=0.07567]\n",
      "Step 500330  [5.397 sec/step, loss=0.07723, avg_loss=0.07568]\n",
      "Step 500331  [5.356 sec/step, loss=0.07867, avg_loss=0.07579]\n",
      "Step 500332  [5.327 sec/step, loss=0.07452, avg_loss=0.07578]\n",
      "Step 500333  [5.324 sec/step, loss=0.07704, avg_loss=0.07581]\n",
      "Step 500334  [5.332 sec/step, loss=0.07553, avg_loss=0.07586]\n",
      "Step 500335  [5.318 sec/step, loss=0.07688, avg_loss=0.07586]\n",
      "Step 500336  [5.336 sec/step, loss=0.07544, avg_loss=0.07583]\n",
      "Step 500337  [5.351 sec/step, loss=0.07599, avg_loss=0.07584]\n",
      "Step 500338  [5.363 sec/step, loss=0.07598, avg_loss=0.07584]\n",
      "Step 500339  [5.375 sec/step, loss=0.07602, avg_loss=0.07583]\n",
      "Step 500340  [5.339 sec/step, loss=0.06791, avg_loss=0.07576]\n",
      "Generated 32 batches of size 32 in 2.475 sec\n",
      "Step 500341  [5.339 sec/step, loss=0.07602, avg_loss=0.07574]\n",
      "Step 500342  [5.356 sec/step, loss=0.07597, avg_loss=0.07576]\n",
      "Step 500343  [5.369 sec/step, loss=0.07755, avg_loss=0.07576]\n",
      "Step 500344  [5.371 sec/step, loss=0.07552, avg_loss=0.07576]\n",
      "Step 500345  [5.377 sec/step, loss=0.07674, avg_loss=0.07575]\n",
      "Step 500346  [5.350 sec/step, loss=0.07336, avg_loss=0.07570]\n",
      "Step 500347  [5.359 sec/step, loss=0.07649, avg_loss=0.07574]\n",
      "Step 500348  [5.356 sec/step, loss=0.07535, avg_loss=0.07574]\n",
      "Step 500349  [5.366 sec/step, loss=0.07603, avg_loss=0.07576]\n",
      "Step 500350  [5.365 sec/step, loss=0.07618, avg_loss=0.07578]\n",
      "Step 500351  [5.365 sec/step, loss=0.07344, avg_loss=0.07576]\n",
      "Step 500352  [5.351 sec/step, loss=0.07593, avg_loss=0.07575]\n",
      "Step 500353  [5.354 sec/step, loss=0.07762, avg_loss=0.07577]\n",
      "Step 500354  [5.330 sec/step, loss=0.06705, avg_loss=0.07566]\n",
      "Step 500355  [5.340 sec/step, loss=0.07753, avg_loss=0.07566]\n",
      "Step 500356  [5.338 sec/step, loss=0.07747, avg_loss=0.07568]\n",
      "Step 500357  [5.350 sec/step, loss=0.07800, avg_loss=0.07570]\n",
      "Step 500358  [5.342 sec/step, loss=0.07638, avg_loss=0.07571]\n",
      "Step 500359  [5.335 sec/step, loss=0.07798, avg_loss=0.07571]\n",
      "Step 500360  [5.332 sec/step, loss=0.07610, avg_loss=0.07568]\n",
      "Step 500361  [5.345 sec/step, loss=0.07473, avg_loss=0.07566]\n",
      "Step 500362  [5.305 sec/step, loss=0.07800, avg_loss=0.07576]\n",
      "Step 500363  [5.290 sec/step, loss=0.07648, avg_loss=0.07576]\n",
      "Step 500364  [5.303 sec/step, loss=0.07738, avg_loss=0.07576]\n",
      "Step 500365  [5.321 sec/step, loss=0.07739, avg_loss=0.07587]\n",
      "Step 500366  [5.329 sec/step, loss=0.07639, avg_loss=0.07590]\n",
      "Step 500367  [5.336 sec/step, loss=0.07419, avg_loss=0.07591]\n",
      "Step 500368  [5.340 sec/step, loss=0.07591, avg_loss=0.07591]\n",
      "Step 500369  [5.341 sec/step, loss=0.07575, avg_loss=0.07589]\n",
      "Step 500370  [5.351 sec/step, loss=0.07618, avg_loss=0.07589]\n",
      "Step 500371  [5.355 sec/step, loss=0.07547, avg_loss=0.07589]\n",
      "Step 500372  [5.348 sec/step, loss=0.07540, avg_loss=0.07589]\n",
      "Generated 32 batches of size 32 in 2.451 sec\n",
      "Step 500373  [5.340 sec/step, loss=0.07535, avg_loss=0.07589]\n",
      "Step 500374  [5.354 sec/step, loss=0.07689, avg_loss=0.07592]\n",
      "Step 500375  [5.339 sec/step, loss=0.07647, avg_loss=0.07591]\n",
      "Step 500376  [5.390 sec/step, loss=0.06704, avg_loss=0.07581]\n",
      "Step 500377  [5.391 sec/step, loss=0.07593, avg_loss=0.07581]\n",
      "Step 500378  [5.390 sec/step, loss=0.07739, avg_loss=0.07582]\n",
      "Step 500379  [5.389 sec/step, loss=0.07789, avg_loss=0.07582]\n",
      "Step 500380  [5.374 sec/step, loss=0.07335, avg_loss=0.07577]\n",
      "Step 500381  [5.374 sec/step, loss=0.07480, avg_loss=0.07577]\n",
      "Step 500382  [5.353 sec/step, loss=0.07662, avg_loss=0.07577]\n",
      "Step 500383  [5.370 sec/step, loss=0.07539, avg_loss=0.07577]\n",
      "Step 500384  [5.366 sec/step, loss=0.07284, avg_loss=0.07573]\n",
      "Step 500385  [5.340 sec/step, loss=0.06734, avg_loss=0.07564]\n",
      "Step 500386  [5.353 sec/step, loss=0.07713, avg_loss=0.07565]\n",
      "Step 500387  [5.354 sec/step, loss=0.07566, avg_loss=0.07567]\n",
      "Step 500388  [5.378 sec/step, loss=0.07429, avg_loss=0.07564]\n",
      "Step 500389  [5.400 sec/step, loss=0.07673, avg_loss=0.07572]\n",
      "Step 500390  [5.400 sec/step, loss=0.07719, avg_loss=0.07572]\n",
      "Step 500391  [5.406 sec/step, loss=0.07590, avg_loss=0.07571]\n",
      "Step 500392  [5.412 sec/step, loss=0.07691, avg_loss=0.07570]\n",
      "Step 500393  [5.392 sec/step, loss=0.07279, avg_loss=0.07564]\n",
      "Step 500394  [5.387 sec/step, loss=0.07597, avg_loss=0.07563]\n",
      "Step 500395  [5.380 sec/step, loss=0.07138, avg_loss=0.07559]\n",
      "Step 500396  [5.426 sec/step, loss=0.06815, avg_loss=0.07550]\n",
      "Step 500397  [5.421 sec/step, loss=0.07734, avg_loss=0.07551]\n",
      "Step 500398  [5.425 sec/step, loss=0.07738, avg_loss=0.07551]\n",
      "Step 500399  [5.410 sec/step, loss=0.07763, avg_loss=0.07551]\n",
      "Step 500400  [5.420 sec/step, loss=0.07620, avg_loss=0.07554]\n",
      "Writing summary at step: 500400\n",
      "Step 500401  [5.433 sec/step, loss=0.07848, avg_loss=0.07559]\n",
      "Step 500402  [5.442 sec/step, loss=0.07796, avg_loss=0.07563]\n",
      "Step 500403  [5.446 sec/step, loss=0.07556, avg_loss=0.07562]\n",
      "Generated 32 batches of size 32 in 2.549 sec\n",
      "Step 500404  [5.422 sec/step, loss=0.07373, avg_loss=0.07562]\n",
      "Step 500405  [5.437 sec/step, loss=0.07586, avg_loss=0.07561]\n",
      "Step 500406  [5.429 sec/step, loss=0.07538, avg_loss=0.07560]\n",
      "Step 500407  [5.386 sec/step, loss=0.07934, avg_loss=0.07571]\n",
      "Step 500408  [5.381 sec/step, loss=0.07793, avg_loss=0.07572]\n",
      "Step 500409  [5.378 sec/step, loss=0.07770, avg_loss=0.07573]\n",
      "Step 500410  [5.379 sec/step, loss=0.07678, avg_loss=0.07573]\n",
      "Step 500411  [5.387 sec/step, loss=0.07667, avg_loss=0.07573]\n",
      "Step 500412  [5.362 sec/step, loss=0.07644, avg_loss=0.07572]\n",
      "Step 500413  [5.368 sec/step, loss=0.07618, avg_loss=0.07573]\n",
      "Step 500414  [5.352 sec/step, loss=0.07573, avg_loss=0.07571]\n",
      "Step 500415  [5.344 sec/step, loss=0.07312, avg_loss=0.07568]\n",
      "Step 500416  [5.384 sec/step, loss=0.06769, avg_loss=0.07561]\n",
      "Step 500417  [5.389 sec/step, loss=0.07884, avg_loss=0.07563]\n",
      "Step 500418  [5.401 sec/step, loss=0.07456, avg_loss=0.07562]\n",
      "Step 500419  [5.407 sec/step, loss=0.07689, avg_loss=0.07562]\n",
      "Step 500420  [5.444 sec/step, loss=0.07538, avg_loss=0.07561]\n",
      "Step 500421  [5.419 sec/step, loss=0.07567, avg_loss=0.07562]\n",
      "Step 500422  [5.434 sec/step, loss=0.07659, avg_loss=0.07566]\n",
      "Step 500423  [5.409 sec/step, loss=0.06790, avg_loss=0.07555]\n",
      "Step 500424  [5.408 sec/step, loss=0.07643, avg_loss=0.07554]\n",
      "Step 500425  [5.355 sec/step, loss=0.07366, avg_loss=0.07560]\n",
      "Step 500426  [5.340 sec/step, loss=0.07622, avg_loss=0.07558]\n",
      "Step 500427  [5.330 sec/step, loss=0.07392, avg_loss=0.07555]\n",
      "Step 500428  [5.316 sec/step, loss=0.07403, avg_loss=0.07552]\n",
      "Step 500429  [5.314 sec/step, loss=0.07595, avg_loss=0.07550]\n",
      "Step 500430  [5.318 sec/step, loss=0.07850, avg_loss=0.07551]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500431  [5.311 sec/step, loss=0.07737, avg_loss=0.07550]\n",
      "Step 500432  [5.320 sec/step, loss=0.07457, avg_loss=0.07550]\n",
      "Step 500433  [5.326 sec/step, loss=0.07841, avg_loss=0.07551]\n",
      "Step 500434  [5.342 sec/step, loss=0.07662, avg_loss=0.07552]\n",
      "Step 500435  [5.345 sec/step, loss=0.07305, avg_loss=0.07548]\n",
      "Generated 32 batches of size 32 in 2.486 sec\n",
      "Step 500436  [5.320 sec/step, loss=0.07742, avg_loss=0.07550]\n",
      "Step 500437  [5.322 sec/step, loss=0.07823, avg_loss=0.07553]\n",
      "Step 500438  [5.311 sec/step, loss=0.07812, avg_loss=0.07555]\n",
      "Step 500439  [5.314 sec/step, loss=0.07789, avg_loss=0.07557]\n",
      "Step 500440  [5.325 sec/step, loss=0.07530, avg_loss=0.07564]\n",
      "Step 500441  [5.330 sec/step, loss=0.07801, avg_loss=0.07566]\n",
      "Step 500442  [5.349 sec/step, loss=0.07594, avg_loss=0.07566]\n",
      "Step 500443  [5.331 sec/step, loss=0.07723, avg_loss=0.07566]\n",
      "Step 500444  [5.343 sec/step, loss=0.07543, avg_loss=0.07566]\n",
      "Step 500445  [5.339 sec/step, loss=0.07774, avg_loss=0.07567]\n",
      "Step 500446  [5.351 sec/step, loss=0.07601, avg_loss=0.07569]\n",
      "Step 500447  [5.343 sec/step, loss=0.07613, avg_loss=0.07569]\n",
      "Step 500448  [5.351 sec/step, loss=0.07645, avg_loss=0.07570]\n",
      "Step 500449  [5.355 sec/step, loss=0.07661, avg_loss=0.07571]\n",
      "Step 500450  [5.346 sec/step, loss=0.07713, avg_loss=0.07571]\n",
      "Step 500451  [5.406 sec/step, loss=0.06894, avg_loss=0.07567]\n",
      "Step 500452  [5.424 sec/step, loss=0.07706, avg_loss=0.07568]\n",
      "Step 500453  [5.426 sec/step, loss=0.07719, avg_loss=0.07568]\n",
      "Step 500454  [5.454 sec/step, loss=0.07795, avg_loss=0.07579]\n",
      "Step 500455  [5.456 sec/step, loss=0.07792, avg_loss=0.07579]\n",
      "Step 500456  [5.448 sec/step, loss=0.07642, avg_loss=0.07578]\n",
      "Step 500457  [5.432 sec/step, loss=0.07189, avg_loss=0.07572]\n",
      "Step 500458  [5.433 sec/step, loss=0.07733, avg_loss=0.07573]\n",
      "Step 500459  [5.430 sec/step, loss=0.07331, avg_loss=0.07568]\n",
      "Step 500460  [5.430 sec/step, loss=0.07575, avg_loss=0.07568]\n",
      "Step 500461  [5.415 sec/step, loss=0.07530, avg_loss=0.07568]\n",
      "Step 500462  [5.386 sec/step, loss=0.06728, avg_loss=0.07558]\n",
      "Step 500463  [5.387 sec/step, loss=0.07736, avg_loss=0.07558]\n",
      "Step 500464  [5.378 sec/step, loss=0.07739, avg_loss=0.07558]\n",
      "Step 500465  [5.363 sec/step, loss=0.07324, avg_loss=0.07554]\n",
      "Step 500466  [5.365 sec/step, loss=0.07391, avg_loss=0.07552]\n",
      "Step 500467  [5.381 sec/step, loss=0.07478, avg_loss=0.07552]\n",
      "Generated 32 batches of size 32 in 2.363 sec\n",
      "Step 500468  [5.388 sec/step, loss=0.07751, avg_loss=0.07554]\n",
      "Step 500469  [5.393 sec/step, loss=0.07638, avg_loss=0.07555]\n",
      "Step 500470  [5.418 sec/step, loss=0.07694, avg_loss=0.07555]\n",
      "Step 500471  [5.414 sec/step, loss=0.07469, avg_loss=0.07555]\n",
      "Step 500472  [5.431 sec/step, loss=0.07738, avg_loss=0.07557]\n",
      "Step 500473  [5.419 sec/step, loss=0.07572, avg_loss=0.07557]\n",
      "Step 500474  [5.406 sec/step, loss=0.07440, avg_loss=0.07555]\n",
      "Step 500475  [5.425 sec/step, loss=0.07807, avg_loss=0.07556]\n",
      "Step 500476  [5.377 sec/step, loss=0.07754, avg_loss=0.07567]\n",
      "Step 500477  [5.382 sec/step, loss=0.07735, avg_loss=0.07568]\n",
      "Step 500478  [5.389 sec/step, loss=0.07815, avg_loss=0.07569]\n",
      "Step 500479  [5.382 sec/step, loss=0.07818, avg_loss=0.07569]\n",
      "Step 500480  [5.409 sec/step, loss=0.07776, avg_loss=0.07574]\n",
      "Step 500481  [5.382 sec/step, loss=0.07626, avg_loss=0.07575]\n",
      "Step 500482  [5.395 sec/step, loss=0.07800, avg_loss=0.07576]\n",
      "Step 500483  [5.378 sec/step, loss=0.07596, avg_loss=0.07577]\n",
      "Step 500484  [5.385 sec/step, loss=0.07720, avg_loss=0.07581]\n",
      "Step 500485  [5.400 sec/step, loss=0.07642, avg_loss=0.07590]\n",
      "Step 500486  [5.393 sec/step, loss=0.07577, avg_loss=0.07589]\n",
      "Step 500487  [5.391 sec/step, loss=0.07530, avg_loss=0.07589]\n",
      "Step 500488  [5.361 sec/step, loss=0.07702, avg_loss=0.07591]\n",
      "Step 500489  [5.348 sec/step, loss=0.07556, avg_loss=0.07590]\n",
      "Step 500490  [5.343 sec/step, loss=0.07758, avg_loss=0.07591]\n",
      "Step 500491  [5.346 sec/step, loss=0.07728, avg_loss=0.07592]\n",
      "Step 500492  [5.322 sec/step, loss=0.07402, avg_loss=0.07589]\n",
      "Step 500493  [5.362 sec/step, loss=0.07459, avg_loss=0.07591]\n",
      "Step 500494  [5.359 sec/step, loss=0.07730, avg_loss=0.07592]\n",
      "Step 500495  [5.352 sec/step, loss=0.06698, avg_loss=0.07588]\n",
      "Step 500496  [5.309 sec/step, loss=0.07669, avg_loss=0.07596]\n",
      "Step 500497  [5.354 sec/step, loss=0.06680, avg_loss=0.07586]\n",
      "Step 500498  [5.356 sec/step, loss=0.07340, avg_loss=0.07582]\n",
      "Step 500499  [5.368 sec/step, loss=0.07752, avg_loss=0.07582]\n",
      "Generated 32 batches of size 32 in 2.598 sec\n",
      "Step 500500  [5.370 sec/step, loss=0.07602, avg_loss=0.07582]\n",
      "Writing summary at step: 500500\n",
      "Step 500501  [5.354 sec/step, loss=0.07290, avg_loss=0.07576]\n",
      "Step 500502  [5.347 sec/step, loss=0.07694, avg_loss=0.07575]\n",
      "Step 500503  [5.352 sec/step, loss=0.07444, avg_loss=0.07574]\n",
      "Step 500504  [5.354 sec/step, loss=0.07638, avg_loss=0.07576]\n",
      "Step 500505  [5.366 sec/step, loss=0.07712, avg_loss=0.07578]\n",
      "Step 500506  [5.375 sec/step, loss=0.07796, avg_loss=0.07580]\n",
      "Step 500507  [5.362 sec/step, loss=0.07542, avg_loss=0.07576]\n",
      "Step 500508  [5.351 sec/step, loss=0.07730, avg_loss=0.07576]\n",
      "Step 500509  [5.349 sec/step, loss=0.07671, avg_loss=0.07575]\n",
      "Step 500510  [5.364 sec/step, loss=0.07756, avg_loss=0.07576]\n",
      "Step 500511  [5.385 sec/step, loss=0.07524, avg_loss=0.07574]\n",
      "Step 500512  [5.400 sec/step, loss=0.07731, avg_loss=0.07575]\n",
      "Step 500513  [5.414 sec/step, loss=0.07610, avg_loss=0.07575]\n",
      "Step 500514  [5.431 sec/step, loss=0.07785, avg_loss=0.07577]\n",
      "Step 500515  [5.430 sec/step, loss=0.07635, avg_loss=0.07580]\n",
      "Step 500516  [5.429 sec/step, loss=0.06672, avg_loss=0.07579]\n",
      "Step 500517  [5.418 sec/step, loss=0.07497, avg_loss=0.07575]\n",
      "Step 500518  [5.421 sec/step, loss=0.07681, avg_loss=0.07578]\n",
      "Step 500519  [5.417 sec/step, loss=0.07629, avg_loss=0.07577]\n",
      "Step 500520  [5.394 sec/step, loss=0.07404, avg_loss=0.07576]\n",
      "Step 500521  [5.410 sec/step, loss=0.07866, avg_loss=0.07579]\n",
      "Step 500522  [5.392 sec/step, loss=0.07332, avg_loss=0.07575]\n",
      "Step 500523  [5.406 sec/step, loss=0.07718, avg_loss=0.07585]\n",
      "Step 500524  [5.399 sec/step, loss=0.07570, avg_loss=0.07584]\n",
      "Step 500525  [5.399 sec/step, loss=0.07489, avg_loss=0.07585]\n",
      "Step 500526  [5.401 sec/step, loss=0.07537, avg_loss=0.07584]\n",
      "Step 500527  [5.419 sec/step, loss=0.07592, avg_loss=0.07586]\n",
      "Step 500528  [5.438 sec/step, loss=0.07752, avg_loss=0.07590]\n",
      "Step 500529  [5.416 sec/step, loss=0.07297, avg_loss=0.07587]\n",
      "Step 500530  [5.424 sec/step, loss=0.07718, avg_loss=0.07586]\n",
      "Generated 32 batches of size 32 in 2.536 sec\n",
      "Step 500531  [5.426 sec/step, loss=0.07616, avg_loss=0.07584]\n",
      "Step 500532  [5.418 sec/step, loss=0.07422, avg_loss=0.07584]\n",
      "Step 500533  [5.421 sec/step, loss=0.07556, avg_loss=0.07581]\n",
      "Step 500534  [5.421 sec/step, loss=0.07783, avg_loss=0.07582]\n",
      "Step 500535  [5.427 sec/step, loss=0.07720, avg_loss=0.07587]\n",
      "Step 500536  [5.437 sec/step, loss=0.07768, avg_loss=0.07587]\n",
      "Step 500537  [5.417 sec/step, loss=0.07569, avg_loss=0.07584]\n",
      "Step 500538  [5.399 sec/step, loss=0.06859, avg_loss=0.07575]\n",
      "Step 500539  [5.386 sec/step, loss=0.07687, avg_loss=0.07574]\n",
      "Step 500540  [5.385 sec/step, loss=0.07358, avg_loss=0.07572]\n",
      "Step 500541  [5.427 sec/step, loss=0.06886, avg_loss=0.07563]\n",
      "Step 500542  [5.389 sec/step, loss=0.07410, avg_loss=0.07561]\n",
      "Step 500543  [5.384 sec/step, loss=0.07505, avg_loss=0.07559]\n",
      "Step 500544  [5.401 sec/step, loss=0.07487, avg_loss=0.07558]\n",
      "Step 500545  [5.409 sec/step, loss=0.07867, avg_loss=0.07559]\n",
      "Step 500546  [5.406 sec/step, loss=0.07553, avg_loss=0.07559]\n",
      "Step 500547  [5.397 sec/step, loss=0.07205, avg_loss=0.07555]\n",
      "Step 500548  [5.394 sec/step, loss=0.07674, avg_loss=0.07555]\n",
      "Step 500549  [5.396 sec/step, loss=0.07569, avg_loss=0.07554]\n",
      "Step 500550  [5.400 sec/step, loss=0.07676, avg_loss=0.07554]\n",
      "Step 500551  [5.340 sec/step, loss=0.07570, avg_loss=0.07560]\n",
      "Step 500552  [5.324 sec/step, loss=0.07601, avg_loss=0.07559]\n",
      "Step 500553  [5.309 sec/step, loss=0.07735, avg_loss=0.07559]\n",
      "Step 500554  [5.308 sec/step, loss=0.07783, avg_loss=0.07559]\n",
      "Step 500555  [5.318 sec/step, loss=0.07447, avg_loss=0.07556]\n",
      "Step 500556  [5.325 sec/step, loss=0.07716, avg_loss=0.07557]\n",
      "Step 500557  [5.339 sec/step, loss=0.07717, avg_loss=0.07562]\n",
      "Step 500558  [5.340 sec/step, loss=0.07497, avg_loss=0.07560]\n",
      "Step 500559  [5.343 sec/step, loss=0.07783, avg_loss=0.07564]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500560  [5.348 sec/step, loss=0.07443, avg_loss=0.07563]\n",
      "Step 500561  [5.362 sec/step, loss=0.07786, avg_loss=0.07565]\n",
      "Step 500562  [5.384 sec/step, loss=0.07609, avg_loss=0.07574]\n",
      "Generated 32 batches of size 32 in 2.368 sec\n",
      "Step 500563  [5.402 sec/step, loss=0.07810, avg_loss=0.07575]\n",
      "Step 500564  [5.403 sec/step, loss=0.07625, avg_loss=0.07574]\n",
      "Step 500565  [5.413 sec/step, loss=0.07718, avg_loss=0.07578]\n",
      "Step 500566  [5.412 sec/step, loss=0.07575, avg_loss=0.07580]\n",
      "Step 500567  [5.415 sec/step, loss=0.07510, avg_loss=0.07580]\n",
      "Step 500568  [5.411 sec/step, loss=0.07802, avg_loss=0.07580]\n",
      "Step 500569  [5.391 sec/step, loss=0.07215, avg_loss=0.07576]\n",
      "Step 500570  [5.370 sec/step, loss=0.07493, avg_loss=0.07574]\n",
      "Step 500571  [5.360 sec/step, loss=0.06754, avg_loss=0.07567]\n",
      "Step 500572  [5.359 sec/step, loss=0.07800, avg_loss=0.07568]\n",
      "Step 500573  [5.393 sec/step, loss=0.07501, avg_loss=0.07567]\n",
      "Step 500574  [5.419 sec/step, loss=0.07571, avg_loss=0.07568]\n",
      "Step 500575  [5.407 sec/step, loss=0.07670, avg_loss=0.07567]\n",
      "Step 500576  [5.404 sec/step, loss=0.07341, avg_loss=0.07563]\n",
      "Step 500577  [5.414 sec/step, loss=0.07561, avg_loss=0.07561]\n",
      "Step 500578  [5.398 sec/step, loss=0.07238, avg_loss=0.07555]\n",
      "Step 500579  [5.390 sec/step, loss=0.07720, avg_loss=0.07554]\n",
      "Step 500580  [5.372 sec/step, loss=0.07546, avg_loss=0.07552]\n",
      "Step 500581  [5.387 sec/step, loss=0.07681, avg_loss=0.07552]\n",
      "Step 500582  [5.385 sec/step, loss=0.07759, avg_loss=0.07552]\n",
      "Step 500583  [5.390 sec/step, loss=0.07510, avg_loss=0.07551]\n",
      "Step 500584  [5.396 sec/step, loss=0.07773, avg_loss=0.07552]\n",
      "Step 500585  [5.400 sec/step, loss=0.07761, avg_loss=0.07553]\n",
      "Step 500586  [5.391 sec/step, loss=0.07567, avg_loss=0.07553]\n",
      "Step 500587  [5.384 sec/step, loss=0.07360, avg_loss=0.07551]\n",
      "Step 500588  [5.393 sec/step, loss=0.07504, avg_loss=0.07549]\n",
      "Step 500589  [5.384 sec/step, loss=0.06842, avg_loss=0.07542]\n",
      "Step 500590  [5.367 sec/step, loss=0.07280, avg_loss=0.07537]\n",
      "Step 500591  [5.359 sec/step, loss=0.07671, avg_loss=0.07537]\n",
      "Step 500592  [5.378 sec/step, loss=0.07826, avg_loss=0.07541]\n",
      "Step 500593  [5.353 sec/step, loss=0.07692, avg_loss=0.07543]\n",
      "Step 500594  [5.360 sec/step, loss=0.07747, avg_loss=0.07543]\n",
      "Generated 32 batches of size 32 in 2.472 sec\n",
      "Step 500595  [5.378 sec/step, loss=0.07542, avg_loss=0.07552]\n",
      "Step 500596  [5.357 sec/step, loss=0.07407, avg_loss=0.07549]\n",
      "Step 500597  [5.323 sec/step, loss=0.07652, avg_loss=0.07559]\n",
      "Step 500598  [5.317 sec/step, loss=0.07615, avg_loss=0.07562]\n",
      "Step 500599  [5.301 sec/step, loss=0.07570, avg_loss=0.07560]\n",
      "Step 500600  [5.349 sec/step, loss=0.06867, avg_loss=0.07552]\n",
      "Writing summary at step: 500600\n",
      "Step 500601  [5.364 sec/step, loss=0.07391, avg_loss=0.07553]\n",
      "Step 500602  [5.368 sec/step, loss=0.07768, avg_loss=0.07554]\n",
      "Step 500603  [5.360 sec/step, loss=0.07434, avg_loss=0.07554]\n",
      "Step 500604  [5.361 sec/step, loss=0.07679, avg_loss=0.07555]\n",
      "Step 500605  [5.355 sec/step, loss=0.07443, avg_loss=0.07552]\n",
      "Step 500606  [5.355 sec/step, loss=0.07616, avg_loss=0.07550]\n",
      "Step 500607  [5.369 sec/step, loss=0.07645, avg_loss=0.07551]\n",
      "Step 500608  [5.365 sec/step, loss=0.07554, avg_loss=0.07549]\n",
      "Step 500609  [5.362 sec/step, loss=0.07614, avg_loss=0.07549]\n",
      "Step 500610  [5.350 sec/step, loss=0.07502, avg_loss=0.07546]\n",
      "Step 500611  [5.315 sec/step, loss=0.07251, avg_loss=0.07543]\n",
      "Step 500612  [5.310 sec/step, loss=0.07670, avg_loss=0.07543]\n",
      "Step 500613  [5.289 sec/step, loss=0.07583, avg_loss=0.07543]\n",
      "Step 500614  [5.284 sec/step, loss=0.07784, avg_loss=0.07543]\n",
      "Step 500615  [5.281 sec/step, loss=0.07272, avg_loss=0.07539]\n",
      "Step 500616  [5.238 sec/step, loss=0.07811, avg_loss=0.07550]\n",
      "Step 500617  [5.246 sec/step, loss=0.07738, avg_loss=0.07553]\n",
      "Step 500618  [5.233 sec/step, loss=0.07555, avg_loss=0.07552]\n",
      "Step 500619  [5.257 sec/step, loss=0.07662, avg_loss=0.07552]\n",
      "Step 500620  [5.254 sec/step, loss=0.07785, avg_loss=0.07556]\n",
      "Step 500621  [5.239 sec/step, loss=0.07316, avg_loss=0.07550]\n",
      "Step 500622  [5.270 sec/step, loss=0.07666, avg_loss=0.07553]\n",
      "Step 500623  [5.285 sec/step, loss=0.07627, avg_loss=0.07553]\n",
      "Step 500624  [5.282 sec/step, loss=0.07628, avg_loss=0.07553]\n",
      "Step 500625  [5.286 sec/step, loss=0.07515, avg_loss=0.07553]\n",
      "Generated 32 batches of size 32 in 2.648 sec\n",
      "Step 500626  [5.295 sec/step, loss=0.07404, avg_loss=0.07552]\n",
      "Step 500627  [5.297 sec/step, loss=0.07650, avg_loss=0.07553]\n",
      "Step 500628  [5.330 sec/step, loss=0.06993, avg_loss=0.07545]\n",
      "Step 500629  [5.346 sec/step, loss=0.07602, avg_loss=0.07548]\n",
      "Step 500630  [5.314 sec/step, loss=0.06702, avg_loss=0.07538]\n",
      "Step 500631  [5.324 sec/step, loss=0.07704, avg_loss=0.07539]\n",
      "Step 500632  [5.322 sec/step, loss=0.07740, avg_loss=0.07542]\n",
      "Step 500633  [5.324 sec/step, loss=0.07845, avg_loss=0.07545]\n",
      "Step 500634  [5.317 sec/step, loss=0.07687, avg_loss=0.07544]\n",
      "Step 500635  [5.298 sec/step, loss=0.07350, avg_loss=0.07540]\n",
      "Step 500636  [5.294 sec/step, loss=0.07768, avg_loss=0.07540]\n",
      "Step 500637  [5.299 sec/step, loss=0.07633, avg_loss=0.07541]\n",
      "Step 500638  [5.314 sec/step, loss=0.07663, avg_loss=0.07549]\n",
      "Step 500639  [5.309 sec/step, loss=0.07627, avg_loss=0.07548]\n",
      "Step 500640  [5.327 sec/step, loss=0.07813, avg_loss=0.07553]\n",
      "Step 500641  [5.266 sec/step, loss=0.07610, avg_loss=0.07560]\n",
      "Step 500642  [5.276 sec/step, loss=0.07493, avg_loss=0.07561]\n",
      "Step 500643  [5.335 sec/step, loss=0.06813, avg_loss=0.07554]\n",
      "Step 500644  [5.330 sec/step, loss=0.07506, avg_loss=0.07554]\n",
      "Step 500645  [5.346 sec/step, loss=0.07574, avg_loss=0.07551]\n",
      "Step 500646  [5.347 sec/step, loss=0.07741, avg_loss=0.07553]\n",
      "Step 500647  [5.353 sec/step, loss=0.07555, avg_loss=0.07557]\n",
      "Step 500648  [5.344 sec/step, loss=0.07556, avg_loss=0.07555]\n",
      "Step 500649  [5.334 sec/step, loss=0.07583, avg_loss=0.07556]\n",
      "Step 500650  [5.340 sec/step, loss=0.07710, avg_loss=0.07556]\n",
      "Step 500651  [5.356 sec/step, loss=0.07728, avg_loss=0.07558]\n",
      "Step 500652  [5.370 sec/step, loss=0.07659, avg_loss=0.07558]\n",
      "Step 500653  [5.384 sec/step, loss=0.07773, avg_loss=0.07558]\n",
      "Step 500654  [5.361 sec/step, loss=0.07237, avg_loss=0.07553]\n",
      "Step 500655  [5.354 sec/step, loss=0.07809, avg_loss=0.07557]\n",
      "Step 500656  [5.357 sec/step, loss=0.07419, avg_loss=0.07554]\n",
      "Step 500657  [5.337 sec/step, loss=0.06802, avg_loss=0.07544]\n",
      "Generated 32 batches of size 32 in 2.532 sec\n",
      "Step 500658  [5.345 sec/step, loss=0.07416, avg_loss=0.07544]\n",
      "Step 500659  [5.353 sec/step, loss=0.07841, avg_loss=0.07544]\n",
      "Step 500660  [5.357 sec/step, loss=0.07792, avg_loss=0.07548]\n",
      "Step 500661  [5.350 sec/step, loss=0.07682, avg_loss=0.07547]\n",
      "Step 500662  [5.357 sec/step, loss=0.07630, avg_loss=0.07547]\n",
      "Step 500663  [5.341 sec/step, loss=0.07740, avg_loss=0.07546]\n",
      "Step 500664  [5.336 sec/step, loss=0.07701, avg_loss=0.07547]\n",
      "Step 500665  [5.342 sec/step, loss=0.07826, avg_loss=0.07548]\n",
      "Step 500666  [5.353 sec/step, loss=0.07698, avg_loss=0.07549]\n",
      "Step 500667  [5.337 sec/step, loss=0.07652, avg_loss=0.07551]\n",
      "Step 500668  [5.375 sec/step, loss=0.06754, avg_loss=0.07540]\n",
      "Step 500669  [5.378 sec/step, loss=0.07635, avg_loss=0.07544]\n",
      "Step 500670  [5.371 sec/step, loss=0.07535, avg_loss=0.07545]\n",
      "Step 500671  [5.373 sec/step, loss=0.06737, avg_loss=0.07545]\n",
      "Step 500672  [5.365 sec/step, loss=0.07661, avg_loss=0.07543]\n",
      "Step 500673  [5.334 sec/step, loss=0.07569, avg_loss=0.07544]\n",
      "Step 500674  [5.316 sec/step, loss=0.07665, avg_loss=0.07545]\n",
      "Step 500675  [5.310 sec/step, loss=0.07514, avg_loss=0.07543]\n",
      "Step 500676  [5.304 sec/step, loss=0.07560, avg_loss=0.07546]\n",
      "Step 500677  [5.298 sec/step, loss=0.07682, avg_loss=0.07547]\n",
      "Step 500678  [5.330 sec/step, loss=0.07438, avg_loss=0.07549]\n",
      "Step 500679  [5.338 sec/step, loss=0.07611, avg_loss=0.07548]\n",
      "Step 500680  [5.343 sec/step, loss=0.07753, avg_loss=0.07550]\n",
      "Step 500681  [5.336 sec/step, loss=0.07545, avg_loss=0.07548]\n",
      "Step 500682  [5.337 sec/step, loss=0.07517, avg_loss=0.07546]\n",
      "Step 500683  [5.338 sec/step, loss=0.07680, avg_loss=0.07548]\n",
      "Step 500684  [5.314 sec/step, loss=0.07329, avg_loss=0.07543]\n",
      "Step 500685  [5.301 sec/step, loss=0.07555, avg_loss=0.07541]\n",
      "Step 500686  [5.302 sec/step, loss=0.07729, avg_loss=0.07543]\n",
      "Step 500687  [5.319 sec/step, loss=0.07854, avg_loss=0.07548]\n",
      "Step 500688  [5.325 sec/step, loss=0.07807, avg_loss=0.07551]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500689  [5.359 sec/step, loss=0.07774, avg_loss=0.07560]\n",
      "Generated 32 batches of size 32 in 2.407 sec\n",
      "Step 500690  [5.380 sec/step, loss=0.07763, avg_loss=0.07565]\n",
      "Step 500691  [5.373 sec/step, loss=0.07401, avg_loss=0.07562]\n",
      "Step 500692  [5.377 sec/step, loss=0.07783, avg_loss=0.07562]\n",
      "Step 500693  [5.366 sec/step, loss=0.07279, avg_loss=0.07558]\n",
      "Step 500694  [5.374 sec/step, loss=0.07530, avg_loss=0.07555]\n",
      "Step 500695  [5.376 sec/step, loss=0.07750, avg_loss=0.07558]\n",
      "Step 500696  [5.389 sec/step, loss=0.07471, avg_loss=0.07558]\n",
      "Step 500697  [5.387 sec/step, loss=0.07855, avg_loss=0.07560]\n",
      "Step 500698  [5.399 sec/step, loss=0.07634, avg_loss=0.07560]\n",
      "Step 500699  [5.402 sec/step, loss=0.07501, avg_loss=0.07560]\n",
      "Step 500700  [5.363 sec/step, loss=0.07875, avg_loss=0.07570]\n",
      "Writing summary at step: 500700\n",
      "Step 500701  [5.357 sec/step, loss=0.07678, avg_loss=0.07573]\n",
      "Step 500702  [5.358 sec/step, loss=0.07799, avg_loss=0.07573]\n",
      "Step 500703  [5.376 sec/step, loss=0.07768, avg_loss=0.07576]\n",
      "Step 500704  [5.369 sec/step, loss=0.07699, avg_loss=0.07577]\n",
      "Step 500705  [5.354 sec/step, loss=0.07707, avg_loss=0.07579]\n",
      "Step 500706  [5.358 sec/step, loss=0.07747, avg_loss=0.07580]\n",
      "Step 500707  [5.362 sec/step, loss=0.07691, avg_loss=0.07581]\n",
      "Step 500708  [5.380 sec/step, loss=0.07730, avg_loss=0.07583]\n",
      "Step 500709  [5.384 sec/step, loss=0.07516, avg_loss=0.07582]\n",
      "Step 500710  [5.388 sec/step, loss=0.07828, avg_loss=0.07585]\n",
      "Step 500711  [5.394 sec/step, loss=0.07554, avg_loss=0.07588]\n",
      "Step 500712  [5.397 sec/step, loss=0.07807, avg_loss=0.07589]\n",
      "Step 500713  [5.403 sec/step, loss=0.07418, avg_loss=0.07588]\n",
      "Step 500714  [5.382 sec/step, loss=0.07358, avg_loss=0.07583]\n",
      "Step 500715  [5.388 sec/step, loss=0.07600, avg_loss=0.07587]\n",
      "Step 500716  [5.371 sec/step, loss=0.07438, avg_loss=0.07583]\n",
      "Step 500717  [5.360 sec/step, loss=0.07732, avg_loss=0.07583]\n",
      "Step 500718  [5.348 sec/step, loss=0.06718, avg_loss=0.07575]\n",
      "Step 500719  [5.323 sec/step, loss=0.07635, avg_loss=0.07574]\n",
      "Step 500720  [5.324 sec/step, loss=0.07734, avg_loss=0.07574]\n",
      "Generated 32 batches of size 32 in 2.649 sec\n",
      "Step 500721  [5.327 sec/step, loss=0.07598, avg_loss=0.07577]\n",
      "Step 500722  [5.322 sec/step, loss=0.07765, avg_loss=0.07578]\n",
      "Step 500723  [5.313 sec/step, loss=0.07732, avg_loss=0.07579]\n",
      "Step 500724  [5.327 sec/step, loss=0.07641, avg_loss=0.07579]\n",
      "Step 500725  [5.335 sec/step, loss=0.07588, avg_loss=0.07580]\n",
      "Step 500726  [5.347 sec/step, loss=0.07648, avg_loss=0.07582]\n",
      "Step 500727  [5.367 sec/step, loss=0.07480, avg_loss=0.07580]\n",
      "Step 500728  [5.314 sec/step, loss=0.07600, avg_loss=0.07586]\n",
      "Step 500729  [5.315 sec/step, loss=0.07873, avg_loss=0.07589]\n",
      "Step 500730  [5.328 sec/step, loss=0.07138, avg_loss=0.07593]\n",
      "Step 500731  [5.317 sec/step, loss=0.07633, avg_loss=0.07593]\n",
      "Step 500732  [5.346 sec/step, loss=0.07528, avg_loss=0.07591]\n",
      "Step 500733  [5.344 sec/step, loss=0.07719, avg_loss=0.07589]\n",
      "Step 500734  [5.344 sec/step, loss=0.07748, avg_loss=0.07590]\n",
      "Step 500735  [5.378 sec/step, loss=0.07623, avg_loss=0.07593]\n",
      "Step 500736  [5.380 sec/step, loss=0.07801, avg_loss=0.07593]\n",
      "Step 500737  [5.399 sec/step, loss=0.07633, avg_loss=0.07593]\n",
      "Step 500738  [5.392 sec/step, loss=0.07240, avg_loss=0.07589]\n",
      "Step 500739  [5.399 sec/step, loss=0.07659, avg_loss=0.07589]\n",
      "Step 500740  [5.387 sec/step, loss=0.07423, avg_loss=0.07585]\n",
      "Step 500741  [5.404 sec/step, loss=0.07741, avg_loss=0.07587]\n",
      "Step 500742  [5.417 sec/step, loss=0.07749, avg_loss=0.07589]\n",
      "Step 500743  [5.367 sec/step, loss=0.07657, avg_loss=0.07598]\n",
      "Step 500744  [5.348 sec/step, loss=0.07678, avg_loss=0.07599]\n",
      "Step 500745  [5.314 sec/step, loss=0.07340, avg_loss=0.07597]\n",
      "Step 500746  [5.314 sec/step, loss=0.07376, avg_loss=0.07593]\n",
      "Step 500747  [5.311 sec/step, loss=0.07594, avg_loss=0.07594]\n",
      "Step 500748  [5.326 sec/step, loss=0.07759, avg_loss=0.07596]\n",
      "Step 500749  [5.334 sec/step, loss=0.07627, avg_loss=0.07596]\n",
      "Step 500750  [5.333 sec/step, loss=0.07376, avg_loss=0.07593]\n",
      "Step 500751  [5.313 sec/step, loss=0.07470, avg_loss=0.07590]\n",
      "Step 500752  [5.357 sec/step, loss=0.06955, avg_loss=0.07583]\n",
      "Generated 32 batches of size 32 in 2.397 sec\n",
      "Step 500753  [5.362 sec/step, loss=0.07530, avg_loss=0.07581]\n",
      "Step 500754  [5.372 sec/step, loss=0.07704, avg_loss=0.07585]\n",
      "Step 500755  [5.375 sec/step, loss=0.07666, avg_loss=0.07584]\n",
      "Step 500756  [5.360 sec/step, loss=0.06879, avg_loss=0.07579]\n",
      "Step 500757  [5.373 sec/step, loss=0.07537, avg_loss=0.07586]\n",
      "Step 500758  [5.370 sec/step, loss=0.07760, avg_loss=0.07589]\n",
      "Step 500759  [5.359 sec/step, loss=0.07495, avg_loss=0.07586]\n",
      "Step 500760  [5.356 sec/step, loss=0.07765, avg_loss=0.07586]\n",
      "Step 500761  [5.351 sec/step, loss=0.07609, avg_loss=0.07585]\n",
      "Step 500762  [5.345 sec/step, loss=0.07721, avg_loss=0.07586]\n",
      "Step 500763  [5.336 sec/step, loss=0.07541, avg_loss=0.07584]\n",
      "Step 500764  [5.344 sec/step, loss=0.07599, avg_loss=0.07583]\n",
      "Step 500765  [5.340 sec/step, loss=0.07687, avg_loss=0.07581]\n",
      "Step 500766  [5.335 sec/step, loss=0.07623, avg_loss=0.07581]\n",
      "Step 500767  [5.343 sec/step, loss=0.07397, avg_loss=0.07578]\n",
      "Step 500768  [5.309 sec/step, loss=0.07648, avg_loss=0.07587]\n",
      "Step 500769  [5.320 sec/step, loss=0.07787, avg_loss=0.07589]\n",
      "Step 500770  [5.349 sec/step, loss=0.07618, avg_loss=0.07589]\n",
      "Step 500771  [5.359 sec/step, loss=0.07214, avg_loss=0.07594]\n",
      "Step 500772  [5.341 sec/step, loss=0.06745, avg_loss=0.07585]\n",
      "Step 500773  [5.344 sec/step, loss=0.07307, avg_loss=0.07582]\n",
      "Step 500774  [5.333 sec/step, loss=0.07598, avg_loss=0.07582]\n",
      "Step 500775  [5.330 sec/step, loss=0.07562, avg_loss=0.07582]\n",
      "Step 500776  [5.323 sec/step, loss=0.07335, avg_loss=0.07580]\n",
      "Step 500777  [5.313 sec/step, loss=0.07547, avg_loss=0.07579]\n",
      "Step 500778  [5.294 sec/step, loss=0.07753, avg_loss=0.07582]\n",
      "Step 500779  [5.339 sec/step, loss=0.06710, avg_loss=0.07573]\n",
      "Step 500780  [5.344 sec/step, loss=0.07831, avg_loss=0.07574]\n",
      "Step 500781  [5.338 sec/step, loss=0.07626, avg_loss=0.07574]\n",
      "Step 500782  [5.345 sec/step, loss=0.07617, avg_loss=0.07575]\n",
      "Step 500783  [5.349 sec/step, loss=0.07715, avg_loss=0.07576]\n",
      "Step 500784  [5.362 sec/step, loss=0.07680, avg_loss=0.07579]\n",
      "Generated 32 batches of size 32 in 2.687 sec\n",
      "Step 500785  [5.365 sec/step, loss=0.07227, avg_loss=0.07576]\n",
      "Step 500786  [5.373 sec/step, loss=0.07628, avg_loss=0.07575]\n",
      "Step 500787  [5.363 sec/step, loss=0.07567, avg_loss=0.07572]\n",
      "Step 500788  [5.350 sec/step, loss=0.07569, avg_loss=0.07570]\n",
      "Step 500789  [5.337 sec/step, loss=0.07453, avg_loss=0.07566]\n",
      "Step 500790  [5.325 sec/step, loss=0.07490, avg_loss=0.07564]\n",
      "Step 500791  [5.337 sec/step, loss=0.07763, avg_loss=0.07567]\n",
      "Step 500792  [5.333 sec/step, loss=0.07833, avg_loss=0.07568]\n",
      "Step 500793  [5.344 sec/step, loss=0.07832, avg_loss=0.07573]\n",
      "Step 500794  [5.358 sec/step, loss=0.07445, avg_loss=0.07573]\n",
      "Step 500795  [5.354 sec/step, loss=0.07715, avg_loss=0.07572]\n",
      "Step 500796  [5.364 sec/step, loss=0.07826, avg_loss=0.07576]\n",
      "Step 500797  [5.350 sec/step, loss=0.07665, avg_loss=0.07574]\n",
      "Step 500798  [5.336 sec/step, loss=0.07482, avg_loss=0.07572]\n",
      "Step 500799  [5.331 sec/step, loss=0.07606, avg_loss=0.07573]\n",
      "Step 500800  [5.330 sec/step, loss=0.07818, avg_loss=0.07573]\n",
      "Writing summary at step: 500800\n",
      "Step 500801  [5.331 sec/step, loss=0.07702, avg_loss=0.07573]\n",
      "Step 500802  [5.327 sec/step, loss=0.07434, avg_loss=0.07569]\n",
      "Step 500803  [5.309 sec/step, loss=0.07565, avg_loss=0.07567]\n",
      "Step 500804  [5.295 sec/step, loss=0.06787, avg_loss=0.07558]\n",
      "Step 500805  [5.297 sec/step, loss=0.07593, avg_loss=0.07557]\n",
      "Step 500806  [5.291 sec/step, loss=0.07359, avg_loss=0.07553]\n",
      "Step 500807  [5.284 sec/step, loss=0.07689, avg_loss=0.07553]\n",
      "Step 500808  [5.283 sec/step, loss=0.07862, avg_loss=0.07554]\n",
      "Step 500809  [5.292 sec/step, loss=0.07617, avg_loss=0.07555]\n",
      "Step 500810  [5.277 sec/step, loss=0.07554, avg_loss=0.07553]\n",
      "Step 500811  [5.271 sec/step, loss=0.07400, avg_loss=0.07551]\n",
      "Step 500812  [5.280 sec/step, loss=0.07622, avg_loss=0.07549]\n",
      "Step 500813  [5.292 sec/step, loss=0.07795, avg_loss=0.07553]\n",
      "Step 500814  [5.318 sec/step, loss=0.07781, avg_loss=0.07557]\n",
      "Step 500815  [5.330 sec/step, loss=0.07663, avg_loss=0.07558]\n",
      "Generated 32 batches of size 32 in 2.826 sec\n",
      "Step 500816  [5.330 sec/step, loss=0.07375, avg_loss=0.07557]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500817  [5.338 sec/step, loss=0.07701, avg_loss=0.07557]\n",
      "Step 500818  [5.352 sec/step, loss=0.07375, avg_loss=0.07564]\n",
      "Step 500819  [5.361 sec/step, loss=0.07842, avg_loss=0.07566]\n",
      "Step 500820  [5.378 sec/step, loss=0.07543, avg_loss=0.07564]\n",
      "Step 500821  [5.431 sec/step, loss=0.06782, avg_loss=0.07556]\n",
      "Step 500822  [5.424 sec/step, loss=0.07745, avg_loss=0.07555]\n",
      "Step 500823  [5.423 sec/step, loss=0.07520, avg_loss=0.07553]\n",
      "Step 500824  [5.415 sec/step, loss=0.07555, avg_loss=0.07552]\n",
      "Step 500825  [5.402 sec/step, loss=0.07556, avg_loss=0.07552]\n",
      "Step 500826  [5.400 sec/step, loss=0.07800, avg_loss=0.07554]\n",
      "Step 500827  [5.377 sec/step, loss=0.07643, avg_loss=0.07555]\n",
      "Step 500828  [5.384 sec/step, loss=0.07708, avg_loss=0.07556]\n",
      "Step 500829  [5.404 sec/step, loss=0.07465, avg_loss=0.07552]\n",
      "Step 500830  [5.398 sec/step, loss=0.07229, avg_loss=0.07553]\n",
      "Step 500831  [5.405 sec/step, loss=0.07806, avg_loss=0.07555]\n",
      "Step 500832  [5.380 sec/step, loss=0.07765, avg_loss=0.07557]\n",
      "Step 500833  [5.364 sec/step, loss=0.07468, avg_loss=0.07555]\n",
      "Step 500834  [5.373 sec/step, loss=0.07494, avg_loss=0.07552]\n",
      "Step 500835  [5.403 sec/step, loss=0.06862, avg_loss=0.07545]\n",
      "Step 500836  [5.401 sec/step, loss=0.07842, avg_loss=0.07545]\n",
      "Step 500837  [5.395 sec/step, loss=0.07793, avg_loss=0.07547]\n",
      "Step 500838  [5.403 sec/step, loss=0.07589, avg_loss=0.07550]\n",
      "Step 500839  [5.406 sec/step, loss=0.07720, avg_loss=0.07551]\n",
      "Step 500840  [5.412 sec/step, loss=0.07701, avg_loss=0.07553]\n",
      "Step 500841  [5.398 sec/step, loss=0.07614, avg_loss=0.07552]\n",
      "Step 500842  [5.387 sec/step, loss=0.07412, avg_loss=0.07549]\n",
      "Step 500843  [5.401 sec/step, loss=0.07737, avg_loss=0.07550]\n",
      "Step 500844  [5.390 sec/step, loss=0.07628, avg_loss=0.07549]\n",
      "Step 500845  [5.401 sec/step, loss=0.07540, avg_loss=0.07551]\n",
      "Step 500846  [5.387 sec/step, loss=0.06746, avg_loss=0.07545]\n",
      "Step 500847  [5.413 sec/step, loss=0.07708, avg_loss=0.07546]\n",
      "Generated 32 batches of size 32 in 2.400 sec\n",
      "Step 500848  [5.414 sec/step, loss=0.07679, avg_loss=0.07545]\n",
      "Step 500849  [5.410 sec/step, loss=0.07687, avg_loss=0.07546]\n",
      "Step 500850  [5.407 sec/step, loss=0.07749, avg_loss=0.07549]\n",
      "Step 500851  [5.430 sec/step, loss=0.07821, avg_loss=0.07553]\n",
      "Step 500852  [5.371 sec/step, loss=0.07592, avg_loss=0.07559]\n",
      "Step 500853  [5.338 sec/step, loss=0.07311, avg_loss=0.07557]\n",
      "Step 500854  [5.345 sec/step, loss=0.07690, avg_loss=0.07557]\n",
      "Step 500855  [5.340 sec/step, loss=0.07642, avg_loss=0.07557]\n",
      "Step 500856  [5.353 sec/step, loss=0.07707, avg_loss=0.07565]\n",
      "Step 500857  [5.351 sec/step, loss=0.07555, avg_loss=0.07565]\n",
      "Step 500858  [5.360 sec/step, loss=0.07534, avg_loss=0.07563]\n",
      "Step 500859  [5.371 sec/step, loss=0.07778, avg_loss=0.07566]\n",
      "Step 500860  [5.347 sec/step, loss=0.07496, avg_loss=0.07563]\n",
      "Step 500861  [5.360 sec/step, loss=0.07764, avg_loss=0.07565]\n",
      "Step 500862  [5.368 sec/step, loss=0.07761, avg_loss=0.07565]\n",
      "Step 500863  [5.379 sec/step, loss=0.07349, avg_loss=0.07563]\n",
      "Step 500864  [5.408 sec/step, loss=0.07082, avg_loss=0.07558]\n",
      "Step 500865  [5.401 sec/step, loss=0.07209, avg_loss=0.07553]\n",
      "Step 500866  [5.401 sec/step, loss=0.07683, avg_loss=0.07554]\n",
      "Step 500867  [5.406 sec/step, loss=0.07568, avg_loss=0.07556]\n",
      "Step 500868  [5.390 sec/step, loss=0.07549, avg_loss=0.07555]\n",
      "Step 500869  [5.378 sec/step, loss=0.07680, avg_loss=0.07553]\n",
      "Step 500870  [5.358 sec/step, loss=0.07619, avg_loss=0.07553]\n",
      "Step 500871  [5.357 sec/step, loss=0.07585, avg_loss=0.07557]\n",
      "Step 500872  [5.377 sec/step, loss=0.07709, avg_loss=0.07567]\n",
      "Step 500873  [5.380 sec/step, loss=0.07426, avg_loss=0.07568]\n",
      "Step 500874  [5.380 sec/step, loss=0.07594, avg_loss=0.07568]\n",
      "Step 500875  [5.381 sec/step, loss=0.07455, avg_loss=0.07567]\n",
      "Step 500876  [5.402 sec/step, loss=0.07727, avg_loss=0.07571]\n",
      "Step 500877  [5.413 sec/step, loss=0.07756, avg_loss=0.07573]\n",
      "Step 500878  [5.390 sec/step, loss=0.06792, avg_loss=0.07563]\n",
      "Step 500879  [5.348 sec/step, loss=0.07791, avg_loss=0.07574]\n",
      "Generated 32 batches of size 32 in 2.404 sec\n",
      "Step 500880  [5.373 sec/step, loss=0.07495, avg_loss=0.07571]\n",
      "Step 500881  [5.377 sec/step, loss=0.07644, avg_loss=0.07571]\n",
      "Step 500882  [5.362 sec/step, loss=0.07767, avg_loss=0.07572]\n",
      "Step 500883  [5.378 sec/step, loss=0.07818, avg_loss=0.07573]\n",
      "Step 500884  [5.393 sec/step, loss=0.07784, avg_loss=0.07575]\n",
      "Step 500885  [5.397 sec/step, loss=0.07708, avg_loss=0.07579]\n",
      "Step 500886  [5.385 sec/step, loss=0.07374, avg_loss=0.07577]\n",
      "Step 500887  [5.386 sec/step, loss=0.07583, avg_loss=0.07577]\n",
      "Step 500888  [5.385 sec/step, loss=0.07739, avg_loss=0.07579]\n",
      "Step 500889  [5.378 sec/step, loss=0.07711, avg_loss=0.07581]\n",
      "Step 500890  [5.378 sec/step, loss=0.07695, avg_loss=0.07583]\n",
      "Step 500891  [5.374 sec/step, loss=0.07717, avg_loss=0.07583]\n",
      "Step 500892  [5.382 sec/step, loss=0.07659, avg_loss=0.07581]\n",
      "Step 500893  [5.395 sec/step, loss=0.07691, avg_loss=0.07580]\n",
      "Step 500894  [5.370 sec/step, loss=0.07772, avg_loss=0.07583]\n",
      "Step 500895  [5.371 sec/step, loss=0.07254, avg_loss=0.07578]\n",
      "Step 500896  [5.367 sec/step, loss=0.07640, avg_loss=0.07576]\n",
      "Step 500897  [5.394 sec/step, loss=0.07430, avg_loss=0.07574]\n",
      "Step 500898  [5.391 sec/step, loss=0.07555, avg_loss=0.07575]\n",
      "Step 500899  [5.409 sec/step, loss=0.07734, avg_loss=0.07576]\n",
      "Step 500900  [5.399 sec/step, loss=0.07603, avg_loss=0.07574]\n",
      "Writing summary at step: 500900\n",
      "Step 500901  [5.448 sec/step, loss=0.06782, avg_loss=0.07565]\n",
      "Step 500902  [5.430 sec/step, loss=0.07234, avg_loss=0.07563]\n",
      "Step 500903  [5.442 sec/step, loss=0.07832, avg_loss=0.07565]\n",
      "Step 500904  [5.454 sec/step, loss=0.07541, avg_loss=0.07573]\n",
      "Step 500905  [5.446 sec/step, loss=0.07605, avg_loss=0.07573]\n",
      "Step 500906  [5.445 sec/step, loss=0.07735, avg_loss=0.07577]\n",
      "Step 500907  [5.455 sec/step, loss=0.07806, avg_loss=0.07578]\n",
      "Step 500908  [5.451 sec/step, loss=0.07511, avg_loss=0.07575]\n",
      "Step 500909  [5.435 sec/step, loss=0.07428, avg_loss=0.07573]\n",
      "Step 500910  [5.442 sec/step, loss=0.07588, avg_loss=0.07573]\n",
      "Generated 32 batches of size 32 in 2.473 sec\n",
      "Step 500911  [5.449 sec/step, loss=0.07487, avg_loss=0.07574]\n",
      "Step 500912  [5.444 sec/step, loss=0.07645, avg_loss=0.07574]\n",
      "Step 500913  [5.438 sec/step, loss=0.07576, avg_loss=0.07572]\n",
      "Step 500914  [5.427 sec/step, loss=0.07722, avg_loss=0.07571]\n",
      "Step 500915  [5.405 sec/step, loss=0.06712, avg_loss=0.07562]\n",
      "Step 500916  [5.425 sec/step, loss=0.07568, avg_loss=0.07564]\n",
      "Step 500917  [5.429 sec/step, loss=0.07830, avg_loss=0.07565]\n",
      "Step 500918  [5.423 sec/step, loss=0.07158, avg_loss=0.07563]\n",
      "Step 500919  [5.426 sec/step, loss=0.07760, avg_loss=0.07562]\n",
      "Step 500920  [5.418 sec/step, loss=0.07551, avg_loss=0.07562]\n",
      "Step 500921  [5.378 sec/step, loss=0.07782, avg_loss=0.07572]\n",
      "Step 500922  [5.375 sec/step, loss=0.07358, avg_loss=0.07568]\n",
      "Step 500923  [5.359 sec/step, loss=0.07368, avg_loss=0.07567]\n",
      "Step 500924  [5.389 sec/step, loss=0.07449, avg_loss=0.07566]\n",
      "Step 500925  [5.411 sec/step, loss=0.07724, avg_loss=0.07567]\n",
      "Step 500926  [5.407 sec/step, loss=0.07718, avg_loss=0.07567]\n",
      "Step 500927  [5.400 sec/step, loss=0.07753, avg_loss=0.07568]\n",
      "Step 500928  [5.399 sec/step, loss=0.07431, avg_loss=0.07565]\n",
      "Step 500929  [5.376 sec/step, loss=0.07734, avg_loss=0.07568]\n",
      "Step 500930  [5.386 sec/step, loss=0.07608, avg_loss=0.07571]\n",
      "Step 500931  [5.360 sec/step, loss=0.06700, avg_loss=0.07560]\n",
      "Step 500932  [5.410 sec/step, loss=0.06760, avg_loss=0.07550]\n",
      "Step 500933  [5.405 sec/step, loss=0.07243, avg_loss=0.07548]\n",
      "Step 500934  [5.386 sec/step, loss=0.07600, avg_loss=0.07549]\n",
      "Step 500935  [5.352 sec/step, loss=0.07655, avg_loss=0.07557]\n",
      "Step 500936  [5.348 sec/step, loss=0.07647, avg_loss=0.07555]\n",
      "Step 500937  [5.339 sec/step, loss=0.07725, avg_loss=0.07554]\n",
      "Step 500938  [5.339 sec/step, loss=0.07626, avg_loss=0.07555]\n",
      "Step 500939  [5.346 sec/step, loss=0.07667, avg_loss=0.07554]\n",
      "Step 500940  [5.333 sec/step, loss=0.07564, avg_loss=0.07553]\n",
      "Step 500941  [5.345 sec/step, loss=0.07705, avg_loss=0.07554]\n",
      "Step 500942  [5.341 sec/step, loss=0.07463, avg_loss=0.07554]\n",
      "Generated 32 batches of size 32 in 2.390 sec\n",
      "Step 500943  [5.346 sec/step, loss=0.07728, avg_loss=0.07554]\n",
      "Step 500944  [5.354 sec/step, loss=0.07783, avg_loss=0.07556]\n",
      "Step 500945  [5.353 sec/step, loss=0.07394, avg_loss=0.07554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500946  [5.367 sec/step, loss=0.07684, avg_loss=0.07564]\n",
      "Step 500947  [5.354 sec/step, loss=0.07718, avg_loss=0.07564]\n",
      "Step 500948  [5.353 sec/step, loss=0.07780, avg_loss=0.07565]\n",
      "Step 500949  [5.361 sec/step, loss=0.07751, avg_loss=0.07565]\n",
      "Step 500950  [5.355 sec/step, loss=0.07565, avg_loss=0.07564]\n",
      "Step 500951  [5.354 sec/step, loss=0.07763, avg_loss=0.07563]\n",
      "Step 500952  [5.360 sec/step, loss=0.07492, avg_loss=0.07562]\n",
      "Step 500953  [5.357 sec/step, loss=0.06718, avg_loss=0.07556]\n",
      "Step 500954  [5.358 sec/step, loss=0.07642, avg_loss=0.07556]\n",
      "Step 500955  [5.334 sec/step, loss=0.07350, avg_loss=0.07553]\n",
      "Step 500956  [5.331 sec/step, loss=0.07605, avg_loss=0.07552]\n",
      "Step 500957  [5.358 sec/step, loss=0.07544, avg_loss=0.07551]\n",
      "Step 500958  [5.345 sec/step, loss=0.07463, avg_loss=0.07551]\n",
      "Step 500959  [5.329 sec/step, loss=0.07308, avg_loss=0.07546]\n",
      "Step 500960  [5.345 sec/step, loss=0.07626, avg_loss=0.07547]\n",
      "Step 500961  [5.340 sec/step, loss=0.07664, avg_loss=0.07546]\n",
      "Step 500962  [5.332 sec/step, loss=0.07562, avg_loss=0.07544]\n",
      "Step 500963  [5.330 sec/step, loss=0.07747, avg_loss=0.07548]\n",
      "Step 500964  [5.301 sec/step, loss=0.07797, avg_loss=0.07556]\n",
      "Step 500965  [5.308 sec/step, loss=0.07625, avg_loss=0.07560]\n",
      "Step 500966  [5.299 sec/step, loss=0.07590, avg_loss=0.07559]\n",
      "Step 500967  [5.298 sec/step, loss=0.07812, avg_loss=0.07561]\n",
      "Step 500968  [5.296 sec/step, loss=0.07721, avg_loss=0.07563]\n",
      "Step 500969  [5.296 sec/step, loss=0.07694, avg_loss=0.07563]\n",
      "Step 500970  [5.287 sec/step, loss=0.07715, avg_loss=0.07564]\n",
      "Step 500971  [5.301 sec/step, loss=0.07745, avg_loss=0.07566]\n",
      "Step 500972  [5.292 sec/step, loss=0.07509, avg_loss=0.07564]\n",
      "Step 500973  [5.314 sec/step, loss=0.07545, avg_loss=0.07565]\n",
      "Step 500974  [5.334 sec/step, loss=0.07831, avg_loss=0.07567]\n",
      "Generated 32 batches of size 32 in 2.692 sec\n",
      "Step 500975  [5.338 sec/step, loss=0.07388, avg_loss=0.07566]\n",
      "Step 500976  [5.340 sec/step, loss=0.07787, avg_loss=0.07567]\n",
      "Step 500977  [5.341 sec/step, loss=0.07770, avg_loss=0.07567]\n",
      "Step 500978  [5.369 sec/step, loss=0.07634, avg_loss=0.07576]\n",
      "Step 500979  [5.366 sec/step, loss=0.07767, avg_loss=0.07575]\n",
      "Step 500980  [5.331 sec/step, loss=0.07360, avg_loss=0.07574]\n",
      "Step 500981  [5.326 sec/step, loss=0.07674, avg_loss=0.07574]\n",
      "Step 500982  [5.333 sec/step, loss=0.07759, avg_loss=0.07574]\n",
      "Step 500983  [5.365 sec/step, loss=0.06796, avg_loss=0.07564]\n",
      "Step 500984  [5.401 sec/step, loss=0.06807, avg_loss=0.07554]\n",
      "Step 500985  [5.393 sec/step, loss=0.07350, avg_loss=0.07551]\n",
      "Step 500986  [5.385 sec/step, loss=0.07598, avg_loss=0.07553]\n",
      "Step 500987  [5.405 sec/step, loss=0.07732, avg_loss=0.07554]\n",
      "Step 500988  [5.409 sec/step, loss=0.07639, avg_loss=0.07553]\n",
      "Step 500989  [5.417 sec/step, loss=0.07722, avg_loss=0.07554]\n",
      "Step 500990  [5.414 sec/step, loss=0.07285, avg_loss=0.07549]\n",
      "Step 500991  [5.414 sec/step, loss=0.07601, avg_loss=0.07548]\n",
      "Step 500992  [5.390 sec/step, loss=0.07625, avg_loss=0.07548]\n",
      "Step 500993  [5.377 sec/step, loss=0.07489, avg_loss=0.07546]\n",
      "Step 500994  [5.373 sec/step, loss=0.07739, avg_loss=0.07546]\n",
      "Step 500995  [5.376 sec/step, loss=0.07675, avg_loss=0.07550]\n",
      "Step 500996  [5.356 sec/step, loss=0.06789, avg_loss=0.07541]\n",
      "Step 500997  [5.356 sec/step, loss=0.07513, avg_loss=0.07542]\n",
      "Step 500998  [5.358 sec/step, loss=0.07547, avg_loss=0.07542]\n",
      "Step 500999  [5.346 sec/step, loss=0.07430, avg_loss=0.07539]\n",
      "Step 501000  [5.358 sec/step, loss=0.07767, avg_loss=0.07541]\n",
      "Writing summary at step: 501000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-501000\n",
      "Saving audio and alignment...\n",
      "/home/itu/Desktop/UrduTTSNew/UrduTTS/tvenv/lib/python3.6/site-packages/librosa/util/utils.py:1725: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if np.issubdtype(x.dtype, float) or np.issubdtype(x.dtype, complex):\n",
      "Input: ayshjaaii zon kay diifans vaalay allaaqoon kii viiman dzazbaatd miin mahvijjatd pir naazaan or ggaraaray lagaanay vaalii hiin~________\n",
      "Step 501001  [5.307 sec/step, loss=0.07634, avg_loss=0.07549]\n",
      "Step 501002  [5.328 sec/step, loss=0.07760, avg_loss=0.07554]\n",
      "Step 501003  [5.317 sec/step, loss=0.07524, avg_loss=0.07551]\n",
      "Step 501004  [5.333 sec/step, loss=0.07801, avg_loss=0.07554]\n",
      "Generated 32 batches of size 32 in 2.356 sec\n",
      "Step 501005  [5.355 sec/step, loss=0.07813, avg_loss=0.07556]\n",
      "Step 501006  [5.351 sec/step, loss=0.07513, avg_loss=0.07554]\n",
      "Step 501007  [5.347 sec/step, loss=0.07632, avg_loss=0.07552]\n",
      "Step 501008  [5.338 sec/step, loss=0.07757, avg_loss=0.07555]\n",
      "Step 501009  [5.330 sec/step, loss=0.07343, avg_loss=0.07554]\n",
      "Step 501010  [5.338 sec/step, loss=0.07735, avg_loss=0.07555]\n",
      "Step 501011  [5.345 sec/step, loss=0.07396, avg_loss=0.07554]\n",
      "Step 501012  [5.352 sec/step, loss=0.07825, avg_loss=0.07556]\n",
      "Step 501013  [5.353 sec/step, loss=0.07462, avg_loss=0.07555]\n",
      "Step 501014  [5.377 sec/step, loss=0.07500, avg_loss=0.07553]\n",
      "Step 501015  [5.387 sec/step, loss=0.07437, avg_loss=0.07560]\n",
      "Step 501016  [5.372 sec/step, loss=0.07693, avg_loss=0.07561]\n",
      "Step 501017  [5.367 sec/step, loss=0.07624, avg_loss=0.07559]\n",
      "Step 501018  [5.387 sec/step, loss=0.07528, avg_loss=0.07563]\n",
      "Step 501019  [5.371 sec/step, loss=0.07558, avg_loss=0.07561]\n",
      "Step 501020  [5.410 sec/step, loss=0.06786, avg_loss=0.07553]\n",
      "Step 501021  [5.399 sec/step, loss=0.07755, avg_loss=0.07553]\n",
      "Step 501022  [5.406 sec/step, loss=0.07719, avg_loss=0.07557]\n",
      "Step 501023  [5.415 sec/step, loss=0.07515, avg_loss=0.07558]\n",
      "Step 501024  [5.388 sec/step, loss=0.07674, avg_loss=0.07560]\n",
      "Step 501025  [5.381 sec/step, loss=0.07814, avg_loss=0.07561]\n",
      "Step 501026  [5.362 sec/step, loss=0.07601, avg_loss=0.07560]\n",
      "Step 501027  [5.373 sec/step, loss=0.07795, avg_loss=0.07560]\n",
      "Step 501028  [5.381 sec/step, loss=0.07715, avg_loss=0.07563]\n",
      "Step 501029  [5.380 sec/step, loss=0.07618, avg_loss=0.07562]\n",
      "Step 501030  [5.381 sec/step, loss=0.07674, avg_loss=0.07563]\n",
      "Step 501031  [5.381 sec/step, loss=0.06813, avg_loss=0.07564]\n",
      "Step 501032  [5.325 sec/step, loss=0.07337, avg_loss=0.07570]\n",
      "Step 501033  [5.354 sec/step, loss=0.07499, avg_loss=0.07572]\n",
      "Step 501034  [5.361 sec/step, loss=0.07353, avg_loss=0.07570]\n",
      "Step 501035  [5.338 sec/step, loss=0.07579, avg_loss=0.07569]\n",
      "Step 501036  [5.337 sec/step, loss=0.07568, avg_loss=0.07568]\n",
      "Generated 32 batches of size 32 in 2.722 sec\n",
      "Step 501037  [5.325 sec/step, loss=0.07377, avg_loss=0.07565]\n",
      "Step 501038  [5.322 sec/step, loss=0.07660, avg_loss=0.07565]\n",
      "Step 501039  [5.320 sec/step, loss=0.07811, avg_loss=0.07566]\n",
      "Step 501040  [5.340 sec/step, loss=0.07736, avg_loss=0.07568]\n",
      "Step 501041  [5.334 sec/step, loss=0.07622, avg_loss=0.07567]\n",
      "Step 501042  [5.353 sec/step, loss=0.07722, avg_loss=0.07570]\n",
      "Step 501043  [5.324 sec/step, loss=0.07176, avg_loss=0.07564]\n",
      "Step 501044  [5.334 sec/step, loss=0.07767, avg_loss=0.07564]\n",
      "Step 501045  [5.340 sec/step, loss=0.07644, avg_loss=0.07567]\n",
      "Step 501046  [5.336 sec/step, loss=0.07386, avg_loss=0.07564]\n",
      "Step 501047  [5.336 sec/step, loss=0.07615, avg_loss=0.07563]\n",
      "Step 501048  [5.332 sec/step, loss=0.07605, avg_loss=0.07561]\n",
      "Step 501049  [5.311 sec/step, loss=0.07210, avg_loss=0.07556]\n",
      "Step 501050  [5.323 sec/step, loss=0.07768, avg_loss=0.07558]\n",
      "Step 501051  [5.315 sec/step, loss=0.07729, avg_loss=0.07557]\n",
      "Step 501052  [5.318 sec/step, loss=0.07647, avg_loss=0.07559]\n",
      "Step 501053  [5.342 sec/step, loss=0.07774, avg_loss=0.07569]\n",
      "Step 501054  [5.321 sec/step, loss=0.06549, avg_loss=0.07558]\n",
      "Step 501055  [5.335 sec/step, loss=0.07776, avg_loss=0.07563]\n",
      "Step 501056  [5.366 sec/step, loss=0.07638, avg_loss=0.07563]\n",
      "Step 501057  [5.355 sec/step, loss=0.07736, avg_loss=0.07565]\n",
      "Step 501058  [5.364 sec/step, loss=0.07795, avg_loss=0.07568]\n",
      "Step 501059  [5.361 sec/step, loss=0.07237, avg_loss=0.07568]\n",
      "Step 501060  [5.351 sec/step, loss=0.07566, avg_loss=0.07567]\n",
      "Step 501061  [5.350 sec/step, loss=0.07643, avg_loss=0.07567]\n",
      "Step 501062  [5.335 sec/step, loss=0.07612, avg_loss=0.07567]\n",
      "Step 501063  [5.346 sec/step, loss=0.07672, avg_loss=0.07567]\n",
      "Step 501064  [5.383 sec/step, loss=0.06640, avg_loss=0.07555]\n",
      "Step 501065  [5.377 sec/step, loss=0.07607, avg_loss=0.07555]\n",
      "Step 501066  [5.377 sec/step, loss=0.07720, avg_loss=0.07556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 501067  [5.364 sec/step, loss=0.07715, avg_loss=0.07555]\n",
      "Step 501068  [5.384 sec/step, loss=0.07718, avg_loss=0.07555]\n",
      "Generated 32 batches of size 32 in 2.405 sec\n",
      "Step 501069  [5.389 sec/step, loss=0.07682, avg_loss=0.07555]\n",
      "Step 501070  [5.393 sec/step, loss=0.07618, avg_loss=0.07554]\n",
      "Step 501071  [5.393 sec/step, loss=0.07671, avg_loss=0.07553]\n",
      "Step 501072  [5.406 sec/step, loss=0.07697, avg_loss=0.07555]\n",
      "Step 501073  [5.393 sec/step, loss=0.07781, avg_loss=0.07558]\n",
      "Step 501074  [5.379 sec/step, loss=0.07598, avg_loss=0.07555]\n",
      "Step 501075  [5.396 sec/step, loss=0.07774, avg_loss=0.07559]\n",
      "Step 501076  [5.385 sec/step, loss=0.07351, avg_loss=0.07555]\n",
      "Step 501077  [5.373 sec/step, loss=0.07525, avg_loss=0.07552]\n",
      "Step 501078  [5.356 sec/step, loss=0.07522, avg_loss=0.07551]\n",
      "Step 501079  [5.348 sec/step, loss=0.07655, avg_loss=0.07550]\n",
      "Step 501080  [5.343 sec/step, loss=0.07206, avg_loss=0.07548]\n",
      "Step 501081  [5.344 sec/step, loss=0.07770, avg_loss=0.07549]\n",
      "Step 501082  [5.346 sec/step, loss=0.07822, avg_loss=0.07550]\n",
      "Step 501083  [5.292 sec/step, loss=0.07476, avg_loss=0.07557]\n",
      "Step 501084  [5.239 sec/step, loss=0.07348, avg_loss=0.07562]\n",
      "Step 501085  [5.242 sec/step, loss=0.07580, avg_loss=0.07565]\n",
      "Step 501086  [5.274 sec/step, loss=0.07596, avg_loss=0.07565]\n",
      "Step 501087  [5.256 sec/step, loss=0.07543, avg_loss=0.07563]\n",
      "Step 501088  [5.258 sec/step, loss=0.07442, avg_loss=0.07561]\n",
      "Step 501089  [5.252 sec/step, loss=0.07670, avg_loss=0.07560]\n",
      "Step 501090  [5.304 sec/step, loss=0.06880, avg_loss=0.07556]\n",
      "Step 501091  [5.295 sec/step, loss=0.07696, avg_loss=0.07557]\n",
      "Step 501092  [5.307 sec/step, loss=0.07500, avg_loss=0.07556]\n",
      "Step 501093  [5.309 sec/step, loss=0.07640, avg_loss=0.07557]\n",
      "Step 501094  [5.319 sec/step, loss=0.07759, avg_loss=0.07557]\n",
      "Step 501095  [5.302 sec/step, loss=0.07051, avg_loss=0.07551]\n",
      "Step 501096  [5.325 sec/step, loss=0.07817, avg_loss=0.07562]\n",
      "Step 501097  [5.291 sec/step, loss=0.07619, avg_loss=0.07563]\n",
      "Step 501098  [5.281 sec/step, loss=0.07433, avg_loss=0.07561]\n",
      "Step 501099  [5.289 sec/step, loss=0.07682, avg_loss=0.07564]\n",
      "Step 501100  [5.299 sec/step, loss=0.07498, avg_loss=0.07561]\n",
      "Writing summary at step: 501100\n",
      "Generated 32 batches of size 32 in 2.306 sec\n",
      "Step 501101  [5.309 sec/step, loss=0.07818, avg_loss=0.07563]\n",
      "Step 501102  [5.312 sec/step, loss=0.07655, avg_loss=0.07562]\n",
      "Step 501103  [5.329 sec/step, loss=0.07590, avg_loss=0.07563]\n",
      "Step 501104  [5.318 sec/step, loss=0.07709, avg_loss=0.07562]\n",
      "Step 501105  [5.300 sec/step, loss=0.07516, avg_loss=0.07559]\n",
      "Step 501106  [5.297 sec/step, loss=0.07374, avg_loss=0.07557]\n",
      "Step 501107  [5.297 sec/step, loss=0.07803, avg_loss=0.07559]\n",
      "Step 501108  [5.305 sec/step, loss=0.07776, avg_loss=0.07559]\n",
      "Step 501109  [5.322 sec/step, loss=0.07652, avg_loss=0.07562]\n",
      "Step 501110  [5.307 sec/step, loss=0.07647, avg_loss=0.07562]\n",
      "Step 501111  [5.314 sec/step, loss=0.07531, avg_loss=0.07563]\n",
      "Step 501112  [5.311 sec/step, loss=0.07786, avg_loss=0.07563]\n",
      "Step 501113  [5.355 sec/step, loss=0.06777, avg_loss=0.07556]\n",
      "Step 501114  [5.313 sec/step, loss=0.06664, avg_loss=0.07547]\n",
      "Step 501115  [5.332 sec/step, loss=0.07560, avg_loss=0.07549]\n",
      "Step 501116  [5.341 sec/step, loss=0.07658, avg_loss=0.07548]\n",
      "Step 501117  [5.332 sec/step, loss=0.07291, avg_loss=0.07545]\n",
      "Step 501118  [5.308 sec/step, loss=0.07349, avg_loss=0.07543]\n",
      "Step 501119  [5.318 sec/step, loss=0.07578, avg_loss=0.07543]\n",
      "Step 501120  [5.264 sec/step, loss=0.07710, avg_loss=0.07553]\n",
      "Step 501121  [5.254 sec/step, loss=0.07290, avg_loss=0.07548]\n",
      "Step 501122  [5.247 sec/step, loss=0.07744, avg_loss=0.07548]\n",
      "Step 501123  [5.256 sec/step, loss=0.07794, avg_loss=0.07551]\n",
      "Step 501124  [5.253 sec/step, loss=0.07724, avg_loss=0.07551]\n",
      "Step 501125  [5.243 sec/step, loss=0.07611, avg_loss=0.07549]\n",
      "Step 501126  [5.259 sec/step, loss=0.07718, avg_loss=0.07551]\n",
      "Step 501127  [5.250 sec/step, loss=0.07485, avg_loss=0.07547]\n",
      "Step 501128  [5.262 sec/step, loss=0.07793, avg_loss=0.07548]\n",
      "Step 501129  [5.260 sec/step, loss=0.07651, avg_loss=0.07549]\n",
      "Step 501130  [5.254 sec/step, loss=0.07490, avg_loss=0.07547]\n",
      "Step 501131  [5.259 sec/step, loss=0.07586, avg_loss=0.07554]\n",
      "Generated 32 batches of size 32 in 2.321 sec\n",
      "Step 501132  [5.284 sec/step, loss=0.07764, avg_loss=0.07559]\n",
      "Step 501133  [5.257 sec/step, loss=0.07451, avg_loss=0.07558]\n",
      "Step 501134  [5.283 sec/step, loss=0.07611, avg_loss=0.07561]\n",
      "Step 501135  [5.302 sec/step, loss=0.07767, avg_loss=0.07563]\n",
      "Step 501136  [5.303 sec/step, loss=0.07686, avg_loss=0.07564]\n",
      "Step 501137  [5.311 sec/step, loss=0.07613, avg_loss=0.07566]\n",
      "Step 501138  [5.310 sec/step, loss=0.07551, avg_loss=0.07565]\n",
      "Step 501139  [5.304 sec/step, loss=0.07758, avg_loss=0.07565]\n",
      "Step 501140  [5.302 sec/step, loss=0.07828, avg_loss=0.07566]\n",
      "Step 501141  [5.295 sec/step, loss=0.07568, avg_loss=0.07565]\n",
      "Step 501142  [5.286 sec/step, loss=0.07636, avg_loss=0.07564]\n",
      "Step 501143  [5.306 sec/step, loss=0.07508, avg_loss=0.07567]\n",
      "Step 501144  [5.288 sec/step, loss=0.07610, avg_loss=0.07566]\n",
      "Step 501145  [5.300 sec/step, loss=0.07653, avg_loss=0.07566]\n",
      "Step 501146  [5.312 sec/step, loss=0.07804, avg_loss=0.07570]\n",
      "Step 501147  [5.302 sec/step, loss=0.07542, avg_loss=0.07569]\n",
      "Step 501148  [5.299 sec/step, loss=0.07793, avg_loss=0.07571]\n",
      "Step 501149  [5.310 sec/step, loss=0.07725, avg_loss=0.07576]\n",
      "Step 501150  [5.328 sec/step, loss=0.07432, avg_loss=0.07573]\n",
      "Step 501151  [5.327 sec/step, loss=0.07698, avg_loss=0.07573]\n",
      "Step 501152  [5.327 sec/step, loss=0.07597, avg_loss=0.07572]\n",
      "Step 501153  [5.316 sec/step, loss=0.07714, avg_loss=0.07572]\n",
      "Step 501154  [5.340 sec/step, loss=0.07879, avg_loss=0.07585]\n",
      "Step 501155  [5.329 sec/step, loss=0.07380, avg_loss=0.07581]\n",
      "Step 501156  [5.313 sec/step, loss=0.07753, avg_loss=0.07582]\n",
      "Step 501157  [5.287 sec/step, loss=0.07290, avg_loss=0.07578]\n",
      "Step 501158  [5.286 sec/step, loss=0.07667, avg_loss=0.07576]\n",
      "Step 501159  [5.309 sec/step, loss=0.07797, avg_loss=0.07582]\n",
      "Step 501160  [5.321 sec/step, loss=0.07711, avg_loss=0.07583]\n",
      "Step 501161  [5.314 sec/step, loss=0.07649, avg_loss=0.07584]\n",
      "Step 501162  [5.329 sec/step, loss=0.07657, avg_loss=0.07584]\n",
      "Step 501163  [5.311 sec/step, loss=0.07307, avg_loss=0.07580]\n",
      "Generated 32 batches of size 32 in 2.366 sec\n",
      "Step 501164  [5.276 sec/step, loss=0.07819, avg_loss=0.07592]\n",
      "Step 501165  [5.282 sec/step, loss=0.07528, avg_loss=0.07591]\n",
      "Step 501166  [5.335 sec/step, loss=0.06773, avg_loss=0.07582]\n",
      "Step 501167  [5.323 sec/step, loss=0.06726, avg_loss=0.07572]\n",
      "Step 501168  [5.308 sec/step, loss=0.07670, avg_loss=0.07571]\n",
      "Step 501169  [5.317 sec/step, loss=0.07740, avg_loss=0.07572]\n",
      "Step 501170  [5.311 sec/step, loss=0.07475, avg_loss=0.07571]\n",
      "Step 501171  [5.293 sec/step, loss=0.07583, avg_loss=0.07570]\n",
      "Step 501172  [5.298 sec/step, loss=0.07809, avg_loss=0.07571]\n",
      "Step 501173  [5.313 sec/step, loss=0.07497, avg_loss=0.07568]\n",
      "Step 501174  [5.363 sec/step, loss=0.06779, avg_loss=0.07560]\n",
      "Step 501175  [5.338 sec/step, loss=0.07325, avg_loss=0.07555]\n",
      "Step 501176  [5.351 sec/step, loss=0.07728, avg_loss=0.07559]\n",
      "Step 501177  [5.348 sec/step, loss=0.07563, avg_loss=0.07559]\n",
      "Step 501178  [5.358 sec/step, loss=0.07605, avg_loss=0.07560]\n",
      "Step 501179  [5.374 sec/step, loss=0.07910, avg_loss=0.07563]\n",
      "Step 501180  [5.381 sec/step, loss=0.07604, avg_loss=0.07567]\n",
      "Step 501181  [5.390 sec/step, loss=0.07590, avg_loss=0.07565]\n",
      "Step 501182  [5.371 sec/step, loss=0.07473, avg_loss=0.07562]\n",
      "Step 501183  [5.379 sec/step, loss=0.07714, avg_loss=0.07564]\n",
      "Step 501184  [5.370 sec/step, loss=0.07297, avg_loss=0.07563]\n",
      "Step 501185  [5.382 sec/step, loss=0.07659, avg_loss=0.07564]\n",
      "Step 501186  [5.350 sec/step, loss=0.07599, avg_loss=0.07564]\n",
      "Step 501187  [5.346 sec/step, loss=0.07434, avg_loss=0.07563]\n",
      "Step 501188  [5.344 sec/step, loss=0.07695, avg_loss=0.07566]\n",
      "Step 501189  [5.350 sec/step, loss=0.07592, avg_loss=0.07565]\n",
      "Step 501190  [5.294 sec/step, loss=0.07716, avg_loss=0.07573]\n",
      "Step 501191  [5.295 sec/step, loss=0.07531, avg_loss=0.07572]\n",
      "Step 501192  [5.290 sec/step, loss=0.07792, avg_loss=0.07575]\n",
      "Step 501193  [5.282 sec/step, loss=0.07271, avg_loss=0.07571]\n",
      "Step 501194  [5.293 sec/step, loss=0.07566, avg_loss=0.07569]\n",
      "Step 501195  [5.309 sec/step, loss=0.07389, avg_loss=0.07572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 32 batches of size 32 in 2.425 sec\n",
      "Step 501196  [5.316 sec/step, loss=0.07594, avg_loss=0.07570]\n",
      "Step 501197  [5.337 sec/step, loss=0.07513, avg_loss=0.07569]\n",
      "Step 501198  [5.335 sec/step, loss=0.06718, avg_loss=0.07562]\n",
      "Step 501199  [5.342 sec/step, loss=0.07812, avg_loss=0.07563]\n",
      "Step 501200  [5.318 sec/step, loss=0.07681, avg_loss=0.07565]\n",
      "Writing summary at step: 501200\n",
      "Step 501201  [5.310 sec/step, loss=0.07695, avg_loss=0.07564]\n",
      "Step 501202  [5.299 sec/step, loss=0.07573, avg_loss=0.07563]\n",
      "Step 501203  [5.293 sec/step, loss=0.07751, avg_loss=0.07565]\n",
      "Step 501204  [5.288 sec/step, loss=0.07415, avg_loss=0.07562]\n",
      "Step 501205  [5.292 sec/step, loss=0.07616, avg_loss=0.07563]\n",
      "Step 501206  [5.290 sec/step, loss=0.07478, avg_loss=0.07564]\n",
      "Step 501207  [5.328 sec/step, loss=0.06777, avg_loss=0.07553]\n",
      "Step 501208  [5.323 sec/step, loss=0.07341, avg_loss=0.07549]\n",
      "Step 501209  [5.326 sec/step, loss=0.07748, avg_loss=0.07550]\n",
      "Step 501210  [5.329 sec/step, loss=0.07502, avg_loss=0.07549]\n",
      "Step 501211  [5.323 sec/step, loss=0.07743, avg_loss=0.07551]\n",
      "Step 501212  [5.322 sec/step, loss=0.07682, avg_loss=0.07550]\n",
      "Step 501213  [5.268 sec/step, loss=0.07703, avg_loss=0.07559]\n",
      "Step 501214  [5.278 sec/step, loss=0.07578, avg_loss=0.07568]\n",
      "Step 501215  [5.281 sec/step, loss=0.07861, avg_loss=0.07571]\n",
      "Step 501216  [5.265 sec/step, loss=0.07421, avg_loss=0.07569]\n",
      "Step 501217  [5.269 sec/step, loss=0.07725, avg_loss=0.07573]\n",
      "Step 501218  [5.277 sec/step, loss=0.07526, avg_loss=0.07575]\n",
      "Step 501219  [5.263 sec/step, loss=0.07251, avg_loss=0.07571]\n",
      "Step 501220  [5.294 sec/step, loss=0.07502, avg_loss=0.07569]\n",
      "Step 501221  [5.308 sec/step, loss=0.07619, avg_loss=0.07573]\n",
      "Step 501222  [5.324 sec/step, loss=0.07759, avg_loss=0.07573]\n",
      "Step 501223  [5.317 sec/step, loss=0.07641, avg_loss=0.07571]\n",
      "Step 501224  [5.313 sec/step, loss=0.07614, avg_loss=0.07570]\n",
      "Step 501225  [5.322 sec/step, loss=0.07616, avg_loss=0.07570]\n",
      "Step 501226  [5.316 sec/step, loss=0.07748, avg_loss=0.07571]\n",
      "Generated 32 batches of size 32 in 2.328 sec\n",
      "Step 501227  [5.330 sec/step, loss=0.07776, avg_loss=0.07573]\n",
      "Step 501228  [5.326 sec/step, loss=0.07534, avg_loss=0.07571]\n",
      "Step 501229  [5.311 sec/step, loss=0.06865, avg_loss=0.07563]\n",
      "Step 501230  [5.311 sec/step, loss=0.07704, avg_loss=0.07565]\n",
      "Step 501231  [5.328 sec/step, loss=0.07805, avg_loss=0.07567]\n",
      "Step 501232  [5.313 sec/step, loss=0.07706, avg_loss=0.07567]\n",
      "Step 501233  [5.334 sec/step, loss=0.07801, avg_loss=0.07570]\n",
      "Step 501234  [5.321 sec/step, loss=0.07763, avg_loss=0.07572]\n",
      "Step 501235  [5.310 sec/step, loss=0.07415, avg_loss=0.07568]\n",
      "Step 501236  [5.322 sec/step, loss=0.07658, avg_loss=0.07568]\n",
      "Step 501237  [5.324 sec/step, loss=0.07630, avg_loss=0.07568]\n",
      "Step 501238  [5.328 sec/step, loss=0.07654, avg_loss=0.07569]\n",
      "Step 501239  [5.335 sec/step, loss=0.07796, avg_loss=0.07570]\n",
      "Step 501240  [5.323 sec/step, loss=0.07556, avg_loss=0.07567]\n",
      "Step 501241  [5.341 sec/step, loss=0.07601, avg_loss=0.07567]\n",
      "Step 501242  [5.319 sec/step, loss=0.06605, avg_loss=0.07557]\n",
      "Step 501243  [5.306 sec/step, loss=0.07745, avg_loss=0.07559]\n",
      "Step 501244  [5.304 sec/step, loss=0.07419, avg_loss=0.07557]\n",
      "Step 501245  [5.287 sec/step, loss=0.07538, avg_loss=0.07556]\n",
      "Step 501246  [5.265 sec/step, loss=0.07216, avg_loss=0.07550]\n",
      "Step 501247  [5.276 sec/step, loss=0.07798, avg_loss=0.07553]\n",
      "Step 501248  [5.271 sec/step, loss=0.07234, avg_loss=0.07547]\n",
      "Step 501249  [5.283 sec/step, loss=0.07776, avg_loss=0.07548]\n",
      "Step 501250  [5.307 sec/step, loss=0.06837, avg_loss=0.07542]\n",
      "Step 501251  [5.310 sec/step, loss=0.07613, avg_loss=0.07541]\n",
      "Step 501252  [5.338 sec/step, loss=0.07489, avg_loss=0.07540]\n",
      "Step 501253  [5.345 sec/step, loss=0.07736, avg_loss=0.07540]\n",
      "Step 501254  [5.348 sec/step, loss=0.07778, avg_loss=0.07539]\n",
      "Step 501255  [5.362 sec/step, loss=0.07565, avg_loss=0.07541]\n",
      "Step 501256  [5.349 sec/step, loss=0.07574, avg_loss=0.07539]\n",
      "Step 501257  [5.353 sec/step, loss=0.07615, avg_loss=0.07542]\n",
      "Step 501258  [5.341 sec/step, loss=0.07693, avg_loss=0.07543]\n",
      "Generated 32 batches of size 32 in 2.499 sec\n",
      "Step 501259  [5.326 sec/step, loss=0.07551, avg_loss=0.07540]\n",
      "Step 501260  [5.337 sec/step, loss=0.07716, avg_loss=0.07540]\n",
      "Step 501261  [5.328 sec/step, loss=0.07158, avg_loss=0.07535]\n",
      "Step 501262  [5.332 sec/step, loss=0.07574, avg_loss=0.07535]\n",
      "Step 501263  [5.338 sec/step, loss=0.07720, avg_loss=0.07539]\n",
      "Step 501264  [5.329 sec/step, loss=0.07733, avg_loss=0.07538]\n",
      "Step 501265  [5.342 sec/step, loss=0.07793, avg_loss=0.07540]\n",
      "Step 501266  [5.293 sec/step, loss=0.07824, avg_loss=0.07551]\n",
      "Step 501267  [5.305 sec/step, loss=0.07469, avg_loss=0.07558]\n",
      "Step 501268  [5.294 sec/step, loss=0.07191, avg_loss=0.07554]\n",
      "Step 501269  [5.286 sec/step, loss=0.07729, avg_loss=0.07553]\n",
      "Step 501270  [5.284 sec/step, loss=0.07652, avg_loss=0.07555]\n",
      "Step 501271  [5.306 sec/step, loss=0.07725, avg_loss=0.07557]\n",
      "Step 501272  [5.310 sec/step, loss=0.07714, avg_loss=0.07556]\n",
      "Step 501273  [5.289 sec/step, loss=0.07634, avg_loss=0.07557]\n",
      "Step 501274  [5.236 sec/step, loss=0.07707, avg_loss=0.07566]\n",
      "Step 501275  [5.248 sec/step, loss=0.07705, avg_loss=0.07570]\n",
      "Step 501276  [5.234 sec/step, loss=0.07619, avg_loss=0.07569]\n",
      "Step 501277  [5.238 sec/step, loss=0.07720, avg_loss=0.07571]\n",
      "Step 501278  [5.242 sec/step, loss=0.07687, avg_loss=0.07571]\n",
      "Step 501279  [5.238 sec/step, loss=0.07585, avg_loss=0.07568]\n",
      "Step 501280  [5.243 sec/step, loss=0.07711, avg_loss=0.07569]\n",
      "Step 501281  [5.236 sec/step, loss=0.07692, avg_loss=0.07570]\n",
      "Step 501282  [5.245 sec/step, loss=0.07713, avg_loss=0.07573]\n",
      "Step 501283  [5.238 sec/step, loss=0.07561, avg_loss=0.07571]\n",
      "Step 501284  [5.258 sec/step, loss=0.07812, avg_loss=0.07576]\n",
      "Step 501285  [5.261 sec/step, loss=0.07839, avg_loss=0.07578]\n",
      "Step 501286  [5.272 sec/step, loss=0.07826, avg_loss=0.07580]\n",
      "Step 501287  [5.271 sec/step, loss=0.07564, avg_loss=0.07582]\n",
      "Step 501288  [5.255 sec/step, loss=0.06909, avg_loss=0.07574]\n",
      "Step 501289  [5.247 sec/step, loss=0.07390, avg_loss=0.07572]\n",
      "Step 501290  [5.267 sec/step, loss=0.07752, avg_loss=0.07572]\n",
      "Generated 32 batches of size 32 in 2.362 sec\n",
      "Step 501291  [5.278 sec/step, loss=0.07665, avg_loss=0.07574]\n",
      "Step 501292  [5.272 sec/step, loss=0.07477, avg_loss=0.07570]\n",
      "Step 501293  [5.326 sec/step, loss=0.06811, avg_loss=0.07566]\n",
      "Step 501294  [5.294 sec/step, loss=0.07464, avg_loss=0.07565]\n",
      "Step 501295  [5.320 sec/step, loss=0.07484, avg_loss=0.07566]\n",
      "Step 501296  [5.299 sec/step, loss=0.07564, avg_loss=0.07565]\n",
      "Step 501297  [5.275 sec/step, loss=0.07317, avg_loss=0.07563]\n",
      "Step 501298  [5.302 sec/step, loss=0.07832, avg_loss=0.07575]\n",
      "Step 501299  [5.299 sec/step, loss=0.07768, avg_loss=0.07574]\n",
      "Step 501300  [5.303 sec/step, loss=0.07735, avg_loss=0.07575]\n",
      "Writing summary at step: 501300\n",
      "Step 501301  [5.312 sec/step, loss=0.07709, avg_loss=0.07575]\n",
      "Step 501302  [5.305 sec/step, loss=0.07505, avg_loss=0.07574]\n",
      "Step 501303  [5.296 sec/step, loss=0.07366, avg_loss=0.07570]\n",
      "Step 501304  [5.301 sec/step, loss=0.07548, avg_loss=0.07572]\n",
      "Step 501305  [5.314 sec/step, loss=0.07826, avg_loss=0.07574]\n",
      "Step 501306  [5.329 sec/step, loss=0.07783, avg_loss=0.07577]\n",
      "Step 501307  [5.276 sec/step, loss=0.07486, avg_loss=0.07584]\n",
      "Step 501308  [5.272 sec/step, loss=0.07685, avg_loss=0.07587]\n",
      "Step 501309  [5.263 sec/step, loss=0.07708, avg_loss=0.07587]\n",
      "Step 501310  [5.264 sec/step, loss=0.07594, avg_loss=0.07588]\n",
      "Step 501311  [5.252 sec/step, loss=0.07642, avg_loss=0.07587]\n",
      "Step 501312  [5.254 sec/step, loss=0.07701, avg_loss=0.07587]\n",
      "Step 501313  [5.265 sec/step, loss=0.07848, avg_loss=0.07588]\n",
      "Step 501314  [5.262 sec/step, loss=0.07640, avg_loss=0.07589]\n",
      "Step 501315  [5.273 sec/step, loss=0.07564, avg_loss=0.07586]\n",
      "Step 501316  [5.302 sec/step, loss=0.07638, avg_loss=0.07588]\n",
      "Step 501317  [5.285 sec/step, loss=0.06631, avg_loss=0.07577]\n",
      "Step 501318  [5.277 sec/step, loss=0.07286, avg_loss=0.07575]\n",
      "Step 501319  [5.295 sec/step, loss=0.07538, avg_loss=0.07578]\n",
      "Step 501320  [5.264 sec/step, loss=0.07464, avg_loss=0.07577]\n",
      "Step 501321  [5.270 sec/step, loss=0.07859, avg_loss=0.07580]\n",
      "Generated 32 batches of size 32 in 2.626 sec\n",
      "Step 501322  [5.308 sec/step, loss=0.06909, avg_loss=0.07571]\n",
      "Step 501323  [5.310 sec/step, loss=0.07802, avg_loss=0.07573]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 501324  [5.327 sec/step, loss=0.07760, avg_loss=0.07574]\n",
      "Step 501325  [5.307 sec/step, loss=0.07268, avg_loss=0.07571]\n",
      "Step 501326  [5.311 sec/step, loss=0.07755, avg_loss=0.07571]\n",
      "Step 501327  [5.306 sec/step, loss=0.07825, avg_loss=0.07572]\n",
      "Step 501328  [5.297 sec/step, loss=0.07732, avg_loss=0.07573]\n",
      "Step 501329  [5.309 sec/step, loss=0.07696, avg_loss=0.07582]\n",
      "Step 501330  [5.328 sec/step, loss=0.07927, avg_loss=0.07584]\n",
      "Step 501331  [5.320 sec/step, loss=0.07258, avg_loss=0.07579]\n",
      "Step 501332  [5.331 sec/step, loss=0.07697, avg_loss=0.07578]\n",
      "Step 501333  [5.314 sec/step, loss=0.07729, avg_loss=0.07578]\n",
      "Step 501334  [5.309 sec/step, loss=0.07648, avg_loss=0.07577]\n",
      "Step 501335  [5.298 sec/step, loss=0.07610, avg_loss=0.07579]\n",
      "Step 501336  [5.278 sec/step, loss=0.07718, avg_loss=0.07579]\n",
      "Step 501337  [5.277 sec/step, loss=0.07623, avg_loss=0.07579]\n",
      "Step 501338  [5.290 sec/step, loss=0.07771, avg_loss=0.07580]\n",
      "Step 501339  [5.294 sec/step, loss=0.07783, avg_loss=0.07580]\n",
      "Step 501340  [5.291 sec/step, loss=0.07555, avg_loss=0.07580]\n",
      "Step 501341  [5.280 sec/step, loss=0.07683, avg_loss=0.07581]\n",
      "Step 501342  [5.306 sec/step, loss=0.07702, avg_loss=0.07592]\n",
      "Step 501343  [5.303 sec/step, loss=0.07546, avg_loss=0.07590]\n",
      "Step 501344  [5.330 sec/step, loss=0.07758, avg_loss=0.07593]\n",
      "Step 501345  [5.336 sec/step, loss=0.07715, avg_loss=0.07595]\n",
      "Step 501346  [5.334 sec/step, loss=0.06901, avg_loss=0.07592]\n",
      "Step 501347  [5.338 sec/step, loss=0.07848, avg_loss=0.07592]\n",
      "Step 501348  [5.336 sec/step, loss=0.07640, avg_loss=0.07596]\n",
      "Step 501349  [5.317 sec/step, loss=0.07355, avg_loss=0.07592]\n",
      "Step 501350  [5.261 sec/step, loss=0.07397, avg_loss=0.07598]\n",
      "Step 501351  [5.265 sec/step, loss=0.07760, avg_loss=0.07599]\n",
      "Step 501352  [5.239 sec/step, loss=0.07722, avg_loss=0.07602]\n",
      "Step 501353  [5.233 sec/step, loss=0.07478, avg_loss=0.07599]\n",
      "Generated 32 batches of size 32 in 2.299 sec\n",
      "Step 501354  [5.227 sec/step, loss=0.07640, avg_loss=0.07598]\n",
      "Step 501355  [5.209 sec/step, loss=0.07423, avg_loss=0.07596]\n",
      "Step 501356  [5.221 sec/step, loss=0.07678, avg_loss=0.07597]\n",
      "Step 501357  [5.282 sec/step, loss=0.06795, avg_loss=0.07589]\n",
      "Step 501358  [5.292 sec/step, loss=0.07807, avg_loss=0.07590]\n",
      "Step 501359  [5.299 sec/step, loss=0.07638, avg_loss=0.07591]\n",
      "Step 501360  [5.286 sec/step, loss=0.07745, avg_loss=0.07591]\n",
      "Step 501361  [5.322 sec/step, loss=0.07490, avg_loss=0.07595]\n",
      "Step 501362  [5.325 sec/step, loss=0.07571, avg_loss=0.07595]\n",
      "Step 501363  [5.326 sec/step, loss=0.07735, avg_loss=0.07595]\n",
      "Step 501364  [5.306 sec/step, loss=0.07234, avg_loss=0.07590]\n",
      "Step 501365  [5.298 sec/step, loss=0.07621, avg_loss=0.07588]\n",
      "Step 501366  [5.291 sec/step, loss=0.07548, avg_loss=0.07585]\n",
      "Step 501367  [5.280 sec/step, loss=0.06764, avg_loss=0.07578]\n",
      "Step 501368  [5.288 sec/step, loss=0.07592, avg_loss=0.07582]\n",
      "Step 501369  [5.288 sec/step, loss=0.07856, avg_loss=0.07584]\n",
      "Step 501370  [5.345 sec/step, loss=0.06659, avg_loss=0.07574]\n",
      "Step 501371  [5.343 sec/step, loss=0.07756, avg_loss=0.07574]\n",
      "Step 501372  [5.332 sec/step, loss=0.07730, avg_loss=0.07574]\n",
      "Step 501373  [5.335 sec/step, loss=0.07801, avg_loss=0.07576]\n",
      "Step 501374  [5.355 sec/step, loss=0.07703, avg_loss=0.07576]\n",
      "Step 501375  [5.368 sec/step, loss=0.07829, avg_loss=0.07577]\n",
      "Step 501376  [5.372 sec/step, loss=0.07664, avg_loss=0.07577]\n",
      "Step 501377  [5.399 sec/step, loss=0.07578, avg_loss=0.07576]\n",
      "Step 501378  [5.389 sec/step, loss=0.07672, avg_loss=0.07576]\n",
      "Step 501379  [5.390 sec/step, loss=0.07718, avg_loss=0.07577]\n",
      "Step 501380  [5.376 sec/step, loss=0.07617, avg_loss=0.07576]\n",
      "Step 501381  [5.372 sec/step, loss=0.07317, avg_loss=0.07573]\n",
      "Step 501382  [5.375 sec/step, loss=0.07613, avg_loss=0.07572]\n",
      "Step 501383  [5.371 sec/step, loss=0.07591, avg_loss=0.07572]\n",
      "Step 501384  [5.369 sec/step, loss=0.07780, avg_loss=0.07572]\n",
      "Step 501385  [5.366 sec/step, loss=0.07798, avg_loss=0.07571]\n",
      "Generated 32 batches of size 32 in 2.455 sec\n",
      "Step 501386  [5.368 sec/step, loss=0.07318, avg_loss=0.07566]\n",
      "Step 501387  [5.366 sec/step, loss=0.07470, avg_loss=0.07565]\n",
      "Step 501388  [5.397 sec/step, loss=0.07732, avg_loss=0.07573]\n",
      "Step 501389  [5.400 sec/step, loss=0.07630, avg_loss=0.07576]\n",
      "Step 501390  [5.391 sec/step, loss=0.07519, avg_loss=0.07573]\n",
      "Step 501391  [5.391 sec/step, loss=0.07504, avg_loss=0.07572]\n",
      "Step 501392  [5.398 sec/step, loss=0.07622, avg_loss=0.07573]\n",
      "Step 501393  [5.346 sec/step, loss=0.07719, avg_loss=0.07582]\n",
      "Step 501394  [5.348 sec/step, loss=0.07176, avg_loss=0.07579]\n",
      "Step 501395  [5.328 sec/step, loss=0.07451, avg_loss=0.07579]\n",
      "Step 501396  [5.339 sec/step, loss=0.07636, avg_loss=0.07580]\n",
      "Step 501397  [5.332 sec/step, loss=0.06735, avg_loss=0.07574]\n",
      "Step 501398  [5.317 sec/step, loss=0.07689, avg_loss=0.07573]\n",
      "Step 501399  [5.310 sec/step, loss=0.07495, avg_loss=0.07570]\n",
      "Step 501400  [5.305 sec/step, loss=0.07483, avg_loss=0.07567]\n",
      "Writing summary at step: 501400\n",
      "Step 501401  [5.286 sec/step, loss=0.07257, avg_loss=0.07563]\n",
      "Step 501402  [5.302 sec/step, loss=0.07752, avg_loss=0.07565]\n",
      "Step 501403  [5.299 sec/step, loss=0.07722, avg_loss=0.07569]\n",
      "Step 501404  [5.300 sec/step, loss=0.07714, avg_loss=0.07570]\n",
      "Step 501405  [5.283 sec/step, loss=0.07511, avg_loss=0.07567]\n",
      "Step 501406  [5.276 sec/step, loss=0.07212, avg_loss=0.07562]\n",
      "Step 501407  [5.269 sec/step, loss=0.07363, avg_loss=0.07560]\n",
      "Step 501408  [5.320 sec/step, loss=0.06834, avg_loss=0.07552]\n",
      "Step 501409  [5.320 sec/step, loss=0.07687, avg_loss=0.07552]\n",
      "Step 501410  [5.339 sec/step, loss=0.07775, avg_loss=0.07553]\n",
      "Step 501411  [5.348 sec/step, loss=0.07689, avg_loss=0.07554]\n",
      "Step 501412  [5.339 sec/step, loss=0.07377, avg_loss=0.07551]\n",
      "Step 501413  [5.345 sec/step, loss=0.07574, avg_loss=0.07548]\n",
      "Step 501414  [5.382 sec/step, loss=0.07542, avg_loss=0.07547]\n",
      "Step 501415  [5.350 sec/step, loss=0.07647, avg_loss=0.07548]\n",
      "Step 501416  [5.343 sec/step, loss=0.07794, avg_loss=0.07549]\n",
      "Generated 32 batches of size 32 in 2.409 sec\n",
      "Step 501417  [5.372 sec/step, loss=0.07767, avg_loss=0.07561]\n",
      "Step 501418  [5.396 sec/step, loss=0.07480, avg_loss=0.07563]\n",
      "Step 501419  [5.388 sec/step, loss=0.07631, avg_loss=0.07564]\n",
      "Step 501420  [5.402 sec/step, loss=0.07798, avg_loss=0.07567]\n",
      "Step 501421  [5.406 sec/step, loss=0.07813, avg_loss=0.07567]\n",
      "Step 501422  [5.348 sec/step, loss=0.07559, avg_loss=0.07573]\n",
      "Step 501423  [5.352 sec/step, loss=0.07689, avg_loss=0.07572]\n",
      "Step 501424  [5.362 sec/step, loss=0.07773, avg_loss=0.07572]\n",
      "Step 501425  [5.359 sec/step, loss=0.07393, avg_loss=0.07573]\n",
      "Step 501426  [5.344 sec/step, loss=0.07336, avg_loss=0.07569]\n",
      "Step 501427  [5.345 sec/step, loss=0.07787, avg_loss=0.07569]\n",
      "Step 501428  [5.336 sec/step, loss=0.07397, avg_loss=0.07565]\n",
      "Step 501429  [5.344 sec/step, loss=0.07660, avg_loss=0.07565]\n",
      "Step 501430  [5.333 sec/step, loss=0.07646, avg_loss=0.07562]\n",
      "Step 501431  [5.341 sec/step, loss=0.07655, avg_loss=0.07566]\n",
      "Step 501432  [5.337 sec/step, loss=0.07737, avg_loss=0.07567]\n",
      "Step 501433  [5.349 sec/step, loss=0.07664, avg_loss=0.07566]\n",
      "Step 501434  [5.345 sec/step, loss=0.07309, avg_loss=0.07562]\n",
      "Step 501435  [5.405 sec/step, loss=0.06700, avg_loss=0.07553]\n",
      "Step 501436  [5.421 sec/step, loss=0.07609, avg_loss=0.07552]\n",
      "Step 501437  [5.407 sec/step, loss=0.07373, avg_loss=0.07550]\n",
      "Step 501438  [5.396 sec/step, loss=0.07699, avg_loss=0.07549]\n",
      "Step 501439  [5.381 sec/step, loss=0.07700, avg_loss=0.07548]\n",
      "Step 501440  [5.386 sec/step, loss=0.07734, avg_loss=0.07550]\n",
      "Step 501441  [5.380 sec/step, loss=0.07483, avg_loss=0.07548]\n",
      "Step 501442  [5.385 sec/step, loss=0.07784, avg_loss=0.07549]\n",
      "Step 501443  [5.385 sec/step, loss=0.07620, avg_loss=0.07550]\n",
      "Step 501444  [5.382 sec/step, loss=0.07818, avg_loss=0.07550]\n",
      "Step 501445  [5.363 sec/step, loss=0.06762, avg_loss=0.07541]\n",
      "Step 501446  [5.379 sec/step, loss=0.07546, avg_loss=0.07547]\n",
      "Step 501447  [5.395 sec/step, loss=0.07496, avg_loss=0.07544]\n",
      "Step 501448  [5.401 sec/step, loss=0.07385, avg_loss=0.07541]\n",
      "Generated 32 batches of size 32 in 2.346 sec\n",
      "Step 501449  [5.418 sec/step, loss=0.07728, avg_loss=0.07545]\n",
      "Step 501450  [5.425 sec/step, loss=0.07580, avg_loss=0.07547]\n",
      "Step 501451  [5.414 sec/step, loss=0.07705, avg_loss=0.07546]\n",
      "Step 501452  [5.424 sec/step, loss=0.07726, avg_loss=0.07546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 501453  [5.437 sec/step, loss=0.07496, avg_loss=0.07546]\n",
      "Step 501454  [5.438 sec/step, loss=0.07859, avg_loss=0.07548]\n",
      "Step 501455  [5.442 sec/step, loss=0.07585, avg_loss=0.07550]\n",
      "Step 501456  [5.432 sec/step, loss=0.07539, avg_loss=0.07549]\n",
      "Step 501457  [5.399 sec/step, loss=0.07759, avg_loss=0.07558]\n",
      "Step 501458  [5.400 sec/step, loss=0.07628, avg_loss=0.07557]\n",
      "Step 501459  [5.389 sec/step, loss=0.07543, avg_loss=0.07556]\n",
      "Step 501460  [5.393 sec/step, loss=0.07669, avg_loss=0.07555]\n",
      "Step 501461  [5.372 sec/step, loss=0.07620, avg_loss=0.07556]\n",
      "Step 501462  [5.388 sec/step, loss=0.07480, avg_loss=0.07555]\n",
      "Step 501463  [5.396 sec/step, loss=0.07532, avg_loss=0.07553]\n",
      "Step 501464  [5.411 sec/step, loss=0.07717, avg_loss=0.07558]\n",
      "Step 501465  [5.391 sec/step, loss=0.06725, avg_loss=0.07549]\n",
      "Step 501466  [5.408 sec/step, loss=0.07759, avg_loss=0.07551]\n",
      "Step 501467  [5.425 sec/step, loss=0.07708, avg_loss=0.07561]\n",
      "Step 501468  [5.412 sec/step, loss=0.07325, avg_loss=0.07558]\n",
      "Step 501469  [5.456 sec/step, loss=0.06751, avg_loss=0.07547]\n",
      "Step 501470  [5.404 sec/step, loss=0.07573, avg_loss=0.07556]\n",
      "Step 501471  [5.404 sec/step, loss=0.07805, avg_loss=0.07557]\n",
      "Step 501472  [5.419 sec/step, loss=0.07683, avg_loss=0.07556]\n",
      "Step 501473  [5.412 sec/step, loss=0.07578, avg_loss=0.07554]\n",
      "Step 501474  [5.397 sec/step, loss=0.07678, avg_loss=0.07554]\n",
      "Step 501475  [5.375 sec/step, loss=0.07290, avg_loss=0.07548]\n",
      "Step 501476  [5.372 sec/step, loss=0.07300, avg_loss=0.07545]\n",
      "Step 501477  [5.346 sec/step, loss=0.07716, avg_loss=0.07546]\n",
      "Step 501478  [5.338 sec/step, loss=0.07613, avg_loss=0.07545]\n",
      "Step 501479  [5.342 sec/step, loss=0.07756, avg_loss=0.07546]\n",
      "Step 501480  [5.356 sec/step, loss=0.07599, avg_loss=0.07546]\n",
      "Generated 32 batches of size 32 in 2.562 sec\n",
      "Step 501481  [5.360 sec/step, loss=0.07373, avg_loss=0.07546]\n",
      "Step 501482  [5.364 sec/step, loss=0.07835, avg_loss=0.07548]\n",
      "Step 501483  [5.369 sec/step, loss=0.07725, avg_loss=0.07550]\n",
      "Step 501484  [5.363 sec/step, loss=0.07727, avg_loss=0.07549]\n",
      "Step 501485  [5.371 sec/step, loss=0.07665, avg_loss=0.07548]\n",
      "Step 501486  [5.366 sec/step, loss=0.07555, avg_loss=0.07550]\n",
      "Step 501487  [5.387 sec/step, loss=0.07695, avg_loss=0.07552]\n",
      "Step 501488  [5.380 sec/step, loss=0.07807, avg_loss=0.07553]\n",
      "Step 501489  [5.374 sec/step, loss=0.07614, avg_loss=0.07553]\n",
      "Step 501490  [5.364 sec/step, loss=0.07600, avg_loss=0.07554]\n",
      "Step 501491  [5.351 sec/step, loss=0.07562, avg_loss=0.07554]\n",
      "Step 501492  [5.346 sec/step, loss=0.07526, avg_loss=0.07553]\n",
      "Step 501493  [5.338 sec/step, loss=0.07564, avg_loss=0.07552]\n",
      "Step 501494  [5.367 sec/step, loss=0.07694, avg_loss=0.07557]\n",
      "Step 501495  [5.362 sec/step, loss=0.07386, avg_loss=0.07556]\n",
      "Step 501496  [5.373 sec/step, loss=0.07726, avg_loss=0.07557]\n",
      "Step 501497  [5.400 sec/step, loss=0.07800, avg_loss=0.07568]\n",
      "Step 501498  [5.429 sec/step, loss=0.07622, avg_loss=0.07567]\n",
      "Step 501499  [5.440 sec/step, loss=0.07794, avg_loss=0.07570]\n",
      "Step 501500  [5.451 sec/step, loss=0.07735, avg_loss=0.07573]\n",
      "Writing summary at step: 501500\n",
      "Step 501501  [5.463 sec/step, loss=0.07409, avg_loss=0.07574]\n",
      "Step 501502  [5.455 sec/step, loss=0.07615, avg_loss=0.07573]\n",
      "Step 501503  [5.455 sec/step, loss=0.07709, avg_loss=0.07573]\n",
      "Step 501504  [5.450 sec/step, loss=0.07358, avg_loss=0.07569]\n",
      "Step 501505  [5.447 sec/step, loss=0.07610, avg_loss=0.07570]\n",
      "Step 501506  [5.460 sec/step, loss=0.07812, avg_loss=0.07576]\n",
      "Step 501507  [5.484 sec/step, loss=0.07737, avg_loss=0.07580]\n",
      "Step 501508  [5.443 sec/step, loss=0.07662, avg_loss=0.07588]\n",
      "Step 501509  [5.455 sec/step, loss=0.07765, avg_loss=0.07589]\n",
      "Step 501510  [5.492 sec/step, loss=0.06711, avg_loss=0.07578]\n",
      "Step 501511  [5.493 sec/step, loss=0.07735, avg_loss=0.07579]\n",
      "Generated 32 batches of size 32 in 2.355 sec\n",
      "Step 501512  [5.499 sec/step, loss=0.07626, avg_loss=0.07581]\n",
      "Step 501513  [5.491 sec/step, loss=0.07677, avg_loss=0.07582]\n",
      "Step 501514  [5.463 sec/step, loss=0.07683, avg_loss=0.07584]\n",
      "Step 501515  [5.456 sec/step, loss=0.07423, avg_loss=0.07582]\n",
      "Step 501516  [5.435 sec/step, loss=0.07436, avg_loss=0.07578]\n",
      "Step 501517  [5.427 sec/step, loss=0.07693, avg_loss=0.07577]\n",
      "Step 501518  [5.413 sec/step, loss=0.07519, avg_loss=0.07578]\n",
      "Step 501519  [5.415 sec/step, loss=0.07730, avg_loss=0.07579]\n",
      "Step 501520  [5.417 sec/step, loss=0.07577, avg_loss=0.07576]\n",
      "Step 501521  [5.415 sec/step, loss=0.07801, avg_loss=0.07576]\n",
      "Step 501522  [5.419 sec/step, loss=0.07721, avg_loss=0.07578]\n",
      "Step 501523  [5.410 sec/step, loss=0.07231, avg_loss=0.07573]\n",
      "Step 501524  [5.391 sec/step, loss=0.07676, avg_loss=0.07572]\n",
      "Step 501525  [5.413 sec/step, loss=0.07782, avg_loss=0.07576]\n",
      "Step 501526  [5.427 sec/step, loss=0.07684, avg_loss=0.07580]\n",
      "Step 501527  [5.421 sec/step, loss=0.07571, avg_loss=0.07578]\n",
      "Step 501528  [5.419 sec/step, loss=0.07559, avg_loss=0.07579]\n",
      "Step 501529  [5.465 sec/step, loss=0.06884, avg_loss=0.07571]\n",
      "Step 501530  [5.458 sec/step, loss=0.07432, avg_loss=0.07569]\n",
      "Step 501531  [5.464 sec/step, loss=0.07547, avg_loss=0.07568]\n",
      "Step 501532  [5.452 sec/step, loss=0.07616, avg_loss=0.07567]\n",
      "Step 501533  [5.439 sec/step, loss=0.07565, avg_loss=0.07566]\n",
      "Step 501534  [5.435 sec/step, loss=0.07662, avg_loss=0.07570]\n",
      "Step 501535  [5.413 sec/step, loss=0.07495, avg_loss=0.07578]\n",
      "Step 501536  [5.421 sec/step, loss=0.07540, avg_loss=0.07577]\n",
      "Step 501537  [5.433 sec/step, loss=0.07677, avg_loss=0.07580]\n",
      "Step 501538  [5.423 sec/step, loss=0.07439, avg_loss=0.07577]\n",
      "Step 501539  [5.434 sec/step, loss=0.07806, avg_loss=0.07578]\n",
      "Step 501540  [5.439 sec/step, loss=0.07824, avg_loss=0.07579]\n",
      "Step 501541  [5.461 sec/step, loss=0.07759, avg_loss=0.07582]\n",
      "Step 501542  [5.458 sec/step, loss=0.07537, avg_loss=0.07580]\n",
      "Step 501543  [5.467 sec/step, loss=0.07762, avg_loss=0.07581]\n",
      "Generated 32 batches of size 32 in 2.925 sec\n",
      "Step 501544  [5.441 sec/step, loss=0.06799, avg_loss=0.07571]\n",
      "Step 501545  [5.461 sec/step, loss=0.07750, avg_loss=0.07581]\n",
      "Step 501546  [5.475 sec/step, loss=0.07520, avg_loss=0.07580]\n",
      "Step 501547  [5.447 sec/step, loss=0.07494, avg_loss=0.07580]\n",
      "Step 501548  [5.438 sec/step, loss=0.07150, avg_loss=0.07578]\n",
      "Step 501549  [5.421 sec/step, loss=0.07612, avg_loss=0.07577]\n",
      "Step 501550  [5.407 sec/step, loss=0.07281, avg_loss=0.07574]\n",
      "Step 501551  [5.413 sec/step, loss=0.07545, avg_loss=0.07572]\n",
      "Step 501552  [5.415 sec/step, loss=0.07775, avg_loss=0.07573]\n",
      "Step 501553  [5.412 sec/step, loss=0.07788, avg_loss=0.07576]\n",
      "Step 501554  [5.399 sec/step, loss=0.07637, avg_loss=0.07573]\n",
      "Step 501555  [5.406 sec/step, loss=0.07692, avg_loss=0.07575]\n",
      "Step 501556  [5.410 sec/step, loss=0.07753, avg_loss=0.07577]\n",
      "Step 501557  [5.420 sec/step, loss=0.07483, avg_loss=0.07574]\n",
      "Step 501558  [5.422 sec/step, loss=0.07816, avg_loss=0.07576]\n",
      "Step 501559  [5.430 sec/step, loss=0.07689, avg_loss=0.07577]\n",
      "Step 501560  [5.412 sec/step, loss=0.07317, avg_loss=0.07574]\n",
      "Step 501561  [5.416 sec/step, loss=0.07823, avg_loss=0.07576]\n",
      "Step 501562  [5.387 sec/step, loss=0.07677, avg_loss=0.07578]\n",
      "Step 501563  [5.381 sec/step, loss=0.07585, avg_loss=0.07578]\n",
      "Step 501564  [5.380 sec/step, loss=0.07666, avg_loss=0.07578]\n",
      "Step 501565  [5.416 sec/step, loss=0.07470, avg_loss=0.07585]\n",
      "Step 501566  [5.403 sec/step, loss=0.07533, avg_loss=0.07583]\n",
      "Step 501567  [5.398 sec/step, loss=0.07571, avg_loss=0.07582]\n",
      "Step 501568  [5.422 sec/step, loss=0.07551, avg_loss=0.07584]\n",
      "Step 501569  [5.382 sec/step, loss=0.07764, avg_loss=0.07594]\n",
      "Step 501570  [5.386 sec/step, loss=0.07372, avg_loss=0.07592]\n",
      "Step 501571  [5.367 sec/step, loss=0.07208, avg_loss=0.07586]\n",
      "Step 501572  [5.399 sec/step, loss=0.06765, avg_loss=0.07577]\n",
      "Step 501573  [5.382 sec/step, loss=0.06751, avg_loss=0.07569]\n",
      "Step 501574  [5.391 sec/step, loss=0.07747, avg_loss=0.07569]\n",
      "Step 501575  [5.416 sec/step, loss=0.07610, avg_loss=0.07572]\n",
      "Generated 32 batches of size 32 in 2.550 sec\n",
      "Step 501576  [5.416 sec/step, loss=0.07507, avg_loss=0.07575]\n",
      "Step 501577  [5.416 sec/step, loss=0.07412, avg_loss=0.07571]\n",
      "Step 501578  [5.431 sec/step, loss=0.07714, avg_loss=0.07572]\n",
      "Step 501579  [5.415 sec/step, loss=0.07633, avg_loss=0.07571]\n",
      "Step 501580  [5.401 sec/step, loss=0.07346, avg_loss=0.07569]\n",
      "Step 501581  [5.415 sec/step, loss=0.07771, avg_loss=0.07573]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 501582  [5.400 sec/step, loss=0.07540, avg_loss=0.07570]\n",
      "Step 501583  [5.407 sec/step, loss=0.07654, avg_loss=0.07569]\n",
      "Step 501584  [5.411 sec/step, loss=0.07549, avg_loss=0.07567]\n",
      "Step 501585  [5.408 sec/step, loss=0.07800, avg_loss=0.07569]\n",
      "Step 501586  [5.414 sec/step, loss=0.07654, avg_loss=0.07570]\n",
      "Step 501587  [5.399 sec/step, loss=0.07731, avg_loss=0.07570]\n",
      "Step 501588  [5.382 sec/step, loss=0.07463, avg_loss=0.07567]\n",
      "Step 501589  [5.384 sec/step, loss=0.07541, avg_loss=0.07566]\n",
      "Step 501590  [5.396 sec/step, loss=0.07619, avg_loss=0.07566]\n",
      "Step 501591  [5.389 sec/step, loss=0.07299, avg_loss=0.07563]\n",
      "Step 501592  [5.393 sec/step, loss=0.07617, avg_loss=0.07564]\n",
      "Step 501593  [5.410 sec/step, loss=0.07780, avg_loss=0.07566]\n",
      "Step 501594  [5.418 sec/step, loss=0.07476, avg_loss=0.07564]\n",
      "Step 501595  [5.419 sec/step, loss=0.07693, avg_loss=0.07567]\n",
      "Step 501596  [5.409 sec/step, loss=0.07839, avg_loss=0.07568]\n",
      "Step 501597  [5.410 sec/step, loss=0.07795, avg_loss=0.07568]\n",
      "Step 501598  [5.367 sec/step, loss=0.06664, avg_loss=0.07559]\n",
      "Step 501599  [5.371 sec/step, loss=0.07759, avg_loss=0.07558]\n",
      "Step 501600  [5.370 sec/step, loss=0.07453, avg_loss=0.07556]\n",
      "Writing summary at step: 501600\n",
      "Step 501601  [5.380 sec/step, loss=0.07834, avg_loss=0.07560]\n",
      "Step 501602  [5.370 sec/step, loss=0.07203, avg_loss=0.07556]\n",
      "Step 501603  [5.386 sec/step, loss=0.07624, avg_loss=0.07555]\n",
      "Step 501604  [5.441 sec/step, loss=0.06778, avg_loss=0.07549]\n",
      "Step 501605  [5.441 sec/step, loss=0.07575, avg_loss=0.07549]\n",
      "Step 501606  [5.438 sec/step, loss=0.07754, avg_loss=0.07548]\n",
      "Generated 32 batches of size 32 in 2.404 sec\n",
      "Step 501607  [5.432 sec/step, loss=0.07753, avg_loss=0.07548]\n",
      "Step 501608  [5.421 sec/step, loss=0.07281, avg_loss=0.07545]\n",
      "Step 501609  [5.410 sec/step, loss=0.07637, avg_loss=0.07543]\n",
      "Step 501610  [5.348 sec/step, loss=0.07634, avg_loss=0.07553]\n",
      "Step 501611  [5.350 sec/step, loss=0.07439, avg_loss=0.07550]\n",
      "Step 501612  [5.342 sec/step, loss=0.07430, avg_loss=0.07548]\n",
      "Step 501613  [5.333 sec/step, loss=0.07699, avg_loss=0.07548]\n",
      "Step 501614  [5.332 sec/step, loss=0.07416, avg_loss=0.07545]\n",
      "Step 501615  [5.358 sec/step, loss=0.07797, avg_loss=0.07549]\n",
      "Step 501616  [5.370 sec/step, loss=0.07745, avg_loss=0.07552]\n",
      "Step 501617  [5.370 sec/step, loss=0.07796, avg_loss=0.07553]\n",
      "Step 501618  [5.357 sec/step, loss=0.06784, avg_loss=0.07546]\n",
      "Step 501619  [5.358 sec/step, loss=0.07515, avg_loss=0.07543]\n",
      "Step 501620  [5.373 sec/step, loss=0.07486, avg_loss=0.07543]\n",
      "Step 501621  [5.363 sec/step, loss=0.07692, avg_loss=0.07541]\n",
      "Step 501622  [5.373 sec/step, loss=0.07785, avg_loss=0.07542]\n",
      "Step 501623  [5.382 sec/step, loss=0.07787, avg_loss=0.07548]\n",
      "Step 501624  [5.393 sec/step, loss=0.07584, avg_loss=0.07547]\n",
      "Step 501625  [5.434 sec/step, loss=0.06770, avg_loss=0.07537]\n",
      "Step 501626  [5.423 sec/step, loss=0.07557, avg_loss=0.07535]\n",
      "Step 501627  [5.433 sec/step, loss=0.07545, avg_loss=0.07535]\n",
      "Step 501628  [5.449 sec/step, loss=0.07636, avg_loss=0.07536]\n",
      "Step 501629  [5.401 sec/step, loss=0.07747, avg_loss=0.07545]\n",
      "Step 501630  [5.403 sec/step, loss=0.07362, avg_loss=0.07544]\n",
      "Step 501631  [5.389 sec/step, loss=0.07477, avg_loss=0.07543]\n",
      "Step 501632  [5.406 sec/step, loss=0.07721, avg_loss=0.07544]\n",
      "Step 501633  [5.427 sec/step, loss=0.07737, avg_loss=0.07546]\n",
      "Step 501634  [5.417 sec/step, loss=0.07372, avg_loss=0.07543]\n",
      "Step 501635  [5.390 sec/step, loss=0.07654, avg_loss=0.07545]\n",
      "Step 501636  [5.359 sec/step, loss=0.07312, avg_loss=0.07542]\n",
      "Step 501637  [5.361 sec/step, loss=0.07596, avg_loss=0.07541]\n",
      "Step 501638  [5.363 sec/step, loss=0.07615, avg_loss=0.07543]\n",
      "Generated 32 batches of size 32 in 2.421 sec\n",
      "Step 501639  [5.364 sec/step, loss=0.07600, avg_loss=0.07541]\n",
      "Step 501640  [5.357 sec/step, loss=0.07627, avg_loss=0.07539]\n",
      "Step 501641  [5.341 sec/step, loss=0.07653, avg_loss=0.07538]\n",
      "Step 501642  [5.327 sec/step, loss=0.07546, avg_loss=0.07538]\n",
      "Step 501643  [5.338 sec/step, loss=0.07830, avg_loss=0.07539]\n",
      "Step 501644  [5.354 sec/step, loss=0.07747, avg_loss=0.07548]\n",
      "Step 501645  [5.346 sec/step, loss=0.07770, avg_loss=0.07549]\n",
      "Step 501646  [5.323 sec/step, loss=0.07625, avg_loss=0.07550]\n",
      "Step 501647  [5.333 sec/step, loss=0.07824, avg_loss=0.07553]\n",
      "Step 501648  [5.336 sec/step, loss=0.07515, avg_loss=0.07557]\n",
      "Step 501649  [5.359 sec/step, loss=0.07792, avg_loss=0.07558]\n",
      "Step 501650  [5.380 sec/step, loss=0.07771, avg_loss=0.07563]\n",
      "Step 501651  [5.365 sec/step, loss=0.07204, avg_loss=0.07560]\n",
      "Step 501652  [5.356 sec/step, loss=0.07552, avg_loss=0.07558]\n",
      "Step 501653  [5.361 sec/step, loss=0.07852, avg_loss=0.07558]\n",
      "Step 501654  [5.390 sec/step, loss=0.07502, avg_loss=0.07557]\n",
      "Step 501655  [5.389 sec/step, loss=0.07457, avg_loss=0.07555]\n",
      "Step 501656  [5.389 sec/step, loss=0.07718, avg_loss=0.07554]\n",
      "Step 501657  [5.360 sec/step, loss=0.07537, avg_loss=0.07555]\n",
      "Step 501658  [5.399 sec/step, loss=0.06802, avg_loss=0.07545]\n",
      "Step 501659  [5.408 sec/step, loss=0.07836, avg_loss=0.07546]\n",
      "Step 501660  [5.406 sec/step, loss=0.06694, avg_loss=0.07540]\n",
      "Step 501661  [5.409 sec/step, loss=0.07791, avg_loss=0.07540]\n",
      "Step 501662  [5.424 sec/step, loss=0.07626, avg_loss=0.07539]\n",
      "Step 501663  [5.444 sec/step, loss=0.07546, avg_loss=0.07539]\n",
      "Step 501664  [5.429 sec/step, loss=0.07332, avg_loss=0.07535]\n",
      "Step 501665  [5.422 sec/step, loss=0.07557, avg_loss=0.07536]\n",
      "Step 501666  [5.421 sec/step, loss=0.07365, avg_loss=0.07535]\n",
      "Step 501667  [5.425 sec/step, loss=0.07574, avg_loss=0.07535]\n",
      "Step 501668  [5.408 sec/step, loss=0.07500, avg_loss=0.07534]\n",
      "Step 501669  [5.396 sec/step, loss=0.07657, avg_loss=0.07533]\n",
      "Step 501670  [5.392 sec/step, loss=0.07572, avg_loss=0.07535]\n",
      "Generated 32 batches of size 32 in 2.318 sec\n",
      "Step 501671  [5.407 sec/step, loss=0.07763, avg_loss=0.07540]\n",
      "Step 501672  [5.354 sec/step, loss=0.07689, avg_loss=0.07550]\n",
      "Step 501673  [5.377 sec/step, loss=0.07619, avg_loss=0.07558]\n",
      "Step 501674  [5.375 sec/step, loss=0.07737, avg_loss=0.07558]\n",
      "Step 501675  [5.350 sec/step, loss=0.07621, avg_loss=0.07558]\n",
      "Step 501676  [5.357 sec/step, loss=0.07661, avg_loss=0.07560]\n",
      "Step 501677  [5.360 sec/step, loss=0.07556, avg_loss=0.07561]\n",
      "Step 501678  [5.366 sec/step, loss=0.07664, avg_loss=0.07561]\n",
      "Step 501679  [5.371 sec/step, loss=0.07659, avg_loss=0.07561]\n",
      "Step 501680  [5.407 sec/step, loss=0.07463, avg_loss=0.07562]\n",
      "Step 501681  [5.404 sec/step, loss=0.07760, avg_loss=0.07562]\n",
      "Step 501682  [5.427 sec/step, loss=0.07701, avg_loss=0.07564]\n",
      "Step 501683  [5.415 sec/step, loss=0.07613, avg_loss=0.07563]\n",
      "Step 501684  [5.413 sec/step, loss=0.07536, avg_loss=0.07563]\n",
      "Step 501685  [5.409 sec/step, loss=0.07711, avg_loss=0.07562]\n",
      "Step 501686  [5.411 sec/step, loss=0.07572, avg_loss=0.07562]\n",
      "Step 501687  [5.419 sec/step, loss=0.07515, avg_loss=0.07559]\n",
      "Step 501688  [5.432 sec/step, loss=0.07586, avg_loss=0.07561]\n",
      "Step 501689  [5.437 sec/step, loss=0.07584, avg_loss=0.07561]\n",
      "Step 501690  [5.438 sec/step, loss=0.07743, avg_loss=0.07562]\n",
      "Step 501691  [5.450 sec/step, loss=0.07699, avg_loss=0.07566]\n",
      "Step 501692  [5.449 sec/step, loss=0.07399, avg_loss=0.07564]\n",
      "Step 501693  [5.436 sec/step, loss=0.07455, avg_loss=0.07561]\n",
      "Step 501694  [5.410 sec/step, loss=0.07635, avg_loss=0.07562]\n",
      "Step 501695  [5.398 sec/step, loss=0.07318, avg_loss=0.07559]\n",
      "Step 501696  [5.385 sec/step, loss=0.07565, avg_loss=0.07556]\n",
      "Step 501697  [5.375 sec/step, loss=0.07599, avg_loss=0.07554]\n",
      "Step 501698  [5.394 sec/step, loss=0.07586, avg_loss=0.07563]\n",
      "Step 501699  [5.386 sec/step, loss=0.07774, avg_loss=0.07563]\n",
      "Step 501700  [5.395 sec/step, loss=0.07737, avg_loss=0.07566]\n",
      "Writing summary at step: 501700\n",
      "Step 501701  [5.432 sec/step, loss=0.06714, avg_loss=0.07555]\n",
      "Generated 32 batches of size 32 in 2.801 sec\n",
      "Step 501702  [5.430 sec/step, loss=0.06790, avg_loss=0.07551]\n",
      "Step 501703  [5.414 sec/step, loss=0.07733, avg_loss=0.07552]\n",
      "Step 501704  [5.378 sec/step, loss=0.07787, avg_loss=0.07562]\n",
      "Step 501705  [5.380 sec/step, loss=0.07508, avg_loss=0.07561]\n",
      "Step 501706  [5.362 sec/step, loss=0.07339, avg_loss=0.07557]\n",
      "Step 501707  [5.357 sec/step, loss=0.07731, avg_loss=0.07557]\n",
      "Step 501708  [5.351 sec/step, loss=0.07248, avg_loss=0.07557]\n",
      "Step 501709  [5.359 sec/step, loss=0.07829, avg_loss=0.07559]\n",
      "Step 501710  [5.377 sec/step, loss=0.07856, avg_loss=0.07561]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 501711  [5.386 sec/step, loss=0.07707, avg_loss=0.07564]\n",
      "Step 501712  [5.385 sec/step, loss=0.07746, avg_loss=0.07567]\n",
      "Step 501713  [5.390 sec/step, loss=0.07483, avg_loss=0.07565]\n",
      "Step 501714  [5.388 sec/step, loss=0.07559, avg_loss=0.07566]\n",
      "Step 501715  [5.363 sec/step, loss=0.07375, avg_loss=0.07562]\n",
      "Step 501716  [5.364 sec/step, loss=0.07627, avg_loss=0.07561]\n",
      "Step 501717  [5.383 sec/step, loss=0.07657, avg_loss=0.07559]\n",
      "Step 501718  [5.449 sec/step, loss=0.06795, avg_loss=0.07559]\n",
      "Step 501719  [5.438 sec/step, loss=0.07597, avg_loss=0.07560]\n",
      "Step 501720  [5.423 sec/step, loss=0.07550, avg_loss=0.07561]\n",
      "Step 501721  [5.430 sec/step, loss=0.07755, avg_loss=0.07561]\n",
      "Step 501722  [5.427 sec/step, loss=0.07739, avg_loss=0.07561]\n",
      "Step 501723  [5.422 sec/step, loss=0.07653, avg_loss=0.07560]\n",
      "Step 501724  [5.427 sec/step, loss=0.07692, avg_loss=0.07561]\n",
      "Step 501725  [5.394 sec/step, loss=0.07677, avg_loss=0.07570]\n",
      "Step 501726  [5.413 sec/step, loss=0.07825, avg_loss=0.07572]\n",
      "Step 501727  [5.403 sec/step, loss=0.07653, avg_loss=0.07574]\n",
      "Step 501728  [5.389 sec/step, loss=0.07525, avg_loss=0.07572]\n",
      "Step 501729  [5.387 sec/step, loss=0.07617, avg_loss=0.07571]\n",
      "Step 501730  [5.393 sec/step, loss=0.07714, avg_loss=0.07575]\n",
      "Step 501731  [5.400 sec/step, loss=0.07440, avg_loss=0.07574]\n",
      "Step 501732  [5.375 sec/step, loss=0.07590, avg_loss=0.07573]\n",
      "Step 501733  [5.358 sec/step, loss=0.07323, avg_loss=0.07569]\n",
      "Generated 32 batches of size 32 in 2.393 sec\n",
      "Step 501734  [5.383 sec/step, loss=0.07835, avg_loss=0.07573]\n",
      "Step 501735  [5.374 sec/step, loss=0.07234, avg_loss=0.07569]\n",
      "Step 501736  [5.374 sec/step, loss=0.07384, avg_loss=0.07570]\n",
      "Step 501737  [5.387 sec/step, loss=0.07745, avg_loss=0.07571]\n",
      "Step 501738  [5.378 sec/step, loss=0.06785, avg_loss=0.07563]\n",
      "Step 501739  [5.366 sec/step, loss=0.07709, avg_loss=0.07564]\n",
      "Step 501740  [5.372 sec/step, loss=0.07641, avg_loss=0.07564]\n",
      "Step 501741  [5.382 sec/step, loss=0.07678, avg_loss=0.07565]\n",
      "Step 501742  [5.385 sec/step, loss=0.07465, avg_loss=0.07564]\n",
      "Step 501743  [5.369 sec/step, loss=0.07708, avg_loss=0.07563]\n",
      "Step 501744  [5.384 sec/step, loss=0.07569, avg_loss=0.07561]\n",
      "Step 501745  [5.392 sec/step, loss=0.07596, avg_loss=0.07559]\n",
      "Step 501746  [5.413 sec/step, loss=0.07521, avg_loss=0.07558]\n",
      "Step 501747  [5.388 sec/step, loss=0.07301, avg_loss=0.07553]\n",
      "Step 501748  [5.398 sec/step, loss=0.07788, avg_loss=0.07556]\n",
      "Step 501749  [5.390 sec/step, loss=0.07730, avg_loss=0.07555]\n",
      "Step 501750  [5.392 sec/step, loss=0.07648, avg_loss=0.07554]\n",
      "Step 501751  [5.412 sec/step, loss=0.07792, avg_loss=0.07560]\n",
      "Step 501752  [5.418 sec/step, loss=0.07574, avg_loss=0.07560]\n",
      "Step 501753  [5.389 sec/step, loss=0.06800, avg_loss=0.07549]\n",
      "Step 501754  [5.380 sec/step, loss=0.07568, avg_loss=0.07550]\n",
      "Step 501755  [5.386 sec/step, loss=0.07746, avg_loss=0.07553]\n",
      "Step 501756  [5.383 sec/step, loss=0.07293, avg_loss=0.07549]\n",
      "Step 501757  [5.374 sec/step, loss=0.07252, avg_loss=0.07546]\n",
      "Step 501758  [5.337 sec/step, loss=0.07810, avg_loss=0.07556]\n",
      "Step 501759  [5.320 sec/step, loss=0.07502, avg_loss=0.07552]\n",
      "Step 501760  [5.331 sec/step, loss=0.07358, avg_loss=0.07559]\n",
      "Step 501761  [5.318 sec/step, loss=0.07485, avg_loss=0.07556]\n",
      "Step 501762  [5.295 sec/step, loss=0.07563, avg_loss=0.07555]\n",
      "Step 501763  [5.273 sec/step, loss=0.07632, avg_loss=0.07556]\n",
      "Step 501764  [5.279 sec/step, loss=0.07606, avg_loss=0.07559]\n",
      "Step 501765  [5.316 sec/step, loss=0.06824, avg_loss=0.07552]\n",
      "Generated 32 batches of size 32 in 2.353 sec\n",
      "Step 501766  [5.336 sec/step, loss=0.07735, avg_loss=0.07555]\n",
      "Step 501767  [5.363 sec/step, loss=0.07499, avg_loss=0.07555]\n",
      "Step 501768  [5.367 sec/step, loss=0.07720, avg_loss=0.07557]\n",
      "Step 501769  [5.380 sec/step, loss=0.07818, avg_loss=0.07558]\n",
      "Step 501770  [5.382 sec/step, loss=0.07643, avg_loss=0.07559]\n",
      "Step 501771  [5.383 sec/step, loss=0.07740, avg_loss=0.07559]\n",
      "Step 501772  [5.385 sec/step, loss=0.07626, avg_loss=0.07558]\n",
      "Step 501773  [5.374 sec/step, loss=0.07497, avg_loss=0.07557]\n",
      "Step 501774  [5.371 sec/step, loss=0.07661, avg_loss=0.07556]\n",
      "Step 501775  [5.368 sec/step, loss=0.07315, avg_loss=0.07553]\n",
      "Step 501776  [5.360 sec/step, loss=0.07707, avg_loss=0.07554]\n",
      "Step 501777  [5.376 sec/step, loss=0.07662, avg_loss=0.07555]\n",
      "Step 501778  [5.415 sec/step, loss=0.06713, avg_loss=0.07545]\n",
      "Step 501779  [5.415 sec/step, loss=0.07735, avg_loss=0.07546]\n",
      "Step 501780  [5.385 sec/step, loss=0.07572, avg_loss=0.07547]\n",
      "Step 501781  [5.378 sec/step, loss=0.07525, avg_loss=0.07545]\n",
      "Step 501782  [5.370 sec/step, loss=0.07644, avg_loss=0.07544]\n",
      "Step 501783  [5.370 sec/step, loss=0.07543, avg_loss=0.07543]\n",
      "Step 501784  [5.365 sec/step, loss=0.07619, avg_loss=0.07544]\n",
      "Step 501785  [5.345 sec/step, loss=0.06717, avg_loss=0.07534]\n",
      "Step 501786  [5.362 sec/step, loss=0.07683, avg_loss=0.07535]\n",
      "Step 501787  [5.369 sec/step, loss=0.07795, avg_loss=0.07538]\n",
      "Step 501788  [5.361 sec/step, loss=0.07377, avg_loss=0.07536]\n",
      "Step 501789  [5.373 sec/step, loss=0.07745, avg_loss=0.07538]\n",
      "Step 501790  [5.359 sec/step, loss=0.07286, avg_loss=0.07533]\n",
      "Step 501791  [5.371 sec/step, loss=0.07520, avg_loss=0.07531]\n",
      "Step 501792  [5.374 sec/step, loss=0.07642, avg_loss=0.07534]\n",
      "Step 501793  [5.377 sec/step, loss=0.07671, avg_loss=0.07536]\n",
      "Step 501794  [5.379 sec/step, loss=0.07760, avg_loss=0.07537]\n",
      "Step 501795  [5.383 sec/step, loss=0.07583, avg_loss=0.07540]\n",
      "Step 501796  [5.391 sec/step, loss=0.07799, avg_loss=0.07542]\n",
      "Step 501797  [5.380 sec/step, loss=0.07231, avg_loss=0.07539]\n",
      "Generated 32 batches of size 32 in 2.353 sec\n",
      "Step 501798  [5.391 sec/step, loss=0.07762, avg_loss=0.07540]\n",
      "Step 501799  [5.395 sec/step, loss=0.07775, avg_loss=0.07540]\n",
      "Step 501800  [5.380 sec/step, loss=0.07487, avg_loss=0.07538]\n",
      "Writing summary at step: 501800\n",
      "Step 501801  [5.320 sec/step, loss=0.07618, avg_loss=0.07547]\n",
      "Step 501802  [5.339 sec/step, loss=0.07803, avg_loss=0.07557]\n",
      "Step 501803  [5.357 sec/step, loss=0.07754, avg_loss=0.07557]\n",
      "Step 501804  [5.351 sec/step, loss=0.07632, avg_loss=0.07556]\n",
      "Step 501805  [5.360 sec/step, loss=0.07627, avg_loss=0.07557]\n",
      "Step 501806  [5.358 sec/step, loss=0.06822, avg_loss=0.07552]\n",
      "Step 501807  [5.369 sec/step, loss=0.07623, avg_loss=0.07551]\n",
      "Step 501808  [5.386 sec/step, loss=0.07754, avg_loss=0.07556]\n",
      "Step 501809  [5.370 sec/step, loss=0.07598, avg_loss=0.07553]\n",
      "Step 501810  [5.368 sec/step, loss=0.07444, avg_loss=0.07549]\n",
      "Step 501811  [5.352 sec/step, loss=0.07512, avg_loss=0.07547]\n",
      "Step 501812  [5.362 sec/step, loss=0.07594, avg_loss=0.07546]\n",
      "Step 501813  [5.347 sec/step, loss=0.07290, avg_loss=0.07544]\n",
      "Step 501814  [5.361 sec/step, loss=0.07797, avg_loss=0.07546]\n",
      "Step 501815  [5.376 sec/step, loss=0.07416, avg_loss=0.07547]\n",
      "Step 501816  [5.362 sec/step, loss=0.07361, avg_loss=0.07544]\n",
      "Step 501817  [5.338 sec/step, loss=0.07676, avg_loss=0.07544]\n",
      "Step 501818  [5.278 sec/step, loss=0.07561, avg_loss=0.07552]\n",
      "Step 501819  [5.289 sec/step, loss=0.07610, avg_loss=0.07552]\n",
      "Step 501820  [5.283 sec/step, loss=0.07435, avg_loss=0.07551]\n",
      "Step 501821  [5.271 sec/step, loss=0.07680, avg_loss=0.07550]\n",
      "Step 501822  [5.256 sec/step, loss=0.07548, avg_loss=0.07548]\n",
      "Step 501823  [5.268 sec/step, loss=0.07833, avg_loss=0.07550]\n",
      "Step 501824  [5.252 sec/step, loss=0.07624, avg_loss=0.07549]\n",
      "Step 501825  [5.247 sec/step, loss=0.07769, avg_loss=0.07550]\n",
      "Step 501826  [5.239 sec/step, loss=0.07649, avg_loss=0.07548]\n",
      "Step 501827  [5.234 sec/step, loss=0.07547, avg_loss=0.07547]\n",
      "Step 501828  [5.245 sec/step, loss=0.07757, avg_loss=0.07550]\n",
      "Generated 32 batches of size 32 in 2.402 sec\n",
      "Step 501829  [5.251 sec/step, loss=0.07727, avg_loss=0.07551]\n",
      "Step 501830  [5.242 sec/step, loss=0.07741, avg_loss=0.07551]\n",
      "Step 501831  [5.247 sec/step, loss=0.07800, avg_loss=0.07555]\n",
      "Step 501832  [5.303 sec/step, loss=0.06787, avg_loss=0.07547]\n",
      "Step 501833  [5.331 sec/step, loss=0.07516, avg_loss=0.07548]\n",
      "Step 501834  [5.331 sec/step, loss=0.07776, avg_loss=0.07548]\n",
      "Step 501835  [5.336 sec/step, loss=0.07499, avg_loss=0.07551]\n",
      "Step 501836  [5.365 sec/step, loss=0.07525, avg_loss=0.07552]\n",
      "Step 501837  [5.355 sec/step, loss=0.07742, avg_loss=0.07552]\n",
      "Step 501838  [5.386 sec/step, loss=0.07748, avg_loss=0.07562]\n",
      "Step 501839  [5.379 sec/step, loss=0.07310, avg_loss=0.07558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 501840  [5.377 sec/step, loss=0.07783, avg_loss=0.07559]\n",
      "Step 501841  [5.367 sec/step, loss=0.07545, avg_loss=0.07558]\n",
      "Step 501842  [5.363 sec/step, loss=0.07453, avg_loss=0.07558]\n",
      "Step 501843  [5.391 sec/step, loss=0.07423, avg_loss=0.07555]\n",
      "Step 501844  [5.382 sec/step, loss=0.07787, avg_loss=0.07557]\n",
      "Step 501845  [5.394 sec/step, loss=0.07692, avg_loss=0.07558]\n",
      "Step 501846  [5.382 sec/step, loss=0.07569, avg_loss=0.07558]\n",
      "Step 501847  [5.407 sec/step, loss=0.07747, avg_loss=0.07563]\n",
      "Step 501848  [5.411 sec/step, loss=0.07328, avg_loss=0.07558]\n",
      "Step 501849  [5.458 sec/step, loss=0.06809, avg_loss=0.07549]\n",
      "Step 501850  [5.450 sec/step, loss=0.07436, avg_loss=0.07547]\n",
      "Step 501851  [5.434 sec/step, loss=0.07581, avg_loss=0.07545]\n",
      "Step 501852  [5.434 sec/step, loss=0.07824, avg_loss=0.07547]\n",
      "Step 501853  [5.458 sec/step, loss=0.07815, avg_loss=0.07557]\n",
      "Step 501854  [5.452 sec/step, loss=0.07653, avg_loss=0.07558]\n",
      "Step 501855  [5.448 sec/step, loss=0.07267, avg_loss=0.07553]\n",
      "Step 501856  [5.437 sec/step, loss=0.07383, avg_loss=0.07554]\n",
      "Step 501857  [5.462 sec/step, loss=0.07751, avg_loss=0.07559]\n",
      "Step 501858  [5.446 sec/step, loss=0.07741, avg_loss=0.07559]\n",
      "Step 501859  [5.450 sec/step, loss=0.07575, avg_loss=0.07559]\n",
      "Step 501860  [5.467 sec/step, loss=0.07511, avg_loss=0.07561]\n",
      "Generated 32 batches of size 32 in 2.592 sec\n",
      "Step 501861  [5.471 sec/step, loss=0.07675, avg_loss=0.07563]\n",
      "Step 501862  [5.486 sec/step, loss=0.07745, avg_loss=0.07565]\n",
      "Step 501863  [5.470 sec/step, loss=0.06752, avg_loss=0.07556]\n",
      "Step 501864  [5.470 sec/step, loss=0.07540, avg_loss=0.07555]\n",
      "Step 501865  [5.423 sec/step, loss=0.07734, avg_loss=0.07564]\n",
      "Step 501866  [5.407 sec/step, loss=0.07782, avg_loss=0.07565]\n",
      "Step 501867  [5.371 sec/step, loss=0.07569, avg_loss=0.07565]\n",
      "Step 501868  [5.378 sec/step, loss=0.07639, avg_loss=0.07565]\n",
      "Step 501869  [5.371 sec/step, loss=0.07344, avg_loss=0.07560]\n",
      "Step 501870  [5.368 sec/step, loss=0.07551, avg_loss=0.07559]\n",
      "Step 501871  [5.354 sec/step, loss=0.07542, avg_loss=0.07557]\n",
      "Step 501872  [5.369 sec/step, loss=0.07731, avg_loss=0.07558]\n",
      "Step 501873  [5.364 sec/step, loss=0.07243, avg_loss=0.07556]\n",
      "Step 501874  [5.354 sec/step, loss=0.07293, avg_loss=0.07552]\n",
      "Step 501875  [5.371 sec/step, loss=0.07468, avg_loss=0.07553]\n",
      "Step 501876  [5.375 sec/step, loss=0.07764, avg_loss=0.07554]\n",
      "Step 501877  [5.357 sec/step, loss=0.07606, avg_loss=0.07553]\n",
      "Step 501878  [5.306 sec/step, loss=0.07618, avg_loss=0.07562]\n",
      "Step 501879  [5.309 sec/step, loss=0.07811, avg_loss=0.07563]\n",
      "Step 501880  [5.366 sec/step, loss=0.06795, avg_loss=0.07555]\n",
      "Step 501881  [5.366 sec/step, loss=0.07705, avg_loss=0.07557]\n",
      "Step 501882  [5.350 sec/step, loss=0.07560, avg_loss=0.07556]\n",
      "Step 501883  [5.354 sec/step, loss=0.07245, avg_loss=0.07553]\n",
      "Step 501884  [5.363 sec/step, loss=0.07792, avg_loss=0.07555]\n",
      "Step 501885  [5.392 sec/step, loss=0.07771, avg_loss=0.07566]\n",
      "Step 501886  [5.368 sec/step, loss=0.07486, avg_loss=0.07564]\n",
      "Step 501887  [5.354 sec/step, loss=0.07722, avg_loss=0.07563]\n",
      "Step 501888  [5.359 sec/step, loss=0.07790, avg_loss=0.07567]\n",
      "Step 501889  [5.348 sec/step, loss=0.07547, avg_loss=0.07565]\n",
      "Step 501890  [5.358 sec/step, loss=0.07439, avg_loss=0.07567]\n",
      "Step 501891  [5.359 sec/step, loss=0.07780, avg_loss=0.07569]\n",
      "Step 501892  [5.386 sec/step, loss=0.07513, avg_loss=0.07568]\n",
      "Generated 32 batches of size 32 in 2.564 sec\n",
      "Step 501893  [5.382 sec/step, loss=0.07572, avg_loss=0.07567]\n",
      "Step 501894  [5.389 sec/step, loss=0.07812, avg_loss=0.07568]\n",
      "Step 501895  [5.392 sec/step, loss=0.07689, avg_loss=0.07569]\n",
      "Step 501896  [5.396 sec/step, loss=0.07667, avg_loss=0.07567]\n",
      "Step 501897  [5.417 sec/step, loss=0.07777, avg_loss=0.07573]\n",
      "Step 501898  [5.391 sec/step, loss=0.07394, avg_loss=0.07569]\n",
      "Step 501899  [5.361 sec/step, loss=0.06840, avg_loss=0.07560]\n",
      "Step 501900  [5.374 sec/step, loss=0.07744, avg_loss=0.07562]\n",
      "Writing summary at step: 501900\n",
      "Step 501901  [5.377 sec/step, loss=0.07532, avg_loss=0.07561]\n",
      "Step 501902  [5.382 sec/step, loss=0.07772, avg_loss=0.07561]\n",
      "Step 501903  [5.356 sec/step, loss=0.07281, avg_loss=0.07556]\n",
      "Step 501904  [5.356 sec/step, loss=0.07645, avg_loss=0.07556]\n",
      "Step 501905  [5.354 sec/step, loss=0.07711, avg_loss=0.07557]\n",
      "Step 501906  [5.368 sec/step, loss=0.07693, avg_loss=0.07566]\n",
      "Step 501907  [5.359 sec/step, loss=0.07631, avg_loss=0.07566]\n",
      "Step 501908  [5.361 sec/step, loss=0.07657, avg_loss=0.07565]\n",
      "Step 501909  [5.358 sec/step, loss=0.07602, avg_loss=0.07565]\n",
      "Step 501910  [5.351 sec/step, loss=0.07681, avg_loss=0.07568]\n",
      "Step 501911  [5.352 sec/step, loss=0.07543, avg_loss=0.07568]\n",
      "Step 501912  [5.325 sec/step, loss=0.06674, avg_loss=0.07559]\n",
      "Step 501913  [5.353 sec/step, loss=0.07771, avg_loss=0.07563]\n",
      "Step 501914  [5.339 sec/step, loss=0.07537, avg_loss=0.07561]\n",
      "Step 501915  [5.335 sec/step, loss=0.07726, avg_loss=0.07564]\n",
      "Step 501916  [5.352 sec/step, loss=0.07789, avg_loss=0.07568]\n",
      "Step 501917  [5.355 sec/step, loss=0.07679, avg_loss=0.07568]\n",
      "Step 501918  [5.357 sec/step, loss=0.07608, avg_loss=0.07569]\n",
      "Step 501919  [5.372 sec/step, loss=0.07532, avg_loss=0.07568]\n",
      "Step 501920  [5.365 sec/step, loss=0.07510, avg_loss=0.07569]\n",
      "Step 501921  [5.382 sec/step, loss=0.07546, avg_loss=0.07567]\n",
      "Step 501922  [5.413 sec/step, loss=0.07494, avg_loss=0.07567]\n",
      "Step 501923  [5.405 sec/step, loss=0.07816, avg_loss=0.07567]\n",
      "Generated 32 batches of size 32 in 2.676 sec\n",
      "Step 501924  [5.398 sec/step, loss=0.07341, avg_loss=0.07564]\n",
      "Step 501925  [5.390 sec/step, loss=0.07684, avg_loss=0.07563]\n",
      "Step 501926  [5.390 sec/step, loss=0.07691, avg_loss=0.07563]\n",
      "Step 501927  [5.405 sec/step, loss=0.07535, avg_loss=0.07563]\n",
      "Step 501928  [5.411 sec/step, loss=0.07787, avg_loss=0.07564]\n",
      "Step 501929  [5.401 sec/step, loss=0.07259, avg_loss=0.07559]\n",
      "Step 501930  [5.407 sec/step, loss=0.07718, avg_loss=0.07559]\n",
      "Step 501931  [5.449 sec/step, loss=0.06784, avg_loss=0.07549]\n",
      "Step 501932  [5.400 sec/step, loss=0.07665, avg_loss=0.07557]\n",
      "Step 501933  [5.375 sec/step, loss=0.07455, avg_loss=0.07557]\n",
      "Step 501934  [5.371 sec/step, loss=0.07838, avg_loss=0.07557]\n",
      "Step 501935  [5.401 sec/step, loss=0.07488, avg_loss=0.07557]\n",
      "Step 501936  [5.383 sec/step, loss=0.07618, avg_loss=0.07558]\n",
      "Step 501937  [5.383 sec/step, loss=0.07691, avg_loss=0.07558]\n",
      "Step 501938  [5.361 sec/step, loss=0.07520, avg_loss=0.07555]\n",
      "Step 501939  [5.384 sec/step, loss=0.07537, avg_loss=0.07558]\n",
      "Step 501940  [5.378 sec/step, loss=0.07365, avg_loss=0.07553]\n",
      "Step 501941  [5.370 sec/step, loss=0.07329, avg_loss=0.07551]\n",
      "Step 501942  [5.382 sec/step, loss=0.07772, avg_loss=0.07554]\n",
      "Step 501943  [5.350 sec/step, loss=0.07540, avg_loss=0.07556]\n",
      "Step 501944  [5.344 sec/step, loss=0.07649, avg_loss=0.07554]\n",
      "Step 501945  [5.326 sec/step, loss=0.07504, avg_loss=0.07552]\n",
      "Step 501946  [5.319 sec/step, loss=0.07211, avg_loss=0.07549]\n",
      "Step 501947  [5.304 sec/step, loss=0.07520, avg_loss=0.07547]\n",
      "Step 501948  [5.309 sec/step, loss=0.07767, avg_loss=0.07551]\n",
      "Step 501949  [5.255 sec/step, loss=0.07676, avg_loss=0.07560]\n",
      "Step 501950  [5.255 sec/step, loss=0.07767, avg_loss=0.07563]\n",
      "Step 501951  [5.270 sec/step, loss=0.07674, avg_loss=0.07564]\n",
      "Step 501952  [5.261 sec/step, loss=0.07673, avg_loss=0.07562]\n",
      "Step 501953  [5.249 sec/step, loss=0.07556, avg_loss=0.07560]\n",
      "Step 501954  [5.242 sec/step, loss=0.07431, avg_loss=0.07557]\n",
      "Step 501955  [5.262 sec/step, loss=0.07480, avg_loss=0.07560]\n",
      "Generated 32 batches of size 32 in 2.354 sec\n",
      "Step 501956  [5.288 sec/step, loss=0.07792, avg_loss=0.07564]\n",
      "Step 501957  [5.258 sec/step, loss=0.06792, avg_loss=0.07554]\n",
      "Step 501958  [5.274 sec/step, loss=0.07785, avg_loss=0.07555]\n",
      "Step 501959  [5.282 sec/step, loss=0.07719, avg_loss=0.07556]\n",
      "Step 501960  [5.257 sec/step, loss=0.07350, avg_loss=0.07554]\n",
      "Step 501961  [5.268 sec/step, loss=0.07870, avg_loss=0.07556]\n",
      "Step 501962  [5.269 sec/step, loss=0.07390, avg_loss=0.07553]\n",
      "Step 501963  [5.295 sec/step, loss=0.07632, avg_loss=0.07562]\n",
      "Step 501964  [5.353 sec/step, loss=0.06845, avg_loss=0.07555]\n",
      "Step 501965  [5.348 sec/step, loss=0.07533, avg_loss=0.07553]\n",
      "Step 501966  [5.346 sec/step, loss=0.07391, avg_loss=0.07549]\n",
      "Step 501967  [5.358 sec/step, loss=0.07576, avg_loss=0.07549]\n",
      "Step 501968  [5.352 sec/step, loss=0.07691, avg_loss=0.07549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 501969  [5.357 sec/step, loss=0.07632, avg_loss=0.07552]\n",
      "Step 501970  [5.370 sec/step, loss=0.07846, avg_loss=0.07555]\n",
      "Step 501971  [5.383 sec/step, loss=0.07723, avg_loss=0.07557]\n",
      "Step 501972  [5.380 sec/step, loss=0.07753, avg_loss=0.07557]\n",
      "Step 501973  [5.391 sec/step, loss=0.07739, avg_loss=0.07562]\n",
      "Step 501974  [5.379 sec/step, loss=0.06795, avg_loss=0.07557]\n",
      "Step 501975  [5.373 sec/step, loss=0.07180, avg_loss=0.07554]\n",
      "Step 501976  [5.380 sec/step, loss=0.07797, avg_loss=0.07555]\n",
      "Step 501977  [5.367 sec/step, loss=0.07320, avg_loss=0.07552]\n",
      "Step 501978  [5.394 sec/step, loss=0.07439, avg_loss=0.07550]\n",
      "Step 501979  [5.383 sec/step, loss=0.07699, avg_loss=0.07549]\n",
      "Step 501980  [5.349 sec/step, loss=0.07757, avg_loss=0.07558]\n",
      "Step 501981  [5.351 sec/step, loss=0.07646, avg_loss=0.07558]\n",
      "Step 501982  [5.350 sec/step, loss=0.07531, avg_loss=0.07558]\n",
      "Step 501983  [5.404 sec/step, loss=0.06776, avg_loss=0.07553]\n",
      "Step 501984  [5.414 sec/step, loss=0.07670, avg_loss=0.07552]\n",
      "Step 501985  [5.398 sec/step, loss=0.07304, avg_loss=0.07547]\n",
      "Step 501986  [5.412 sec/step, loss=0.07783, avg_loss=0.07550]\n",
      "Step 501987  [5.414 sec/step, loss=0.07649, avg_loss=0.07549]\n",
      "Generated 32 batches of size 32 in 2.537 sec\n",
      "Step 501988  [5.410 sec/step, loss=0.07572, avg_loss=0.07547]\n",
      "Step 501989  [5.411 sec/step, loss=0.07721, avg_loss=0.07549]\n",
      "Step 501990  [5.416 sec/step, loss=0.07556, avg_loss=0.07550]\n",
      "Step 501991  [5.407 sec/step, loss=0.07551, avg_loss=0.07548]\n",
      "Step 501992  [5.370 sec/step, loss=0.07195, avg_loss=0.07544]\n",
      "Step 501993  [5.376 sec/step, loss=0.07692, avg_loss=0.07546]\n",
      "Step 501994  [5.376 sec/step, loss=0.07798, avg_loss=0.07546]\n",
      "Step 501995  [5.369 sec/step, loss=0.07602, avg_loss=0.07545]\n",
      "Step 501996  [5.374 sec/step, loss=0.07839, avg_loss=0.07546]\n",
      "Step 501997  [5.359 sec/step, loss=0.07550, avg_loss=0.07544]\n",
      "Step 501998  [5.358 sec/step, loss=0.07285, avg_loss=0.07543]\n",
      "Step 501999  [5.380 sec/step, loss=0.07572, avg_loss=0.07550]\n",
      "Step 502000  [5.357 sec/step, loss=0.07241, avg_loss=0.07545]\n",
      "Writing summary at step: 502000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-502000\n",
      "Saving audio and alignment...\n",
      "Input: sagay bhaaiioon kay khoo dzaanay pir sultdaan zarqaavii dhay gijaa or mooslaaddhaar dduaaayn kiin~__________________________________\n",
      "Step 502001  [5.364 sec/step, loss=0.07592, avg_loss=0.07546]\n",
      "Step 502002  [5.336 sec/step, loss=0.06781, avg_loss=0.07536]\n",
      "Step 502003  [5.344 sec/step, loss=0.07713, avg_loss=0.07540]\n",
      "Step 502004  [5.341 sec/step, loss=0.07648, avg_loss=0.07540]\n",
      "Step 502005  [5.341 sec/step, loss=0.07715, avg_loss=0.07540]\n",
      "Step 502006  [5.359 sec/step, loss=0.07652, avg_loss=0.07540]\n",
      "Step 502007  [5.364 sec/step, loss=0.07614, avg_loss=0.07540]\n",
      "Step 502008  [5.357 sec/step, loss=0.07732, avg_loss=0.07541]\n",
      "Step 502009  [5.362 sec/step, loss=0.07688, avg_loss=0.07541]\n",
      "Step 502010  [5.367 sec/step, loss=0.07600, avg_loss=0.07541]\n",
      "Step 502011  [5.365 sec/step, loss=0.07405, avg_loss=0.07539]\n",
      "Step 502012  [5.378 sec/step, loss=0.07590, avg_loss=0.07548]\n",
      "Step 502013  [5.388 sec/step, loss=0.07751, avg_loss=0.07548]\n",
      "Step 502014  [5.399 sec/step, loss=0.07798, avg_loss=0.07551]\n",
      "Step 502015  [5.394 sec/step, loss=0.07456, avg_loss=0.07548]\n",
      "Step 502016  [5.396 sec/step, loss=0.07730, avg_loss=0.07548]\n",
      "Step 502017  [5.445 sec/step, loss=0.06713, avg_loss=0.07538]\n",
      "Generated 32 batches of size 32 in 2.404 sec\n",
      "Step 502018  [5.471 sec/step, loss=0.07699, avg_loss=0.07539]\n",
      "Step 502019  [5.452 sec/step, loss=0.07621, avg_loss=0.07540]\n",
      "Step 502020  [5.449 sec/step, loss=0.07568, avg_loss=0.07540]\n",
      "Step 502021  [5.448 sec/step, loss=0.07678, avg_loss=0.07542]\n",
      "Step 502022  [5.431 sec/step, loss=0.07750, avg_loss=0.07544]\n",
      "Step 502023  [5.436 sec/step, loss=0.07836, avg_loss=0.07544]\n",
      "Step 502024  [5.447 sec/step, loss=0.07736, avg_loss=0.07548]\n",
      "Step 502025  [5.449 sec/step, loss=0.07797, avg_loss=0.07549]\n",
      "Step 502026  [5.435 sec/step, loss=0.07586, avg_loss=0.07548]\n",
      "Step 502027  [5.421 sec/step, loss=0.07475, avg_loss=0.07548]\n",
      "Step 502028  [5.404 sec/step, loss=0.07554, avg_loss=0.07545]\n",
      "Step 502029  [5.405 sec/step, loss=0.07415, avg_loss=0.07547]\n",
      "Step 502030  [5.421 sec/step, loss=0.07776, avg_loss=0.07548]\n",
      "Step 502031  [5.382 sec/step, loss=0.07580, avg_loss=0.07556]\n",
      "Step 502032  [5.394 sec/step, loss=0.07770, avg_loss=0.07557]\n",
      "Step 502033  [5.374 sec/step, loss=0.06787, avg_loss=0.07550]\n",
      "Step 502034  [5.375 sec/step, loss=0.07668, avg_loss=0.07548]\n",
      "Step 502035  [5.340 sec/step, loss=0.07219, avg_loss=0.07546]\n",
      "Step 502036  [5.340 sec/step, loss=0.07400, avg_loss=0.07543]\n",
      "Step 502037  [5.349 sec/step, loss=0.07558, avg_loss=0.07542]\n",
      "Step 502038  [5.348 sec/step, loss=0.07603, avg_loss=0.07543]\n",
      "Step 502039  [5.335 sec/step, loss=0.07770, avg_loss=0.07545]\n",
      "Step 502040  [5.333 sec/step, loss=0.07671, avg_loss=0.07548]\n",
      "Step 502041  [5.350 sec/step, loss=0.07795, avg_loss=0.07553]\n",
      "Step 502042  [5.347 sec/step, loss=0.07684, avg_loss=0.07552]\n",
      "Step 502043  [5.353 sec/step, loss=0.07617, avg_loss=0.07553]\n",
      "Step 502044  [5.356 sec/step, loss=0.07778, avg_loss=0.07554]\n",
      "Step 502045  [5.368 sec/step, loss=0.07811, avg_loss=0.07557]\n",
      "Step 502046  [5.368 sec/step, loss=0.07569, avg_loss=0.07561]\n",
      "Step 502047  [5.370 sec/step, loss=0.07607, avg_loss=0.07562]\n",
      "Step 502048  [5.385 sec/step, loss=0.07505, avg_loss=0.07559]\n",
      "Step 502049  [5.438 sec/step, loss=0.06738, avg_loss=0.07550]\n",
      "Generated 32 batches of size 32 in 2.565 sec\n",
      "Step 502050  [5.438 sec/step, loss=0.07767, avg_loss=0.07550]\n",
      "Step 502051  [5.420 sec/step, loss=0.07346, avg_loss=0.07546]\n",
      "Step 502052  [5.422 sec/step, loss=0.07730, avg_loss=0.07547]\n",
      "Step 502053  [5.432 sec/step, loss=0.07657, avg_loss=0.07548]\n",
      "Step 502054  [5.436 sec/step, loss=0.07816, avg_loss=0.07552]\n",
      "Step 502055  [5.422 sec/step, loss=0.07664, avg_loss=0.07554]\n",
      "Step 502056  [5.396 sec/step, loss=0.07377, avg_loss=0.07549]\n",
      "Step 502057  [5.414 sec/step, loss=0.07716, avg_loss=0.07559]\n",
      "Step 502058  [5.416 sec/step, loss=0.07837, avg_loss=0.07559]\n",
      "Step 502059  [5.421 sec/step, loss=0.07575, avg_loss=0.07558]\n",
      "Step 502060  [5.442 sec/step, loss=0.07798, avg_loss=0.07562]\n",
      "Step 502061  [5.433 sec/step, loss=0.07658, avg_loss=0.07560]\n",
      "Step 502062  [5.411 sec/step, loss=0.06740, avg_loss=0.07554]\n",
      "Step 502063  [5.402 sec/step, loss=0.07733, avg_loss=0.07555]\n",
      "Step 502064  [5.348 sec/step, loss=0.07704, avg_loss=0.07563]\n",
      "Step 502065  [5.370 sec/step, loss=0.07551, avg_loss=0.07563]\n",
      "Step 502066  [5.363 sec/step, loss=0.07612, avg_loss=0.07566]\n",
      "Step 502067  [5.362 sec/step, loss=0.07694, avg_loss=0.07567]\n",
      "Step 502068  [5.377 sec/step, loss=0.07823, avg_loss=0.07568]\n",
      "Step 502069  [5.367 sec/step, loss=0.07624, avg_loss=0.07568]\n",
      "Step 502070  [5.354 sec/step, loss=0.07259, avg_loss=0.07562]\n",
      "Step 502071  [5.344 sec/step, loss=0.07543, avg_loss=0.07560]\n",
      "Step 502072  [5.319 sec/step, loss=0.07538, avg_loss=0.07558]\n",
      "Step 502073  [5.321 sec/step, loss=0.07729, avg_loss=0.07558]\n",
      "Step 502074  [5.348 sec/step, loss=0.07732, avg_loss=0.07568]\n",
      "Step 502075  [5.399 sec/step, loss=0.06707, avg_loss=0.07563]\n",
      "Step 502076  [5.390 sec/step, loss=0.07536, avg_loss=0.07560]\n",
      "Step 502077  [5.416 sec/step, loss=0.07781, avg_loss=0.07565]\n",
      "Step 502078  [5.380 sec/step, loss=0.07595, avg_loss=0.07566]\n",
      "Step 502079  [5.393 sec/step, loss=0.07766, avg_loss=0.07567]\n",
      "Step 502080  [5.384 sec/step, loss=0.07764, avg_loss=0.07567]\n",
      "Step 502081  [5.367 sec/step, loss=0.07344, avg_loss=0.07564]\n",
      "Generated 32 batches of size 32 in 2.335 sec\n",
      "Step 502082  [5.386 sec/step, loss=0.07776, avg_loss=0.07566]\n",
      "Step 502083  [5.348 sec/step, loss=0.07599, avg_loss=0.07575]\n",
      "Step 502084  [5.325 sec/step, loss=0.07402, avg_loss=0.07572]\n",
      "Step 502085  [5.331 sec/step, loss=0.07387, avg_loss=0.07573]\n",
      "Step 502086  [5.318 sec/step, loss=0.07701, avg_loss=0.07572]\n",
      "Step 502087  [5.316 sec/step, loss=0.07704, avg_loss=0.07573]\n",
      "Step 502088  [5.346 sec/step, loss=0.07472, avg_loss=0.07572]\n",
      "Step 502089  [5.348 sec/step, loss=0.07613, avg_loss=0.07571]\n",
      "Step 502090  [5.329 sec/step, loss=0.07541, avg_loss=0.07570]\n",
      "Step 502091  [5.339 sec/step, loss=0.07509, avg_loss=0.07570]\n",
      "Step 502092  [5.343 sec/step, loss=0.07497, avg_loss=0.07573]\n",
      "Step 502093  [5.346 sec/step, loss=0.07659, avg_loss=0.07573]\n",
      "Step 502094  [5.333 sec/step, loss=0.07644, avg_loss=0.07571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 502095  [5.345 sec/step, loss=0.07716, avg_loss=0.07572]\n",
      "Step 502096  [5.349 sec/step, loss=0.07736, avg_loss=0.07571]\n",
      "Step 502097  [5.342 sec/step, loss=0.07322, avg_loss=0.07569]\n",
      "Step 502098  [5.362 sec/step, loss=0.07693, avg_loss=0.07573]\n",
      "Step 502099  [5.364 sec/step, loss=0.07649, avg_loss=0.07574]\n",
      "Step 502100  [5.373 sec/step, loss=0.07543, avg_loss=0.07577]\n",
      "Writing summary at step: 502100\n",
      "Step 502101  [5.377 sec/step, loss=0.07684, avg_loss=0.07578]\n",
      "Step 502102  [5.405 sec/step, loss=0.07787, avg_loss=0.07588]\n",
      "Step 502103  [5.403 sec/step, loss=0.07715, avg_loss=0.07588]\n",
      "Step 502104  [5.415 sec/step, loss=0.07750, avg_loss=0.07589]\n",
      "Step 502105  [5.407 sec/step, loss=0.07547, avg_loss=0.07587]\n",
      "Step 502106  [5.394 sec/step, loss=0.07395, avg_loss=0.07585]\n",
      "Step 502107  [5.433 sec/step, loss=0.06822, avg_loss=0.07577]\n",
      "Step 502108  [5.440 sec/step, loss=0.07809, avg_loss=0.07577]\n",
      "Step 502109  [5.468 sec/step, loss=0.07396, avg_loss=0.07575]\n",
      "Step 502110  [5.459 sec/step, loss=0.07419, avg_loss=0.07573]\n",
      "Step 502111  [5.475 sec/step, loss=0.07518, avg_loss=0.07574]\n",
      "Step 502112  [5.486 sec/step, loss=0.07799, avg_loss=0.07576]\n",
      "Generated 32 batches of size 32 in 2.528 sec\n",
      "Step 502113  [5.460 sec/step, loss=0.07602, avg_loss=0.07574]\n",
      "Step 502114  [5.465 sec/step, loss=0.07752, avg_loss=0.07574]\n",
      "Step 502115  [5.456 sec/step, loss=0.06926, avg_loss=0.07569]\n",
      "Step 502116  [5.437 sec/step, loss=0.07562, avg_loss=0.07567]\n",
      "Step 502117  [5.387 sec/step, loss=0.07407, avg_loss=0.07574]\n",
      "Step 502118  [5.372 sec/step, loss=0.07680, avg_loss=0.07574]\n",
      "Step 502119  [5.374 sec/step, loss=0.07671, avg_loss=0.07574]\n",
      "Step 502120  [5.378 sec/step, loss=0.07646, avg_loss=0.07575]\n",
      "Step 502121  [5.356 sec/step, loss=0.07292, avg_loss=0.07571]\n",
      "Step 502122  [5.341 sec/step, loss=0.07547, avg_loss=0.07569]\n",
      "Step 502123  [5.320 sec/step, loss=0.07566, avg_loss=0.07566]\n",
      "Step 502124  [5.369 sec/step, loss=0.06786, avg_loss=0.07557]\n",
      "Step 502125  [5.347 sec/step, loss=0.06745, avg_loss=0.07546]\n",
      "Step 502126  [5.364 sec/step, loss=0.07633, avg_loss=0.07547]\n",
      "Step 502127  [5.369 sec/step, loss=0.07389, avg_loss=0.07546]\n",
      "Step 502128  [5.368 sec/step, loss=0.07189, avg_loss=0.07542]\n",
      "Step 502129  [5.366 sec/step, loss=0.07501, avg_loss=0.07543]\n",
      "Step 502130  [5.338 sec/step, loss=0.07305, avg_loss=0.07539]\n",
      "Step 502131  [5.312 sec/step, loss=0.07281, avg_loss=0.07536]\n",
      "Step 502132  [5.307 sec/step, loss=0.07639, avg_loss=0.07534]\n",
      "Step 502133  [5.329 sec/step, loss=0.07672, avg_loss=0.07543]\n",
      "Step 502134  [5.330 sec/step, loss=0.07566, avg_loss=0.07542]\n",
      "Step 502135  [5.339 sec/step, loss=0.07736, avg_loss=0.07547]\n",
      "Step 502136  [5.351 sec/step, loss=0.07513, avg_loss=0.07548]\n",
      "Step 502137  [5.354 sec/step, loss=0.07764, avg_loss=0.07550]\n",
      "Step 502138  [5.378 sec/step, loss=0.07677, avg_loss=0.07551]\n",
      "Step 502139  [5.375 sec/step, loss=0.07357, avg_loss=0.07547]\n",
      "Step 502140  [5.374 sec/step, loss=0.07775, avg_loss=0.07548]\n",
      "Step 502141  [5.372 sec/step, loss=0.07808, avg_loss=0.07548]\n",
      "Step 502142  [5.366 sec/step, loss=0.07664, avg_loss=0.07548]\n",
      "Step 502143  [5.379 sec/step, loss=0.07759, avg_loss=0.07549]\n",
      "Step 502144  [5.375 sec/step, loss=0.07564, avg_loss=0.07547]\n",
      "Generated 32 batches of size 32 in 2.512 sec\n",
      "Step 502145  [5.366 sec/step, loss=0.07610, avg_loss=0.07545]\n",
      "Step 502146  [5.382 sec/step, loss=0.07813, avg_loss=0.07548]\n",
      "Step 502147  [5.385 sec/step, loss=0.07736, avg_loss=0.07549]\n",
      "Step 502148  [5.363 sec/step, loss=0.07733, avg_loss=0.07551]\n",
      "Step 502149  [5.338 sec/step, loss=0.07713, avg_loss=0.07561]\n",
      "Step 502150  [5.334 sec/step, loss=0.07672, avg_loss=0.07560]\n",
      "Step 502151  [5.356 sec/step, loss=0.07823, avg_loss=0.07565]\n",
      "Step 502152  [5.356 sec/step, loss=0.07700, avg_loss=0.07565]\n",
      "Step 502153  [5.350 sec/step, loss=0.07623, avg_loss=0.07564]\n",
      "Step 502154  [5.353 sec/step, loss=0.07829, avg_loss=0.07564]\n",
      "Step 502155  [5.359 sec/step, loss=0.07765, avg_loss=0.07565]\n",
      "Step 502156  [5.372 sec/step, loss=0.07501, avg_loss=0.07567]\n",
      "Step 502157  [5.385 sec/step, loss=0.07725, avg_loss=0.07567]\n",
      "Step 502158  [5.365 sec/step, loss=0.07375, avg_loss=0.07562]\n",
      "Step 502159  [5.352 sec/step, loss=0.07222, avg_loss=0.07559]\n",
      "Step 502160  [5.354 sec/step, loss=0.07786, avg_loss=0.07558]\n",
      "Step 502161  [5.358 sec/step, loss=0.07640, avg_loss=0.07558]\n",
      "Step 502162  [5.357 sec/step, loss=0.06717, avg_loss=0.07558]\n",
      "Step 502163  [5.356 sec/step, loss=0.07718, avg_loss=0.07558]\n",
      "Step 502164  [5.359 sec/step, loss=0.07667, avg_loss=0.07557]\n",
      "Step 502165  [5.333 sec/step, loss=0.07459, avg_loss=0.07557]\n",
      "Step 502166  [5.339 sec/step, loss=0.07603, avg_loss=0.07556]\n",
      "Step 502167  [5.339 sec/step, loss=0.07594, avg_loss=0.07555]\n",
      "Step 502168  [5.326 sec/step, loss=0.07746, avg_loss=0.07555]\n",
      "Step 502169  [5.316 sec/step, loss=0.07303, avg_loss=0.07551]\n",
      "Step 502170  [5.311 sec/step, loss=0.07516, avg_loss=0.07554]\n",
      "Step 502171  [5.319 sec/step, loss=0.07727, avg_loss=0.07556]\n",
      "Step 502172  [5.382 sec/step, loss=0.06773, avg_loss=0.07548]\n",
      "Step 502173  [5.368 sec/step, loss=0.07598, avg_loss=0.07547]\n",
      "Step 502174  [5.364 sec/step, loss=0.07787, avg_loss=0.07547]\n",
      "Step 502175  [5.324 sec/step, loss=0.07763, avg_loss=0.07558]\n",
      "Step 502176  [5.350 sec/step, loss=0.07519, avg_loss=0.07558]\n",
      "Generated 32 batches of size 32 in 2.427 sec\n",
      "Step 502177  [5.357 sec/step, loss=0.07770, avg_loss=0.07558]\n",
      "Step 502178  [5.353 sec/step, loss=0.07357, avg_loss=0.07555]\n",
      "Step 502179  [5.348 sec/step, loss=0.07586, avg_loss=0.07554]\n",
      "Step 502180  [5.335 sec/step, loss=0.07686, avg_loss=0.07553]\n",
      "Step 502181  [5.343 sec/step, loss=0.07700, avg_loss=0.07556]\n",
      "Step 502182  [5.328 sec/step, loss=0.07564, avg_loss=0.07554]\n",
      "Step 502183  [5.324 sec/step, loss=0.07730, avg_loss=0.07556]\n",
      "Step 502184  [5.332 sec/step, loss=0.07750, avg_loss=0.07559]\n",
      "Step 502185  [5.351 sec/step, loss=0.07533, avg_loss=0.07560]\n",
      "Step 502186  [5.350 sec/step, loss=0.07412, avg_loss=0.07558]\n",
      "Step 502187  [5.363 sec/step, loss=0.07630, avg_loss=0.07557]\n",
      "Step 502188  [5.343 sec/step, loss=0.07804, avg_loss=0.07560]\n",
      "Step 502189  [5.347 sec/step, loss=0.07716, avg_loss=0.07561]\n",
      "Step 502190  [5.354 sec/step, loss=0.07654, avg_loss=0.07562]\n",
      "Step 502191  [5.336 sec/step, loss=0.07690, avg_loss=0.07564]\n",
      "Step 502192  [5.344 sec/step, loss=0.07764, avg_loss=0.07567]\n",
      "Step 502193  [5.349 sec/step, loss=0.07578, avg_loss=0.07566]\n",
      "Step 502194  [5.403 sec/step, loss=0.06743, avg_loss=0.07557]\n",
      "Step 502195  [5.409 sec/step, loss=0.07764, avg_loss=0.07557]\n",
      "Step 502196  [5.385 sec/step, loss=0.07603, avg_loss=0.07556]\n",
      "Step 502197  [5.395 sec/step, loss=0.07678, avg_loss=0.07560]\n",
      "Step 502198  [5.373 sec/step, loss=0.06720, avg_loss=0.07550]\n",
      "Step 502199  [5.370 sec/step, loss=0.07633, avg_loss=0.07550]\n",
      "Step 502200  [5.381 sec/step, loss=0.07773, avg_loss=0.07552]\n",
      "Writing summary at step: 502200\n",
      "Step 502201  [5.391 sec/step, loss=0.07768, avg_loss=0.07553]\n",
      "Step 502202  [5.370 sec/step, loss=0.07277, avg_loss=0.07548]\n",
      "Step 502203  [5.400 sec/step, loss=0.07559, avg_loss=0.07546]\n",
      "Step 502204  [5.374 sec/step, loss=0.07534, avg_loss=0.07544]\n",
      "Step 502205  [5.377 sec/step, loss=0.07453, avg_loss=0.07543]\n",
      "Step 502206  [5.367 sec/step, loss=0.07613, avg_loss=0.07545]\n",
      "Step 502207  [5.330 sec/step, loss=0.07773, avg_loss=0.07555]\n",
      "Generated 32 batches of size 32 in 2.373 sec\n",
      "Step 502208  [5.348 sec/step, loss=0.07568, avg_loss=0.07552]\n",
      "Step 502209  [5.331 sec/step, loss=0.07378, avg_loss=0.07552]\n",
      "Step 502210  [5.341 sec/step, loss=0.07627, avg_loss=0.07554]\n",
      "Step 502211  [5.327 sec/step, loss=0.07585, avg_loss=0.07555]\n",
      "Step 502212  [5.321 sec/step, loss=0.07791, avg_loss=0.07555]\n",
      "Step 502213  [5.333 sec/step, loss=0.07796, avg_loss=0.07557]\n",
      "Step 502214  [5.320 sec/step, loss=0.07606, avg_loss=0.07555]\n",
      "Step 502215  [5.334 sec/step, loss=0.07507, avg_loss=0.07561]\n",
      "Step 502216  [5.330 sec/step, loss=0.07366, avg_loss=0.07559]\n",
      "Step 502217  [5.327 sec/step, loss=0.07672, avg_loss=0.07562]\n",
      "Step 502218  [5.319 sec/step, loss=0.07695, avg_loss=0.07562]\n",
      "Step 502219  [5.310 sec/step, loss=0.07616, avg_loss=0.07562]\n",
      "Step 502220  [5.326 sec/step, loss=0.07742, avg_loss=0.07562]\n",
      "Step 502221  [5.343 sec/step, loss=0.07596, avg_loss=0.07566]\n",
      "Step 502222  [5.340 sec/step, loss=0.07549, avg_loss=0.07566]\n",
      "Step 502223  [5.399 sec/step, loss=0.06769, avg_loss=0.07558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 502224  [5.339 sec/step, loss=0.07537, avg_loss=0.07565]\n",
      "Step 502225  [5.367 sec/step, loss=0.07557, avg_loss=0.07573]\n",
      "Step 502226  [5.368 sec/step, loss=0.07802, avg_loss=0.07575]\n",
      "Step 502227  [5.374 sec/step, loss=0.07807, avg_loss=0.07579]\n",
      "Step 502228  [5.379 sec/step, loss=0.07769, avg_loss=0.07585]\n",
      "Step 502229  [5.411 sec/step, loss=0.07455, avg_loss=0.07584]\n",
      "Step 502230  [5.427 sec/step, loss=0.07620, avg_loss=0.07588]\n",
      "Step 502231  [5.454 sec/step, loss=0.07810, avg_loss=0.07593]\n",
      "Step 502232  [5.457 sec/step, loss=0.07523, avg_loss=0.07592]\n",
      "Step 502233  [5.468 sec/step, loss=0.07688, avg_loss=0.07592]\n",
      "Step 502234  [5.458 sec/step, loss=0.07651, avg_loss=0.07593]\n",
      "Step 502235  [5.454 sec/step, loss=0.07328, avg_loss=0.07589]\n",
      "Step 502236  [5.446 sec/step, loss=0.07580, avg_loss=0.07589]\n",
      "Step 502237  [5.445 sec/step, loss=0.07762, avg_loss=0.07589]\n",
      "Step 502238  [5.411 sec/step, loss=0.06846, avg_loss=0.07581]\n",
      "Step 502239  [5.415 sec/step, loss=0.07753, avg_loss=0.07585]\n",
      "Generated 32 batches of size 32 in 2.754 sec\n",
      "Step 502240  [5.411 sec/step, loss=0.07367, avg_loss=0.07581]\n",
      "Step 502241  [5.403 sec/step, loss=0.07421, avg_loss=0.07577]\n",
      "Step 502242  [5.408 sec/step, loss=0.07685, avg_loss=0.07577]\n",
      "Step 502243  [5.385 sec/step, loss=0.07345, avg_loss=0.07573]\n",
      "Step 502244  [5.378 sec/step, loss=0.07500, avg_loss=0.07572]\n",
      "Step 502245  [5.381 sec/step, loss=0.07711, avg_loss=0.07573]\n",
      "Step 502246  [5.372 sec/step, loss=0.07585, avg_loss=0.07571]\n",
      "Step 502247  [5.373 sec/step, loss=0.07639, avg_loss=0.07570]\n",
      "Step 502248  [5.368 sec/step, loss=0.07565, avg_loss=0.07568]\n",
      "Step 502249  [5.348 sec/step, loss=0.07599, avg_loss=0.07567]\n",
      "Step 502250  [5.340 sec/step, loss=0.07266, avg_loss=0.07563]\n",
      "Step 502251  [5.329 sec/step, loss=0.07738, avg_loss=0.07562]\n",
      "Step 502252  [5.330 sec/step, loss=0.07393, avg_loss=0.07559]\n",
      "Step 502253  [5.323 sec/step, loss=0.07534, avg_loss=0.07558]\n",
      "Step 502254  [5.331 sec/step, loss=0.07668, avg_loss=0.07557]\n",
      "Step 502255  [5.312 sec/step, loss=0.07582, avg_loss=0.07555]\n",
      "Step 502256  [5.322 sec/step, loss=0.07755, avg_loss=0.07558]\n",
      "Step 502257  [5.313 sec/step, loss=0.07694, avg_loss=0.07557]\n",
      "Step 502258  [5.314 sec/step, loss=0.07686, avg_loss=0.07560]\n",
      "Step 502259  [5.321 sec/step, loss=0.07660, avg_loss=0.07565]\n",
      "Step 502260  [5.319 sec/step, loss=0.07610, avg_loss=0.07563]\n",
      "Step 502261  [5.306 sec/step, loss=0.07541, avg_loss=0.07562]\n",
      "Step 502262  [5.336 sec/step, loss=0.07704, avg_loss=0.07572]\n",
      "Step 502263  [5.345 sec/step, loss=0.07775, avg_loss=0.07572]\n",
      "Step 502264  [5.358 sec/step, loss=0.07679, avg_loss=0.07573]\n",
      "Step 502265  [5.375 sec/step, loss=0.07497, avg_loss=0.07573]\n",
      "Step 502266  [5.372 sec/step, loss=0.07494, avg_loss=0.07572]\n",
      "Step 502267  [5.360 sec/step, loss=0.07602, avg_loss=0.07572]\n",
      "Step 502268  [5.346 sec/step, loss=0.07302, avg_loss=0.07567]\n",
      "Step 502269  [5.355 sec/step, loss=0.07570, avg_loss=0.07570]\n",
      "Step 502270  [5.365 sec/step, loss=0.07696, avg_loss=0.07572]\n",
      "Step 502271  [5.410 sec/step, loss=0.06693, avg_loss=0.07562]\n",
      "Generated 32 batches of size 32 in 2.371 sec\n",
      "Step 502272  [5.390 sec/step, loss=0.07664, avg_loss=0.07571]\n",
      "Step 502273  [5.401 sec/step, loss=0.07385, avg_loss=0.07568]\n",
      "Step 502274  [5.392 sec/step, loss=0.07295, avg_loss=0.07563]\n",
      "Step 502275  [5.368 sec/step, loss=0.06708, avg_loss=0.07553]\n",
      "Step 502276  [5.339 sec/step, loss=0.07753, avg_loss=0.07555]\n",
      "Step 502277  [5.328 sec/step, loss=0.07676, avg_loss=0.07554]\n",
      "Step 502278  [5.341 sec/step, loss=0.07592, avg_loss=0.07557]\n",
      "Step 502279  [5.344 sec/step, loss=0.07830, avg_loss=0.07559]\n",
      "Step 502280  [5.357 sec/step, loss=0.07830, avg_loss=0.07561]\n",
      "Step 502281  [5.372 sec/step, loss=0.07591, avg_loss=0.07559]\n",
      "Step 502282  [5.372 sec/step, loss=0.07369, avg_loss=0.07558]\n",
      "Step 502283  [5.378 sec/step, loss=0.07816, avg_loss=0.07558]\n",
      "Step 502284  [5.378 sec/step, loss=0.07655, avg_loss=0.07557]\n",
      "Step 502285  [5.370 sec/step, loss=0.07781, avg_loss=0.07560]\n",
      "Step 502286  [5.370 sec/step, loss=0.07678, avg_loss=0.07563]\n",
      "Step 502287  [5.358 sec/step, loss=0.07421, avg_loss=0.07560]\n",
      "Step 502288  [5.360 sec/step, loss=0.07643, avg_loss=0.07559]\n",
      "Step 502289  [5.342 sec/step, loss=0.07247, avg_loss=0.07554]\n",
      "Step 502290  [5.351 sec/step, loss=0.07750, avg_loss=0.07555]\n",
      "Step 502291  [5.354 sec/step, loss=0.07693, avg_loss=0.07555]\n",
      "Step 502292  [5.349 sec/step, loss=0.07591, avg_loss=0.07553]\n",
      "Step 502293  [5.348 sec/step, loss=0.07656, avg_loss=0.07554]\n",
      "Step 502294  [5.287 sec/step, loss=0.07634, avg_loss=0.07563]\n",
      "Step 502295  [5.290 sec/step, loss=0.07838, avg_loss=0.07564]\n",
      "Step 502296  [5.320 sec/step, loss=0.07593, avg_loss=0.07564]\n",
      "Step 502297  [5.321 sec/step, loss=0.07737, avg_loss=0.07564]\n",
      "Step 502298  [5.324 sec/step, loss=0.07348, avg_loss=0.07571]\n",
      "Step 502299  [5.325 sec/step, loss=0.07746, avg_loss=0.07572]\n",
      "Step 502300  [5.299 sec/step, loss=0.06762, avg_loss=0.07562]\n",
      "Writing summary at step: 502300\n",
      "Step 502301  [5.284 sec/step, loss=0.07713, avg_loss=0.07561]\n",
      "Step 502302  [5.287 sec/step, loss=0.07674, avg_loss=0.07565]\n",
      "Generated 32 batches of size 32 in 2.514 sec\n",
      "Step 502303  [5.257 sec/step, loss=0.07478, avg_loss=0.07564]\n",
      "Step 502304  [5.269 sec/step, loss=0.07707, avg_loss=0.07566]\n",
      "Step 502305  [5.272 sec/step, loss=0.07731, avg_loss=0.07569]\n",
      "Step 502306  [5.273 sec/step, loss=0.07447, avg_loss=0.07567]\n",
      "Step 502307  [5.287 sec/step, loss=0.07593, avg_loss=0.07565]\n",
      "Step 502308  [5.262 sec/step, loss=0.07772, avg_loss=0.07567]\n",
      "Step 502309  [5.262 sec/step, loss=0.07861, avg_loss=0.07572]\n",
      "Step 502310  [5.266 sec/step, loss=0.07609, avg_loss=0.07572]\n",
      "Step 502311  [5.266 sec/step, loss=0.07757, avg_loss=0.07574]\n",
      "Step 502312  [5.261 sec/step, loss=0.07730, avg_loss=0.07573]\n",
      "Step 502313  [5.262 sec/step, loss=0.07621, avg_loss=0.07571]\n",
      "Step 502314  [5.248 sec/step, loss=0.07393, avg_loss=0.07569]\n",
      "Step 502315  [5.253 sec/step, loss=0.07705, avg_loss=0.07571]\n",
      "Step 502316  [5.257 sec/step, loss=0.07617, avg_loss=0.07574]\n",
      "Step 502317  [5.258 sec/step, loss=0.07628, avg_loss=0.07573]\n",
      "Step 502318  [5.270 sec/step, loss=0.07711, avg_loss=0.07573]\n",
      "Step 502319  [5.287 sec/step, loss=0.07783, avg_loss=0.07575]\n",
      "Step 502320  [5.275 sec/step, loss=0.07599, avg_loss=0.07574]\n",
      "Step 502321  [5.260 sec/step, loss=0.07541, avg_loss=0.07573]\n",
      "Step 502322  [5.278 sec/step, loss=0.07594, avg_loss=0.07574]\n",
      "Step 502323  [5.242 sec/step, loss=0.07747, avg_loss=0.07583]\n",
      "Step 502324  [5.274 sec/step, loss=0.07650, avg_loss=0.07585]\n",
      "Step 502325  [5.272 sec/step, loss=0.07824, avg_loss=0.07587]\n",
      "Step 502326  [5.266 sec/step, loss=0.07668, avg_loss=0.07586]\n",
      "Step 502327  [5.263 sec/step, loss=0.07738, avg_loss=0.07585]\n",
      "Step 502328  [5.245 sec/step, loss=0.06785, avg_loss=0.07575]\n",
      "Step 502329  [5.212 sec/step, loss=0.07600, avg_loss=0.07577]\n",
      "Step 502330  [5.210 sec/step, loss=0.07705, avg_loss=0.07578]\n",
      "Step 502331  [5.248 sec/step, loss=0.06709, avg_loss=0.07567]\n",
      "Step 502332  [5.236 sec/step, loss=0.07774, avg_loss=0.07569]\n",
      "Step 502333  [5.216 sec/step, loss=0.07449, avg_loss=0.07567]\n",
      "Step 502334  [5.218 sec/step, loss=0.07351, avg_loss=0.07564]\n",
      "Generated 32 batches of size 32 in 2.471 sec\n",
      "Step 502335  [5.226 sec/step, loss=0.07500, avg_loss=0.07565]\n",
      "Step 502336  [5.219 sec/step, loss=0.07324, avg_loss=0.07563]\n",
      "Step 502337  [5.204 sec/step, loss=0.07550, avg_loss=0.07561]\n",
      "Step 502338  [5.229 sec/step, loss=0.07832, avg_loss=0.07571]\n",
      "Step 502339  [5.224 sec/step, loss=0.07725, avg_loss=0.07570]\n",
      "Step 502340  [5.223 sec/step, loss=0.07424, avg_loss=0.07571]\n",
      "Step 502341  [5.244 sec/step, loss=0.07679, avg_loss=0.07573]\n",
      "Step 502342  [5.253 sec/step, loss=0.07775, avg_loss=0.07574]\n",
      "Step 502343  [5.273 sec/step, loss=0.07864, avg_loss=0.07580]\n",
      "Step 502344  [5.269 sec/step, loss=0.07588, avg_loss=0.07580]\n",
      "Step 502345  [5.269 sec/step, loss=0.07700, avg_loss=0.07580]\n",
      "Step 502346  [5.270 sec/step, loss=0.07659, avg_loss=0.07581]\n",
      "Step 502347  [5.264 sec/step, loss=0.07561, avg_loss=0.07580]\n",
      "Step 502348  [5.291 sec/step, loss=0.07446, avg_loss=0.07579]\n",
      "Step 502349  [5.285 sec/step, loss=0.07403, avg_loss=0.07577]\n",
      "Step 502350  [5.304 sec/step, loss=0.07600, avg_loss=0.07580]\n",
      "Step 502351  [5.299 sec/step, loss=0.07491, avg_loss=0.07578]\n",
      "Step 502352  [5.296 sec/step, loss=0.07647, avg_loss=0.07581]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 502353  [5.289 sec/step, loss=0.07319, avg_loss=0.07578]\n",
      "Step 502354  [5.282 sec/step, loss=0.07808, avg_loss=0.07580]\n",
      "Step 502355  [5.283 sec/step, loss=0.07531, avg_loss=0.07579]\n",
      "Step 502356  [5.263 sec/step, loss=0.07243, avg_loss=0.07574]\n",
      "Step 502357  [5.260 sec/step, loss=0.07796, avg_loss=0.07575]\n",
      "Step 502358  [5.280 sec/step, loss=0.07788, avg_loss=0.07576]\n",
      "Step 502359  [5.280 sec/step, loss=0.07656, avg_loss=0.07576]\n",
      "Step 502360  [5.283 sec/step, loss=0.07810, avg_loss=0.07578]\n",
      "Step 502361  [5.288 sec/step, loss=0.07797, avg_loss=0.07581]\n",
      "Step 502362  [5.278 sec/step, loss=0.07635, avg_loss=0.07580]\n",
      "Step 502363  [5.267 sec/step, loss=0.07529, avg_loss=0.07578]\n",
      "Step 502364  [5.251 sec/step, loss=0.07723, avg_loss=0.07578]\n",
      "Step 502365  [5.248 sec/step, loss=0.07706, avg_loss=0.07580]\n",
      "Step 502366  [5.248 sec/step, loss=0.07743, avg_loss=0.07583]\n",
      "Generated 32 batches of size 32 in 2.326 sec\n",
      "Step 502367  [5.275 sec/step, loss=0.07755, avg_loss=0.07584]\n",
      "Step 502368  [5.300 sec/step, loss=0.07839, avg_loss=0.07590]\n",
      "Step 502369  [5.352 sec/step, loss=0.06829, avg_loss=0.07582]\n",
      "Step 502370  [5.334 sec/step, loss=0.06775, avg_loss=0.07573]\n",
      "Step 502371  [5.293 sec/step, loss=0.07852, avg_loss=0.07584]\n",
      "Step 502372  [5.276 sec/step, loss=0.07535, avg_loss=0.07583]\n",
      "Step 502373  [5.275 sec/step, loss=0.07664, avg_loss=0.07586]\n",
      "Step 502374  [5.268 sec/step, loss=0.07613, avg_loss=0.07589]\n",
      "Step 502375  [5.297 sec/step, loss=0.07833, avg_loss=0.07600]\n",
      "Step 502376  [5.312 sec/step, loss=0.07807, avg_loss=0.07601]\n",
      "Step 502377  [5.304 sec/step, loss=0.07724, avg_loss=0.07601]\n",
      "Step 502378  [5.331 sec/step, loss=0.07822, avg_loss=0.07604]\n",
      "Step 502379  [5.318 sec/step, loss=0.07618, avg_loss=0.07602]\n",
      "Step 502380  [5.308 sec/step, loss=0.07574, avg_loss=0.07599]\n",
      "Step 502381  [5.309 sec/step, loss=0.08053, avg_loss=0.07604]\n",
      "Step 502382  [5.313 sec/step, loss=0.07924, avg_loss=0.07609]\n",
      "Step 502383  [5.301 sec/step, loss=0.08044, avg_loss=0.07612]\n",
      "Step 502384  [5.290 sec/step, loss=0.07812, avg_loss=0.07613]\n",
      "Step 502385  [5.275 sec/step, loss=0.07877, avg_loss=0.07614]\n",
      "Step 502386  [5.274 sec/step, loss=0.07726, avg_loss=0.07615]\n",
      "Step 502387  [5.280 sec/step, loss=0.08018, avg_loss=0.07620]\n",
      "Step 502388  [5.284 sec/step, loss=0.08125, avg_loss=0.07625]\n",
      "Step 502389  [5.281 sec/step, loss=0.07710, avg_loss=0.07630]\n",
      "Step 502390  [5.269 sec/step, loss=0.07966, avg_loss=0.07632]\n",
      "Step 502391  [5.263 sec/step, loss=0.07792, avg_loss=0.07633]\n",
      "Step 502392  [5.256 sec/step, loss=0.07829, avg_loss=0.07635]\n",
      "Step 502393  [5.298 sec/step, loss=0.07724, avg_loss=0.07636]\n",
      "Step 502394  [5.303 sec/step, loss=0.07686, avg_loss=0.07637]\n",
      "Step 502395  [5.293 sec/step, loss=0.08033, avg_loss=0.07639]\n",
      "Step 502396  [5.293 sec/step, loss=0.08125, avg_loss=0.07644]\n",
      "Step 502397  [5.286 sec/step, loss=0.07727, avg_loss=0.07644]\n",
      "Step 502398  [5.304 sec/step, loss=0.08118, avg_loss=0.07652]\n",
      "Generated 32 batches of size 32 in 2.311 sec\n",
      "Step 502399  [5.314 sec/step, loss=0.08014, avg_loss=0.07654]\n",
      "Step 502400  [5.338 sec/step, loss=0.08128, avg_loss=0.07668]\n",
      "Writing summary at step: 502400\n",
      "Step 502401  [5.343 sec/step, loss=0.08082, avg_loss=0.07672]\n",
      "Step 502402  [5.364 sec/step, loss=0.08038, avg_loss=0.07675]\n",
      "Step 502403  [5.377 sec/step, loss=0.08117, avg_loss=0.07682]\n",
      "Step 502404  [5.358 sec/step, loss=0.07581, avg_loss=0.07680]\n",
      "Step 502405  [5.355 sec/step, loss=0.07954, avg_loss=0.07683]\n",
      "Step 502406  [5.368 sec/step, loss=0.07999, avg_loss=0.07688]\n",
      "Step 502407  [5.348 sec/step, loss=0.08045, avg_loss=0.07693]\n",
      "Step 502408  [5.372 sec/step, loss=0.08066, avg_loss=0.07696]\n",
      "Step 502409  [5.365 sec/step, loss=0.08018, avg_loss=0.07697]\n",
      "Step 502410  [5.357 sec/step, loss=0.07870, avg_loss=0.07700]\n",
      "Step 502411  [5.368 sec/step, loss=0.08037, avg_loss=0.07703]\n",
      "Step 502412  [5.363 sec/step, loss=0.07865, avg_loss=0.07704]\n",
      "Step 502413  [5.351 sec/step, loss=0.07967, avg_loss=0.07707]\n",
      "Step 502414  [5.373 sec/step, loss=0.07989, avg_loss=0.07713]\n",
      "Step 502415  [5.368 sec/step, loss=0.07763, avg_loss=0.07714]\n",
      "Step 502416  [5.389 sec/step, loss=0.08075, avg_loss=0.07718]\n",
      "Step 502417  [5.381 sec/step, loss=0.07643, avg_loss=0.07719]\n",
      "Step 502418  [5.375 sec/step, loss=0.07937, avg_loss=0.07721]\n",
      "Step 502419  [5.349 sec/step, loss=0.07117, avg_loss=0.07714]\n",
      "Step 502420  [5.350 sec/step, loss=0.07811, avg_loss=0.07716]\n",
      "Step 502421  [5.347 sec/step, loss=0.07785, avg_loss=0.07719]\n",
      "Step 502422  [5.343 sec/step, loss=0.07920, avg_loss=0.07722]\n",
      "Step 502423  [5.380 sec/step, loss=0.07296, avg_loss=0.07717]\n",
      "Step 502424  [5.343 sec/step, loss=0.07538, avg_loss=0.07716]\n",
      "Step 502425  [5.344 sec/step, loss=0.07942, avg_loss=0.07718]\n",
      "Step 502426  [5.335 sec/step, loss=0.07795, avg_loss=0.07719]\n",
      "Step 502427  [5.343 sec/step, loss=0.07848, avg_loss=0.07720]\n",
      "Step 502428  [5.380 sec/step, loss=0.07680, avg_loss=0.07729]\n",
      "Step 502429  [5.388 sec/step, loss=0.07612, avg_loss=0.07729]\n",
      "Generated 32 batches of size 32 in 2.439 sec\n",
      "Step 502430  [5.387 sec/step, loss=0.07670, avg_loss=0.07729]\n",
      "Step 502431  [5.336 sec/step, loss=0.07799, avg_loss=0.07740]\n",
      "Step 502432  [5.333 sec/step, loss=0.07820, avg_loss=0.07740]\n",
      "Step 502433  [5.341 sec/step, loss=0.07866, avg_loss=0.07744]\n",
      "Step 502434  [5.341 sec/step, loss=0.07870, avg_loss=0.07749]\n",
      "Step 502435  [5.346 sec/step, loss=0.07755, avg_loss=0.07752]\n",
      "Step 502436  [5.343 sec/step, loss=0.07690, avg_loss=0.07756]\n",
      "Step 502437  [5.355 sec/step, loss=0.07868, avg_loss=0.07759]\n",
      "Step 502438  [5.362 sec/step, loss=0.07775, avg_loss=0.07758]\n",
      "Step 502439  [5.373 sec/step, loss=0.07904, avg_loss=0.07760]\n",
      "Step 502440  [5.392 sec/step, loss=0.07854, avg_loss=0.07764]\n",
      "Step 502441  [5.367 sec/step, loss=0.07659, avg_loss=0.07764]\n",
      "Step 502442  [5.369 sec/step, loss=0.07594, avg_loss=0.07762]\n",
      "Step 502443  [5.354 sec/step, loss=0.07609, avg_loss=0.07760]\n",
      "Step 502444  [5.350 sec/step, loss=0.07703, avg_loss=0.07761]\n",
      "Step 502445  [5.396 sec/step, loss=0.07047, avg_loss=0.07754]\n",
      "Step 502446  [5.398 sec/step, loss=0.07508, avg_loss=0.07753]\n",
      "Step 502447  [5.402 sec/step, loss=0.07769, avg_loss=0.07755]\n",
      "Step 502448  [5.374 sec/step, loss=0.07659, avg_loss=0.07757]\n",
      "Step 502449  [5.388 sec/step, loss=0.07758, avg_loss=0.07761]\n",
      "Step 502450  [5.364 sec/step, loss=0.06892, avg_loss=0.07753]\n",
      "Step 502451  [5.374 sec/step, loss=0.07545, avg_loss=0.07754]\n",
      "Step 502452  [5.401 sec/step, loss=0.07584, avg_loss=0.07753]\n",
      "Step 502453  [5.419 sec/step, loss=0.07802, avg_loss=0.07758]\n",
      "Step 502454  [5.412 sec/step, loss=0.07816, avg_loss=0.07758]\n",
      "Step 502455  [5.436 sec/step, loss=0.07859, avg_loss=0.07762]\n",
      "Step 502456  [5.452 sec/step, loss=0.07794, avg_loss=0.07767]\n",
      "Step 502457  [5.446 sec/step, loss=0.07500, avg_loss=0.07764]\n",
      "Step 502458  [5.426 sec/step, loss=0.07434, avg_loss=0.07761]\n",
      "Step 502459  [5.428 sec/step, loss=0.07849, avg_loss=0.07763]\n",
      "Step 502460  [5.405 sec/step, loss=0.07452, avg_loss=0.07759]\n",
      "Step 502461  [5.394 sec/step, loss=0.07425, avg_loss=0.07755]\n",
      "Generated 32 batches of size 32 in 2.384 sec\n",
      "Step 502462  [5.404 sec/step, loss=0.07892, avg_loss=0.07758]\n",
      "Step 502463  [5.398 sec/step, loss=0.07614, avg_loss=0.07759]\n",
      "Step 502464  [5.416 sec/step, loss=0.07941, avg_loss=0.07761]\n",
      "Step 502465  [5.412 sec/step, loss=0.07803, avg_loss=0.07762]\n",
      "Step 502466  [5.428 sec/step, loss=0.07930, avg_loss=0.07764]\n",
      "Step 502467  [5.425 sec/step, loss=0.07675, avg_loss=0.07763]\n",
      "Step 502468  [5.415 sec/step, loss=0.07418, avg_loss=0.07759]\n",
      "Step 502469  [5.362 sec/step, loss=0.07764, avg_loss=0.07768]\n",
      "Step 502470  [5.376 sec/step, loss=0.07764, avg_loss=0.07778]\n",
      "Step 502471  [5.367 sec/step, loss=0.07606, avg_loss=0.07775]\n",
      "Step 502472  [5.359 sec/step, loss=0.07556, avg_loss=0.07776]\n",
      "Step 502473  [5.363 sec/step, loss=0.07728, avg_loss=0.07776]\n",
      "Step 502474  [5.384 sec/step, loss=0.07782, avg_loss=0.07778]\n",
      "Step 502475  [5.371 sec/step, loss=0.07589, avg_loss=0.07776]\n",
      "Step 502476  [5.356 sec/step, loss=0.07596, avg_loss=0.07773]\n",
      "Step 502477  [5.346 sec/step, loss=0.07615, avg_loss=0.07772]\n",
      "Step 502478  [5.326 sec/step, loss=0.07797, avg_loss=0.07772]\n",
      "Step 502479  [5.335 sec/step, loss=0.07580, avg_loss=0.07772]\n",
      "Step 502480  [5.328 sec/step, loss=0.07620, avg_loss=0.07772]\n",
      "Step 502481  [5.319 sec/step, loss=0.07730, avg_loss=0.07769]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 502482  [5.314 sec/step, loss=0.07721, avg_loss=0.07767]\n",
      "Step 502483  [5.313 sec/step, loss=0.07640, avg_loss=0.07763]\n",
      "Step 502484  [5.318 sec/step, loss=0.07767, avg_loss=0.07762]\n",
      "Step 502485  [5.332 sec/step, loss=0.07798, avg_loss=0.07762]\n",
      "Step 502486  [5.382 sec/step, loss=0.06891, avg_loss=0.07753]\n",
      "Step 502487  [5.378 sec/step, loss=0.07805, avg_loss=0.07751]\n",
      "Step 502488  [5.384 sec/step, loss=0.07472, avg_loss=0.07745]\n",
      "Step 502489  [5.409 sec/step, loss=0.07800, avg_loss=0.07746]\n",
      "Step 502490  [5.421 sec/step, loss=0.07835, avg_loss=0.07744]\n",
      "Step 502491  [5.412 sec/step, loss=0.06908, avg_loss=0.07735]\n",
      "Step 502492  [5.416 sec/step, loss=0.07560, avg_loss=0.07733]\n",
      "Step 502493  [5.380 sec/step, loss=0.07746, avg_loss=0.07733]\n",
      "Generated 32 batches of size 32 in 2.368 sec\n",
      "Step 502494  [5.397 sec/step, loss=0.07840, avg_loss=0.07734]\n",
      "Step 502495  [5.398 sec/step, loss=0.07715, avg_loss=0.07731]\n",
      "Step 502496  [5.365 sec/step, loss=0.07399, avg_loss=0.07724]\n",
      "Step 502497  [5.368 sec/step, loss=0.07468, avg_loss=0.07721]\n",
      "Step 502498  [5.352 sec/step, loss=0.07241, avg_loss=0.07713]\n",
      "Step 502499  [5.366 sec/step, loss=0.07551, avg_loss=0.07708]\n",
      "Step 502500  [5.367 sec/step, loss=0.07781, avg_loss=0.07705]\n",
      "Writing summary at step: 502500\n",
      "Step 502501  [5.363 sec/step, loss=0.07757, avg_loss=0.07701]\n",
      "Step 502502  [5.333 sec/step, loss=0.06850, avg_loss=0.07689]\n",
      "Step 502503  [5.322 sec/step, loss=0.07498, avg_loss=0.07683]\n",
      "Step 502504  [5.336 sec/step, loss=0.07781, avg_loss=0.07685]\n",
      "Step 502505  [5.347 sec/step, loss=0.07892, avg_loss=0.07685]\n",
      "Step 502506  [5.333 sec/step, loss=0.07292, avg_loss=0.07678]\n",
      "Step 502507  [5.319 sec/step, loss=0.07650, avg_loss=0.07674]\n",
      "Step 502508  [5.298 sec/step, loss=0.07656, avg_loss=0.07669]\n",
      "Step 502509  [5.302 sec/step, loss=0.07710, avg_loss=0.07666]\n",
      "Step 502510  [5.310 sec/step, loss=0.07841, avg_loss=0.07666]\n",
      "Step 502511  [5.301 sec/step, loss=0.07654, avg_loss=0.07662]\n",
      "Step 502512  [5.311 sec/step, loss=0.07359, avg_loss=0.07657]\n",
      "Step 502513  [5.321 sec/step, loss=0.07799, avg_loss=0.07656]\n",
      "Step 502514  [5.304 sec/step, loss=0.07620, avg_loss=0.07652]\n",
      "Step 502515  [5.321 sec/step, loss=0.07800, avg_loss=0.07652]\n",
      "Step 502516  [5.298 sec/step, loss=0.07333, avg_loss=0.07645]\n",
      "Step 502517  [5.321 sec/step, loss=0.07624, avg_loss=0.07645]\n",
      "Step 502518  [5.314 sec/step, loss=0.07549, avg_loss=0.07641]\n",
      "Step 502519  [5.378 sec/step, loss=0.06834, avg_loss=0.07638]\n",
      "Step 502520  [5.385 sec/step, loss=0.07643, avg_loss=0.07636]\n",
      "Step 502521  [5.397 sec/step, loss=0.07785, avg_loss=0.07636]\n",
      "Step 502522  [5.420 sec/step, loss=0.07525, avg_loss=0.07632]\n",
      "Step 502523  [5.371 sec/step, loss=0.07412, avg_loss=0.07633]\n",
      "Step 502524  [5.391 sec/step, loss=0.07772, avg_loss=0.07636]\n",
      "Generated 32 batches of size 32 in 2.820 sec\n",
      "Step 502525  [5.370 sec/step, loss=0.07329, avg_loss=0.07630]\n",
      "Step 502526  [5.387 sec/step, loss=0.07869, avg_loss=0.07630]\n",
      "Step 502527  [5.370 sec/step, loss=0.07606, avg_loss=0.07628]\n",
      "Step 502528  [5.352 sec/step, loss=0.07731, avg_loss=0.07628]\n",
      "Step 502529  [5.354 sec/step, loss=0.07690, avg_loss=0.07629]\n",
      "Step 502530  [5.346 sec/step, loss=0.07753, avg_loss=0.07630]\n",
      "Step 502531  [5.359 sec/step, loss=0.07814, avg_loss=0.07630]\n",
      "Step 502532  [5.379 sec/step, loss=0.07733, avg_loss=0.07629]\n",
      "Step 502533  [5.373 sec/step, loss=0.07745, avg_loss=0.07628]\n",
      "Step 502534  [5.358 sec/step, loss=0.07405, avg_loss=0.07624]\n",
      "Step 502535  [5.347 sec/step, loss=0.07639, avg_loss=0.07622]\n",
      "Step 502536  [5.349 sec/step, loss=0.07251, avg_loss=0.07618]\n",
      "Step 502537  [5.367 sec/step, loss=0.07472, avg_loss=0.07614]\n",
      "Step 502538  [5.402 sec/step, loss=0.06877, avg_loss=0.07605]\n",
      "Step 502539  [5.406 sec/step, loss=0.07811, avg_loss=0.07604]\n",
      "Step 502540  [5.397 sec/step, loss=0.07631, avg_loss=0.07602]\n",
      "Step 502541  [5.406 sec/step, loss=0.07708, avg_loss=0.07602]\n",
      "Step 502542  [5.399 sec/step, loss=0.07670, avg_loss=0.07603]\n",
      "Step 502543  [5.410 sec/step, loss=0.07683, avg_loss=0.07604]\n",
      "Step 502544  [5.422 sec/step, loss=0.07784, avg_loss=0.07605]\n",
      "Step 502545  [5.365 sec/step, loss=0.07528, avg_loss=0.07609]\n",
      "Step 502546  [5.364 sec/step, loss=0.07717, avg_loss=0.07612]\n",
      "Step 502547  [5.374 sec/step, loss=0.07799, avg_loss=0.07612]\n",
      "Step 502548  [5.368 sec/step, loss=0.07616, avg_loss=0.07611]\n",
      "Step 502549  [5.364 sec/step, loss=0.07610, avg_loss=0.07610]\n",
      "Step 502550  [5.376 sec/step, loss=0.07545, avg_loss=0.07616]\n",
      "Step 502551  [5.379 sec/step, loss=0.07821, avg_loss=0.07619]\n",
      "Step 502552  [5.347 sec/step, loss=0.07656, avg_loss=0.07620]\n",
      "Step 502553  [5.340 sec/step, loss=0.07735, avg_loss=0.07619]\n",
      "Step 502554  [5.327 sec/step, loss=0.07329, avg_loss=0.07614]\n",
      "Step 502555  [5.305 sec/step, loss=0.07475, avg_loss=0.07611]\n",
      "Step 502556  [5.311 sec/step, loss=0.07772, avg_loss=0.07610]\n",
      "Generated 32 batches of size 32 in 2.407 sec\n",
      "Step 502557  [5.338 sec/step, loss=0.07737, avg_loss=0.07613]\n",
      "Step 502558  [5.339 sec/step, loss=0.07529, avg_loss=0.07614]\n",
      "Step 502559  [5.324 sec/step, loss=0.07618, avg_loss=0.07611]\n",
      "Step 502560  [5.342 sec/step, loss=0.07596, avg_loss=0.07613]\n",
      "Step 502561  [5.366 sec/step, loss=0.07705, avg_loss=0.07616]\n",
      "Step 502562  [5.364 sec/step, loss=0.07763, avg_loss=0.07614]\n",
      "Step 502563  [5.379 sec/step, loss=0.07761, avg_loss=0.07616]\n",
      "Step 502564  [5.370 sec/step, loss=0.07703, avg_loss=0.07613]\n",
      "Step 502565  [5.351 sec/step, loss=0.06694, avg_loss=0.07602]\n",
      "Step 502566  [5.383 sec/step, loss=0.07068, avg_loss=0.07594]\n",
      "Step 502567  [5.365 sec/step, loss=0.07309, avg_loss=0.07590]\n",
      "Step 502568  [5.357 sec/step, loss=0.07535, avg_loss=0.07591]\n",
      "Step 502569  [5.371 sec/step, loss=0.07748, avg_loss=0.07591]\n",
      "Step 502570  [5.363 sec/step, loss=0.07597, avg_loss=0.07589]\n",
      "Step 502571  [5.363 sec/step, loss=0.07617, avg_loss=0.07590]\n",
      "Step 502572  [5.345 sec/step, loss=0.07358, avg_loss=0.07588]\n",
      "Step 502573  [5.342 sec/step, loss=0.07733, avg_loss=0.07588]\n",
      "Step 502574  [5.350 sec/step, loss=0.07530, avg_loss=0.07585]\n",
      "Step 502575  [5.351 sec/step, loss=0.07714, avg_loss=0.07586]\n",
      "Step 502576  [5.355 sec/step, loss=0.07350, avg_loss=0.07584]\n",
      "Step 502577  [5.374 sec/step, loss=0.07624, avg_loss=0.07584]\n",
      "Step 502578  [5.375 sec/step, loss=0.07761, avg_loss=0.07584]\n",
      "Step 502579  [5.363 sec/step, loss=0.07631, avg_loss=0.07584]\n",
      "Step 502580  [5.386 sec/step, loss=0.07814, avg_loss=0.07586]\n",
      "Step 502581  [5.382 sec/step, loss=0.07754, avg_loss=0.07586]\n",
      "Step 502582  [5.377 sec/step, loss=0.07260, avg_loss=0.07582]\n",
      "Step 502583  [5.376 sec/step, loss=0.07587, avg_loss=0.07581]\n",
      "Step 502584  [5.383 sec/step, loss=0.07705, avg_loss=0.07581]\n",
      "Step 502585  [5.397 sec/step, loss=0.07523, avg_loss=0.07578]\n",
      "Step 502586  [5.353 sec/step, loss=0.07692, avg_loss=0.07586]\n",
      "Step 502587  [5.357 sec/step, loss=0.07820, avg_loss=0.07586]\n",
      "Step 502588  [5.319 sec/step, loss=0.06773, avg_loss=0.07579]\n",
      "Generated 32 batches of size 32 in 2.513 sec\n",
      "Step 502589  [5.309 sec/step, loss=0.07484, avg_loss=0.07576]\n",
      "Step 502590  [5.295 sec/step, loss=0.07480, avg_loss=0.07572]\n",
      "Step 502591  [5.307 sec/step, loss=0.07707, avg_loss=0.07580]\n",
      "Step 502592  [5.310 sec/step, loss=0.07718, avg_loss=0.07582]\n",
      "Step 502593  [5.307 sec/step, loss=0.07797, avg_loss=0.07582]\n",
      "Step 502594  [5.309 sec/step, loss=0.07827, avg_loss=0.07582]\n",
      "Step 502595  [5.315 sec/step, loss=0.07757, avg_loss=0.07583]\n",
      "Step 502596  [5.332 sec/step, loss=0.07711, avg_loss=0.07586]\n",
      "Step 502597  [5.346 sec/step, loss=0.07636, avg_loss=0.07587]\n",
      "Step 502598  [5.370 sec/step, loss=0.07659, avg_loss=0.07592]\n",
      "Step 502599  [5.347 sec/step, loss=0.07614, avg_loss=0.07592]\n",
      "Step 502600  [5.334 sec/step, loss=0.07478, avg_loss=0.07589]\n",
      "Writing summary at step: 502600\n",
      "Step 502601  [5.333 sec/step, loss=0.07600, avg_loss=0.07588]\n",
      "Step 502602  [5.345 sec/step, loss=0.07705, avg_loss=0.07596]\n",
      "Step 502603  [5.346 sec/step, loss=0.07565, avg_loss=0.07597]\n",
      "Step 502604  [5.332 sec/step, loss=0.06766, avg_loss=0.07587]\n",
      "Step 502605  [5.340 sec/step, loss=0.07705, avg_loss=0.07585]\n",
      "Step 502606  [5.357 sec/step, loss=0.07590, avg_loss=0.07588]\n",
      "Step 502607  [5.354 sec/step, loss=0.07597, avg_loss=0.07587]\n",
      "Step 502608  [5.345 sec/step, loss=0.07673, avg_loss=0.07587]\n",
      "Step 502609  [5.333 sec/step, loss=0.07512, avg_loss=0.07585]\n",
      "Step 502610  [5.374 sec/step, loss=0.06805, avg_loss=0.07575]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 502611  [5.379 sec/step, loss=0.07636, avg_loss=0.07575]\n",
      "Step 502612  [5.377 sec/step, loss=0.07675, avg_loss=0.07578]\n",
      "Step 502613  [5.381 sec/step, loss=0.07789, avg_loss=0.07578]\n",
      "Step 502614  [5.401 sec/step, loss=0.07779, avg_loss=0.07580]\n",
      "Step 502615  [5.381 sec/step, loss=0.07561, avg_loss=0.07577]\n",
      "Step 502616  [5.394 sec/step, loss=0.07714, avg_loss=0.07581]\n",
      "Step 502617  [5.409 sec/step, loss=0.07433, avg_loss=0.07579]\n",
      "Step 502618  [5.414 sec/step, loss=0.07673, avg_loss=0.07580]\n",
      "Step 502619  [5.373 sec/step, loss=0.07811, avg_loss=0.07590]\n",
      "Generated 32 batches of size 32 in 2.343 sec\n",
      "Step 502620  [5.369 sec/step, loss=0.07789, avg_loss=0.07592]\n",
      "Step 502621  [5.371 sec/step, loss=0.07774, avg_loss=0.07591]\n",
      "Step 502622  [5.331 sec/step, loss=0.07300, avg_loss=0.07589]\n",
      "Step 502623  [5.321 sec/step, loss=0.07275, avg_loss=0.07588]\n",
      "Step 502624  [5.324 sec/step, loss=0.07758, avg_loss=0.07588]\n",
      "Step 502625  [5.330 sec/step, loss=0.07444, avg_loss=0.07589]\n",
      "Step 502626  [5.329 sec/step, loss=0.07754, avg_loss=0.07588]\n",
      "Step 502627  [5.330 sec/step, loss=0.07510, avg_loss=0.07587]\n",
      "Step 502628  [5.336 sec/step, loss=0.07701, avg_loss=0.07586]\n",
      "Step 502629  [5.326 sec/step, loss=0.07595, avg_loss=0.07585]\n",
      "Step 502630  [5.327 sec/step, loss=0.07575, avg_loss=0.07584]\n",
      "Step 502631  [5.326 sec/step, loss=0.07565, avg_loss=0.07581]\n",
      "Step 502632  [5.319 sec/step, loss=0.07804, avg_loss=0.07582]\n",
      "Step 502633  [5.332 sec/step, loss=0.07474, avg_loss=0.07579]\n",
      "Step 502634  [5.335 sec/step, loss=0.07575, avg_loss=0.07581]\n",
      "Step 502635  [5.324 sec/step, loss=0.07183, avg_loss=0.07576]\n",
      "Step 502636  [5.335 sec/step, loss=0.07606, avg_loss=0.07580]\n",
      "Step 502637  [5.310 sec/step, loss=0.07651, avg_loss=0.07582]\n",
      "Step 502638  [5.251 sec/step, loss=0.07313, avg_loss=0.07586]\n",
      "Step 502639  [5.255 sec/step, loss=0.07746, avg_loss=0.07585]\n",
      "Step 502640  [5.254 sec/step, loss=0.07607, avg_loss=0.07585]\n",
      "Step 502641  [5.249 sec/step, loss=0.07641, avg_loss=0.07584]\n",
      "Step 502642  [5.246 sec/step, loss=0.07706, avg_loss=0.07585]\n",
      "Step 502643  [5.252 sec/step, loss=0.07799, avg_loss=0.07586]\n",
      "Step 502644  [5.246 sec/step, loss=0.07272, avg_loss=0.07581]\n",
      "Step 502645  [5.256 sec/step, loss=0.07676, avg_loss=0.07582]\n",
      "Step 502646  [5.237 sec/step, loss=0.06574, avg_loss=0.07571]\n",
      "Step 502647  [5.217 sec/step, loss=0.07070, avg_loss=0.07564]\n",
      "Step 502648  [5.228 sec/step, loss=0.07717, avg_loss=0.07565]\n",
      "Step 502649  [5.219 sec/step, loss=0.07660, avg_loss=0.07565]\n",
      "Step 502650  [5.222 sec/step, loss=0.07750, avg_loss=0.07567]\n",
      "Step 502651  [5.217 sec/step, loss=0.07641, avg_loss=0.07565]\n",
      "Generated 32 batches of size 32 in 2.370 sec\n",
      "Step 502652  [5.251 sec/step, loss=0.07521, avg_loss=0.07564]\n",
      "Step 502653  [5.273 sec/step, loss=0.07561, avg_loss=0.07562]\n",
      "Step 502654  [5.280 sec/step, loss=0.07568, avg_loss=0.07565]\n",
      "Step 502655  [5.298 sec/step, loss=0.07773, avg_loss=0.07568]\n",
      "Step 502656  [5.294 sec/step, loss=0.07840, avg_loss=0.07568]\n",
      "Step 502657  [5.277 sec/step, loss=0.07663, avg_loss=0.07568]\n",
      "Step 502658  [5.328 sec/step, loss=0.06792, avg_loss=0.07560]\n",
      "Step 502659  [5.335 sec/step, loss=0.07760, avg_loss=0.07562]\n",
      "Step 502660  [5.330 sec/step, loss=0.07566, avg_loss=0.07561]\n",
      "Step 502661  [5.321 sec/step, loss=0.07805, avg_loss=0.07562]\n",
      "Step 502662  [5.322 sec/step, loss=0.07536, avg_loss=0.07560]\n",
      "Step 502663  [5.298 sec/step, loss=0.06968, avg_loss=0.07552]\n",
      "Step 502664  [5.287 sec/step, loss=0.07507, avg_loss=0.07550]\n",
      "Step 502665  [5.300 sec/step, loss=0.07687, avg_loss=0.07560]\n",
      "Step 502666  [5.268 sec/step, loss=0.07816, avg_loss=0.07568]\n",
      "Step 502667  [5.269 sec/step, loss=0.07509, avg_loss=0.07570]\n",
      "Step 502668  [5.280 sec/step, loss=0.07726, avg_loss=0.07571]\n",
      "Step 502669  [5.266 sec/step, loss=0.07717, avg_loss=0.07571]\n",
      "Step 502670  [5.277 sec/step, loss=0.07748, avg_loss=0.07573]\n",
      "Step 502671  [5.275 sec/step, loss=0.07527, avg_loss=0.07572]\n",
      "Step 502672  [5.301 sec/step, loss=0.07818, avg_loss=0.07576]\n",
      "Step 502673  [5.309 sec/step, loss=0.07656, avg_loss=0.07576]\n",
      "Step 502674  [5.284 sec/step, loss=0.07521, avg_loss=0.07576]\n",
      "Step 502675  [5.276 sec/step, loss=0.07238, avg_loss=0.07571]\n",
      "Step 502676  [5.293 sec/step, loss=0.07669, avg_loss=0.07574]\n",
      "Step 502677  [5.271 sec/step, loss=0.07542, avg_loss=0.07573]\n",
      "Step 502678  [5.254 sec/step, loss=0.07405, avg_loss=0.07570]\n",
      "Step 502679  [5.262 sec/step, loss=0.07772, avg_loss=0.07571]\n",
      "Step 502680  [5.260 sec/step, loss=0.07782, avg_loss=0.07571]\n",
      "Step 502681  [5.282 sec/step, loss=0.07716, avg_loss=0.07570]\n",
      "Step 502682  [5.297 sec/step, loss=0.07632, avg_loss=0.07574]\n",
      "Step 502683  [5.305 sec/step, loss=0.07817, avg_loss=0.07576]\n",
      "Generated 32 batches of size 32 in 2.323 sec\n",
      "Step 502684  [5.305 sec/step, loss=0.07656, avg_loss=0.07576]\n",
      "Step 502685  [5.284 sec/step, loss=0.07654, avg_loss=0.07577]\n",
      "Step 502686  [5.280 sec/step, loss=0.07401, avg_loss=0.07574]\n",
      "Step 502687  [5.264 sec/step, loss=0.07532, avg_loss=0.07571]\n",
      "Step 502688  [5.274 sec/step, loss=0.07576, avg_loss=0.07579]\n",
      "Step 502689  [5.283 sec/step, loss=0.07531, avg_loss=0.07580]\n",
      "Step 502690  [5.338 sec/step, loss=0.06819, avg_loss=0.07573]\n",
      "Step 502691  [5.347 sec/step, loss=0.07699, avg_loss=0.07573]\n",
      "Step 502692  [5.351 sec/step, loss=0.07723, avg_loss=0.07573]\n",
      "Step 502693  [5.333 sec/step, loss=0.07548, avg_loss=0.07571]\n",
      "Step 502694  [5.328 sec/step, loss=0.07758, avg_loss=0.07570]\n",
      "Step 502695  [5.319 sec/step, loss=0.07310, avg_loss=0.07566]\n",
      "Step 502696  [5.329 sec/step, loss=0.07816, avg_loss=0.07567]\n",
      "Step 502697  [5.304 sec/step, loss=0.06850, avg_loss=0.07559]\n",
      "Step 502698  [5.294 sec/step, loss=0.07654, avg_loss=0.07559]\n",
      "Step 502699  [5.283 sec/step, loss=0.07578, avg_loss=0.07558]\n",
      "Step 502700  [5.278 sec/step, loss=0.07548, avg_loss=0.07559]\n",
      "Writing summary at step: 502700\n",
      "Step 502701  [5.276 sec/step, loss=0.07533, avg_loss=0.07558]\n",
      "Step 502702  [5.328 sec/step, loss=0.06880, avg_loss=0.07550]\n",
      "Step 502703  [5.348 sec/step, loss=0.07566, avg_loss=0.07550]\n",
      "Step 502704  [5.353 sec/step, loss=0.07599, avg_loss=0.07558]\n",
      "Step 502705  [5.347 sec/step, loss=0.07651, avg_loss=0.07558]\n",
      "Step 502706  [5.335 sec/step, loss=0.07655, avg_loss=0.07559]\n",
      "Step 502707  [5.353 sec/step, loss=0.07787, avg_loss=0.07560]\n",
      "Step 502708  [5.354 sec/step, loss=0.07311, avg_loss=0.07557]\n",
      "Step 502709  [5.372 sec/step, loss=0.07569, avg_loss=0.07557]\n",
      "Step 502710  [5.349 sec/step, loss=0.07561, avg_loss=0.07565]\n",
      "Step 502711  [5.356 sec/step, loss=0.07787, avg_loss=0.07566]\n",
      "Step 502712  [5.341 sec/step, loss=0.07302, avg_loss=0.07563]\n",
      "Step 502713  [5.320 sec/step, loss=0.07323, avg_loss=0.07558]\n",
      "Step 502714  [5.303 sec/step, loss=0.07468, avg_loss=0.07555]\n",
      "Generated 32 batches of size 32 in 2.283 sec\n",
      "Step 502715  [5.316 sec/step, loss=0.07779, avg_loss=0.07557]\n",
      "Step 502716  [5.328 sec/step, loss=0.07804, avg_loss=0.07558]\n",
      "Step 502717  [5.302 sec/step, loss=0.07700, avg_loss=0.07561]\n",
      "Step 502718  [5.303 sec/step, loss=0.07707, avg_loss=0.07561]\n",
      "Step 502719  [5.290 sec/step, loss=0.07733, avg_loss=0.07560]\n",
      "Step 502720  [5.280 sec/step, loss=0.07681, avg_loss=0.07559]\n",
      "Step 502721  [5.281 sec/step, loss=0.07728, avg_loss=0.07559]\n",
      "Step 502722  [5.299 sec/step, loss=0.07662, avg_loss=0.07562]\n",
      "Step 502723  [5.321 sec/step, loss=0.07537, avg_loss=0.07565]\n",
      "Step 502724  [5.301 sec/step, loss=0.07567, avg_loss=0.07563]\n",
      "Step 502725  [5.312 sec/step, loss=0.07771, avg_loss=0.07566]\n",
      "Step 502726  [5.293 sec/step, loss=0.07113, avg_loss=0.07560]\n",
      "Step 502727  [5.289 sec/step, loss=0.07411, avg_loss=0.07559]\n",
      "Step 502728  [5.292 sec/step, loss=0.07771, avg_loss=0.07560]\n",
      "Step 502729  [5.296 sec/step, loss=0.07276, avg_loss=0.07556]\n",
      "Step 502730  [5.310 sec/step, loss=0.07577, avg_loss=0.07556]\n",
      "Step 502731  [5.285 sec/step, loss=0.07262, avg_loss=0.07553]\n",
      "Step 502732  [5.281 sec/step, loss=0.07602, avg_loss=0.07551]\n",
      "Step 502733  [5.272 sec/step, loss=0.07730, avg_loss=0.07554]\n",
      "Step 502734  [5.286 sec/step, loss=0.07679, avg_loss=0.07555]\n",
      "Step 502735  [5.299 sec/step, loss=0.07637, avg_loss=0.07560]\n",
      "Step 502736  [5.293 sec/step, loss=0.07728, avg_loss=0.07561]\n",
      "Step 502737  [5.290 sec/step, loss=0.07596, avg_loss=0.07560]\n",
      "Step 502738  [5.310 sec/step, loss=0.07749, avg_loss=0.07565]\n",
      "Step 502739  [5.305 sec/step, loss=0.07823, avg_loss=0.07565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 502740  [5.322 sec/step, loss=0.07689, avg_loss=0.07566]\n",
      "Step 502741  [5.334 sec/step, loss=0.07441, avg_loss=0.07564]\n",
      "Step 502742  [5.341 sec/step, loss=0.07716, avg_loss=0.07564]\n",
      "Step 502743  [5.325 sec/step, loss=0.07582, avg_loss=0.07562]\n",
      "Step 502744  [5.322 sec/step, loss=0.07556, avg_loss=0.07565]\n",
      "Step 502745  [5.323 sec/step, loss=0.07660, avg_loss=0.07565]\n",
      "Step 502746  [5.382 sec/step, loss=0.07074, avg_loss=0.07570]\n",
      "Generated 32 batches of size 32 in 2.362 sec\n",
      "Step 502747  [5.399 sec/step, loss=0.07687, avg_loss=0.07576]\n",
      "Step 502748  [5.390 sec/step, loss=0.07349, avg_loss=0.07572]\n",
      "Step 502749  [5.416 sec/step, loss=0.07529, avg_loss=0.07571]\n",
      "Step 502750  [5.422 sec/step, loss=0.07671, avg_loss=0.07570]\n",
      "Step 502751  [5.425 sec/step, loss=0.07775, avg_loss=0.07572]\n",
      "Step 502752  [5.396 sec/step, loss=0.07314, avg_loss=0.07569]\n",
      "Step 502753  [5.375 sec/step, loss=0.07689, avg_loss=0.07571]\n",
      "Step 502754  [5.364 sec/step, loss=0.06643, avg_loss=0.07562]\n",
      "Step 502755  [5.346 sec/step, loss=0.07703, avg_loss=0.07561]\n",
      "Step 502756  [5.327 sec/step, loss=0.07462, avg_loss=0.07557]\n",
      "Step 502757  [5.333 sec/step, loss=0.07391, avg_loss=0.07554]\n",
      "Step 502758  [5.290 sec/step, loss=0.07809, avg_loss=0.07564]\n",
      "Step 502759  [5.306 sec/step, loss=0.07772, avg_loss=0.07565]\n",
      "Step 502760  [5.305 sec/step, loss=0.07664, avg_loss=0.07566]\n",
      "Step 502761  [5.293 sec/step, loss=0.07170, avg_loss=0.07559]\n",
      "Step 502762  [5.306 sec/step, loss=0.07505, avg_loss=0.07559]\n",
      "Step 502763  [5.326 sec/step, loss=0.07657, avg_loss=0.07566]\n",
      "Step 502764  [5.326 sec/step, loss=0.07516, avg_loss=0.07566]\n",
      "Step 502765  [5.342 sec/step, loss=0.07468, avg_loss=0.07564]\n",
      "Step 502766  [5.333 sec/step, loss=0.07717, avg_loss=0.07563]\n",
      "Step 502767  [5.331 sec/step, loss=0.07694, avg_loss=0.07565]\n",
      "Step 502768  [5.335 sec/step, loss=0.07777, avg_loss=0.07565]\n",
      "Step 502769  [5.338 sec/step, loss=0.07579, avg_loss=0.07564]\n",
      "Step 502770  [5.323 sec/step, loss=0.07427, avg_loss=0.07560]\n",
      "Step 502771  [5.329 sec/step, loss=0.07664, avg_loss=0.07562]\n",
      "Step 502772  [5.322 sec/step, loss=0.07791, avg_loss=0.07562]\n",
      "Step 502773  [5.330 sec/step, loss=0.07542, avg_loss=0.07560]\n",
      "Step 502774  [5.336 sec/step, loss=0.07328, avg_loss=0.07559]\n",
      "Step 502775  [5.343 sec/step, loss=0.07743, avg_loss=0.07564]\n",
      "Step 502776  [5.322 sec/step, loss=0.07549, avg_loss=0.07562]\n",
      "Step 502777  [5.343 sec/step, loss=0.07691, avg_loss=0.07564]\n",
      "Step 502778  [5.348 sec/step, loss=0.07673, avg_loss=0.07567]\n",
      "Generated 32 batches of size 32 in 2.381 sec\n",
      "Step 502779  [5.364 sec/step, loss=0.07769, avg_loss=0.07567]\n",
      "Step 502780  [5.402 sec/step, loss=0.06819, avg_loss=0.07557]\n",
      "Step 502781  [5.384 sec/step, loss=0.07641, avg_loss=0.07556]\n",
      "Step 502782  [5.377 sec/step, loss=0.07397, avg_loss=0.07554]\n",
      "Step 502783  [5.381 sec/step, loss=0.07800, avg_loss=0.07554]\n",
      "Step 502784  [5.361 sec/step, loss=0.06850, avg_loss=0.07546]\n",
      "Step 502785  [5.347 sec/step, loss=0.07376, avg_loss=0.07543]\n",
      "Step 502786  [5.339 sec/step, loss=0.07589, avg_loss=0.07545]\n",
      "Step 502787  [5.353 sec/step, loss=0.07378, avg_loss=0.07543]\n",
      "Step 502788  [5.361 sec/step, loss=0.07795, avg_loss=0.07545]\n",
      "Step 502789  [5.362 sec/step, loss=0.07784, avg_loss=0.07548]\n",
      "Step 502790  [5.311 sec/step, loss=0.07614, avg_loss=0.07556]\n",
      "Step 502791  [5.296 sec/step, loss=0.07597, avg_loss=0.07555]\n",
      "Step 502792  [5.296 sec/step, loss=0.07623, avg_loss=0.07554]\n",
      "Step 502793  [5.295 sec/step, loss=0.07594, avg_loss=0.07554]\n",
      "Step 502794  [5.295 sec/step, loss=0.07876, avg_loss=0.07555]\n",
      "Step 502795  [5.297 sec/step, loss=0.07678, avg_loss=0.07559]\n",
      "Step 502796  [5.269 sec/step, loss=0.07195, avg_loss=0.07553]\n",
      "Step 502797  [5.284 sec/step, loss=0.07731, avg_loss=0.07562]\n",
      "Step 502798  [5.287 sec/step, loss=0.07682, avg_loss=0.07562]\n",
      "Step 502799  [5.297 sec/step, loss=0.07682, avg_loss=0.07563]\n",
      "Step 502800  [5.316 sec/step, loss=0.07575, avg_loss=0.07563]\n",
      "Writing summary at step: 502800\n",
      "Step 502801  [5.331 sec/step, loss=0.07525, avg_loss=0.07563]\n",
      "Step 502802  [5.291 sec/step, loss=0.07690, avg_loss=0.07571]\n",
      "Step 502803  [5.269 sec/step, loss=0.07360, avg_loss=0.07569]\n",
      "Step 502804  [5.285 sec/step, loss=0.07770, avg_loss=0.07571]\n",
      "Step 502805  [5.271 sec/step, loss=0.07708, avg_loss=0.07572]\n",
      "Step 502806  [5.267 sec/step, loss=0.07583, avg_loss=0.07571]\n",
      "Step 502807  [5.273 sec/step, loss=0.07736, avg_loss=0.07570]\n",
      "Step 502808  [5.269 sec/step, loss=0.07323, avg_loss=0.07570]\n",
      "Step 502809  [5.255 sec/step, loss=0.07254, avg_loss=0.07567]\n",
      "Generated 32 batches of size 32 in 2.369 sec\n",
      "Step 502810  [5.246 sec/step, loss=0.07747, avg_loss=0.07569]\n",
      "Step 502811  [5.224 sec/step, loss=0.07053, avg_loss=0.07562]\n",
      "Step 502812  [5.236 sec/step, loss=0.07606, avg_loss=0.07565]\n",
      "Step 502813  [5.265 sec/step, loss=0.07470, avg_loss=0.07566]\n",
      "Step 502814  [5.274 sec/step, loss=0.07684, avg_loss=0.07568]\n",
      "Step 502815  [5.269 sec/step, loss=0.07366, avg_loss=0.07564]\n",
      "Step 502816  [5.261 sec/step, loss=0.07602, avg_loss=0.07562]\n",
      "Step 502817  [5.242 sec/step, loss=0.06635, avg_loss=0.07552]\n",
      "Step 502818  [5.267 sec/step, loss=0.07509, avg_loss=0.07550]\n",
      "Step 502819  [5.284 sec/step, loss=0.07737, avg_loss=0.07550]\n",
      "Step 502820  [5.285 sec/step, loss=0.07587, avg_loss=0.07549]\n",
      "Step 502821  [5.295 sec/step, loss=0.07658, avg_loss=0.07548]\n",
      "Step 502822  [5.296 sec/step, loss=0.07475, avg_loss=0.07546]\n",
      "Step 502823  [5.280 sec/step, loss=0.07342, avg_loss=0.07544]\n",
      "Step 502824  [5.286 sec/step, loss=0.07656, avg_loss=0.07545]\n",
      "Step 502825  [5.288 sec/step, loss=0.07559, avg_loss=0.07543]\n",
      "Step 502826  [5.299 sec/step, loss=0.07516, avg_loss=0.07547]\n",
      "Step 502827  [5.307 sec/step, loss=0.07675, avg_loss=0.07550]\n",
      "Step 502828  [5.289 sec/step, loss=0.07547, avg_loss=0.07547]\n",
      "Step 502829  [5.282 sec/step, loss=0.07336, avg_loss=0.07548]\n",
      "Step 502830  [5.282 sec/step, loss=0.07716, avg_loss=0.07549]\n",
      "Step 502831  [5.302 sec/step, loss=0.07756, avg_loss=0.07554]\n",
      "Step 502832  [5.290 sec/step, loss=0.07495, avg_loss=0.07553]\n",
      "Step 502833  [5.292 sec/step, loss=0.07709, avg_loss=0.07553]\n",
      "Step 502834  [5.284 sec/step, loss=0.07163, avg_loss=0.07548]\n",
      "Step 502835  [5.334 sec/step, loss=0.06776, avg_loss=0.07539]\n",
      "Step 502836  [5.340 sec/step, loss=0.07771, avg_loss=0.07540]\n",
      "Step 502837  [5.343 sec/step, loss=0.07729, avg_loss=0.07541]\n",
      "Step 502838  [5.315 sec/step, loss=0.06933, avg_loss=0.07533]\n",
      "Step 502839  [5.315 sec/step, loss=0.07554, avg_loss=0.07530]\n",
      "Step 502840  [5.299 sec/step, loss=0.07634, avg_loss=0.07530]\n",
      "Step 502841  [5.301 sec/step, loss=0.07776, avg_loss=0.07533]\n",
      "Generated 32 batches of size 32 in 2.451 sec\n",
      "Step 502842  [5.292 sec/step, loss=0.07432, avg_loss=0.07530]\n",
      "Step 502843  [5.315 sec/step, loss=0.07626, avg_loss=0.07531]\n",
      "Step 502844  [5.320 sec/step, loss=0.07540, avg_loss=0.07530]\n",
      "Step 502845  [5.321 sec/step, loss=0.07659, avg_loss=0.07530]\n",
      "Step 502846  [5.307 sec/step, loss=0.07392, avg_loss=0.07534]\n",
      "Step 502847  [5.311 sec/step, loss=0.07845, avg_loss=0.07535]\n",
      "Step 502848  [5.314 sec/step, loss=0.07689, avg_loss=0.07539]\n",
      "Step 502849  [5.279 sec/step, loss=0.07559, avg_loss=0.07539]\n",
      "Step 502850  [5.260 sec/step, loss=0.07337, avg_loss=0.07536]\n",
      "Step 502851  [5.258 sec/step, loss=0.07706, avg_loss=0.07535]\n",
      "Step 502852  [5.253 sec/step, loss=0.07579, avg_loss=0.07538]\n",
      "Step 502853  [5.264 sec/step, loss=0.07699, avg_loss=0.07538]\n",
      "Step 502854  [5.264 sec/step, loss=0.06814, avg_loss=0.07539]\n",
      "Step 502855  [5.258 sec/step, loss=0.07307, avg_loss=0.07535]\n",
      "Step 502856  [5.281 sec/step, loss=0.07824, avg_loss=0.07539]\n",
      "Step 502857  [5.271 sec/step, loss=0.07692, avg_loss=0.07542]\n",
      "Step 502858  [5.265 sec/step, loss=0.07686, avg_loss=0.07541]\n",
      "Step 502859  [5.251 sec/step, loss=0.07377, avg_loss=0.07537]\n",
      "Step 502860  [5.244 sec/step, loss=0.07523, avg_loss=0.07535]\n",
      "Step 502861  [5.256 sec/step, loss=0.07786, avg_loss=0.07542]\n",
      "Step 502862  [5.227 sec/step, loss=0.07685, avg_loss=0.07543]\n",
      "Step 502863  [5.229 sec/step, loss=0.07649, avg_loss=0.07543]\n",
      "Step 502864  [5.243 sec/step, loss=0.07546, avg_loss=0.07544]\n",
      "Step 502865  [5.234 sec/step, loss=0.07663, avg_loss=0.07546]\n",
      "Step 502866  [5.227 sec/step, loss=0.07480, avg_loss=0.07543]\n",
      "Step 502867  [5.242 sec/step, loss=0.07524, avg_loss=0.07542]\n",
      "Step 502868  [5.246 sec/step, loss=0.07768, avg_loss=0.07541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 502869  [5.272 sec/step, loss=0.07488, avg_loss=0.07541]\n",
      "Step 502870  [5.288 sec/step, loss=0.07279, avg_loss=0.07539]\n",
      "Step 502871  [5.275 sec/step, loss=0.07593, avg_loss=0.07538]\n",
      "Step 502872  [5.281 sec/step, loss=0.07790, avg_loss=0.07538]\n",
      "Step 502873  [5.313 sec/step, loss=0.06860, avg_loss=0.07531]\n",
      "Generated 32 batches of size 32 in 2.387 sec\n",
      "Step 502874  [5.318 sec/step, loss=0.07674, avg_loss=0.07535]\n",
      "Step 502875  [5.337 sec/step, loss=0.07460, avg_loss=0.07532]\n",
      "Step 502876  [5.341 sec/step, loss=0.07569, avg_loss=0.07532]\n",
      "Step 502877  [5.327 sec/step, loss=0.07565, avg_loss=0.07531]\n",
      "Step 502878  [5.329 sec/step, loss=0.07661, avg_loss=0.07531]\n",
      "Step 502879  [5.299 sec/step, loss=0.07354, avg_loss=0.07527]\n",
      "Step 502880  [5.246 sec/step, loss=0.07252, avg_loss=0.07531]\n",
      "Step 502881  [5.246 sec/step, loss=0.07648, avg_loss=0.07531]\n",
      "Step 502882  [5.256 sec/step, loss=0.07771, avg_loss=0.07535]\n",
      "Step 502883  [5.246 sec/step, loss=0.07775, avg_loss=0.07535]\n",
      "Step 502884  [5.259 sec/step, loss=0.07651, avg_loss=0.07543]\n",
      "Step 502885  [5.254 sec/step, loss=0.07269, avg_loss=0.07542]\n",
      "Step 502886  [5.253 sec/step, loss=0.07501, avg_loss=0.07541]\n",
      "Step 502887  [5.268 sec/step, loss=0.07410, avg_loss=0.07541]\n",
      "Step 502888  [5.268 sec/step, loss=0.07749, avg_loss=0.07541]\n",
      "Step 502889  [5.257 sec/step, loss=0.07554, avg_loss=0.07538]\n",
      "Step 502890  [5.252 sec/step, loss=0.07467, avg_loss=0.07537]\n",
      "Step 502891  [5.269 sec/step, loss=0.07745, avg_loss=0.07538]\n",
      "Step 502892  [5.258 sec/step, loss=0.07264, avg_loss=0.07535]\n",
      "Step 502893  [5.278 sec/step, loss=0.07722, avg_loss=0.07536]\n",
      "Step 502894  [5.276 sec/step, loss=0.07779, avg_loss=0.07535]\n",
      "Step 502895  [5.284 sec/step, loss=0.07612, avg_loss=0.07534]\n",
      "Step 502896  [5.297 sec/step, loss=0.07587, avg_loss=0.07538]\n",
      "Step 502897  [5.291 sec/step, loss=0.07543, avg_loss=0.07536]\n",
      "Step 502898  [5.279 sec/step, loss=0.07702, avg_loss=0.07537]\n",
      "Step 502899  [5.301 sec/step, loss=0.07446, avg_loss=0.07534]\n",
      "Step 502900  [5.295 sec/step, loss=0.07629, avg_loss=0.07535]\n",
      "Writing summary at step: 502900\n",
      "Step 502901  [5.290 sec/step, loss=0.07669, avg_loss=0.07536]\n",
      "Step 502902  [5.287 sec/step, loss=0.07686, avg_loss=0.07536]\n",
      "Step 502903  [5.301 sec/step, loss=0.07775, avg_loss=0.07540]\n",
      "Step 502904  [5.281 sec/step, loss=0.06666, avg_loss=0.07529]\n",
      "Generated 32 batches of size 32 in 2.337 sec\n",
      "Step 502905  [5.298 sec/step, loss=0.07830, avg_loss=0.07531]\n",
      "Step 502906  [5.299 sec/step, loss=0.07424, avg_loss=0.07529]\n",
      "Step 502907  [5.282 sec/step, loss=0.07531, avg_loss=0.07527]\n",
      "Step 502908  [5.290 sec/step, loss=0.07699, avg_loss=0.07531]\n",
      "Step 502909  [5.302 sec/step, loss=0.07741, avg_loss=0.07536]\n",
      "Step 502910  [5.274 sec/step, loss=0.07599, avg_loss=0.07534]\n",
      "Step 502911  [5.289 sec/step, loss=0.07644, avg_loss=0.07540]\n",
      "Step 502912  [5.289 sec/step, loss=0.07674, avg_loss=0.07541]\n",
      "Step 502913  [5.320 sec/step, loss=0.06728, avg_loss=0.07533]\n",
      "Step 502914  [5.314 sec/step, loss=0.07602, avg_loss=0.07532]\n",
      "Step 502915  [5.361 sec/step, loss=0.06761, avg_loss=0.07526]\n",
      "Step 502916  [5.358 sec/step, loss=0.07430, avg_loss=0.07525]\n",
      "Step 502917  [5.373 sec/step, loss=0.07686, avg_loss=0.07535]\n",
      "Step 502918  [5.364 sec/step, loss=0.07715, avg_loss=0.07537]\n",
      "Step 502919  [5.375 sec/step, loss=0.07661, avg_loss=0.07536]\n",
      "Step 502920  [5.369 sec/step, loss=0.07609, avg_loss=0.07537]\n",
      "Step 502921  [5.366 sec/step, loss=0.07775, avg_loss=0.07538]\n",
      "Step 502922  [5.357 sec/step, loss=0.07560, avg_loss=0.07539]\n",
      "Step 502923  [5.361 sec/step, loss=0.07628, avg_loss=0.07542]\n",
      "Step 502924  [5.373 sec/step, loss=0.07724, avg_loss=0.07542]\n",
      "Step 502925  [5.359 sec/step, loss=0.07647, avg_loss=0.07543]\n",
      "Step 502926  [5.345 sec/step, loss=0.07282, avg_loss=0.07541]\n",
      "Step 502927  [5.358 sec/step, loss=0.07703, avg_loss=0.07541]\n",
      "Step 502928  [5.367 sec/step, loss=0.07520, avg_loss=0.07541]\n",
      "Step 502929  [5.379 sec/step, loss=0.07743, avg_loss=0.07545]\n",
      "Step 502930  [5.366 sec/step, loss=0.07360, avg_loss=0.07541]\n",
      "Step 502931  [5.364 sec/step, loss=0.07292, avg_loss=0.07537]\n",
      "Step 502932  [5.371 sec/step, loss=0.07604, avg_loss=0.07538]\n",
      "Step 502933  [5.363 sec/step, loss=0.07450, avg_loss=0.07535]\n",
      "Step 502934  [5.379 sec/step, loss=0.07784, avg_loss=0.07541]\n",
      "Step 502935  [5.312 sec/step, loss=0.06698, avg_loss=0.07541]\n",
      "Step 502936  [5.293 sec/step, loss=0.07578, avg_loss=0.07539]\n",
      "Generated 32 batches of size 32 in 2.358 sec\n",
      "Step 502937  [5.305 sec/step, loss=0.07756, avg_loss=0.07539]\n",
      "Step 502938  [5.333 sec/step, loss=0.07597, avg_loss=0.07546]\n",
      "Step 502939  [5.327 sec/step, loss=0.07676, avg_loss=0.07547]\n",
      "Step 502940  [5.338 sec/step, loss=0.07516, avg_loss=0.07546]\n",
      "Step 502941  [5.320 sec/step, loss=0.07507, avg_loss=0.07543]\n",
      "Step 502942  [5.306 sec/step, loss=0.07255, avg_loss=0.07541]\n",
      "Step 502943  [5.292 sec/step, loss=0.07670, avg_loss=0.07542]\n",
      "Step 502944  [5.303 sec/step, loss=0.07733, avg_loss=0.07543]\n",
      "Step 502945  [5.306 sec/step, loss=0.07797, avg_loss=0.07545]\n",
      "Step 502946  [5.273 sec/step, loss=0.07240, avg_loss=0.07543]\n",
      "Step 502947  [5.258 sec/step, loss=0.07507, avg_loss=0.07540]\n",
      "Step 502948  [5.262 sec/step, loss=0.07522, avg_loss=0.07538]\n",
      "Step 502949  [5.270 sec/step, loss=0.07485, avg_loss=0.07538]\n",
      "Step 502950  [5.332 sec/step, loss=0.06728, avg_loss=0.07531]\n",
      "Step 502951  [5.340 sec/step, loss=0.07802, avg_loss=0.07532]\n",
      "Step 502952  [5.350 sec/step, loss=0.07589, avg_loss=0.07533]\n",
      "Step 502953  [5.361 sec/step, loss=0.07483, avg_loss=0.07530]\n",
      "Step 502954  [5.381 sec/step, loss=0.07625, avg_loss=0.07538]\n",
      "Step 502955  [5.390 sec/step, loss=0.07610, avg_loss=0.07541]\n",
      "Step 502956  [5.391 sec/step, loss=0.07650, avg_loss=0.07540]\n",
      "Step 502957  [5.379 sec/step, loss=0.07293, avg_loss=0.07536]\n",
      "Step 502958  [5.379 sec/step, loss=0.07671, avg_loss=0.07536]\n",
      "Step 502959  [5.390 sec/step, loss=0.07789, avg_loss=0.07540]\n",
      "Step 502960  [5.407 sec/step, loss=0.07598, avg_loss=0.07540]\n",
      "Step 502961  [5.403 sec/step, loss=0.07709, avg_loss=0.07540]\n",
      "Step 502962  [5.414 sec/step, loss=0.07760, avg_loss=0.07540]\n",
      "Step 502963  [5.398 sec/step, loss=0.07590, avg_loss=0.07540]\n",
      "Step 502964  [5.375 sec/step, loss=0.07333, avg_loss=0.07538]\n",
      "Step 502965  [5.369 sec/step, loss=0.07696, avg_loss=0.07538]\n",
      "Step 502966  [5.383 sec/step, loss=0.07796, avg_loss=0.07541]\n",
      "Step 502967  [5.381 sec/step, loss=0.07811, avg_loss=0.07544]\n",
      "Step 502968  [5.364 sec/step, loss=0.07666, avg_loss=0.07543]\n",
      "Generated 32 batches of size 32 in 2.329 sec\n",
      "Step 502969  [5.348 sec/step, loss=0.07634, avg_loss=0.07545]\n",
      "Step 502970  [5.341 sec/step, loss=0.07418, avg_loss=0.07546]\n",
      "Step 502971  [5.344 sec/step, loss=0.07553, avg_loss=0.07546]\n",
      "Step 502972  [5.345 sec/step, loss=0.07766, avg_loss=0.07545]\n",
      "Step 502973  [5.288 sec/step, loss=0.07451, avg_loss=0.07551]\n",
      "Step 502974  [5.266 sec/step, loss=0.06729, avg_loss=0.07542]\n",
      "Step 502975  [5.273 sec/step, loss=0.07556, avg_loss=0.07543]\n",
      "Step 502976  [5.273 sec/step, loss=0.07588, avg_loss=0.07543]\n",
      "Step 502977  [5.281 sec/step, loss=0.07634, avg_loss=0.07544]\n",
      "Step 502978  [5.288 sec/step, loss=0.07676, avg_loss=0.07544]\n",
      "Step 502979  [5.294 sec/step, loss=0.07501, avg_loss=0.07545]\n",
      "Step 502980  [5.306 sec/step, loss=0.07827, avg_loss=0.07551]\n",
      "Step 502981  [5.291 sec/step, loss=0.07329, avg_loss=0.07548]\n",
      "Step 502982  [5.278 sec/step, loss=0.07348, avg_loss=0.07544]\n",
      "Step 502983  [5.287 sec/step, loss=0.07466, avg_loss=0.07540]\n",
      "Step 502984  [5.287 sec/step, loss=0.07692, avg_loss=0.07541]\n",
      "Step 502985  [5.309 sec/step, loss=0.07809, avg_loss=0.07546]\n",
      "Step 502986  [5.326 sec/step, loss=0.07748, avg_loss=0.07549]\n",
      "Step 502987  [5.312 sec/step, loss=0.07666, avg_loss=0.07551]\n",
      "Step 502988  [5.303 sec/step, loss=0.07610, avg_loss=0.07550]\n",
      "Step 502989  [5.311 sec/step, loss=0.07771, avg_loss=0.07552]\n",
      "Step 502990  [5.323 sec/step, loss=0.07643, avg_loss=0.07554]\n",
      "Step 502991  [5.319 sec/step, loss=0.07615, avg_loss=0.07553]\n",
      "Step 502992  [5.330 sec/step, loss=0.07366, avg_loss=0.07554]\n",
      "Step 502993  [5.312 sec/step, loss=0.07407, avg_loss=0.07550]\n",
      "Step 502994  [5.319 sec/step, loss=0.07690, avg_loss=0.07549]\n",
      "Step 502995  [5.337 sec/step, loss=0.07409, avg_loss=0.07547]\n",
      "Step 502996  [5.338 sec/step, loss=0.07526, avg_loss=0.07547]\n",
      "Step 502997  [5.336 sec/step, loss=0.07588, avg_loss=0.07547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 502998  [5.336 sec/step, loss=0.07561, avg_loss=0.07546]\n",
      "Step 502999  [5.305 sec/step, loss=0.07709, avg_loss=0.07549]\n",
      "Step 503000  [5.315 sec/step, loss=0.07621, avg_loss=0.07548]\n",
      "Writing summary at step: 503000\n",
      "Generated 32 batches of size 32 in 2.275 sec\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-503000\n",
      "Saving audio and alignment...\n",
      "Input: vaairlaes raevinjuu kii mansuuxii simfanii daaiigaraam koo mosuul hoo gaii hae~_____________________________________________\n",
      "Step 503001  [5.294 sec/step, loss=0.07339, avg_loss=0.07545]\n",
      "Step 503002  [5.287 sec/step, loss=0.07620, avg_loss=0.07544]\n",
      "Step 503003  [5.276 sec/step, loss=0.07745, avg_loss=0.07544]\n",
      "Step 503004  [5.307 sec/step, loss=0.07687, avg_loss=0.07554]\n",
      "Step 503005  [5.277 sec/step, loss=0.06733, avg_loss=0.07543]\n",
      "Step 503006  [5.331 sec/step, loss=0.06697, avg_loss=0.07536]\n",
      "Step 503007  [5.342 sec/step, loss=0.07549, avg_loss=0.07536]\n",
      "Step 503008  [5.336 sec/step, loss=0.07644, avg_loss=0.07536]\n",
      "Step 503009  [5.335 sec/step, loss=0.07647, avg_loss=0.07535]\n",
      "Step 503010  [5.347 sec/step, loss=0.07708, avg_loss=0.07536]\n",
      "Step 503011  [5.352 sec/step, loss=0.07644, avg_loss=0.07536]\n",
      "Step 503012  [5.349 sec/step, loss=0.07331, avg_loss=0.07532]\n",
      "Step 503013  [5.308 sec/step, loss=0.07746, avg_loss=0.07543]\n",
      "Step 503014  [5.314 sec/step, loss=0.07542, avg_loss=0.07542]\n",
      "Step 503015  [5.282 sec/step, loss=0.07526, avg_loss=0.07550]\n",
      "Step 503016  [5.271 sec/step, loss=0.07562, avg_loss=0.07551]\n",
      "Step 503017  [5.264 sec/step, loss=0.07263, avg_loss=0.07547]\n",
      "Step 503018  [5.239 sec/step, loss=0.07578, avg_loss=0.07545]\n",
      "Step 503019  [5.222 sec/step, loss=0.07830, avg_loss=0.07547]\n",
      "Step 503020  [5.253 sec/step, loss=0.07708, avg_loss=0.07548]\n",
      "Step 503021  [5.240 sec/step, loss=0.07569, avg_loss=0.07546]\n",
      "Step 503022  [5.257 sec/step, loss=0.07879, avg_loss=0.07549]\n",
      "Step 503023  [5.257 sec/step, loss=0.07697, avg_loss=0.07550]\n",
      "Step 503024  [5.248 sec/step, loss=0.07654, avg_loss=0.07549]\n",
      "Step 503025  [5.248 sec/step, loss=0.07687, avg_loss=0.07550]\n",
      "Step 503026  [5.273 sec/step, loss=0.07565, avg_loss=0.07552]\n",
      "Step 503027  [5.271 sec/step, loss=0.07765, avg_loss=0.07553]\n",
      "Step 503028  [5.256 sec/step, loss=0.07381, avg_loss=0.07552]\n",
      "Step 503029  [5.263 sec/step, loss=0.07454, avg_loss=0.07549]\n",
      "Step 503030  [5.270 sec/step, loss=0.07699, avg_loss=0.07552]\n",
      "Generated 32 batches of size 32 in 2.511 sec\n",
      "Step 503031  [5.264 sec/step, loss=0.07421, avg_loss=0.07553]\n",
      "Step 503032  [5.264 sec/step, loss=0.07571, avg_loss=0.07553]\n",
      "Step 503033  [5.276 sec/step, loss=0.07808, avg_loss=0.07557]\n",
      "Step 503034  [5.254 sec/step, loss=0.07450, avg_loss=0.07553]\n",
      "Step 503035  [5.254 sec/step, loss=0.06665, avg_loss=0.07553]\n",
      "Step 503036  [5.313 sec/step, loss=0.06744, avg_loss=0.07545]\n",
      "Step 503037  [5.300 sec/step, loss=0.07699, avg_loss=0.07544]\n",
      "Step 503038  [5.276 sec/step, loss=0.07555, avg_loss=0.07544]\n",
      "Step 503039  [5.276 sec/step, loss=0.07740, avg_loss=0.07544]\n",
      "Step 503040  [5.273 sec/step, loss=0.07717, avg_loss=0.07546]\n",
      "Step 503041  [5.274 sec/step, loss=0.07585, avg_loss=0.07547]\n",
      "Step 503042  [5.286 sec/step, loss=0.07361, avg_loss=0.07548]\n",
      "Step 503043  [5.331 sec/step, loss=0.06828, avg_loss=0.07540]\n",
      "Step 503044  [5.340 sec/step, loss=0.07704, avg_loss=0.07539]\n",
      "Step 503045  [5.337 sec/step, loss=0.07755, avg_loss=0.07539]\n",
      "Step 503046  [5.346 sec/step, loss=0.07586, avg_loss=0.07543]\n",
      "Step 503047  [5.347 sec/step, loss=0.07702, avg_loss=0.07544]\n",
      "Step 503048  [5.327 sec/step, loss=0.06765, avg_loss=0.07537]\n",
      "Step 503049  [5.324 sec/step, loss=0.07473, avg_loss=0.07537]\n",
      "Step 503050  [5.274 sec/step, loss=0.07619, avg_loss=0.07546]\n",
      "Step 503051  [5.253 sec/step, loss=0.07563, avg_loss=0.07543]\n",
      "Step 503052  [5.237 sec/step, loss=0.07593, avg_loss=0.07543]\n",
      "Step 503053  [5.229 sec/step, loss=0.07770, avg_loss=0.07546]\n",
      "Step 503054  [5.225 sec/step, loss=0.07369, avg_loss=0.07544]\n",
      "Step 503055  [5.233 sec/step, loss=0.07759, avg_loss=0.07545]\n",
      "Step 503056  [5.245 sec/step, loss=0.07540, avg_loss=0.07544]\n",
      "Step 503057  [5.247 sec/step, loss=0.07532, avg_loss=0.07546]\n",
      "Step 503058  [5.246 sec/step, loss=0.07757, avg_loss=0.07547]\n",
      "Step 503059  [5.245 sec/step, loss=0.07811, avg_loss=0.07548]\n",
      "Step 503060  [5.236 sec/step, loss=0.07685, avg_loss=0.07548]\n",
      "Step 503061  [5.225 sec/step, loss=0.07240, avg_loss=0.07544]\n",
      "Step 503062  [5.203 sec/step, loss=0.07348, avg_loss=0.07540]\n",
      "Generated 32 batches of size 32 in 2.305 sec\n",
      "Step 503063  [5.222 sec/step, loss=0.07663, avg_loss=0.07540]\n",
      "Step 503064  [5.242 sec/step, loss=0.07833, avg_loss=0.07545]\n",
      "Step 503065  [5.250 sec/step, loss=0.07670, avg_loss=0.07545]\n",
      "Step 503066  [5.239 sec/step, loss=0.07335, avg_loss=0.07540]\n",
      "Step 503067  [5.243 sec/step, loss=0.07670, avg_loss=0.07539]\n",
      "Step 503068  [5.246 sec/step, loss=0.07641, avg_loss=0.07539]\n",
      "Step 503069  [5.249 sec/step, loss=0.07775, avg_loss=0.07540]\n",
      "Step 503070  [5.251 sec/step, loss=0.07663, avg_loss=0.07543]\n",
      "Step 503071  [5.265 sec/step, loss=0.07644, avg_loss=0.07544]\n",
      "Step 503072  [5.261 sec/step, loss=0.07785, avg_loss=0.07544]\n",
      "Step 503073  [5.279 sec/step, loss=0.07610, avg_loss=0.07545]\n",
      "Step 503074  [5.292 sec/step, loss=0.07710, avg_loss=0.07555]\n",
      "Step 503075  [5.269 sec/step, loss=0.07608, avg_loss=0.07556]\n",
      "Step 503076  [5.273 sec/step, loss=0.07699, avg_loss=0.07557]\n",
      "Step 503077  [5.258 sec/step, loss=0.07289, avg_loss=0.07553]\n",
      "Step 503078  [5.261 sec/step, loss=0.07827, avg_loss=0.07555]\n",
      "Step 503079  [5.257 sec/step, loss=0.07198, avg_loss=0.07552]\n",
      "Step 503080  [5.246 sec/step, loss=0.07456, avg_loss=0.07548]\n",
      "Step 503081  [5.267 sec/step, loss=0.07761, avg_loss=0.07552]\n",
      "Step 503082  [5.264 sec/step, loss=0.07345, avg_loss=0.07552]\n",
      "Step 503083  [5.246 sec/step, loss=0.07466, avg_loss=0.07552]\n",
      "Step 503084  [5.253 sec/step, loss=0.07618, avg_loss=0.07552]\n",
      "Step 503085  [5.238 sec/step, loss=0.07508, avg_loss=0.07549]\n",
      "Step 503086  [5.218 sec/step, loss=0.07549, avg_loss=0.07547]\n",
      "Step 503087  [5.215 sec/step, loss=0.07725, avg_loss=0.07547]\n",
      "Step 503088  [5.272 sec/step, loss=0.06758, avg_loss=0.07539]\n",
      "Step 503089  [5.265 sec/step, loss=0.07549, avg_loss=0.07536]\n",
      "Step 503090  [5.253 sec/step, loss=0.07537, avg_loss=0.07535]\n",
      "Step 503091  [5.260 sec/step, loss=0.07755, avg_loss=0.07537]\n",
      "Step 503092  [5.254 sec/step, loss=0.07598, avg_loss=0.07539]\n",
      "Step 503093  [5.267 sec/step, loss=0.07725, avg_loss=0.07542]\n",
      "Step 503094  [5.279 sec/step, loss=0.07517, avg_loss=0.07541]\n",
      "Generated 32 batches of size 32 in 2.258 sec\n",
      "Step 503095  [5.275 sec/step, loss=0.07517, avg_loss=0.07542]\n",
      "Step 503096  [5.258 sec/step, loss=0.06707, avg_loss=0.07533]\n",
      "Step 503097  [5.266 sec/step, loss=0.07681, avg_loss=0.07534]\n",
      "Step 503098  [5.269 sec/step, loss=0.07674, avg_loss=0.07536]\n",
      "Step 503099  [5.288 sec/step, loss=0.07833, avg_loss=0.07537]\n",
      "Step 503100  [5.263 sec/step, loss=0.07543, avg_loss=0.07536]\n",
      "Writing summary at step: 503100\n",
      "Step 503101  [5.281 sec/step, loss=0.07780, avg_loss=0.07540]\n",
      "Step 503102  [5.296 sec/step, loss=0.07469, avg_loss=0.07539]\n",
      "Step 503103  [5.304 sec/step, loss=0.07823, avg_loss=0.07540]\n",
      "Step 503104  [5.279 sec/step, loss=0.07543, avg_loss=0.07538]\n",
      "Step 503105  [5.293 sec/step, loss=0.07443, avg_loss=0.07545]\n",
      "Step 503106  [5.267 sec/step, loss=0.07680, avg_loss=0.07555]\n",
      "Step 503107  [5.258 sec/step, loss=0.07640, avg_loss=0.07556]\n",
      "Step 503108  [5.269 sec/step, loss=0.07753, avg_loss=0.07557]\n",
      "Step 503109  [5.269 sec/step, loss=0.07768, avg_loss=0.07558]\n",
      "Step 503110  [5.281 sec/step, loss=0.07576, avg_loss=0.07557]\n",
      "Step 503111  [5.322 sec/step, loss=0.06736, avg_loss=0.07548]\n",
      "Step 503112  [5.330 sec/step, loss=0.07614, avg_loss=0.07551]\n",
      "Step 503113  [5.317 sec/step, loss=0.07440, avg_loss=0.07548]\n",
      "Step 503114  [5.308 sec/step, loss=0.07509, avg_loss=0.07547]\n",
      "Step 503115  [5.295 sec/step, loss=0.07592, avg_loss=0.07548]\n",
      "Step 503116  [5.311 sec/step, loss=0.07712, avg_loss=0.07550]\n",
      "Step 503117  [5.328 sec/step, loss=0.07538, avg_loss=0.07552]\n",
      "Step 503118  [5.322 sec/step, loss=0.07329, avg_loss=0.07550]\n",
      "Step 503119  [5.313 sec/step, loss=0.07508, avg_loss=0.07547]\n",
      "Step 503120  [5.284 sec/step, loss=0.07507, avg_loss=0.07545]\n",
      "Step 503121  [5.275 sec/step, loss=0.07614, avg_loss=0.07545]\n",
      "Step 503122  [5.274 sec/step, loss=0.07663, avg_loss=0.07543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 503123  [5.285 sec/step, loss=0.07716, avg_loss=0.07543]\n",
      "Step 503124  [5.268 sec/step, loss=0.06681, avg_loss=0.07533]\n",
      "Step 503125  [5.261 sec/step, loss=0.07258, avg_loss=0.07529]\n",
      "Generated 32 batches of size 32 in 2.328 sec\n",
      "Step 503126  [5.256 sec/step, loss=0.07729, avg_loss=0.07531]\n",
      "Step 503127  [5.242 sec/step, loss=0.07693, avg_loss=0.07530]\n",
      "Step 503128  [5.256 sec/step, loss=0.07737, avg_loss=0.07534]\n",
      "Step 503129  [5.258 sec/step, loss=0.07815, avg_loss=0.07537]\n",
      "Step 503130  [5.251 sec/step, loss=0.07716, avg_loss=0.07537]\n",
      "Step 503131  [5.251 sec/step, loss=0.07731, avg_loss=0.07540]\n",
      "Step 503132  [5.270 sec/step, loss=0.07693, avg_loss=0.07542]\n",
      "Step 503133  [5.263 sec/step, loss=0.07662, avg_loss=0.07540]\n",
      "Step 503134  [5.264 sec/step, loss=0.07594, avg_loss=0.07542]\n",
      "Step 503135  [5.273 sec/step, loss=0.07584, avg_loss=0.07551]\n",
      "Step 503136  [5.210 sec/step, loss=0.07386, avg_loss=0.07557]\n",
      "Step 503137  [5.215 sec/step, loss=0.07682, avg_loss=0.07557]\n",
      "Step 503138  [5.227 sec/step, loss=0.07708, avg_loss=0.07559]\n",
      "Step 503139  [5.222 sec/step, loss=0.07595, avg_loss=0.07557]\n",
      "Step 503140  [5.225 sec/step, loss=0.07577, avg_loss=0.07556]\n",
      "Step 503141  [5.239 sec/step, loss=0.07749, avg_loss=0.07557]\n",
      "Step 503142  [5.260 sec/step, loss=0.07493, avg_loss=0.07559]\n",
      "Step 503143  [5.220 sec/step, loss=0.07540, avg_loss=0.07566]\n",
      "Step 503144  [5.207 sec/step, loss=0.07702, avg_loss=0.07566]\n",
      "Step 503145  [5.210 sec/step, loss=0.07805, avg_loss=0.07566]\n",
      "Step 503146  [5.201 sec/step, loss=0.07677, avg_loss=0.07567]\n",
      "Step 503147  [5.201 sec/step, loss=0.07590, avg_loss=0.07566]\n",
      "Step 503148  [5.203 sec/step, loss=0.06909, avg_loss=0.07568]\n",
      "Step 503149  [5.204 sec/step, loss=0.07666, avg_loss=0.07570]\n",
      "Step 503150  [5.218 sec/step, loss=0.07806, avg_loss=0.07571]\n",
      "Step 503151  [5.230 sec/step, loss=0.07605, avg_loss=0.07572]\n",
      "Step 503152  [5.235 sec/step, loss=0.07542, avg_loss=0.07571]\n",
      "Step 503153  [5.215 sec/step, loss=0.07569, avg_loss=0.07569]\n",
      "Step 503154  [5.215 sec/step, loss=0.07722, avg_loss=0.07573]\n",
      "Step 503155  [5.232 sec/step, loss=0.07498, avg_loss=0.07570]\n",
      "Step 503156  [5.214 sec/step, loss=0.07732, avg_loss=0.07572]\n",
      "Step 503157  [5.217 sec/step, loss=0.07440, avg_loss=0.07571]\n",
      "Generated 32 batches of size 32 in 2.350 sec\n",
      "Step 503158  [5.231 sec/step, loss=0.07732, avg_loss=0.07571]\n",
      "Step 503159  [5.222 sec/step, loss=0.07711, avg_loss=0.07570]\n",
      "Step 503160  [5.215 sec/step, loss=0.07512, avg_loss=0.07568]\n",
      "Step 503161  [5.275 sec/step, loss=0.06838, avg_loss=0.07564]\n",
      "Step 503162  [5.280 sec/step, loss=0.07265, avg_loss=0.07563]\n",
      "Step 503163  [5.261 sec/step, loss=0.07428, avg_loss=0.07561]\n",
      "Step 503164  [5.260 sec/step, loss=0.07689, avg_loss=0.07560]\n",
      "Step 503165  [5.267 sec/step, loss=0.07826, avg_loss=0.07561]\n",
      "Step 503166  [5.270 sec/step, loss=0.07662, avg_loss=0.07564]\n",
      "Step 503167  [5.248 sec/step, loss=0.07561, avg_loss=0.07563]\n",
      "Step 503168  [5.275 sec/step, loss=0.07438, avg_loss=0.07561]\n",
      "Step 503169  [5.281 sec/step, loss=0.07433, avg_loss=0.07558]\n",
      "Step 503170  [5.294 sec/step, loss=0.07806, avg_loss=0.07559]\n",
      "Step 503171  [5.283 sec/step, loss=0.07537, avg_loss=0.07558]\n",
      "Step 503172  [5.263 sec/step, loss=0.07278, avg_loss=0.07553]\n",
      "Step 503173  [5.258 sec/step, loss=0.07836, avg_loss=0.07555]\n",
      "Step 503174  [5.262 sec/step, loss=0.07591, avg_loss=0.07554]\n",
      "Step 503175  [5.253 sec/step, loss=0.07492, avg_loss=0.07553]\n",
      "Step 503176  [5.246 sec/step, loss=0.07547, avg_loss=0.07552]\n",
      "Step 503177  [5.248 sec/step, loss=0.07534, avg_loss=0.07554]\n",
      "Step 503178  [5.252 sec/step, loss=0.07835, avg_loss=0.07554]\n",
      "Step 503179  [5.251 sec/step, loss=0.07312, avg_loss=0.07555]\n",
      "Step 503180  [5.251 sec/step, loss=0.07355, avg_loss=0.07554]\n",
      "Step 503181  [5.244 sec/step, loss=0.07687, avg_loss=0.07553]\n",
      "Step 503182  [5.257 sec/step, loss=0.07714, avg_loss=0.07557]\n",
      "Step 503183  [5.277 sec/step, loss=0.07700, avg_loss=0.07560]\n",
      "Step 503184  [5.322 sec/step, loss=0.06846, avg_loss=0.07552]\n",
      "Step 503185  [5.332 sec/step, loss=0.07685, avg_loss=0.07554]\n",
      "Step 503186  [5.348 sec/step, loss=0.07766, avg_loss=0.07556]\n",
      "Step 503187  [5.349 sec/step, loss=0.07766, avg_loss=0.07556]\n",
      "Step 503188  [5.312 sec/step, loss=0.07524, avg_loss=0.07564]\n",
      "Step 503189  [5.304 sec/step, loss=0.07434, avg_loss=0.07563]\n",
      "Generated 32 batches of size 32 in 2.476 sec\n",
      "Step 503190  [5.311 sec/step, loss=0.07692, avg_loss=0.07564]\n",
      "Step 503191  [5.296 sec/step, loss=0.07302, avg_loss=0.07560]\n",
      "Step 503192  [5.299 sec/step, loss=0.07699, avg_loss=0.07561]\n",
      "Step 503193  [5.276 sec/step, loss=0.06782, avg_loss=0.07551]\n",
      "Step 503194  [5.252 sec/step, loss=0.07681, avg_loss=0.07553]\n",
      "Step 503195  [5.237 sec/step, loss=0.07726, avg_loss=0.07555]\n",
      "Step 503196  [5.266 sec/step, loss=0.07524, avg_loss=0.07563]\n",
      "Step 503197  [5.271 sec/step, loss=0.07496, avg_loss=0.07561]\n",
      "Step 503198  [5.273 sec/step, loss=0.07749, avg_loss=0.07562]\n",
      "Step 503199  [5.264 sec/step, loss=0.07661, avg_loss=0.07560]\n",
      "Step 503200  [5.325 sec/step, loss=0.06635, avg_loss=0.07551]\n",
      "Writing summary at step: 503200\n",
      "Step 503201  [5.321 sec/step, loss=0.07659, avg_loss=0.07550]\n",
      "Step 503202  [5.292 sec/step, loss=0.06758, avg_loss=0.07543]\n",
      "Step 503203  [5.276 sec/step, loss=0.07616, avg_loss=0.07541]\n",
      "Step 503204  [5.289 sec/step, loss=0.07671, avg_loss=0.07542]\n",
      "Step 503205  [5.289 sec/step, loss=0.07614, avg_loss=0.07544]\n",
      "Step 503206  [5.275 sec/step, loss=0.07591, avg_loss=0.07543]\n",
      "Step 503207  [5.268 sec/step, loss=0.07547, avg_loss=0.07542]\n",
      "Step 503208  [5.274 sec/step, loss=0.07668, avg_loss=0.07541]\n",
      "Step 503209  [5.275 sec/step, loss=0.07796, avg_loss=0.07541]\n",
      "Step 503210  [5.272 sec/step, loss=0.07798, avg_loss=0.07544]\n",
      "Step 503211  [5.226 sec/step, loss=0.07621, avg_loss=0.07553]\n",
      "Step 503212  [5.218 sec/step, loss=0.07713, avg_loss=0.07554]\n",
      "Step 503213  [5.223 sec/step, loss=0.07360, avg_loss=0.07553]\n",
      "Step 503214  [5.218 sec/step, loss=0.07261, avg_loss=0.07550]\n",
      "Step 503215  [5.226 sec/step, loss=0.07704, avg_loss=0.07551]\n",
      "Step 503216  [5.225 sec/step, loss=0.07846, avg_loss=0.07553]\n",
      "Step 503217  [5.205 sec/step, loss=0.07296, avg_loss=0.07550]\n",
      "Step 503218  [5.226 sec/step, loss=0.07749, avg_loss=0.07554]\n",
      "Step 503219  [5.229 sec/step, loss=0.07711, avg_loss=0.07557]\n",
      "Step 503220  [5.231 sec/step, loss=0.07691, avg_loss=0.07558]\n",
      "Generated 32 batches of size 32 in 2.486 sec\n",
      "Step 503221  [5.242 sec/step, loss=0.07428, avg_loss=0.07556]\n",
      "Step 503222  [5.247 sec/step, loss=0.07654, avg_loss=0.07556]\n",
      "Step 503223  [5.247 sec/step, loss=0.07724, avg_loss=0.07556]\n",
      "Step 503224  [5.289 sec/step, loss=0.07613, avg_loss=0.07566]\n",
      "Step 503225  [5.296 sec/step, loss=0.07529, avg_loss=0.07569]\n",
      "Step 503226  [5.293 sec/step, loss=0.07665, avg_loss=0.07568]\n",
      "Step 503227  [5.298 sec/step, loss=0.07796, avg_loss=0.07569]\n",
      "Step 503228  [5.292 sec/step, loss=0.07530, avg_loss=0.07567]\n",
      "Step 503229  [5.281 sec/step, loss=0.07642, avg_loss=0.07565]\n",
      "Step 503230  [5.333 sec/step, loss=0.06889, avg_loss=0.07557]\n",
      "Step 503231  [5.337 sec/step, loss=0.07625, avg_loss=0.07556]\n",
      "Step 503232  [5.319 sec/step, loss=0.07504, avg_loss=0.07554]\n",
      "Step 503233  [5.313 sec/step, loss=0.07704, avg_loss=0.07554]\n",
      "Step 503234  [5.311 sec/step, loss=0.07232, avg_loss=0.07551]\n",
      "Step 503235  [5.319 sec/step, loss=0.07653, avg_loss=0.07551]\n",
      "Step 503236  [5.328 sec/step, loss=0.07691, avg_loss=0.07554]\n",
      "Step 503237  [5.332 sec/step, loss=0.07748, avg_loss=0.07555]\n",
      "Step 503238  [5.327 sec/step, loss=0.07537, avg_loss=0.07553]\n",
      "Step 503239  [5.332 sec/step, loss=0.07539, avg_loss=0.07553]\n",
      "Step 503240  [5.333 sec/step, loss=0.07800, avg_loss=0.07555]\n",
      "Step 503241  [5.327 sec/step, loss=0.07736, avg_loss=0.07555]\n",
      "Step 503242  [5.314 sec/step, loss=0.07793, avg_loss=0.07558]\n",
      "Step 503243  [5.296 sec/step, loss=0.07520, avg_loss=0.07558]\n",
      "Step 503244  [5.284 sec/step, loss=0.07454, avg_loss=0.07555]\n",
      "Step 503245  [5.283 sec/step, loss=0.07839, avg_loss=0.07556]\n",
      "Step 503246  [5.276 sec/step, loss=0.07584, avg_loss=0.07555]\n",
      "Step 503247  [5.279 sec/step, loss=0.07753, avg_loss=0.07556]\n",
      "Step 503248  [5.311 sec/step, loss=0.07512, avg_loss=0.07562]\n",
      "Step 503249  [5.322 sec/step, loss=0.07645, avg_loss=0.07562]\n",
      "Step 503250  [5.322 sec/step, loss=0.07810, avg_loss=0.07562]\n",
      "Step 503251  [5.331 sec/step, loss=0.07564, avg_loss=0.07562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 503252  [5.348 sec/step, loss=0.07499, avg_loss=0.07561]\n",
      "Generated 32 batches of size 32 in 2.402 sec\n",
      "Step 503253  [5.388 sec/step, loss=0.07527, avg_loss=0.07561]\n",
      "Step 503254  [5.386 sec/step, loss=0.07591, avg_loss=0.07560]\n",
      "Step 503255  [5.366 sec/step, loss=0.07667, avg_loss=0.07561]\n",
      "Step 503256  [5.346 sec/step, loss=0.07406, avg_loss=0.07558]\n",
      "Step 503257  [5.355 sec/step, loss=0.07733, avg_loss=0.07561]\n",
      "Step 503258  [5.325 sec/step, loss=0.06825, avg_loss=0.07552]\n",
      "Step 503259  [5.326 sec/step, loss=0.07690, avg_loss=0.07552]\n",
      "Step 503260  [5.329 sec/step, loss=0.07296, avg_loss=0.07549]\n",
      "Step 503261  [5.272 sec/step, loss=0.07503, avg_loss=0.07556]\n",
      "Step 503262  [5.275 sec/step, loss=0.07389, avg_loss=0.07557]\n",
      "Step 503263  [5.335 sec/step, loss=0.06745, avg_loss=0.07551]\n",
      "Step 503264  [5.341 sec/step, loss=0.07704, avg_loss=0.07551]\n",
      "Step 503265  [5.325 sec/step, loss=0.07698, avg_loss=0.07549]\n",
      "Step 503266  [5.329 sec/step, loss=0.07713, avg_loss=0.07550]\n",
      "Step 503267  [5.344 sec/step, loss=0.07780, avg_loss=0.07552]\n",
      "Step 503268  [5.319 sec/step, loss=0.07696, avg_loss=0.07555]\n",
      "Step 503269  [5.287 sec/step, loss=0.07259, avg_loss=0.07553]\n",
      "Step 503270  [5.266 sec/step, loss=0.07581, avg_loss=0.07551]\n",
      "Step 503271  [5.259 sec/step, loss=0.07573, avg_loss=0.07551]\n",
      "Step 503272  [5.282 sec/step, loss=0.07614, avg_loss=0.07554]\n",
      "Step 503273  [5.287 sec/step, loss=0.07766, avg_loss=0.07554]\n",
      "Step 503274  [5.282 sec/step, loss=0.07529, avg_loss=0.07553]\n",
      "Step 503275  [5.286 sec/step, loss=0.07581, avg_loss=0.07554]\n",
      "Step 503276  [5.297 sec/step, loss=0.07820, avg_loss=0.07557]\n",
      "Step 503277  [5.314 sec/step, loss=0.07638, avg_loss=0.07558]\n",
      "Step 503278  [5.302 sec/step, loss=0.07614, avg_loss=0.07556]\n",
      "Step 503279  [5.331 sec/step, loss=0.07424, avg_loss=0.07557]\n",
      "Step 503280  [5.343 sec/step, loss=0.07829, avg_loss=0.07561]\n",
      "Step 503281  [5.330 sec/step, loss=0.07223, avg_loss=0.07557]\n",
      "Step 503282  [5.334 sec/step, loss=0.07704, avg_loss=0.07557]\n",
      "Step 503283  [5.314 sec/step, loss=0.07470, avg_loss=0.07554]\n",
      "Step 503284  [5.250 sec/step, loss=0.06717, avg_loss=0.07553]\n",
      "Generated 32 batches of size 32 in 2.461 sec\n",
      "Step 503285  [5.247 sec/step, loss=0.07305, avg_loss=0.07549]\n",
      "Step 503286  [5.267 sec/step, loss=0.07465, avg_loss=0.07546]\n",
      "Step 503287  [5.265 sec/step, loss=0.07474, avg_loss=0.07543]\n",
      "Step 503288  [5.254 sec/step, loss=0.07737, avg_loss=0.07546]\n",
      "Step 503289  [5.266 sec/step, loss=0.07634, avg_loss=0.07548]\n",
      "Step 503290  [5.269 sec/step, loss=0.07595, avg_loss=0.07547]\n",
      "Step 503291  [5.268 sec/step, loss=0.07720, avg_loss=0.07551]\n",
      "Step 503292  [5.272 sec/step, loss=0.07744, avg_loss=0.07551]\n",
      "Step 503293  [5.297 sec/step, loss=0.07439, avg_loss=0.07558]\n",
      "Step 503294  [5.296 sec/step, loss=0.07530, avg_loss=0.07556]\n",
      "Step 503295  [5.279 sec/step, loss=0.07551, avg_loss=0.07554]\n",
      "Step 503296  [5.284 sec/step, loss=0.07694, avg_loss=0.07556]\n",
      "Step 503297  [5.262 sec/step, loss=0.06763, avg_loss=0.07549]\n",
      "Step 503298  [5.258 sec/step, loss=0.07648, avg_loss=0.07548]\n",
      "Step 503299  [5.264 sec/step, loss=0.07756, avg_loss=0.07549]\n",
      "Step 503300  [5.238 sec/step, loss=0.07616, avg_loss=0.07559]\n",
      "Writing summary at step: 503300\n",
      "Step 503301  [5.227 sec/step, loss=0.07593, avg_loss=0.07558]\n",
      "Step 503302  [5.243 sec/step, loss=0.07677, avg_loss=0.07567]\n",
      "Step 503303  [5.258 sec/step, loss=0.07705, avg_loss=0.07568]\n",
      "Step 503304  [5.253 sec/step, loss=0.07666, avg_loss=0.07568]\n",
      "Step 503305  [5.243 sec/step, loss=0.07290, avg_loss=0.07565]\n",
      "Step 503306  [5.245 sec/step, loss=0.07773, avg_loss=0.07567]\n",
      "Step 503307  [5.245 sec/step, loss=0.07559, avg_loss=0.07567]\n",
      "Step 503308  [5.226 sec/step, loss=0.07552, avg_loss=0.07566]\n",
      "Step 503309  [5.267 sec/step, loss=0.06719, avg_loss=0.07555]\n",
      "Step 503310  [5.243 sec/step, loss=0.07332, avg_loss=0.07550]\n",
      "Step 503311  [5.243 sec/step, loss=0.07643, avg_loss=0.07550]\n",
      "Step 503312  [5.248 sec/step, loss=0.07680, avg_loss=0.07550]\n",
      "Step 503313  [5.251 sec/step, loss=0.07631, avg_loss=0.07553]\n",
      "Step 503314  [5.266 sec/step, loss=0.07835, avg_loss=0.07558]\n",
      "Step 503315  [5.249 sec/step, loss=0.07476, avg_loss=0.07556]\n",
      "Generated 32 batches of size 32 in 2.378 sec\n",
      "Step 503316  [5.263 sec/step, loss=0.07688, avg_loss=0.07555]\n",
      "Step 503317  [5.283 sec/step, loss=0.07644, avg_loss=0.07558]\n",
      "Step 503318  [5.276 sec/step, loss=0.07648, avg_loss=0.07557]\n",
      "Step 503319  [5.275 sec/step, loss=0.07609, avg_loss=0.07556]\n",
      "Step 503320  [5.289 sec/step, loss=0.07576, avg_loss=0.07555]\n",
      "Step 503321  [5.285 sec/step, loss=0.07692, avg_loss=0.07558]\n",
      "Step 503322  [5.265 sec/step, loss=0.07261, avg_loss=0.07554]\n",
      "Step 503323  [5.262 sec/step, loss=0.07605, avg_loss=0.07552]\n",
      "Step 503324  [5.247 sec/step, loss=0.07812, avg_loss=0.07554]\n",
      "Step 503325  [5.256 sec/step, loss=0.07755, avg_loss=0.07557]\n",
      "Step 503326  [5.263 sec/step, loss=0.07621, avg_loss=0.07556]\n",
      "Step 503327  [5.261 sec/step, loss=0.07589, avg_loss=0.07554]\n",
      "Step 503328  [5.262 sec/step, loss=0.07675, avg_loss=0.07556]\n",
      "Step 503329  [5.257 sec/step, loss=0.07295, avg_loss=0.07552]\n",
      "Step 503330  [5.210 sec/step, loss=0.07741, avg_loss=0.07561]\n",
      "Step 503331  [5.227 sec/step, loss=0.07530, avg_loss=0.07560]\n",
      "Step 503332  [5.225 sec/step, loss=0.07486, avg_loss=0.07560]\n",
      "Step 503333  [5.235 sec/step, loss=0.07374, avg_loss=0.07556]\n",
      "Step 503334  [5.239 sec/step, loss=0.07526, avg_loss=0.07559]\n",
      "Step 503335  [5.249 sec/step, loss=0.07745, avg_loss=0.07560]\n",
      "Step 503336  [5.245 sec/step, loss=0.07597, avg_loss=0.07559]\n",
      "Step 503337  [5.253 sec/step, loss=0.07624, avg_loss=0.07558]\n",
      "Step 503338  [5.242 sec/step, loss=0.06669, avg_loss=0.07549]\n",
      "Step 503339  [5.262 sec/step, loss=0.07378, avg_loss=0.07548]\n",
      "Step 503340  [5.245 sec/step, loss=0.07485, avg_loss=0.07544]\n",
      "Step 503341  [5.240 sec/step, loss=0.07694, avg_loss=0.07544]\n",
      "Step 503342  [5.240 sec/step, loss=0.07652, avg_loss=0.07543]\n",
      "Step 503343  [5.253 sec/step, loss=0.07638, avg_loss=0.07544]\n",
      "Step 503344  [5.247 sec/step, loss=0.07331, avg_loss=0.07543]\n",
      "Step 503345  [5.289 sec/step, loss=0.06879, avg_loss=0.07533]\n",
      "Step 503346  [5.309 sec/step, loss=0.07762, avg_loss=0.07535]\n",
      "Step 503347  [5.317 sec/step, loss=0.07479, avg_loss=0.07532]\n",
      "Generated 32 batches of size 32 in 2.306 sec\n",
      "Step 503348  [5.306 sec/step, loss=0.07369, avg_loss=0.07531]\n",
      "Step 503349  [5.308 sec/step, loss=0.07821, avg_loss=0.07532]\n",
      "Step 503350  [5.288 sec/step, loss=0.07481, avg_loss=0.07529]\n",
      "Step 503351  [5.265 sec/step, loss=0.07209, avg_loss=0.07526]\n",
      "Step 503352  [5.256 sec/step, loss=0.07712, avg_loss=0.07528]\n",
      "Step 503353  [5.214 sec/step, loss=0.07565, avg_loss=0.07528]\n",
      "Step 503354  [5.223 sec/step, loss=0.07777, avg_loss=0.07530]\n",
      "Step 503355  [5.233 sec/step, loss=0.07484, avg_loss=0.07528]\n",
      "Step 503356  [5.250 sec/step, loss=0.07684, avg_loss=0.07531]\n",
      "Step 503357  [5.241 sec/step, loss=0.07532, avg_loss=0.07529]\n",
      "Step 503358  [5.271 sec/step, loss=0.07627, avg_loss=0.07537]\n",
      "Step 503359  [5.266 sec/step, loss=0.07713, avg_loss=0.07537]\n",
      "Step 503360  [5.279 sec/step, loss=0.07723, avg_loss=0.07541]\n",
      "Step 503361  [5.291 sec/step, loss=0.07795, avg_loss=0.07544]\n",
      "Step 503362  [5.307 sec/step, loss=0.07510, avg_loss=0.07545]\n",
      "Step 503363  [5.241 sec/step, loss=0.06926, avg_loss=0.07547]\n",
      "Step 503364  [5.228 sec/step, loss=0.07646, avg_loss=0.07547]\n",
      "Step 503365  [5.230 sec/step, loss=0.07581, avg_loss=0.07546]\n",
      "Step 503366  [5.217 sec/step, loss=0.07197, avg_loss=0.07540]\n",
      "Step 503367  [5.204 sec/step, loss=0.07388, avg_loss=0.07536]\n",
      "Step 503368  [5.216 sec/step, loss=0.07697, avg_loss=0.07536]\n",
      "Step 503369  [5.238 sec/step, loss=0.07778, avg_loss=0.07542]\n",
      "Step 503370  [5.251 sec/step, loss=0.07618, avg_loss=0.07542]\n",
      "Step 503371  [5.269 sec/step, loss=0.07672, avg_loss=0.07543]\n",
      "Step 503372  [5.251 sec/step, loss=0.07538, avg_loss=0.07542]\n",
      "Step 503373  [5.262 sec/step, loss=0.07614, avg_loss=0.07541]\n",
      "Step 503374  [5.260 sec/step, loss=0.07439, avg_loss=0.07540]\n",
      "Step 503375  [5.262 sec/step, loss=0.07727, avg_loss=0.07541]\n",
      "Step 503376  [5.249 sec/step, loss=0.07706, avg_loss=0.07540]\n",
      "Step 503377  [5.246 sec/step, loss=0.07473, avg_loss=0.07538]\n",
      "Step 503378  [5.248 sec/step, loss=0.07754, avg_loss=0.07540]\n",
      "Step 503379  [5.245 sec/step, loss=0.07735, avg_loss=0.07543]\n",
      "Generated 32 batches of size 32 in 2.306 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 503380  [5.243 sec/step, loss=0.07660, avg_loss=0.07541]\n",
      "Step 503381  [5.253 sec/step, loss=0.07475, avg_loss=0.07544]\n",
      "Step 503382  [5.232 sec/step, loss=0.07550, avg_loss=0.07542]\n",
      "Step 503383  [5.249 sec/step, loss=0.07486, avg_loss=0.07542]\n",
      "Step 503384  [5.262 sec/step, loss=0.07241, avg_loss=0.07548]\n",
      "Step 503385  [5.271 sec/step, loss=0.07817, avg_loss=0.07553]\n",
      "Step 503386  [5.231 sec/step, loss=0.07311, avg_loss=0.07551]\n",
      "Step 503387  [5.234 sec/step, loss=0.07655, avg_loss=0.07553]\n",
      "Step 503388  [5.282 sec/step, loss=0.06694, avg_loss=0.07543]\n",
      "Step 503389  [5.289 sec/step, loss=0.07782, avg_loss=0.07544]\n",
      "Step 503390  [5.276 sec/step, loss=0.07524, avg_loss=0.07543]\n",
      "Step 503391  [5.292 sec/step, loss=0.07750, avg_loss=0.07544]\n",
      "Step 503392  [5.276 sec/step, loss=0.06543, avg_loss=0.07532]\n",
      "Step 503393  [5.275 sec/step, loss=0.07716, avg_loss=0.07534]\n",
      "Step 503394  [5.288 sec/step, loss=0.07729, avg_loss=0.07536]\n",
      "Step 503395  [5.295 sec/step, loss=0.07625, avg_loss=0.07537]\n",
      "Step 503396  [5.276 sec/step, loss=0.07309, avg_loss=0.07533]\n",
      "Step 503397  [5.289 sec/step, loss=0.07653, avg_loss=0.07542]\n",
      "Step 503398  [5.300 sec/step, loss=0.07576, avg_loss=0.07542]\n",
      "Step 503399  [5.294 sec/step, loss=0.07641, avg_loss=0.07540]\n",
      "Step 503400  [5.263 sec/step, loss=0.07578, avg_loss=0.07540]\n",
      "Writing summary at step: 503400\n",
      "Step 503401  [5.270 sec/step, loss=0.07712, avg_loss=0.07541]\n",
      "Step 503402  [5.271 sec/step, loss=0.07724, avg_loss=0.07542]\n",
      "Step 503403  [5.265 sec/step, loss=0.07690, avg_loss=0.07542]\n",
      "Step 503404  [5.256 sec/step, loss=0.07594, avg_loss=0.07541]\n",
      "Step 503405  [5.270 sec/step, loss=0.07636, avg_loss=0.07544]\n",
      "Step 503406  [5.245 sec/step, loss=0.07329, avg_loss=0.07540]\n",
      "Step 503407  [5.260 sec/step, loss=0.07804, avg_loss=0.07542]\n",
      "Step 503408  [5.255 sec/step, loss=0.07253, avg_loss=0.07539]\n",
      "Step 503409  [5.255 sec/step, loss=0.06780, avg_loss=0.07540]\n",
      "Step 503410  [5.267 sec/step, loss=0.07600, avg_loss=0.07543]\n",
      "Generated 32 batches of size 32 in 2.310 sec\n",
      "Step 503411  [5.277 sec/step, loss=0.07468, avg_loss=0.07541]\n",
      "Step 503412  [5.276 sec/step, loss=0.07722, avg_loss=0.07541]\n",
      "Step 503413  [5.278 sec/step, loss=0.07723, avg_loss=0.07542]\n",
      "Step 503414  [5.298 sec/step, loss=0.07494, avg_loss=0.07539]\n",
      "Step 503415  [5.315 sec/step, loss=0.07714, avg_loss=0.07541]\n",
      "Step 503416  [5.299 sec/step, loss=0.07687, avg_loss=0.07541]\n",
      "Step 503417  [5.286 sec/step, loss=0.07527, avg_loss=0.07540]\n",
      "Step 503418  [5.286 sec/step, loss=0.07613, avg_loss=0.07540]\n",
      "Step 503419  [5.281 sec/step, loss=0.07425, avg_loss=0.07538]\n",
      "Step 503420  [5.282 sec/step, loss=0.07737, avg_loss=0.07539]\n",
      "Step 503421  [5.334 sec/step, loss=0.06765, avg_loss=0.07530]\n",
      "Step 503422  [5.341 sec/step, loss=0.07332, avg_loss=0.07531]\n",
      "Step 503423  [5.323 sec/step, loss=0.07327, avg_loss=0.07528]\n",
      "Step 503424  [5.307 sec/step, loss=0.07510, avg_loss=0.07525]\n",
      "Step 503425  [5.291 sec/step, loss=0.07575, avg_loss=0.07523]\n",
      "Step 503426  [5.286 sec/step, loss=0.07665, avg_loss=0.07524]\n",
      "Step 503427  [5.290 sec/step, loss=0.07688, avg_loss=0.07525]\n",
      "Step 503428  [5.310 sec/step, loss=0.07676, avg_loss=0.07525]\n",
      "Step 503429  [5.325 sec/step, loss=0.07563, avg_loss=0.07527]\n",
      "Step 503430  [5.324 sec/step, loss=0.07468, avg_loss=0.07525]\n",
      "Step 503431  [5.310 sec/step, loss=0.07827, avg_loss=0.07528]\n",
      "Step 503432  [5.322 sec/step, loss=0.07761, avg_loss=0.07530]\n",
      "Step 503433  [5.319 sec/step, loss=0.07662, avg_loss=0.07533]\n",
      "Step 503434  [5.350 sec/step, loss=0.07501, avg_loss=0.07533]\n",
      "Step 503435  [5.336 sec/step, loss=0.07713, avg_loss=0.07533]\n",
      "Step 503436  [5.358 sec/step, loss=0.07852, avg_loss=0.07535]\n",
      "Step 503437  [5.340 sec/step, loss=0.07666, avg_loss=0.07536]\n",
      "Step 503438  [5.361 sec/step, loss=0.07758, avg_loss=0.07546]\n",
      "Step 503439  [5.323 sec/step, loss=0.07282, avg_loss=0.07546]\n",
      "Step 503440  [5.322 sec/step, loss=0.07163, avg_loss=0.07542]\n",
      "Step 503441  [5.319 sec/step, loss=0.07547, avg_loss=0.07541]\n",
      "Step 503442  [5.314 sec/step, loss=0.07409, avg_loss=0.07538]\n",
      "Generated 32 batches of size 32 in 2.504 sec\n",
      "Step 503443  [5.311 sec/step, loss=0.07494, avg_loss=0.07537]\n",
      "Step 503444  [5.309 sec/step, loss=0.06788, avg_loss=0.07532]\n",
      "Step 503445  [5.269 sec/step, loss=0.07596, avg_loss=0.07539]\n",
      "Step 503446  [5.265 sec/step, loss=0.07688, avg_loss=0.07538]\n",
      "Step 503447  [5.257 sec/step, loss=0.07741, avg_loss=0.07541]\n",
      "Step 503448  [5.248 sec/step, loss=0.07551, avg_loss=0.07542]\n",
      "Step 503449  [5.247 sec/step, loss=0.07767, avg_loss=0.07542]\n",
      "Step 503450  [5.253 sec/step, loss=0.07603, avg_loss=0.07543]\n",
      "Step 503451  [5.263 sec/step, loss=0.07645, avg_loss=0.07547]\n",
      "Step 503452  [5.255 sec/step, loss=0.07509, avg_loss=0.07545]\n",
      "Step 503453  [5.291 sec/step, loss=0.07456, avg_loss=0.07544]\n",
      "Step 503454  [5.282 sec/step, loss=0.07660, avg_loss=0.07543]\n",
      "Step 503455  [5.257 sec/step, loss=0.07277, avg_loss=0.07541]\n",
      "Step 503456  [5.254 sec/step, loss=0.07784, avg_loss=0.07542]\n",
      "Step 503457  [5.263 sec/step, loss=0.07486, avg_loss=0.07542]\n",
      "Step 503458  [5.247 sec/step, loss=0.07593, avg_loss=0.07541]\n",
      "Step 503459  [5.268 sec/step, loss=0.07715, avg_loss=0.07541]\n",
      "Step 503460  [5.253 sec/step, loss=0.07359, avg_loss=0.07538]\n",
      "Step 503461  [5.249 sec/step, loss=0.07702, avg_loss=0.07537]\n",
      "Step 503462  [5.248 sec/step, loss=0.07839, avg_loss=0.07540]\n",
      "Step 503463  [5.268 sec/step, loss=0.07647, avg_loss=0.07547]\n",
      "Step 503464  [5.277 sec/step, loss=0.07803, avg_loss=0.07549]\n",
      "Step 503465  [5.278 sec/step, loss=0.07386, avg_loss=0.07547]\n",
      "Step 503466  [5.283 sec/step, loss=0.07536, avg_loss=0.07550]\n",
      "Step 503467  [5.279 sec/step, loss=0.07556, avg_loss=0.07552]\n",
      "Step 503468  [5.264 sec/step, loss=0.07713, avg_loss=0.07552]\n",
      "Step 503469  [5.267 sec/step, loss=0.07770, avg_loss=0.07552]\n",
      "Step 503470  [5.255 sec/step, loss=0.07552, avg_loss=0.07551]\n",
      "Step 503471  [5.260 sec/step, loss=0.07773, avg_loss=0.07552]\n",
      "Step 503472  [5.278 sec/step, loss=0.07487, avg_loss=0.07552]\n",
      "Step 503473  [5.264 sec/step, loss=0.07531, avg_loss=0.07551]\n",
      "Step 503474  [5.258 sec/step, loss=0.07384, avg_loss=0.07550]\n",
      "Generated 32 batches of size 32 in 2.871 sec\n",
      "Step 503475  [5.247 sec/step, loss=0.06791, avg_loss=0.07541]\n",
      "Step 503476  [5.254 sec/step, loss=0.07460, avg_loss=0.07539]\n",
      "Step 503477  [5.301 sec/step, loss=0.06831, avg_loss=0.07532]\n",
      "Step 503478  [5.295 sec/step, loss=0.07626, avg_loss=0.07531]\n",
      "Step 503479  [5.275 sec/step, loss=0.07541, avg_loss=0.07529]\n",
      "Step 503480  [5.267 sec/step, loss=0.07527, avg_loss=0.07528]\n",
      "Step 503481  [5.278 sec/step, loss=0.07787, avg_loss=0.07531]\n",
      "Step 503482  [5.301 sec/step, loss=0.07740, avg_loss=0.07533]\n",
      "Step 503483  [5.296 sec/step, loss=0.07695, avg_loss=0.07535]\n",
      "Step 503484  [5.306 sec/step, loss=0.07713, avg_loss=0.07540]\n",
      "Step 503485  [5.293 sec/step, loss=0.07393, avg_loss=0.07535]\n",
      "Step 503486  [5.293 sec/step, loss=0.07396, avg_loss=0.07536]\n",
      "Step 503487  [5.301 sec/step, loss=0.07738, avg_loss=0.07537]\n",
      "Step 503488  [5.264 sec/step, loss=0.07745, avg_loss=0.07547]\n",
      "Step 503489  [5.247 sec/step, loss=0.07517, avg_loss=0.07545]\n",
      "Step 503490  [5.246 sec/step, loss=0.07544, avg_loss=0.07545]\n",
      "Step 503491  [5.242 sec/step, loss=0.07846, avg_loss=0.07546]\n",
      "Step 503492  [5.261 sec/step, loss=0.07481, avg_loss=0.07555]\n",
      "Step 503493  [5.248 sec/step, loss=0.07677, avg_loss=0.07555]\n",
      "Step 503494  [5.227 sec/step, loss=0.07561, avg_loss=0.07553]\n",
      "Step 503495  [5.228 sec/step, loss=0.07515, avg_loss=0.07552]\n",
      "Step 503496  [5.256 sec/step, loss=0.07496, avg_loss=0.07554]\n",
      "Step 503497  [5.310 sec/step, loss=0.06724, avg_loss=0.07545]\n",
      "Step 503498  [5.301 sec/step, loss=0.07655, avg_loss=0.07546]\n",
      "Step 503499  [5.302 sec/step, loss=0.07443, avg_loss=0.07544]\n",
      "Step 503500  [5.312 sec/step, loss=0.07715, avg_loss=0.07545]\n",
      "Writing summary at step: 503500\n",
      "Step 503501  [5.328 sec/step, loss=0.07795, avg_loss=0.07546]\n",
      "Step 503502  [5.339 sec/step, loss=0.07547, avg_loss=0.07544]\n",
      "Step 503503  [5.348 sec/step, loss=0.07749, avg_loss=0.07545]\n",
      "Step 503504  [5.369 sec/step, loss=0.07585, avg_loss=0.07545]\n",
      "Step 503505  [5.357 sec/step, loss=0.07206, avg_loss=0.07540]\n",
      "Generated 32 batches of size 32 in 2.392 sec\n",
      "Step 503506  [5.394 sec/step, loss=0.07523, avg_loss=0.07542]\n",
      "Step 503507  [5.388 sec/step, loss=0.07685, avg_loss=0.07541]\n",
      "Step 503508  [5.390 sec/step, loss=0.07318, avg_loss=0.07542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 503509  [5.345 sec/step, loss=0.07652, avg_loss=0.07550]\n",
      "Step 503510  [5.343 sec/step, loss=0.07523, avg_loss=0.07550]\n",
      "Step 503511  [5.336 sec/step, loss=0.07772, avg_loss=0.07553]\n",
      "Step 503512  [5.336 sec/step, loss=0.07641, avg_loss=0.07552]\n",
      "Step 503513  [5.315 sec/step, loss=0.06826, avg_loss=0.07543]\n",
      "Step 503514  [5.286 sec/step, loss=0.07650, avg_loss=0.07544]\n",
      "Step 503515  [5.270 sec/step, loss=0.07525, avg_loss=0.07542]\n",
      "Step 503516  [5.261 sec/step, loss=0.07508, avg_loss=0.07541]\n",
      "Step 503517  [5.260 sec/step, loss=0.07497, avg_loss=0.07540]\n",
      "Step 503518  [5.273 sec/step, loss=0.07713, avg_loss=0.07541]\n",
      "Step 503519  [5.280 sec/step, loss=0.07779, avg_loss=0.07545]\n",
      "Step 503520  [5.272 sec/step, loss=0.07705, avg_loss=0.07545]\n",
      "Step 503521  [5.207 sec/step, loss=0.06642, avg_loss=0.07543]\n",
      "Step 503522  [5.198 sec/step, loss=0.07344, avg_loss=0.07544]\n",
      "Step 503523  [5.204 sec/step, loss=0.07618, avg_loss=0.07546]\n",
      "Step 503524  [5.208 sec/step, loss=0.07575, avg_loss=0.07547]\n",
      "Step 503525  [5.226 sec/step, loss=0.07804, avg_loss=0.07549]\n",
      "Step 503526  [5.223 sec/step, loss=0.07626, avg_loss=0.07549]\n",
      "Step 503527  [5.205 sec/step, loss=0.07276, avg_loss=0.07545]\n",
      "Step 503528  [5.239 sec/step, loss=0.06771, avg_loss=0.07536]\n",
      "Step 503529  [5.238 sec/step, loss=0.07734, avg_loss=0.07538]\n",
      "Step 503530  [5.248 sec/step, loss=0.07524, avg_loss=0.07538]\n",
      "Step 503531  [5.252 sec/step, loss=0.07711, avg_loss=0.07537]\n",
      "Step 503532  [5.246 sec/step, loss=0.07419, avg_loss=0.07533]\n",
      "Step 503533  [5.240 sec/step, loss=0.07666, avg_loss=0.07534]\n",
      "Step 503534  [5.224 sec/step, loss=0.07598, avg_loss=0.07534]\n",
      "Step 503535  [5.218 sec/step, loss=0.07561, avg_loss=0.07533]\n",
      "Step 503536  [5.194 sec/step, loss=0.07220, avg_loss=0.07527]\n",
      "Step 503537  [5.196 sec/step, loss=0.07383, avg_loss=0.07524]\n",
      "Generated 32 batches of size 32 in 2.444 sec\n",
      "Step 503538  [5.187 sec/step, loss=0.07579, avg_loss=0.07522]\n",
      "Step 503539  [5.202 sec/step, loss=0.07717, avg_loss=0.07526]\n",
      "Step 503540  [5.214 sec/step, loss=0.07602, avg_loss=0.07531]\n",
      "Step 503541  [5.239 sec/step, loss=0.07722, avg_loss=0.07533]\n",
      "Step 503542  [5.233 sec/step, loss=0.07622, avg_loss=0.07535]\n",
      "Step 503543  [5.239 sec/step, loss=0.07777, avg_loss=0.07537]\n",
      "Step 503544  [5.264 sec/step, loss=0.07805, avg_loss=0.07548]\n",
      "Step 503545  [5.255 sec/step, loss=0.07628, avg_loss=0.07548]\n",
      "Step 503546  [5.277 sec/step, loss=0.07472, avg_loss=0.07546]\n",
      "Step 503547  [5.268 sec/step, loss=0.07599, avg_loss=0.07544]\n",
      "Step 503548  [5.271 sec/step, loss=0.07452, avg_loss=0.07543]\n",
      "Step 503549  [5.255 sec/step, loss=0.07488, avg_loss=0.07541]\n",
      "Step 503550  [5.257 sec/step, loss=0.07723, avg_loss=0.07542]\n",
      "Step 503551  [5.262 sec/step, loss=0.07669, avg_loss=0.07542]\n",
      "Step 503552  [5.281 sec/step, loss=0.07758, avg_loss=0.07545]\n",
      "Step 503553  [5.252 sec/step, loss=0.07402, avg_loss=0.07544]\n",
      "Step 503554  [5.261 sec/step, loss=0.07788, avg_loss=0.07545]\n",
      "Step 503555  [5.284 sec/step, loss=0.07751, avg_loss=0.07550]\n",
      "Step 503556  [5.281 sec/step, loss=0.07587, avg_loss=0.07548]\n",
      "Step 503557  [5.282 sec/step, loss=0.07609, avg_loss=0.07549]\n",
      "Step 503558  [5.272 sec/step, loss=0.07217, avg_loss=0.07546]\n",
      "Step 503559  [5.250 sec/step, loss=0.07739, avg_loss=0.07546]\n",
      "Step 503560  [5.266 sec/step, loss=0.07578, avg_loss=0.07548]\n",
      "Step 503561  [5.251 sec/step, loss=0.07348, avg_loss=0.07544]\n",
      "Step 503562  [5.253 sec/step, loss=0.07749, avg_loss=0.07543]\n",
      "Step 503563  [5.255 sec/step, loss=0.07710, avg_loss=0.07544]\n",
      "Step 503564  [5.265 sec/step, loss=0.07442, avg_loss=0.07541]\n",
      "Step 503565  [5.260 sec/step, loss=0.07503, avg_loss=0.07542]\n",
      "Step 503566  [5.264 sec/step, loss=0.07644, avg_loss=0.07543]\n",
      "Step 503567  [5.284 sec/step, loss=0.07519, avg_loss=0.07542]\n",
      "Step 503568  [5.288 sec/step, loss=0.07756, avg_loss=0.07543]\n",
      "Step 503569  [5.270 sec/step, loss=0.07551, avg_loss=0.07541]\n",
      "Generated 32 batches of size 32 in 2.357 sec\n",
      "Step 503570  [5.279 sec/step, loss=0.07698, avg_loss=0.07542]\n",
      "Step 503571  [5.275 sec/step, loss=0.07724, avg_loss=0.07542]\n",
      "Step 503572  [5.315 sec/step, loss=0.06809, avg_loss=0.07535]\n",
      "Step 503573  [5.308 sec/step, loss=0.07443, avg_loss=0.07534]\n",
      "Step 503574  [5.348 sec/step, loss=0.07466, avg_loss=0.07535]\n",
      "Step 503575  [5.368 sec/step, loss=0.07792, avg_loss=0.07545]\n",
      "Step 503576  [5.355 sec/step, loss=0.07584, avg_loss=0.07546]\n",
      "Step 503577  [5.288 sec/step, loss=0.06735, avg_loss=0.07545]\n",
      "Step 503578  [5.298 sec/step, loss=0.07497, avg_loss=0.07544]\n",
      "Step 503579  [5.299 sec/step, loss=0.07722, avg_loss=0.07546]\n",
      "Step 503580  [5.312 sec/step, loss=0.07746, avg_loss=0.07548]\n",
      "Step 503581  [5.291 sec/step, loss=0.07611, avg_loss=0.07546]\n",
      "Step 503582  [5.319 sec/step, loss=0.07041, avg_loss=0.07539]\n",
      "Step 503583  [5.315 sec/step, loss=0.07734, avg_loss=0.07539]\n",
      "Step 503584  [5.305 sec/step, loss=0.07377, avg_loss=0.07536]\n",
      "Step 503585  [5.308 sec/step, loss=0.07670, avg_loss=0.07539]\n",
      "Step 503586  [5.338 sec/step, loss=0.07655, avg_loss=0.07541]\n",
      "Step 503587  [5.323 sec/step, loss=0.07580, avg_loss=0.07540]\n",
      "Step 503588  [5.311 sec/step, loss=0.07483, avg_loss=0.07537]\n",
      "Step 503589  [5.324 sec/step, loss=0.07837, avg_loss=0.07540]\n",
      "Step 503590  [5.342 sec/step, loss=0.07735, avg_loss=0.07542]\n",
      "Step 503591  [5.333 sec/step, loss=0.07583, avg_loss=0.07540]\n",
      "Step 503592  [5.339 sec/step, loss=0.07804, avg_loss=0.07543]\n",
      "Step 503593  [5.340 sec/step, loss=0.07315, avg_loss=0.07539]\n",
      "Step 503594  [5.341 sec/step, loss=0.07616, avg_loss=0.07540]\n",
      "Step 503595  [5.345 sec/step, loss=0.07638, avg_loss=0.07541]\n",
      "Step 503596  [5.346 sec/step, loss=0.07453, avg_loss=0.07541]\n",
      "Step 503597  [5.283 sec/step, loss=0.07319, avg_loss=0.07547]\n",
      "Step 503598  [5.294 sec/step, loss=0.07457, avg_loss=0.07545]\n",
      "Step 503599  [5.280 sec/step, loss=0.07435, avg_loss=0.07545]\n",
      "Step 503600  [5.281 sec/step, loss=0.07659, avg_loss=0.07544]\n",
      "Writing summary at step: 503600\n",
      "Generated 32 batches of size 32 in 2.316 sec\n",
      "Step 503601  [5.273 sec/step, loss=0.07631, avg_loss=0.07542]\n",
      "Step 503602  [5.267 sec/step, loss=0.07479, avg_loss=0.07542]\n",
      "Step 503603  [5.266 sec/step, loss=0.07574, avg_loss=0.07540]\n",
      "Step 503604  [5.251 sec/step, loss=0.07513, avg_loss=0.07539]\n",
      "Step 503605  [5.242 sec/step, loss=0.06747, avg_loss=0.07535]\n",
      "Step 503606  [5.231 sec/step, loss=0.07735, avg_loss=0.07537]\n",
      "Step 503607  [5.241 sec/step, loss=0.07795, avg_loss=0.07538]\n",
      "Step 503608  [5.241 sec/step, loss=0.07286, avg_loss=0.07538]\n",
      "Step 503609  [5.232 sec/step, loss=0.07625, avg_loss=0.07537]\n",
      "Step 503610  [5.221 sec/step, loss=0.06613, avg_loss=0.07528]\n",
      "Step 503611  [5.206 sec/step, loss=0.07565, avg_loss=0.07526]\n",
      "Step 503612  [5.196 sec/step, loss=0.07537, avg_loss=0.07525]\n",
      "Step 503613  [5.262 sec/step, loss=0.06873, avg_loss=0.07526]\n",
      "Step 503614  [5.261 sec/step, loss=0.07495, avg_loss=0.07524]\n",
      "Step 503615  [5.264 sec/step, loss=0.07548, avg_loss=0.07524]\n",
      "Step 503616  [5.274 sec/step, loss=0.07714, avg_loss=0.07526]\n",
      "Step 503617  [5.306 sec/step, loss=0.07494, avg_loss=0.07526]\n",
      "Step 503618  [5.297 sec/step, loss=0.07700, avg_loss=0.07526]\n",
      "Step 503619  [5.305 sec/step, loss=0.07493, avg_loss=0.07523]\n",
      "Step 503620  [5.296 sec/step, loss=0.07697, avg_loss=0.07523]\n",
      "Step 503621  [5.319 sec/step, loss=0.07751, avg_loss=0.07534]\n",
      "Step 503622  [5.336 sec/step, loss=0.07716, avg_loss=0.07538]\n",
      "Step 503623  [5.343 sec/step, loss=0.07582, avg_loss=0.07538]\n",
      "Step 503624  [5.348 sec/step, loss=0.07592, avg_loss=0.07538]\n",
      "Step 503625  [5.348 sec/step, loss=0.07656, avg_loss=0.07536]\n",
      "Step 503626  [5.354 sec/step, loss=0.07787, avg_loss=0.07538]\n",
      "Step 503627  [5.380 sec/step, loss=0.07765, avg_loss=0.07543]\n",
      "Step 503628  [5.320 sec/step, loss=0.07269, avg_loss=0.07548]\n",
      "Step 503629  [5.306 sec/step, loss=0.07651, avg_loss=0.07547]\n",
      "Step 503630  [5.310 sec/step, loss=0.07756, avg_loss=0.07549]\n",
      "Step 503631  [5.299 sec/step, loss=0.07741, avg_loss=0.07550]\n",
      "Step 503632  [5.319 sec/step, loss=0.07458, avg_loss=0.07550]\n",
      "Generated 32 batches of size 32 in 2.532 sec\n",
      "Step 503633  [5.325 sec/step, loss=0.07459, avg_loss=0.07548]\n",
      "Step 503634  [5.312 sec/step, loss=0.07367, avg_loss=0.07546]\n",
      "Step 503635  [5.321 sec/step, loss=0.07606, avg_loss=0.07546]\n",
      "Step 503636  [5.329 sec/step, loss=0.07636, avg_loss=0.07550]\n",
      "Step 503637  [5.339 sec/step, loss=0.07767, avg_loss=0.07554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 503638  [5.343 sec/step, loss=0.07715, avg_loss=0.07555]\n",
      "Step 503639  [5.328 sec/step, loss=0.07268, avg_loss=0.07551]\n",
      "Step 503640  [5.312 sec/step, loss=0.07573, avg_loss=0.07551]\n",
      "Step 503641  [5.295 sec/step, loss=0.07689, avg_loss=0.07550]\n",
      "Step 503642  [5.287 sec/step, loss=0.07486, avg_loss=0.07549]\n",
      "Step 503643  [5.277 sec/step, loss=0.07678, avg_loss=0.07548]\n",
      "Step 503644  [5.279 sec/step, loss=0.07677, avg_loss=0.07547]\n",
      "Step 503645  [5.290 sec/step, loss=0.07726, avg_loss=0.07548]\n",
      "Step 503646  [5.260 sec/step, loss=0.07669, avg_loss=0.07550]\n",
      "Step 503647  [5.276 sec/step, loss=0.07568, avg_loss=0.07549]\n",
      "Step 503648  [5.278 sec/step, loss=0.07668, avg_loss=0.07551]\n",
      "Step 503649  [5.292 sec/step, loss=0.07762, avg_loss=0.07554]\n",
      "Step 503650  [5.292 sec/step, loss=0.07691, avg_loss=0.07554]\n",
      "Step 503651  [5.291 sec/step, loss=0.07601, avg_loss=0.07553]\n",
      "Step 503652  [5.279 sec/step, loss=0.07711, avg_loss=0.07553]\n",
      "Step 503653  [5.286 sec/step, loss=0.07629, avg_loss=0.07555]\n",
      "Step 503654  [5.273 sec/step, loss=0.07524, avg_loss=0.07552]\n",
      "Step 503655  [5.255 sec/step, loss=0.07490, avg_loss=0.07550]\n",
      "Step 503656  [5.245 sec/step, loss=0.07299, avg_loss=0.07547]\n",
      "Step 503657  [5.238 sec/step, loss=0.07361, avg_loss=0.07544]\n",
      "Step 503658  [5.248 sec/step, loss=0.07514, avg_loss=0.07547]\n",
      "Step 503659  [5.276 sec/step, loss=0.07685, avg_loss=0.07547]\n",
      "Step 503660  [5.306 sec/step, loss=0.07000, avg_loss=0.07541]\n",
      "Step 503661  [5.318 sec/step, loss=0.07632, avg_loss=0.07544]\n",
      "Step 503662  [5.312 sec/step, loss=0.07665, avg_loss=0.07543]\n",
      "Step 503663  [5.317 sec/step, loss=0.07726, avg_loss=0.07543]\n",
      "Step 503664  [5.291 sec/step, loss=0.07545, avg_loss=0.07544]\n",
      "Generated 32 batches of size 32 in 2.387 sec\n",
      "Step 503665  [5.312 sec/step, loss=0.07811, avg_loss=0.07547]\n",
      "Step 503666  [5.314 sec/step, loss=0.07433, avg_loss=0.07545]\n",
      "Step 503667  [5.291 sec/step, loss=0.07397, avg_loss=0.07544]\n",
      "Step 503668  [5.297 sec/step, loss=0.07765, avg_loss=0.07544]\n",
      "Step 503669  [5.293 sec/step, loss=0.07633, avg_loss=0.07545]\n",
      "Step 503670  [5.300 sec/step, loss=0.07752, avg_loss=0.07545]\n",
      "Step 503671  [5.302 sec/step, loss=0.07663, avg_loss=0.07545]\n",
      "Step 503672  [5.236 sec/step, loss=0.06898, avg_loss=0.07546]\n",
      "Step 503673  [5.250 sec/step, loss=0.07689, avg_loss=0.07548]\n",
      "Step 503674  [5.213 sec/step, loss=0.07541, avg_loss=0.07549]\n",
      "Step 503675  [5.209 sec/step, loss=0.07648, avg_loss=0.07547]\n",
      "Step 503676  [5.228 sec/step, loss=0.07796, avg_loss=0.07550]\n",
      "Step 503677  [5.294 sec/step, loss=0.06827, avg_loss=0.07550]\n",
      "Step 503678  [5.283 sec/step, loss=0.07407, avg_loss=0.07550]\n",
      "Step 503679  [5.283 sec/step, loss=0.07580, avg_loss=0.07548]\n",
      "Step 503680  [5.271 sec/step, loss=0.07705, avg_loss=0.07548]\n",
      "Step 503681  [5.281 sec/step, loss=0.07321, avg_loss=0.07545]\n",
      "Step 503682  [5.233 sec/step, loss=0.07573, avg_loss=0.07550]\n",
      "Step 503683  [5.224 sec/step, loss=0.07543, avg_loss=0.07548]\n",
      "Step 503684  [5.213 sec/step, loss=0.07276, avg_loss=0.07547]\n",
      "Step 503685  [5.211 sec/step, loss=0.07586, avg_loss=0.07546]\n",
      "Step 503686  [5.204 sec/step, loss=0.07784, avg_loss=0.07548]\n",
      "Step 503687  [5.214 sec/step, loss=0.07701, avg_loss=0.07549]\n",
      "Step 503688  [5.210 sec/step, loss=0.07727, avg_loss=0.07551]\n",
      "Step 503689  [5.219 sec/step, loss=0.07516, avg_loss=0.07548]\n",
      "Step 503690  [5.226 sec/step, loss=0.07750, avg_loss=0.07548]\n",
      "Step 503691  [5.225 sec/step, loss=0.07593, avg_loss=0.07548]\n",
      "Step 503692  [5.241 sec/step, loss=0.07464, avg_loss=0.07545]\n",
      "Step 503693  [5.240 sec/step, loss=0.07356, avg_loss=0.07545]\n",
      "Step 503694  [5.248 sec/step, loss=0.07748, avg_loss=0.07547]\n",
      "Step 503695  [5.249 sec/step, loss=0.07694, avg_loss=0.07547]\n",
      "Step 503696  [5.230 sec/step, loss=0.07857, avg_loss=0.07551]\n",
      "Generated 32 batches of size 32 in 2.379 sec\n",
      "Step 503697  [5.250 sec/step, loss=0.07671, avg_loss=0.07555]\n",
      "Step 503698  [5.250 sec/step, loss=0.07556, avg_loss=0.07556]\n",
      "Step 503699  [5.243 sec/step, loss=0.06720, avg_loss=0.07549]\n",
      "Step 503700  [5.245 sec/step, loss=0.07703, avg_loss=0.07549]\n",
      "Writing summary at step: 503700\n",
      "Step 503701  [5.253 sec/step, loss=0.07753, avg_loss=0.07550]\n",
      "Step 503702  [5.236 sec/step, loss=0.07227, avg_loss=0.07548]\n",
      "Step 503703  [5.224 sec/step, loss=0.07666, avg_loss=0.07549]\n",
      "Step 503704  [5.241 sec/step, loss=0.07551, avg_loss=0.07549]\n",
      "Step 503705  [5.261 sec/step, loss=0.07708, avg_loss=0.07559]\n",
      "Step 503706  [5.250 sec/step, loss=0.07752, avg_loss=0.07559]\n",
      "Step 503707  [5.233 sec/step, loss=0.07593, avg_loss=0.07557]\n",
      "Step 503708  [5.253 sec/step, loss=0.07807, avg_loss=0.07562]\n",
      "Step 503709  [5.267 sec/step, loss=0.07727, avg_loss=0.07563]\n",
      "Step 503710  [5.279 sec/step, loss=0.07667, avg_loss=0.07574]\n",
      "Step 503711  [5.294 sec/step, loss=0.07791, avg_loss=0.07576]\n",
      "Step 503712  [5.304 sec/step, loss=0.07378, avg_loss=0.07574]\n",
      "Step 503713  [5.253 sec/step, loss=0.07662, avg_loss=0.07582]\n",
      "Step 503714  [5.252 sec/step, loss=0.07667, avg_loss=0.07584]\n",
      "Step 503715  [5.302 sec/step, loss=0.06923, avg_loss=0.07578]\n",
      "Step 503716  [5.305 sec/step, loss=0.07775, avg_loss=0.07578]\n",
      "Step 503717  [5.281 sec/step, loss=0.07291, avg_loss=0.07576]\n",
      "Step 503718  [5.291 sec/step, loss=0.07789, avg_loss=0.07577]\n",
      "Step 503719  [5.301 sec/step, loss=0.07483, avg_loss=0.07577]\n",
      "Step 503720  [5.317 sec/step, loss=0.07760, avg_loss=0.07578]\n",
      "Step 503721  [5.300 sec/step, loss=0.07575, avg_loss=0.07576]\n",
      "Step 503722  [5.294 sec/step, loss=0.07669, avg_loss=0.07575]\n",
      "Step 503723  [5.283 sec/step, loss=0.07587, avg_loss=0.07575]\n",
      "Step 503724  [5.264 sec/step, loss=0.06724, avg_loss=0.07567]\n",
      "Step 503725  [5.262 sec/step, loss=0.07452, avg_loss=0.07565]\n",
      "Step 503726  [5.279 sec/step, loss=0.07505, avg_loss=0.07562]\n",
      "Step 503727  [5.275 sec/step, loss=0.07758, avg_loss=0.07562]\n",
      "Generated 32 batches of size 32 in 2.565 sec\n",
      "Step 503728  [5.288 sec/step, loss=0.07371, avg_loss=0.07563]\n",
      "Step 503729  [5.288 sec/step, loss=0.07408, avg_loss=0.07560]\n",
      "Step 503730  [5.261 sec/step, loss=0.07362, avg_loss=0.07557]\n",
      "Step 503731  [5.251 sec/step, loss=0.07530, avg_loss=0.07554]\n",
      "Step 503732  [5.235 sec/step, loss=0.07785, avg_loss=0.07558]\n",
      "Step 503733  [5.223 sec/step, loss=0.07385, avg_loss=0.07557]\n",
      "Step 503734  [5.231 sec/step, loss=0.07651, avg_loss=0.07560]\n",
      "Step 503735  [5.231 sec/step, loss=0.07530, avg_loss=0.07559]\n",
      "Step 503736  [5.244 sec/step, loss=0.07524, avg_loss=0.07558]\n",
      "Step 503737  [5.235 sec/step, loss=0.07751, avg_loss=0.07558]\n",
      "Step 503738  [5.238 sec/step, loss=0.07687, avg_loss=0.07557]\n",
      "Step 503739  [5.278 sec/step, loss=0.07371, avg_loss=0.07559]\n",
      "Step 503740  [5.298 sec/step, loss=0.07746, avg_loss=0.07560]\n",
      "Step 503741  [5.298 sec/step, loss=0.07654, avg_loss=0.07560]\n",
      "Step 503742  [5.299 sec/step, loss=0.07507, avg_loss=0.07560]\n",
      "Step 503743  [5.302 sec/step, loss=0.07582, avg_loss=0.07559]\n",
      "Step 503744  [5.296 sec/step, loss=0.07782, avg_loss=0.07560]\n",
      "Step 503745  [5.287 sec/step, loss=0.07568, avg_loss=0.07559]\n",
      "Step 503746  [5.287 sec/step, loss=0.07503, avg_loss=0.07557]\n",
      "Step 503747  [5.288 sec/step, loss=0.07482, avg_loss=0.07556]\n",
      "Step 503748  [5.276 sec/step, loss=0.07546, avg_loss=0.07555]\n",
      "Step 503749  [5.279 sec/step, loss=0.07569, avg_loss=0.07553]\n",
      "Step 503750  [5.272 sec/step, loss=0.07387, avg_loss=0.07550]\n",
      "Step 503751  [5.265 sec/step, loss=0.07303, avg_loss=0.07547]\n",
      "Step 503752  [5.276 sec/step, loss=0.07790, avg_loss=0.07548]\n",
      "Step 503753  [5.285 sec/step, loss=0.07657, avg_loss=0.07548]\n",
      "Step 503754  [5.301 sec/step, loss=0.07744, avg_loss=0.07550]\n",
      "Step 503755  [5.306 sec/step, loss=0.07669, avg_loss=0.07552]\n",
      "Step 503756  [5.322 sec/step, loss=0.07628, avg_loss=0.07555]\n",
      "Step 503757  [5.335 sec/step, loss=0.07539, avg_loss=0.07557]\n",
      "Step 503758  [5.323 sec/step, loss=0.07306, avg_loss=0.07555]\n",
      "Step 503759  [5.301 sec/step, loss=0.07680, avg_loss=0.07555]\n",
      "Generated 32 batches of size 32 in 3.025 sec\n",
      "Step 503760  [5.247 sec/step, loss=0.06728, avg_loss=0.07552]\n",
      "Step 503761  [5.299 sec/step, loss=0.06657, avg_loss=0.07542]\n",
      "Step 503762  [5.283 sec/step, loss=0.07199, avg_loss=0.07538]\n",
      "Step 503763  [5.280 sec/step, loss=0.07815, avg_loss=0.07539]\n",
      "Step 503764  [5.284 sec/step, loss=0.07700, avg_loss=0.07540]\n",
      "Step 503765  [5.265 sec/step, loss=0.07713, avg_loss=0.07539]\n",
      "Step 503766  [5.262 sec/step, loss=0.07509, avg_loss=0.07540]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 503767  [5.293 sec/step, loss=0.07761, avg_loss=0.07544]\n",
      "Step 503768  [5.278 sec/step, loss=0.07569, avg_loss=0.07542]\n",
      "Step 503769  [5.288 sec/step, loss=0.07605, avg_loss=0.07541]\n",
      "Step 503770  [5.276 sec/step, loss=0.07446, avg_loss=0.07538]\n",
      "Step 503771  [5.256 sec/step, loss=0.07359, avg_loss=0.07535]\n",
      "Step 503772  [5.272 sec/step, loss=0.07460, avg_loss=0.07541]\n",
      "Step 503773  [5.272 sec/step, loss=0.07655, avg_loss=0.07541]\n",
      "Step 503774  [5.272 sec/step, loss=0.07575, avg_loss=0.07541]\n",
      "Step 503775  [5.252 sec/step, loss=0.06886, avg_loss=0.07533]\n",
      "Step 503776  [5.230 sec/step, loss=0.07405, avg_loss=0.07529]\n",
      "Step 503777  [5.185 sec/step, loss=0.07762, avg_loss=0.07539]\n",
      "Step 503778  [5.193 sec/step, loss=0.07607, avg_loss=0.07541]\n",
      "Step 503779  [5.208 sec/step, loss=0.07824, avg_loss=0.07543]\n",
      "Step 503780  [5.203 sec/step, loss=0.07541, avg_loss=0.07542]\n",
      "Step 503781  [5.210 sec/step, loss=0.07738, avg_loss=0.07546]\n",
      "Step 503782  [5.231 sec/step, loss=0.07707, avg_loss=0.07547]\n",
      "Step 503783  [5.241 sec/step, loss=0.07523, avg_loss=0.07547]\n",
      "Step 503784  [5.254 sec/step, loss=0.07745, avg_loss=0.07552]\n",
      "Step 503785  [5.263 sec/step, loss=0.07704, avg_loss=0.07553]\n",
      "Step 503786  [5.261 sec/step, loss=0.07646, avg_loss=0.07551]\n",
      "Step 503787  [5.252 sec/step, loss=0.07682, avg_loss=0.07551]\n",
      "Step 503788  [5.256 sec/step, loss=0.07677, avg_loss=0.07551]\n",
      "Step 503789  [5.248 sec/step, loss=0.07745, avg_loss=0.07553]\n",
      "Step 503790  [5.227 sec/step, loss=0.07515, avg_loss=0.07551]\n",
      "Step 503791  [5.240 sec/step, loss=0.07740, avg_loss=0.07552]\n",
      "Generated 32 batches of size 32 in 2.558 sec\n",
      "Step 503792  [5.271 sec/step, loss=0.06716, avg_loss=0.07545]\n",
      "Step 503793  [5.282 sec/step, loss=0.07726, avg_loss=0.07548]\n",
      "Step 503794  [5.308 sec/step, loss=0.07719, avg_loss=0.07548]\n",
      "Step 503795  [5.315 sec/step, loss=0.07798, avg_loss=0.07549]\n",
      "Step 503796  [5.313 sec/step, loss=0.07745, avg_loss=0.07548]\n",
      "Step 503797  [5.302 sec/step, loss=0.07228, avg_loss=0.07543]\n",
      "Step 503798  [5.286 sec/step, loss=0.07589, avg_loss=0.07544]\n",
      "Step 503799  [5.306 sec/step, loss=0.07779, avg_loss=0.07554]\n",
      "Step 503800  [5.299 sec/step, loss=0.07706, avg_loss=0.07554]\n",
      "Writing summary at step: 503800\n",
      "Step 503801  [5.277 sec/step, loss=0.07285, avg_loss=0.07550]\n",
      "Step 503802  [5.303 sec/step, loss=0.07953, avg_loss=0.07557]\n",
      "Step 503803  [5.318 sec/step, loss=0.07832, avg_loss=0.07559]\n",
      "Step 503804  [5.308 sec/step, loss=0.07703, avg_loss=0.07560]\n",
      "Step 503805  [5.316 sec/step, loss=0.07694, avg_loss=0.07560]\n",
      "Step 503806  [5.320 sec/step, loss=0.07676, avg_loss=0.07559]\n",
      "Step 503807  [5.313 sec/step, loss=0.07368, avg_loss=0.07557]\n",
      "Step 503808  [5.312 sec/step, loss=0.07895, avg_loss=0.07558]\n",
      "Step 503809  [5.287 sec/step, loss=0.06740, avg_loss=0.07548]\n",
      "Step 503810  [5.289 sec/step, loss=0.07775, avg_loss=0.07549]\n",
      "Step 503811  [5.272 sec/step, loss=0.07633, avg_loss=0.07548]\n",
      "Step 503812  [5.277 sec/step, loss=0.07726, avg_loss=0.07551]\n",
      "Step 503813  [5.274 sec/step, loss=0.07527, avg_loss=0.07550]\n",
      "Step 503814  [5.330 sec/step, loss=0.06885, avg_loss=0.07542]\n",
      "Step 503815  [5.285 sec/step, loss=0.07638, avg_loss=0.07549]\n",
      "Step 503816  [5.271 sec/step, loss=0.07585, avg_loss=0.07547]\n",
      "Step 503817  [5.275 sec/step, loss=0.07789, avg_loss=0.07552]\n",
      "Step 503818  [5.270 sec/step, loss=0.07853, avg_loss=0.07553]\n",
      "Step 503819  [5.244 sec/step, loss=0.07599, avg_loss=0.07554]\n",
      "Step 503820  [5.253 sec/step, loss=0.07527, avg_loss=0.07552]\n",
      "Step 503821  [5.259 sec/step, loss=0.07580, avg_loss=0.07552]\n",
      "Step 503822  [5.255 sec/step, loss=0.07533, avg_loss=0.07550]\n",
      "Generated 32 batches of size 32 in 2.396 sec\n",
      "Step 503823  [5.270 sec/step, loss=0.07762, avg_loss=0.07552]\n",
      "Step 503824  [5.289 sec/step, loss=0.07739, avg_loss=0.07562]\n",
      "Step 503825  [5.285 sec/step, loss=0.07682, avg_loss=0.07564]\n",
      "Step 503826  [5.272 sec/step, loss=0.07624, avg_loss=0.07566]\n",
      "Step 503827  [5.263 sec/step, loss=0.07741, avg_loss=0.07565]\n",
      "Step 503828  [5.275 sec/step, loss=0.07796, avg_loss=0.07570]\n",
      "Step 503829  [5.278 sec/step, loss=0.07270, avg_loss=0.07568]\n",
      "Step 503830  [5.301 sec/step, loss=0.07697, avg_loss=0.07572]\n",
      "Step 503831  [5.319 sec/step, loss=0.07565, avg_loss=0.07572]\n",
      "Step 503832  [5.318 sec/step, loss=0.07714, avg_loss=0.07571]\n",
      "Step 503833  [5.339 sec/step, loss=0.07803, avg_loss=0.07575]\n",
      "Step 503834  [5.333 sec/step, loss=0.07430, avg_loss=0.07573]\n",
      "Step 503835  [5.336 sec/step, loss=0.07743, avg_loss=0.07575]\n",
      "Step 503836  [5.353 sec/step, loss=0.07446, avg_loss=0.07575]\n",
      "Step 503837  [5.355 sec/step, loss=0.07816, avg_loss=0.07575]\n",
      "Step 503838  [5.349 sec/step, loss=0.07726, avg_loss=0.07576]\n",
      "Step 503839  [5.333 sec/step, loss=0.07835, avg_loss=0.07580]\n",
      "Step 503840  [5.374 sec/step, loss=0.06787, avg_loss=0.07571]\n",
      "Step 503841  [5.358 sec/step, loss=0.06758, avg_loss=0.07562]\n",
      "Step 503842  [5.359 sec/step, loss=0.07533, avg_loss=0.07562]\n",
      "Step 503843  [5.368 sec/step, loss=0.07750, avg_loss=0.07564]\n",
      "Step 503844  [5.366 sec/step, loss=0.07379, avg_loss=0.07560]\n",
      "Step 503845  [5.365 sec/step, loss=0.07396, avg_loss=0.07558]\n",
      "Step 503846  [5.360 sec/step, loss=0.07593, avg_loss=0.07559]\n",
      "Step 503847  [5.366 sec/step, loss=0.07832, avg_loss=0.07562]\n",
      "Step 503848  [5.385 sec/step, loss=0.07742, avg_loss=0.07564]\n",
      "Step 503849  [5.391 sec/step, loss=0.07622, avg_loss=0.07565]\n",
      "Step 503850  [5.408 sec/step, loss=0.07665, avg_loss=0.07568]\n",
      "Step 503851  [5.403 sec/step, loss=0.07315, avg_loss=0.07568]\n",
      "Step 503852  [5.377 sec/step, loss=0.07311, avg_loss=0.07563]\n",
      "Step 503853  [5.365 sec/step, loss=0.07333, avg_loss=0.07560]\n",
      "Step 503854  [5.362 sec/step, loss=0.07802, avg_loss=0.07560]\n",
      "Generated 32 batches of size 32 in 2.394 sec\n",
      "Step 503855  [5.386 sec/step, loss=0.07796, avg_loss=0.07562]\n",
      "Step 503856  [5.384 sec/step, loss=0.07618, avg_loss=0.07561]\n",
      "Step 503857  [5.371 sec/step, loss=0.07579, avg_loss=0.07562]\n",
      "Step 503858  [5.390 sec/step, loss=0.07697, avg_loss=0.07566]\n",
      "Step 503859  [5.403 sec/step, loss=0.07760, avg_loss=0.07567]\n",
      "Step 503860  [5.423 sec/step, loss=0.07447, avg_loss=0.07574]\n",
      "Step 503861  [5.375 sec/step, loss=0.07669, avg_loss=0.07584]\n",
      "Step 503862  [5.381 sec/step, loss=0.07568, avg_loss=0.07588]\n",
      "Step 503863  [5.369 sec/step, loss=0.07529, avg_loss=0.07585]\n",
      "Step 503864  [5.382 sec/step, loss=0.07777, avg_loss=0.07586]\n",
      "Step 503865  [5.382 sec/step, loss=0.07522, avg_loss=0.07584]\n",
      "Step 503866  [5.390 sec/step, loss=0.07808, avg_loss=0.07587]\n",
      "Step 503867  [5.385 sec/step, loss=0.07782, avg_loss=0.07587]\n",
      "Step 503868  [5.385 sec/step, loss=0.07590, avg_loss=0.07587]\n",
      "Step 503869  [5.380 sec/step, loss=0.07566, avg_loss=0.07587]\n",
      "Step 503870  [5.385 sec/step, loss=0.07452, avg_loss=0.07587]\n",
      "Step 503871  [5.392 sec/step, loss=0.07402, avg_loss=0.07587]\n",
      "Step 503872  [5.419 sec/step, loss=0.07373, avg_loss=0.07586]\n",
      "Step 503873  [5.408 sec/step, loss=0.07552, avg_loss=0.07585]\n",
      "Step 503874  [5.418 sec/step, loss=0.07665, avg_loss=0.07586]\n",
      "Step 503875  [5.421 sec/step, loss=0.07434, avg_loss=0.07592]\n",
      "Step 503876  [5.434 sec/step, loss=0.07629, avg_loss=0.07594]\n",
      "Step 503877  [5.435 sec/step, loss=0.07710, avg_loss=0.07593]\n",
      "Step 503878  [5.444 sec/step, loss=0.07720, avg_loss=0.07594]\n",
      "Step 503879  [5.430 sec/step, loss=0.07542, avg_loss=0.07592]\n",
      "Step 503880  [5.437 sec/step, loss=0.07751, avg_loss=0.07594]\n",
      "Step 503881  [5.427 sec/step, loss=0.07671, avg_loss=0.07593]\n",
      "Step 503882  [5.417 sec/step, loss=0.07703, avg_loss=0.07593]\n",
      "Step 503883  [5.467 sec/step, loss=0.06733, avg_loss=0.07585]\n",
      "Step 503884  [5.458 sec/step, loss=0.07555, avg_loss=0.07583]\n",
      "Step 503885  [5.458 sec/step, loss=0.07516, avg_loss=0.07581]\n",
      "Step 503886  [5.442 sec/step, loss=0.07284, avg_loss=0.07578]\n",
      "Generated 32 batches of size 32 in 2.875 sec\n",
      "Step 503887  [5.435 sec/step, loss=0.06752, avg_loss=0.07568]\n",
      "Step 503888  [5.447 sec/step, loss=0.07435, avg_loss=0.07566]\n",
      "Step 503889  [5.452 sec/step, loss=0.07654, avg_loss=0.07565]\n",
      "Step 503890  [5.456 sec/step, loss=0.07681, avg_loss=0.07567]\n",
      "Step 503891  [5.453 sec/step, loss=0.07776, avg_loss=0.07567]\n",
      "Step 503892  [5.399 sec/step, loss=0.07531, avg_loss=0.07575]\n",
      "Step 503893  [5.398 sec/step, loss=0.07709, avg_loss=0.07575]\n",
      "Step 503894  [5.385 sec/step, loss=0.07764, avg_loss=0.07575]\n",
      "Step 503895  [5.393 sec/step, loss=0.07684, avg_loss=0.07574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 503896  [5.380 sec/step, loss=0.07519, avg_loss=0.07572]\n",
      "Step 503897  [5.369 sec/step, loss=0.06887, avg_loss=0.07569]\n",
      "Step 503898  [5.385 sec/step, loss=0.07778, avg_loss=0.07571]\n",
      "Step 503899  [5.390 sec/step, loss=0.07678, avg_loss=0.07570]\n",
      "Step 503900  [5.404 sec/step, loss=0.07554, avg_loss=0.07568]\n",
      "Writing summary at step: 503900\n",
      "Step 503901  [5.438 sec/step, loss=0.07564, avg_loss=0.07571]\n",
      "Step 503902  [5.421 sec/step, loss=0.07463, avg_loss=0.07566]\n",
      "Step 503903  [5.418 sec/step, loss=0.07499, avg_loss=0.07563]\n",
      "Step 503904  [5.419 sec/step, loss=0.07404, avg_loss=0.07560]\n",
      "Step 503905  [5.424 sec/step, loss=0.07819, avg_loss=0.07561]\n",
      "Step 503906  [5.442 sec/step, loss=0.07546, avg_loss=0.07560]\n",
      "Step 503907  [5.452 sec/step, loss=0.07574, avg_loss=0.07562]\n",
      "Step 503908  [5.501 sec/step, loss=0.06801, avg_loss=0.07551]\n",
      "Step 503909  [5.514 sec/step, loss=0.07557, avg_loss=0.07559]\n",
      "Step 503910  [5.535 sec/step, loss=0.07741, avg_loss=0.07559]\n",
      "Step 503911  [5.551 sec/step, loss=0.07479, avg_loss=0.07557]\n",
      "Step 503912  [5.560 sec/step, loss=0.07797, avg_loss=0.07558]\n",
      "Step 503913  [5.570 sec/step, loss=0.07320, avg_loss=0.07556]\n",
      "Step 503914  [5.534 sec/step, loss=0.07806, avg_loss=0.07565]\n",
      "Step 503915  [5.538 sec/step, loss=0.07724, avg_loss=0.07566]\n",
      "Step 503916  [5.545 sec/step, loss=0.07710, avg_loss=0.07567]\n",
      "Step 503917  [5.547 sec/step, loss=0.07690, avg_loss=0.07566]\n",
      "Generated 32 batches of size 32 in 2.457 sec\n",
      "Step 503918  [5.552 sec/step, loss=0.07677, avg_loss=0.07564]\n",
      "Step 503919  [5.570 sec/step, loss=0.07751, avg_loss=0.07566]\n",
      "Step 503920  [5.556 sec/step, loss=0.07634, avg_loss=0.07567]\n",
      "Step 503921  [5.559 sec/step, loss=0.07554, avg_loss=0.07567]\n",
      "Step 503922  [5.549 sec/step, loss=0.07605, avg_loss=0.07567]\n",
      "Step 503923  [5.542 sec/step, loss=0.07575, avg_loss=0.07565]\n",
      "Step 503924  [5.531 sec/step, loss=0.07288, avg_loss=0.07561]\n",
      "Step 503925  [5.533 sec/step, loss=0.07640, avg_loss=0.07560]\n",
      "Step 503926  [5.507 sec/step, loss=0.07407, avg_loss=0.07558]\n",
      "Step 503927  [5.514 sec/step, loss=0.07622, avg_loss=0.07557]\n",
      "Step 503928  [5.511 sec/step, loss=0.07855, avg_loss=0.07558]\n",
      "Step 503929  [5.557 sec/step, loss=0.07152, avg_loss=0.07556]\n",
      "Step 503930  [5.557 sec/step, loss=0.07699, avg_loss=0.07557]\n",
      "Step 503931  [5.560 sec/step, loss=0.07510, avg_loss=0.07556]\n",
      "Step 503932  [5.560 sec/step, loss=0.07718, avg_loss=0.07556]\n",
      "Step 503933  [5.547 sec/step, loss=0.07471, avg_loss=0.07553]\n",
      "Step 503934  [5.548 sec/step, loss=0.07684, avg_loss=0.07555]\n",
      "Step 503935  [5.573 sec/step, loss=0.07686, avg_loss=0.07555]\n",
      "Step 503936  [5.536 sec/step, loss=0.07172, avg_loss=0.07552]\n",
      "Step 503937  [5.555 sec/step, loss=0.07733, avg_loss=0.07551]\n",
      "Step 503938  [5.541 sec/step, loss=0.06583, avg_loss=0.07540]\n",
      "Step 503939  [5.547 sec/step, loss=0.07780, avg_loss=0.07539]\n",
      "Step 503940  [5.494 sec/step, loss=0.07375, avg_loss=0.07545]\n",
      "Step 503941  [5.509 sec/step, loss=0.07316, avg_loss=0.07551]\n",
      "Step 503942  [5.511 sec/step, loss=0.07481, avg_loss=0.07550]\n",
      "Step 503943  [5.510 sec/step, loss=0.07687, avg_loss=0.07549]\n",
      "Step 503944  [5.509 sec/step, loss=0.07512, avg_loss=0.07551]\n",
      "Step 503945  [5.499 sec/step, loss=0.07604, avg_loss=0.07553]\n",
      "Step 503946  [5.503 sec/step, loss=0.07574, avg_loss=0.07553]\n",
      "Step 503947  [5.490 sec/step, loss=0.07542, avg_loss=0.07550]\n",
      "Step 503948  [5.490 sec/step, loss=0.07800, avg_loss=0.07550]\n",
      "Step 503949  [5.479 sec/step, loss=0.07732, avg_loss=0.07551]\n",
      "Generated 32 batches of size 32 in 2.541 sec\n",
      "Step 503950  [5.485 sec/step, loss=0.07769, avg_loss=0.07552]\n",
      "Step 503951  [5.482 sec/step, loss=0.07334, avg_loss=0.07553]\n",
      "Step 503952  [5.500 sec/step, loss=0.07737, avg_loss=0.07557]\n",
      "Step 503953  [5.513 sec/step, loss=0.07810, avg_loss=0.07562]\n",
      "Step 503954  [5.513 sec/step, loss=0.07700, avg_loss=0.07561]\n",
      "Step 503955  [5.494 sec/step, loss=0.07753, avg_loss=0.07560]\n",
      "Step 503956  [5.491 sec/step, loss=0.07734, avg_loss=0.07561]\n",
      "Step 503957  [5.512 sec/step, loss=0.07717, avg_loss=0.07563]\n",
      "Step 503958  [5.517 sec/step, loss=0.07665, avg_loss=0.07562]\n",
      "Step 503959  [5.502 sec/step, loss=0.07700, avg_loss=0.07562]\n",
      "Step 503960  [5.497 sec/step, loss=0.07619, avg_loss=0.07564]\n",
      "Step 503961  [5.507 sec/step, loss=0.07503, avg_loss=0.07562]\n",
      "Step 503962  [5.520 sec/step, loss=0.07432, avg_loss=0.07561]\n",
      "Step 503963  [5.509 sec/step, loss=0.06891, avg_loss=0.07554]\n",
      "Step 503964  [5.505 sec/step, loss=0.07698, avg_loss=0.07553]\n",
      "Step 503965  [5.522 sec/step, loss=0.07578, avg_loss=0.07554]\n",
      "Step 503966  [5.528 sec/step, loss=0.07798, avg_loss=0.07554]\n",
      "Step 503967  [5.528 sec/step, loss=0.07758, avg_loss=0.07554]\n",
      "Step 503968  [5.546 sec/step, loss=0.07608, avg_loss=0.07554]\n",
      "Step 503969  [5.547 sec/step, loss=0.07597, avg_loss=0.07554]\n",
      "Step 503970  [5.566 sec/step, loss=0.07837, avg_loss=0.07558]\n",
      "Step 503971  [5.645 sec/step, loss=0.06885, avg_loss=0.07553]\n",
      "Step 503972  [5.618 sec/step, loss=0.07506, avg_loss=0.07554]\n",
      "Step 503973  [5.622 sec/step, loss=0.07773, avg_loss=0.07556]\n",
      "Step 503974  [5.623 sec/step, loss=0.07650, avg_loss=0.07556]\n",
      "Step 503975  [5.627 sec/step, loss=0.07395, avg_loss=0.07556]\n",
      "Step 503976  [5.632 sec/step, loss=0.07648, avg_loss=0.07556]\n",
      "Step 503977  [5.625 sec/step, loss=0.07445, avg_loss=0.07553]\n",
      "Step 503978  [5.638 sec/step, loss=0.07575, avg_loss=0.07552]\n",
      "Step 503979  [5.636 sec/step, loss=0.07525, avg_loss=0.07552]\n",
      "Step 503980  [5.630 sec/step, loss=0.07533, avg_loss=0.07549]\n",
      "Step 503981  [5.658 sec/step, loss=0.07533, avg_loss=0.07548]\n",
      "Generated 32 batches of size 32 in 2.603 sec\n",
      "Step 503982  [5.666 sec/step, loss=0.07595, avg_loss=0.07547]\n",
      "Step 503983  [5.620 sec/step, loss=0.07688, avg_loss=0.07557]\n",
      "Step 503984  [5.627 sec/step, loss=0.07510, avg_loss=0.07556]\n",
      "Step 503985  [5.632 sec/step, loss=0.07767, avg_loss=0.07559]\n",
      "Step 503986  [5.628 sec/step, loss=0.07345, avg_loss=0.07559]\n",
      "Step 503987  [5.638 sec/step, loss=0.07723, avg_loss=0.07569]\n",
      "Step 503988  [5.631 sec/step, loss=0.07716, avg_loss=0.07572]\n",
      "Step 503989  [5.608 sec/step, loss=0.07216, avg_loss=0.07567]\n",
      "Step 503990  [5.620 sec/step, loss=0.07651, avg_loss=0.07567]\n",
      "Step 503991  [5.623 sec/step, loss=0.07751, avg_loss=0.07567]\n",
      "Step 503992  [5.627 sec/step, loss=0.07772, avg_loss=0.07569]\n",
      "Step 503993  [5.620 sec/step, loss=0.07681, avg_loss=0.07569]\n",
      "Step 503994  [5.609 sec/step, loss=0.07535, avg_loss=0.07567]\n",
      "Step 503995  [5.581 sec/step, loss=0.07571, avg_loss=0.07566]\n",
      "Step 503996  [5.589 sec/step, loss=0.07550, avg_loss=0.07566]\n",
      "Step 503997  [5.598 sec/step, loss=0.07500, avg_loss=0.07572]\n",
      "Step 503998  [5.584 sec/step, loss=0.07467, avg_loss=0.07569]\n",
      "Step 503999  [5.573 sec/step, loss=0.07491, avg_loss=0.07567]\n",
      "Step 504000  [5.574 sec/step, loss=0.07725, avg_loss=0.07569]\n",
      "Writing summary at step: 504000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-504000\n",
      "Saving audio and alignment...\n",
      "Input: hatdhaylii pir pauudar kii aamayzish sae afiim kii muassar xushbuu aaii~___________________________\n",
      "Step 504001  [5.555 sec/step, loss=0.07609, avg_loss=0.07569]\n",
      "Step 504002  [5.554 sec/step, loss=0.07659, avg_loss=0.07571]\n",
      "Step 504003  [5.554 sec/step, loss=0.07792, avg_loss=0.07574]\n",
      "Step 504004  [5.569 sec/step, loss=0.07686, avg_loss=0.07577]\n",
      "Step 504005  [5.562 sec/step, loss=0.07773, avg_loss=0.07576]\n",
      "Step 504006  [5.531 sec/step, loss=0.07527, avg_loss=0.07576]\n",
      "Step 504007  [5.558 sec/step, loss=0.07597, avg_loss=0.07576]\n",
      "Step 504008  [5.500 sec/step, loss=0.07714, avg_loss=0.07586]\n",
      "Step 504009  [5.512 sec/step, loss=0.07740, avg_loss=0.07587]\n",
      "Step 504010  [5.503 sec/step, loss=0.07493, avg_loss=0.07585]\n",
      "Step 504011  [5.502 sec/step, loss=0.07722, avg_loss=0.07587]\n",
      "Generated 32 batches of size 32 in 2.329 sec\n",
      "Step 504012  [5.493 sec/step, loss=0.07795, avg_loss=0.07587]\n",
      "Step 504013  [5.487 sec/step, loss=0.07621, avg_loss=0.07590]\n",
      "Step 504014  [5.459 sec/step, loss=0.07396, avg_loss=0.07586]\n",
      "Step 504015  [5.466 sec/step, loss=0.07601, avg_loss=0.07585]\n",
      "Step 504016  [5.516 sec/step, loss=0.06671, avg_loss=0.07575]\n",
      "Step 504017  [5.493 sec/step, loss=0.06624, avg_loss=0.07564]\n",
      "Step 504018  [5.484 sec/step, loss=0.07641, avg_loss=0.07564]\n",
      "Step 504019  [5.471 sec/step, loss=0.07267, avg_loss=0.07559]\n",
      "Step 504020  [5.474 sec/step, loss=0.07635, avg_loss=0.07559]\n",
      "Step 504021  [5.462 sec/step, loss=0.07238, avg_loss=0.07556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 504022  [5.472 sec/step, loss=0.07550, avg_loss=0.07555]\n",
      "Step 504023  [5.494 sec/step, loss=0.07528, avg_loss=0.07555]\n",
      "Step 504024  [5.499 sec/step, loss=0.07745, avg_loss=0.07559]\n",
      "Step 504025  [5.489 sec/step, loss=0.07591, avg_loss=0.07559]\n",
      "Step 504026  [5.494 sec/step, loss=0.07459, avg_loss=0.07559]\n",
      "Step 504027  [5.488 sec/step, loss=0.07536, avg_loss=0.07558]\n",
      "Step 504028  [5.460 sec/step, loss=0.06655, avg_loss=0.07546]\n",
      "Step 504029  [5.425 sec/step, loss=0.07754, avg_loss=0.07552]\n",
      "Step 504030  [5.418 sec/step, loss=0.07743, avg_loss=0.07553]\n",
      "Step 504031  [5.415 sec/step, loss=0.07761, avg_loss=0.07555]\n",
      "Step 504032  [5.400 sec/step, loss=0.07556, avg_loss=0.07554]\n",
      "Step 504033  [5.451 sec/step, loss=0.06821, avg_loss=0.07547]\n",
      "Step 504034  [5.448 sec/step, loss=0.07576, avg_loss=0.07546]\n",
      "Step 504035  [5.418 sec/step, loss=0.07635, avg_loss=0.07546]\n",
      "Step 504036  [5.439 sec/step, loss=0.07763, avg_loss=0.07552]\n",
      "Step 504037  [5.426 sec/step, loss=0.07777, avg_loss=0.07552]\n",
      "Step 504038  [5.469 sec/step, loss=0.07475, avg_loss=0.07561]\n",
      "Step 504039  [5.442 sec/step, loss=0.07058, avg_loss=0.07554]\n",
      "Step 504040  [5.449 sec/step, loss=0.07688, avg_loss=0.07557]\n",
      "Step 504041  [5.459 sec/step, loss=0.07771, avg_loss=0.07561]\n",
      "Step 504042  [5.459 sec/step, loss=0.07371, avg_loss=0.07560]\n",
      "Step 504043  [5.466 sec/step, loss=0.07774, avg_loss=0.07561]\n",
      "Generated 32 batches of size 32 in 2.623 sec\n",
      "Step 504044  [5.465 sec/step, loss=0.07253, avg_loss=0.07559]\n",
      "Step 504045  [5.479 sec/step, loss=0.07491, avg_loss=0.07557]\n",
      "Step 504046  [5.487 sec/step, loss=0.07673, avg_loss=0.07558]\n",
      "Step 504047  [5.485 sec/step, loss=0.07654, avg_loss=0.07560]\n",
      "Step 504048  [5.476 sec/step, loss=0.07408, avg_loss=0.07556]\n",
      "Step 504049  [5.479 sec/step, loss=0.07689, avg_loss=0.07555]\n",
      "Step 504050  [5.470 sec/step, loss=0.07802, avg_loss=0.07556]\n",
      "Step 504051  [5.487 sec/step, loss=0.07632, avg_loss=0.07559]\n",
      "Step 504052  [5.493 sec/step, loss=0.07592, avg_loss=0.07557]\n",
      "Step 504053  [5.472 sec/step, loss=0.07537, avg_loss=0.07554]\n",
      "Step 504054  [5.481 sec/step, loss=0.07473, avg_loss=0.07552]\n",
      "Step 504055  [5.462 sec/step, loss=0.07417, avg_loss=0.07549]\n",
      "Step 504056  [5.465 sec/step, loss=0.07729, avg_loss=0.07549]\n",
      "Step 504057  [5.448 sec/step, loss=0.07654, avg_loss=0.07548]\n",
      "Step 504058  [5.448 sec/step, loss=0.07724, avg_loss=0.07549]\n",
      "Step 504059  [5.459 sec/step, loss=0.07781, avg_loss=0.07549]\n",
      "Step 504060  [5.456 sec/step, loss=0.07676, avg_loss=0.07550]\n",
      "Step 504061  [5.444 sec/step, loss=0.07602, avg_loss=0.07551]\n",
      "Step 504062  [5.451 sec/step, loss=0.07864, avg_loss=0.07555]\n",
      "Step 504063  [5.475 sec/step, loss=0.07756, avg_loss=0.07564]\n",
      "Step 504064  [5.475 sec/step, loss=0.07539, avg_loss=0.07562]\n",
      "Step 504065  [5.455 sec/step, loss=0.07189, avg_loss=0.07558]\n",
      "Step 504066  [5.439 sec/step, loss=0.07475, avg_loss=0.07555]\n",
      "Step 504067  [5.431 sec/step, loss=0.07558, avg_loss=0.07553]\n",
      "Step 504068  [5.425 sec/step, loss=0.07682, avg_loss=0.07554]\n",
      "Step 504069  [5.437 sec/step, loss=0.07526, avg_loss=0.07553]\n",
      "Step 504070  [5.415 sec/step, loss=0.07369, avg_loss=0.07549]\n",
      "Step 504071  [5.350 sec/step, loss=0.07676, avg_loss=0.07557]\n",
      "Step 504072  [5.375 sec/step, loss=0.07466, avg_loss=0.07556]\n",
      "Step 504073  [5.362 sec/step, loss=0.07557, avg_loss=0.07554]\n",
      "Step 504074  [5.362 sec/step, loss=0.07503, avg_loss=0.07552]\n",
      "Step 504075  [5.357 sec/step, loss=0.06878, avg_loss=0.07547]\n",
      "Generated 32 batches of size 32 in 2.566 sec\n",
      "Step 504076  [5.355 sec/step, loss=0.07492, avg_loss=0.07546]\n",
      "Step 504077  [5.346 sec/step, loss=0.07391, avg_loss=0.07545]\n",
      "Step 504078  [5.327 sec/step, loss=0.07745, avg_loss=0.07547]\n",
      "Step 504079  [5.347 sec/step, loss=0.07566, avg_loss=0.07547]\n",
      "Step 504080  [5.370 sec/step, loss=0.07470, avg_loss=0.07547]\n",
      "Step 504081  [5.353 sec/step, loss=0.07707, avg_loss=0.07548]\n",
      "Step 504082  [5.340 sec/step, loss=0.07745, avg_loss=0.07550]\n",
      "Step 504083  [5.327 sec/step, loss=0.07217, avg_loss=0.07545]\n",
      "Step 504084  [5.380 sec/step, loss=0.06700, avg_loss=0.07537]\n",
      "Step 504085  [5.377 sec/step, loss=0.07750, avg_loss=0.07537]\n",
      "Step 504086  [5.395 sec/step, loss=0.07728, avg_loss=0.07541]\n",
      "Step 504087  [5.407 sec/step, loss=0.07786, avg_loss=0.07541]\n",
      "Step 504088  [5.406 sec/step, loss=0.07732, avg_loss=0.07542]\n",
      "Step 504089  [5.427 sec/step, loss=0.07593, avg_loss=0.07545]\n",
      "Step 504090  [5.426 sec/step, loss=0.07799, avg_loss=0.07547]\n",
      "Step 504091  [5.409 sec/step, loss=0.07478, avg_loss=0.07544]\n",
      "Step 504092  [5.396 sec/step, loss=0.07564, avg_loss=0.07542]\n",
      "Step 504093  [5.424 sec/step, loss=0.07498, avg_loss=0.07540]\n",
      "Step 504094  [5.434 sec/step, loss=0.07765, avg_loss=0.07542]\n",
      "Step 504095  [5.437 sec/step, loss=0.07450, avg_loss=0.07541]\n",
      "Step 504096  [5.435 sec/step, loss=0.07590, avg_loss=0.07542]\n",
      "Step 504097  [5.442 sec/step, loss=0.07742, avg_loss=0.07544]\n",
      "Step 504098  [5.497 sec/step, loss=0.06864, avg_loss=0.07538]\n",
      "Step 504099  [5.503 sec/step, loss=0.07493, avg_loss=0.07538]\n",
      "Step 504100  [5.479 sec/step, loss=0.07601, avg_loss=0.07537]\n",
      "Writing summary at step: 504100\n",
      "Step 504101  [5.482 sec/step, loss=0.07466, avg_loss=0.07535]\n",
      "Step 504102  [5.481 sec/step, loss=0.07279, avg_loss=0.07532]\n",
      "Step 504103  [5.479 sec/step, loss=0.07767, avg_loss=0.07531]\n",
      "Step 504104  [5.464 sec/step, loss=0.07533, avg_loss=0.07530]\n",
      "Step 504105  [5.440 sec/step, loss=0.06701, avg_loss=0.07519]\n",
      "Step 504106  [5.446 sec/step, loss=0.07609, avg_loss=0.07520]\n",
      "Generated 32 batches of size 32 in 2.413 sec\n",
      "Step 504107  [5.434 sec/step, loss=0.07530, avg_loss=0.07519]\n",
      "Step 504108  [5.420 sec/step, loss=0.07277, avg_loss=0.07515]\n",
      "Step 504109  [5.422 sec/step, loss=0.07463, avg_loss=0.07512]\n",
      "Step 504110  [5.411 sec/step, loss=0.07674, avg_loss=0.07514]\n",
      "Step 504111  [5.396 sec/step, loss=0.07257, avg_loss=0.07509]\n",
      "Step 504112  [5.393 sec/step, loss=0.07641, avg_loss=0.07508]\n",
      "Step 504113  [5.411 sec/step, loss=0.07711, avg_loss=0.07509]\n",
      "Step 504114  [5.424 sec/step, loss=0.07621, avg_loss=0.07511]\n",
      "Step 504115  [5.406 sec/step, loss=0.07379, avg_loss=0.07509]\n",
      "Step 504116  [5.368 sec/step, loss=0.07748, avg_loss=0.07519]\n",
      "Step 504117  [5.383 sec/step, loss=0.07581, avg_loss=0.07529]\n",
      "Step 504118  [5.388 sec/step, loss=0.07538, avg_loss=0.07528]\n",
      "Step 504119  [5.403 sec/step, loss=0.07764, avg_loss=0.07533]\n",
      "Step 504120  [5.443 sec/step, loss=0.06753, avg_loss=0.07524]\n",
      "Step 504121  [5.460 sec/step, loss=0.07620, avg_loss=0.07528]\n",
      "Step 504122  [5.480 sec/step, loss=0.07475, avg_loss=0.07527]\n",
      "Step 504123  [5.456 sec/step, loss=0.07583, avg_loss=0.07528]\n",
      "Step 504124  [5.449 sec/step, loss=0.07175, avg_loss=0.07522]\n",
      "Step 504125  [5.452 sec/step, loss=0.07551, avg_loss=0.07522]\n",
      "Step 504126  [5.485 sec/step, loss=0.07492, avg_loss=0.07522]\n",
      "Step 504127  [5.478 sec/step, loss=0.07483, avg_loss=0.07521]\n",
      "Step 504128  [5.501 sec/step, loss=0.07631, avg_loss=0.07531]\n",
      "Step 504129  [5.491 sec/step, loss=0.07698, avg_loss=0.07531]\n",
      "Step 504130  [5.498 sec/step, loss=0.07839, avg_loss=0.07532]\n",
      "Step 504131  [5.492 sec/step, loss=0.07668, avg_loss=0.07531]\n",
      "Step 504132  [5.504 sec/step, loss=0.07701, avg_loss=0.07532]\n",
      "Step 504133  [5.465 sec/step, loss=0.07454, avg_loss=0.07538]\n",
      "Step 504134  [5.459 sec/step, loss=0.07613, avg_loss=0.07539]\n",
      "Step 504135  [5.474 sec/step, loss=0.07759, avg_loss=0.07540]\n",
      "Step 504136  [5.445 sec/step, loss=0.06797, avg_loss=0.07530]\n",
      "Step 504137  [5.434 sec/step, loss=0.07306, avg_loss=0.07526]\n",
      "Step 504138  [5.416 sec/step, loss=0.07782, avg_loss=0.07529]\n",
      "Generated 32 batches of size 32 in 2.414 sec\n",
      "Step 504139  [5.439 sec/step, loss=0.07636, avg_loss=0.07534]\n",
      "Step 504140  [5.421 sec/step, loss=0.07352, avg_loss=0.07531]\n",
      "Step 504141  [5.408 sec/step, loss=0.07700, avg_loss=0.07530]\n",
      "Step 504142  [5.412 sec/step, loss=0.07671, avg_loss=0.07533]\n",
      "Step 504143  [5.397 sec/step, loss=0.07634, avg_loss=0.07532]\n",
      "Step 504144  [5.408 sec/step, loss=0.07609, avg_loss=0.07536]\n",
      "Step 504145  [5.407 sec/step, loss=0.07719, avg_loss=0.07538]\n",
      "Step 504146  [5.398 sec/step, loss=0.07554, avg_loss=0.07537]\n",
      "Step 504147  [5.395 sec/step, loss=0.07346, avg_loss=0.07534]\n",
      "Step 504148  [5.407 sec/step, loss=0.07794, avg_loss=0.07537]\n",
      "Step 504149  [5.401 sec/step, loss=0.07602, avg_loss=0.07537]\n",
      "Step 504150  [5.401 sec/step, loss=0.07744, avg_loss=0.07536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 504151  [5.394 sec/step, loss=0.07659, avg_loss=0.07536]\n",
      "Step 504152  [5.381 sec/step, loss=0.07398, avg_loss=0.07534]\n",
      "Step 504153  [5.379 sec/step, loss=0.07288, avg_loss=0.07532]\n",
      "Step 504154  [5.370 sec/step, loss=0.07641, avg_loss=0.07534]\n",
      "Step 504155  [5.385 sec/step, loss=0.07691, avg_loss=0.07536]\n",
      "Step 504156  [5.384 sec/step, loss=0.07653, avg_loss=0.07535]\n",
      "Step 504157  [5.382 sec/step, loss=0.07538, avg_loss=0.07534]\n",
      "Step 504158  [5.361 sec/step, loss=0.07539, avg_loss=0.07532]\n",
      "Step 504159  [5.359 sec/step, loss=0.07601, avg_loss=0.07531]\n",
      "Step 504160  [5.360 sec/step, loss=0.07725, avg_loss=0.07531]\n",
      "Step 504161  [5.356 sec/step, loss=0.07652, avg_loss=0.07532]\n",
      "Step 504162  [5.392 sec/step, loss=0.06699, avg_loss=0.07520]\n",
      "Step 504163  [5.375 sec/step, loss=0.07507, avg_loss=0.07518]\n",
      "Step 504164  [5.388 sec/step, loss=0.07675, avg_loss=0.07519]\n",
      "Step 504165  [5.397 sec/step, loss=0.07594, avg_loss=0.07523]\n",
      "Step 504166  [5.410 sec/step, loss=0.07816, avg_loss=0.07526]\n",
      "Step 504167  [5.417 sec/step, loss=0.07734, avg_loss=0.07528]\n",
      "Step 504168  [5.423 sec/step, loss=0.07470, avg_loss=0.07526]\n",
      "Step 504169  [5.442 sec/step, loss=0.07645, avg_loss=0.07527]\n",
      "Step 504170  [5.448 sec/step, loss=0.07647, avg_loss=0.07530]\n",
      "Generated 32 batches of size 32 in 2.853 sec\n",
      "Step 504171  [5.427 sec/step, loss=0.07337, avg_loss=0.07527]\n",
      "Step 504172  [5.401 sec/step, loss=0.07510, avg_loss=0.07527]\n",
      "Step 504173  [5.419 sec/step, loss=0.07690, avg_loss=0.07528]\n",
      "Step 504174  [5.416 sec/step, loss=0.07255, avg_loss=0.07526]\n",
      "Step 504175  [5.424 sec/step, loss=0.07529, avg_loss=0.07532]\n",
      "Step 504176  [5.418 sec/step, loss=0.07563, avg_loss=0.07533]\n",
      "Step 504177  [5.410 sec/step, loss=0.06610, avg_loss=0.07525]\n",
      "Step 504178  [5.415 sec/step, loss=0.07822, avg_loss=0.07526]\n",
      "Step 504179  [5.405 sec/step, loss=0.07428, avg_loss=0.07525]\n",
      "Step 504180  [5.384 sec/step, loss=0.07649, avg_loss=0.07526]\n",
      "Step 504181  [5.373 sec/step, loss=0.07520, avg_loss=0.07525]\n",
      "Step 504182  [5.387 sec/step, loss=0.08085, avg_loss=0.07528]\n",
      "Step 504183  [5.392 sec/step, loss=0.07901, avg_loss=0.07535]\n",
      "Step 504184  [5.326 sec/step, loss=0.06920, avg_loss=0.07537]\n",
      "Step 504185  [5.301 sec/step, loss=0.07562, avg_loss=0.07535]\n",
      "Step 504186  [5.346 sec/step, loss=0.07366, avg_loss=0.07532]\n",
      "Step 504187  [5.338 sec/step, loss=0.08068, avg_loss=0.07534]\n",
      "Step 504188  [5.363 sec/step, loss=0.07938, avg_loss=0.07536]\n",
      "Step 504189  [5.363 sec/step, loss=0.08079, avg_loss=0.07541]\n",
      "Step 504190  [5.354 sec/step, loss=0.08035, avg_loss=0.07544]\n",
      "Step 504191  [5.355 sec/step, loss=0.07902, avg_loss=0.07548]\n",
      "Step 504192  [5.356 sec/step, loss=0.07882, avg_loss=0.07551]\n",
      "Step 504193  [5.322 sec/step, loss=0.07896, avg_loss=0.07555]\n",
      "Step 504194  [5.313 sec/step, loss=0.07744, avg_loss=0.07555]\n",
      "Step 504195  [5.334 sec/step, loss=0.08327, avg_loss=0.07564]\n",
      "Step 504196  [5.339 sec/step, loss=0.08109, avg_loss=0.07569]\n",
      "Step 504197  [5.346 sec/step, loss=0.08228, avg_loss=0.07574]\n",
      "Step 504198  [5.305 sec/step, loss=0.08091, avg_loss=0.07586]\n",
      "Step 504199  [5.302 sec/step, loss=0.08116, avg_loss=0.07592]\n",
      "Step 504200  [5.302 sec/step, loss=0.07603, avg_loss=0.07592]\n",
      "Writing summary at step: 504200\n",
      "Step 504201  [5.308 sec/step, loss=0.08239, avg_loss=0.07600]\n",
      "Generated 32 batches of size 32 in 2.359 sec\n",
      "Step 504202  [5.318 sec/step, loss=0.07737, avg_loss=0.07604]\n",
      "Step 504203  [5.317 sec/step, loss=0.08144, avg_loss=0.07608]\n",
      "Step 504204  [5.321 sec/step, loss=0.07948, avg_loss=0.07612]\n",
      "Step 504205  [5.339 sec/step, loss=0.07910, avg_loss=0.07624]\n",
      "Step 504206  [5.353 sec/step, loss=0.07748, avg_loss=0.07626]\n",
      "Step 504207  [5.348 sec/step, loss=0.07923, avg_loss=0.07630]\n",
      "Step 504208  [5.361 sec/step, loss=0.07568, avg_loss=0.07633]\n",
      "Step 504209  [5.345 sec/step, loss=0.07721, avg_loss=0.07635]\n",
      "Step 504210  [5.363 sec/step, loss=0.08033, avg_loss=0.07639]\n",
      "Step 504211  [5.385 sec/step, loss=0.08239, avg_loss=0.07649]\n",
      "Step 504212  [5.378 sec/step, loss=0.07936, avg_loss=0.07652]\n",
      "Step 504213  [5.358 sec/step, loss=0.07925, avg_loss=0.07654]\n",
      "Step 504214  [5.362 sec/step, loss=0.07981, avg_loss=0.07657]\n",
      "Step 504215  [5.365 sec/step, loss=0.07965, avg_loss=0.07663]\n",
      "Step 504216  [5.402 sec/step, loss=0.07077, avg_loss=0.07657]\n",
      "Step 504217  [5.410 sec/step, loss=0.07973, avg_loss=0.07660]\n",
      "Step 504218  [5.412 sec/step, loss=0.07948, avg_loss=0.07665]\n",
      "Step 504219  [5.415 sec/step, loss=0.08031, avg_loss=0.07667]\n",
      "Step 504220  [5.371 sec/step, loss=0.08011, avg_loss=0.07680]\n",
      "Step 504221  [5.359 sec/step, loss=0.07409, avg_loss=0.07678]\n",
      "Step 504222  [5.359 sec/step, loss=0.07814, avg_loss=0.07681]\n",
      "Step 504223  [5.366 sec/step, loss=0.07861, avg_loss=0.07684]\n",
      "Step 504224  [5.364 sec/step, loss=0.07529, avg_loss=0.07687]\n",
      "Step 504225  [5.381 sec/step, loss=0.07908, avg_loss=0.07691]\n",
      "Step 504226  [5.352 sec/step, loss=0.07746, avg_loss=0.07694]\n",
      "Step 504227  [5.362 sec/step, loss=0.07593, avg_loss=0.07695]\n",
      "Step 504228  [5.356 sec/step, loss=0.07640, avg_loss=0.07695]\n",
      "Step 504229  [5.351 sec/step, loss=0.07722, avg_loss=0.07695]\n",
      "Step 504230  [5.352 sec/step, loss=0.07920, avg_loss=0.07696]\n",
      "Step 504231  [5.345 sec/step, loss=0.07479, avg_loss=0.07694]\n",
      "Step 504232  [5.325 sec/step, loss=0.06834, avg_loss=0.07685]\n",
      "Step 504233  [5.342 sec/step, loss=0.07703, avg_loss=0.07688]\n",
      "Generated 32 batches of size 32 in 2.536 sec\n",
      "Step 504234  [5.350 sec/step, loss=0.07759, avg_loss=0.07689]\n",
      "Step 504235  [5.334 sec/step, loss=0.07335, avg_loss=0.07685]\n",
      "Step 504236  [5.352 sec/step, loss=0.07839, avg_loss=0.07695]\n",
      "Step 504237  [5.344 sec/step, loss=0.07497, avg_loss=0.07697]\n",
      "Step 504238  [5.334 sec/step, loss=0.07772, avg_loss=0.07697]\n",
      "Step 504239  [5.327 sec/step, loss=0.07893, avg_loss=0.07700]\n",
      "Step 504240  [5.340 sec/step, loss=0.07756, avg_loss=0.07704]\n",
      "Step 504241  [5.352 sec/step, loss=0.08014, avg_loss=0.07707]\n",
      "Step 504242  [5.363 sec/step, loss=0.07745, avg_loss=0.07708]\n",
      "Step 504243  [5.353 sec/step, loss=0.07357, avg_loss=0.07705]\n",
      "Step 504244  [5.337 sec/step, loss=0.07807, avg_loss=0.07707]\n",
      "Step 504245  [5.351 sec/step, loss=0.07889, avg_loss=0.07709]\n",
      "Step 504246  [5.348 sec/step, loss=0.07647, avg_loss=0.07710]\n",
      "Step 504247  [5.361 sec/step, loss=0.07908, avg_loss=0.07715]\n",
      "Step 504248  [5.358 sec/step, loss=0.07854, avg_loss=0.07716]\n",
      "Step 504249  [5.359 sec/step, loss=0.07763, avg_loss=0.07717]\n",
      "Step 504250  [5.342 sec/step, loss=0.07634, avg_loss=0.07716]\n",
      "Step 504251  [5.349 sec/step, loss=0.07639, avg_loss=0.07716]\n",
      "Step 504252  [5.362 sec/step, loss=0.07867, avg_loss=0.07721]\n",
      "Step 504253  [5.372 sec/step, loss=0.07659, avg_loss=0.07724]\n",
      "Step 504254  [5.370 sec/step, loss=0.07759, avg_loss=0.07726]\n",
      "Step 504255  [5.379 sec/step, loss=0.07726, avg_loss=0.07726]\n",
      "Step 504256  [5.374 sec/step, loss=0.07393, avg_loss=0.07723]\n",
      "Step 504257  [5.371 sec/step, loss=0.07645, avg_loss=0.07724]\n",
      "Step 504258  [5.379 sec/step, loss=0.07357, avg_loss=0.07723]\n",
      "Step 504259  [5.385 sec/step, loss=0.07834, avg_loss=0.07725]\n",
      "Step 504260  [5.371 sec/step, loss=0.07417, avg_loss=0.07722]\n",
      "Step 504261  [5.368 sec/step, loss=0.07496, avg_loss=0.07720]\n",
      "Step 504262  [5.322 sec/step, loss=0.07720, avg_loss=0.07731]\n",
      "Step 504263  [5.345 sec/step, loss=0.07771, avg_loss=0.07733]\n",
      "Step 504264  [5.329 sec/step, loss=0.07678, avg_loss=0.07733]\n",
      "Step 504265  [5.310 sec/step, loss=0.06828, avg_loss=0.07726]\n",
      "Generated 32 batches of size 32 in 2.333 sec\n",
      "Step 504266  [5.305 sec/step, loss=0.07718, avg_loss=0.07725]\n",
      "Step 504267  [5.289 sec/step, loss=0.07724, avg_loss=0.07724]\n",
      "Step 504268  [5.279 sec/step, loss=0.07568, avg_loss=0.07725]\n",
      "Step 504269  [5.266 sec/step, loss=0.08071, avg_loss=0.07730]\n",
      "Step 504270  [5.312 sec/step, loss=0.06832, avg_loss=0.07722]\n",
      "Step 504271  [5.323 sec/step, loss=0.07788, avg_loss=0.07726]\n",
      "Step 504272  [5.334 sec/step, loss=0.07651, avg_loss=0.07727]\n",
      "Step 504273  [5.348 sec/step, loss=0.07548, avg_loss=0.07726]\n",
      "Step 504274  [5.358 sec/step, loss=0.08001, avg_loss=0.07734]\n",
      "Step 504275  [5.366 sec/step, loss=0.07678, avg_loss=0.07735]\n",
      "Step 504276  [5.368 sec/step, loss=0.07456, avg_loss=0.07734]\n",
      "Step 504277  [5.375 sec/step, loss=0.07341, avg_loss=0.07741]\n",
      "Step 504278  [5.357 sec/step, loss=0.07616, avg_loss=0.07739]\n",
      "Step 504279  [5.350 sec/step, loss=0.07609, avg_loss=0.07741]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 504280  [5.354 sec/step, loss=0.07777, avg_loss=0.07742]\n",
      "Step 504281  [5.362 sec/step, loss=0.07468, avg_loss=0.07742]\n",
      "Step 504282  [5.352 sec/step, loss=0.07579, avg_loss=0.07737]\n",
      "Step 504283  [5.344 sec/step, loss=0.07609, avg_loss=0.07734]\n",
      "Step 504284  [5.356 sec/step, loss=0.07578, avg_loss=0.07740]\n",
      "Step 504285  [5.397 sec/step, loss=0.07553, avg_loss=0.07740]\n",
      "Step 504286  [5.354 sec/step, loss=0.07807, avg_loss=0.07745]\n",
      "Step 504287  [5.335 sec/step, loss=0.06731, avg_loss=0.07731]\n",
      "Step 504288  [5.320 sec/step, loss=0.07810, avg_loss=0.07730]\n",
      "Step 504289  [5.311 sec/step, loss=0.07687, avg_loss=0.07726]\n",
      "Step 504290  [5.314 sec/step, loss=0.07728, avg_loss=0.07723]\n",
      "Step 504291  [5.315 sec/step, loss=0.07349, avg_loss=0.07718]\n",
      "Step 504292  [5.326 sec/step, loss=0.07708, avg_loss=0.07716]\n",
      "Step 504293  [5.344 sec/step, loss=0.07815, avg_loss=0.07715]\n",
      "Step 504294  [5.348 sec/step, loss=0.07935, avg_loss=0.07717]\n",
      "Step 504295  [5.330 sec/step, loss=0.07706, avg_loss=0.07711]\n",
      "Step 504296  [5.337 sec/step, loss=0.07826, avg_loss=0.07708]\n",
      "Step 504297  [5.315 sec/step, loss=0.07339, avg_loss=0.07699]\n",
      "Generated 32 batches of size 32 in 2.460 sec\n",
      "Step 504298  [5.304 sec/step, loss=0.07592, avg_loss=0.07694]\n",
      "Step 504299  [5.308 sec/step, loss=0.07768, avg_loss=0.07690]\n",
      "Step 504300  [5.335 sec/step, loss=0.07723, avg_loss=0.07692]\n",
      "Writing summary at step: 504300\n",
      "Step 504301  [5.329 sec/step, loss=0.07809, avg_loss=0.07687]\n",
      "Step 504302  [5.333 sec/step, loss=0.07573, avg_loss=0.07686]\n",
      "Step 504303  [5.324 sec/step, loss=0.07700, avg_loss=0.07681]\n",
      "Step 504304  [5.335 sec/step, loss=0.07772, avg_loss=0.07680]\n",
      "Step 504305  [5.335 sec/step, loss=0.07697, avg_loss=0.07677]\n",
      "Step 504306  [5.327 sec/step, loss=0.07490, avg_loss=0.07675]\n",
      "Step 504307  [5.316 sec/step, loss=0.07726, avg_loss=0.07673]\n",
      "Step 504308  [5.324 sec/step, loss=0.07784, avg_loss=0.07675]\n",
      "Step 504309  [5.331 sec/step, loss=0.07772, avg_loss=0.07676]\n",
      "Step 504310  [5.328 sec/step, loss=0.07799, avg_loss=0.07673]\n",
      "Step 504311  [5.307 sec/step, loss=0.07604, avg_loss=0.07667]\n",
      "Step 504312  [5.312 sec/step, loss=0.07590, avg_loss=0.07663]\n",
      "Step 504313  [5.324 sec/step, loss=0.07607, avg_loss=0.07660]\n",
      "Step 504314  [5.322 sec/step, loss=0.07767, avg_loss=0.07658]\n",
      "Step 504315  [5.339 sec/step, loss=0.07779, avg_loss=0.07656]\n",
      "Step 504316  [5.285 sec/step, loss=0.07371, avg_loss=0.07659]\n",
      "Step 504317  [5.287 sec/step, loss=0.07660, avg_loss=0.07656]\n",
      "Step 504318  [5.286 sec/step, loss=0.07661, avg_loss=0.07653]\n",
      "Step 504319  [5.271 sec/step, loss=0.07408, avg_loss=0.07647]\n",
      "Step 504320  [5.276 sec/step, loss=0.07951, avg_loss=0.07646]\n",
      "Step 504321  [5.269 sec/step, loss=0.06891, avg_loss=0.07641]\n",
      "Step 504322  [5.236 sec/step, loss=0.07363, avg_loss=0.07637]\n",
      "Step 504323  [5.223 sec/step, loss=0.07601, avg_loss=0.07634]\n",
      "Step 504324  [5.235 sec/step, loss=0.07626, avg_loss=0.07635]\n",
      "Step 504325  [5.235 sec/step, loss=0.07548, avg_loss=0.07631]\n",
      "Step 504326  [5.252 sec/step, loss=0.07793, avg_loss=0.07632]\n",
      "Step 504327  [5.247 sec/step, loss=0.07628, avg_loss=0.07632]\n",
      "Step 504328  [5.243 sec/step, loss=0.07583, avg_loss=0.07632]\n",
      "Generated 32 batches of size 32 in 2.518 sec\n",
      "Step 504329  [5.249 sec/step, loss=0.07528, avg_loss=0.07630]\n",
      "Step 504330  [5.233 sec/step, loss=0.07561, avg_loss=0.07626]\n",
      "Step 504331  [5.241 sec/step, loss=0.07665, avg_loss=0.07628]\n",
      "Step 504332  [5.262 sec/step, loss=0.07688, avg_loss=0.07637]\n",
      "Step 504333  [5.225 sec/step, loss=0.07274, avg_loss=0.07632]\n",
      "Step 504334  [5.236 sec/step, loss=0.07847, avg_loss=0.07633]\n",
      "Step 504335  [5.236 sec/step, loss=0.07614, avg_loss=0.07636]\n",
      "Step 504336  [5.261 sec/step, loss=0.07497, avg_loss=0.07632]\n",
      "Step 504337  [5.322 sec/step, loss=0.06773, avg_loss=0.07625]\n",
      "Step 504338  [5.338 sec/step, loss=0.07758, avg_loss=0.07625]\n",
      "Step 504339  [5.343 sec/step, loss=0.07474, avg_loss=0.07621]\n",
      "Step 504340  [5.328 sec/step, loss=0.06543, avg_loss=0.07609]\n",
      "Step 504341  [5.338 sec/step, loss=0.07476, avg_loss=0.07603]\n",
      "Step 504342  [5.332 sec/step, loss=0.07678, avg_loss=0.07603]\n",
      "Step 504343  [5.346 sec/step, loss=0.07442, avg_loss=0.07604]\n",
      "Step 504344  [5.357 sec/step, loss=0.07888, avg_loss=0.07604]\n",
      "Step 504345  [5.367 sec/step, loss=0.07524, avg_loss=0.07601]\n",
      "Step 504346  [5.368 sec/step, loss=0.07360, avg_loss=0.07598]\n",
      "Step 504347  [5.357 sec/step, loss=0.07596, avg_loss=0.07595]\n",
      "Step 504348  [5.341 sec/step, loss=0.07541, avg_loss=0.07592]\n",
      "Step 504349  [5.345 sec/step, loss=0.07868, avg_loss=0.07593]\n",
      "Step 504350  [5.362 sec/step, loss=0.07759, avg_loss=0.07594]\n",
      "Step 504351  [5.359 sec/step, loss=0.07755, avg_loss=0.07595]\n",
      "Step 504352  [5.346 sec/step, loss=0.07602, avg_loss=0.07592]\n",
      "Step 504353  [5.357 sec/step, loss=0.07911, avg_loss=0.07595]\n",
      "Step 504354  [5.348 sec/step, loss=0.07498, avg_loss=0.07592]\n",
      "Step 504355  [5.329 sec/step, loss=0.07608, avg_loss=0.07591]\n",
      "Step 504356  [5.344 sec/step, loss=0.07657, avg_loss=0.07594]\n",
      "Step 504357  [5.351 sec/step, loss=0.07401, avg_loss=0.07591]\n",
      "Step 504358  [5.359 sec/step, loss=0.07683, avg_loss=0.07595]\n",
      "Step 504359  [5.338 sec/step, loss=0.07538, avg_loss=0.07592]\n",
      "Step 504360  [5.349 sec/step, loss=0.07712, avg_loss=0.07595]\n",
      "Generated 32 batches of size 32 in 2.529 sec\n",
      "Step 504361  [5.358 sec/step, loss=0.07442, avg_loss=0.07594]\n",
      "Step 504362  [5.366 sec/step, loss=0.07745, avg_loss=0.07594]\n",
      "Step 504363  [5.352 sec/step, loss=0.07423, avg_loss=0.07591]\n",
      "Step 504364  [5.362 sec/step, loss=0.07896, avg_loss=0.07593]\n",
      "Step 504365  [5.364 sec/step, loss=0.07337, avg_loss=0.07598]\n",
      "Step 504366  [5.362 sec/step, loss=0.07715, avg_loss=0.07598]\n",
      "Step 504367  [5.366 sec/step, loss=0.07664, avg_loss=0.07597]\n",
      "Step 504368  [5.361 sec/step, loss=0.07679, avg_loss=0.07599]\n",
      "Step 504369  [5.400 sec/step, loss=0.06802, avg_loss=0.07586]\n",
      "Step 504370  [5.364 sec/step, loss=0.07793, avg_loss=0.07595]\n",
      "Step 504371  [5.379 sec/step, loss=0.07709, avg_loss=0.07595]\n",
      "Step 504372  [5.378 sec/step, loss=0.07658, avg_loss=0.07595]\n",
      "Step 504373  [5.349 sec/step, loss=0.07579, avg_loss=0.07595]\n",
      "Step 504374  [5.338 sec/step, loss=0.07523, avg_loss=0.07590]\n",
      "Step 504375  [5.341 sec/step, loss=0.07668, avg_loss=0.07590]\n",
      "Step 504376  [5.338 sec/step, loss=0.07415, avg_loss=0.07590]\n",
      "Step 504377  [5.359 sec/step, loss=0.07604, avg_loss=0.07592]\n",
      "Step 504378  [5.367 sec/step, loss=0.07390, avg_loss=0.07590]\n",
      "Step 504379  [5.364 sec/step, loss=0.07616, avg_loss=0.07590]\n",
      "Step 504380  [5.372 sec/step, loss=0.07870, avg_loss=0.07591]\n",
      "Step 504381  [5.376 sec/step, loss=0.07721, avg_loss=0.07594]\n",
      "Step 504382  [5.379 sec/step, loss=0.07690, avg_loss=0.07595]\n",
      "Step 504383  [5.394 sec/step, loss=0.07652, avg_loss=0.07595]\n",
      "Step 504384  [5.412 sec/step, loss=0.07640, avg_loss=0.07596]\n",
      "Step 504385  [5.399 sec/step, loss=0.07714, avg_loss=0.07597]\n",
      "Step 504386  [5.381 sec/step, loss=0.07269, avg_loss=0.07592]\n",
      "Step 504387  [5.399 sec/step, loss=0.07719, avg_loss=0.07602]\n",
      "Step 504388  [5.379 sec/step, loss=0.07572, avg_loss=0.07600]\n",
      "Step 504389  [5.358 sec/step, loss=0.06866, avg_loss=0.07591]\n",
      "Step 504390  [5.364 sec/step, loss=0.07801, avg_loss=0.07592]\n",
      "Step 504391  [5.368 sec/step, loss=0.07706, avg_loss=0.07596]\n",
      "Step 504392  [5.360 sec/step, loss=0.07684, avg_loss=0.07595]\n",
      "Generated 32 batches of size 32 in 2.405 sec\n",
      "Step 504393  [5.381 sec/step, loss=0.07391, avg_loss=0.07591]\n",
      "Step 504394  [5.372 sec/step, loss=0.07693, avg_loss=0.07589]\n",
      "Step 504395  [5.368 sec/step, loss=0.07599, avg_loss=0.07588]\n",
      "Step 504396  [5.355 sec/step, loss=0.07372, avg_loss=0.07583]\n",
      "Step 504397  [5.376 sec/step, loss=0.07814, avg_loss=0.07588]\n",
      "Step 504398  [5.366 sec/step, loss=0.07388, avg_loss=0.07586]\n",
      "Step 504399  [5.360 sec/step, loss=0.07638, avg_loss=0.07585]\n",
      "Step 504400  [5.345 sec/step, loss=0.07721, avg_loss=0.07584]\n",
      "Writing summary at step: 504400\n",
      "Step 504401  [5.336 sec/step, loss=0.07617, avg_loss=0.07583]\n",
      "Step 504402  [5.312 sec/step, loss=0.07266, avg_loss=0.07579]\n",
      "Step 504403  [5.305 sec/step, loss=0.07230, avg_loss=0.07575]\n",
      "Step 504404  [5.298 sec/step, loss=0.07598, avg_loss=0.07573]\n",
      "Step 504405  [5.299 sec/step, loss=0.07533, avg_loss=0.07571]\n",
      "Step 504406  [5.304 sec/step, loss=0.07578, avg_loss=0.07572]\n",
      "Step 504407  [5.318 sec/step, loss=0.07748, avg_loss=0.07573]\n",
      "Step 504408  [5.306 sec/step, loss=0.07542, avg_loss=0.07570]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 504409  [5.305 sec/step, loss=0.07675, avg_loss=0.07569]\n",
      "Step 504410  [5.317 sec/step, loss=0.07420, avg_loss=0.07565]\n",
      "Step 504411  [5.327 sec/step, loss=0.07645, avg_loss=0.07566]\n",
      "Step 504412  [5.318 sec/step, loss=0.07565, avg_loss=0.07565]\n",
      "Step 504413  [5.305 sec/step, loss=0.07687, avg_loss=0.07566]\n",
      "Step 504414  [5.314 sec/step, loss=0.07867, avg_loss=0.07567]\n",
      "Step 504415  [5.301 sec/step, loss=0.07685, avg_loss=0.07566]\n",
      "Step 504416  [5.299 sec/step, loss=0.07558, avg_loss=0.07568]\n",
      "Step 504417  [5.289 sec/step, loss=0.07567, avg_loss=0.07567]\n",
      "Step 504418  [5.293 sec/step, loss=0.07668, avg_loss=0.07567]\n",
      "Step 504419  [5.343 sec/step, loss=0.06729, avg_loss=0.07561]\n",
      "Step 504420  [5.327 sec/step, loss=0.07504, avg_loss=0.07556]\n",
      "Step 504421  [5.343 sec/step, loss=0.07701, avg_loss=0.07564]\n",
      "Step 504422  [5.354 sec/step, loss=0.07328, avg_loss=0.07564]\n",
      "Step 504423  [5.362 sec/step, loss=0.07633, avg_loss=0.07564]\n",
      "Generated 32 batches of size 32 in 2.388 sec\n",
      "Step 504424  [5.371 sec/step, loss=0.07671, avg_loss=0.07565]\n",
      "Step 504425  [5.368 sec/step, loss=0.07845, avg_loss=0.07568]\n",
      "Step 504426  [5.369 sec/step, loss=0.07706, avg_loss=0.07567]\n",
      "Step 504427  [5.360 sec/step, loss=0.07264, avg_loss=0.07563]\n",
      "Step 504428  [5.362 sec/step, loss=0.07682, avg_loss=0.07564]\n",
      "Step 504429  [5.369 sec/step, loss=0.07841, avg_loss=0.07567]\n",
      "Step 504430  [5.360 sec/step, loss=0.06831, avg_loss=0.07560]\n",
      "Step 504431  [5.363 sec/step, loss=0.07813, avg_loss=0.07561]\n",
      "Step 504432  [5.377 sec/step, loss=0.07733, avg_loss=0.07562]\n",
      "Step 504433  [5.396 sec/step, loss=0.07728, avg_loss=0.07566]\n",
      "Step 504434  [5.389 sec/step, loss=0.07384, avg_loss=0.07562]\n",
      "Step 504435  [5.386 sec/step, loss=0.07572, avg_loss=0.07561]\n",
      "Step 504436  [5.341 sec/step, loss=0.06782, avg_loss=0.07554]\n",
      "Step 504437  [5.296 sec/step, loss=0.07715, avg_loss=0.07564]\n",
      "Step 504438  [5.292 sec/step, loss=0.07948, avg_loss=0.07565]\n",
      "Step 504439  [5.269 sec/step, loss=0.07323, avg_loss=0.07564]\n",
      "Step 504440  [5.293 sec/step, loss=0.07785, avg_loss=0.07576]\n",
      "Step 504441  [5.300 sec/step, loss=0.07455, avg_loss=0.07576]\n",
      "Step 504442  [5.290 sec/step, loss=0.07577, avg_loss=0.07575]\n",
      "Step 504443  [5.282 sec/step, loss=0.07356, avg_loss=0.07574]\n",
      "Step 504444  [5.286 sec/step, loss=0.07790, avg_loss=0.07573]\n",
      "Step 504445  [5.263 sec/step, loss=0.07565, avg_loss=0.07574]\n",
      "Step 504446  [5.283 sec/step, loss=0.07608, avg_loss=0.07576]\n",
      "Step 504447  [5.279 sec/step, loss=0.07303, avg_loss=0.07573]\n",
      "Step 504448  [5.294 sec/step, loss=0.07874, avg_loss=0.07577]\n",
      "Step 504449  [5.307 sec/step, loss=0.07507, avg_loss=0.07573]\n",
      "Step 504450  [5.350 sec/step, loss=0.06783, avg_loss=0.07563]\n",
      "Step 504451  [5.348 sec/step, loss=0.07587, avg_loss=0.07562]\n",
      "Step 504452  [5.343 sec/step, loss=0.07600, avg_loss=0.07562]\n",
      "Step 504453  [5.336 sec/step, loss=0.07432, avg_loss=0.07557]\n",
      "Step 504454  [5.340 sec/step, loss=0.07723, avg_loss=0.07559]\n",
      "Step 504455  [5.348 sec/step, loss=0.07699, avg_loss=0.07560]\n",
      "Generated 32 batches of size 32 in 2.369 sec\n",
      "Step 504456  [5.343 sec/step, loss=0.07677, avg_loss=0.07560]\n",
      "Step 504457  [5.338 sec/step, loss=0.07673, avg_loss=0.07563]\n",
      "Step 504458  [5.332 sec/step, loss=0.07679, avg_loss=0.07563]\n",
      "Step 504459  [5.344 sec/step, loss=0.07731, avg_loss=0.07565]\n",
      "Step 504460  [5.360 sec/step, loss=0.07624, avg_loss=0.07564]\n",
      "Step 504461  [5.351 sec/step, loss=0.07447, avg_loss=0.07564]\n",
      "Step 504462  [5.348 sec/step, loss=0.07798, avg_loss=0.07564]\n",
      "Step 504463  [5.360 sec/step, loss=0.07878, avg_loss=0.07569]\n",
      "Step 504464  [5.340 sec/step, loss=0.07218, avg_loss=0.07562]\n",
      "Step 504465  [5.354 sec/step, loss=0.07700, avg_loss=0.07566]\n",
      "Step 504466  [5.352 sec/step, loss=0.07644, avg_loss=0.07565]\n",
      "Step 504467  [5.335 sec/step, loss=0.06651, avg_loss=0.07555]\n",
      "Step 504468  [5.337 sec/step, loss=0.07326, avg_loss=0.07551]\n",
      "Step 504469  [5.337 sec/step, loss=0.06775, avg_loss=0.07551]\n",
      "Step 504470  [5.335 sec/step, loss=0.07807, avg_loss=0.07551]\n",
      "Step 504471  [5.314 sec/step, loss=0.07331, avg_loss=0.07548]\n",
      "Step 504472  [5.302 sec/step, loss=0.07696, avg_loss=0.07548]\n",
      "Step 504473  [5.304 sec/step, loss=0.07682, avg_loss=0.07549]\n",
      "Step 504474  [5.313 sec/step, loss=0.07649, avg_loss=0.07550]\n",
      "Step 504475  [5.299 sec/step, loss=0.07559, avg_loss=0.07549]\n",
      "Step 504476  [5.306 sec/step, loss=0.07729, avg_loss=0.07552]\n",
      "Step 504477  [5.295 sec/step, loss=0.07471, avg_loss=0.07551]\n",
      "Step 504478  [5.307 sec/step, loss=0.07693, avg_loss=0.07554]\n",
      "Step 504479  [5.304 sec/step, loss=0.07206, avg_loss=0.07550]\n",
      "Step 504480  [5.304 sec/step, loss=0.07839, avg_loss=0.07550]\n",
      "Step 504481  [5.308 sec/step, loss=0.07696, avg_loss=0.07549]\n",
      "Step 504482  [5.309 sec/step, loss=0.07651, avg_loss=0.07549]\n",
      "Step 504483  [5.291 sec/step, loss=0.07258, avg_loss=0.07545]\n",
      "Step 504484  [5.277 sec/step, loss=0.07330, avg_loss=0.07542]\n",
      "Step 504485  [5.269 sec/step, loss=0.07648, avg_loss=0.07541]\n",
      "Step 504486  [5.274 sec/step, loss=0.07570, avg_loss=0.07544]\n",
      "Step 504487  [5.282 sec/step, loss=0.07535, avg_loss=0.07542]\n",
      "Generated 32 batches of size 32 in 2.515 sec\n",
      "Step 504488  [5.292 sec/step, loss=0.07678, avg_loss=0.07543]\n",
      "Step 504489  [5.316 sec/step, loss=0.07603, avg_loss=0.07551]\n",
      "Step 504490  [5.318 sec/step, loss=0.07778, avg_loss=0.07551]\n",
      "Step 504491  [5.324 sec/step, loss=0.07837, avg_loss=0.07552]\n",
      "Step 504492  [5.354 sec/step, loss=0.07610, avg_loss=0.07551]\n",
      "Step 504493  [5.340 sec/step, loss=0.07646, avg_loss=0.07554]\n",
      "Step 504494  [5.335 sec/step, loss=0.07545, avg_loss=0.07552]\n",
      "Step 504495  [5.335 sec/step, loss=0.07507, avg_loss=0.07551]\n",
      "Step 504496  [5.343 sec/step, loss=0.07613, avg_loss=0.07554]\n",
      "Step 504497  [5.346 sec/step, loss=0.07544, avg_loss=0.07551]\n",
      "Step 504498  [5.359 sec/step, loss=0.07612, avg_loss=0.07553]\n",
      "Step 504499  [5.359 sec/step, loss=0.07644, avg_loss=0.07553]\n",
      "Step 504500  [5.340 sec/step, loss=0.06732, avg_loss=0.07543]\n",
      "Writing summary at step: 504500\n",
      "Step 504501  [5.345 sec/step, loss=0.07664, avg_loss=0.07544]\n",
      "Step 504502  [5.357 sec/step, loss=0.07673, avg_loss=0.07548]\n",
      "Step 504503  [5.365 sec/step, loss=0.07502, avg_loss=0.07551]\n",
      "Step 504504  [5.369 sec/step, loss=0.07788, avg_loss=0.07553]\n",
      "Step 504505  [5.360 sec/step, loss=0.07559, avg_loss=0.07553]\n",
      "Step 504506  [5.358 sec/step, loss=0.07688, avg_loss=0.07554]\n",
      "Step 504507  [5.357 sec/step, loss=0.07811, avg_loss=0.07555]\n",
      "Step 504508  [5.369 sec/step, loss=0.07634, avg_loss=0.07555]\n",
      "Step 504509  [5.361 sec/step, loss=0.07287, avg_loss=0.07552]\n",
      "Step 504510  [5.322 sec/step, loss=0.07376, avg_loss=0.07551]\n",
      "Step 504511  [5.317 sec/step, loss=0.07148, avg_loss=0.07546]\n",
      "Step 504512  [5.329 sec/step, loss=0.07632, avg_loss=0.07547]\n",
      "Step 504513  [5.330 sec/step, loss=0.07690, avg_loss=0.07547]\n",
      "Step 504514  [5.332 sec/step, loss=0.07612, avg_loss=0.07544]\n",
      "Step 504515  [5.334 sec/step, loss=0.07868, avg_loss=0.07546]\n",
      "Step 504516  [5.346 sec/step, loss=0.07685, avg_loss=0.07547]\n",
      "Step 504517  [5.335 sec/step, loss=0.07384, avg_loss=0.07546]\n",
      "Step 504518  [5.346 sec/step, loss=0.07490, avg_loss=0.07544]\n",
      "Generated 32 batches of size 32 in 2.558 sec\n",
      "Step 504519  [5.296 sec/step, loss=0.07494, avg_loss=0.07551]\n",
      "Step 504520  [5.296 sec/step, loss=0.07544, avg_loss=0.07552]\n",
      "Step 504521  [5.296 sec/step, loss=0.07673, avg_loss=0.07552]\n",
      "Step 504522  [5.320 sec/step, loss=0.07463, avg_loss=0.07553]\n",
      "Step 504523  [5.369 sec/step, loss=0.06723, avg_loss=0.07544]\n",
      "Step 504524  [5.377 sec/step, loss=0.07739, avg_loss=0.07545]\n",
      "Step 504525  [5.369 sec/step, loss=0.07707, avg_loss=0.07543]\n",
      "Step 504526  [5.364 sec/step, loss=0.07817, avg_loss=0.07544]\n",
      "Step 504527  [5.373 sec/step, loss=0.07271, avg_loss=0.07544]\n",
      "Step 504528  [5.378 sec/step, loss=0.07658, avg_loss=0.07544]\n",
      "Step 504529  [5.359 sec/step, loss=0.07211, avg_loss=0.07538]\n",
      "Step 504530  [5.386 sec/step, loss=0.07758, avg_loss=0.07547]\n",
      "Step 504531  [5.371 sec/step, loss=0.07458, avg_loss=0.07544]\n",
      "Step 504532  [5.356 sec/step, loss=0.07638, avg_loss=0.07543]\n",
      "Step 504533  [5.349 sec/step, loss=0.07662, avg_loss=0.07542]\n",
      "Step 504534  [5.342 sec/step, loss=0.07406, avg_loss=0.07542]\n",
      "Step 504535  [5.348 sec/step, loss=0.07627, avg_loss=0.07543]\n",
      "Step 504536  [5.358 sec/step, loss=0.07512, avg_loss=0.07550]\n",
      "Step 504537  [5.359 sec/step, loss=0.07787, avg_loss=0.07551]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 504538  [5.355 sec/step, loss=0.07848, avg_loss=0.07550]\n",
      "Step 504539  [5.390 sec/step, loss=0.07514, avg_loss=0.07552]\n",
      "Step 504540  [5.394 sec/step, loss=0.07756, avg_loss=0.07551]\n",
      "Step 504541  [5.372 sec/step, loss=0.07571, avg_loss=0.07553]\n",
      "Step 504542  [5.377 sec/step, loss=0.07250, avg_loss=0.07549]\n",
      "Step 504543  [5.393 sec/step, loss=0.07858, avg_loss=0.07554]\n",
      "Step 504544  [5.378 sec/step, loss=0.07537, avg_loss=0.07552]\n",
      "Step 504545  [5.384 sec/step, loss=0.07789, avg_loss=0.07554]\n",
      "Step 504546  [5.370 sec/step, loss=0.07288, avg_loss=0.07551]\n",
      "Step 504547  [5.381 sec/step, loss=0.07774, avg_loss=0.07555]\n",
      "Step 504548  [5.399 sec/step, loss=0.07491, avg_loss=0.07552]\n",
      "Step 504549  [5.432 sec/step, loss=0.06815, avg_loss=0.07545]\n",
      "Step 504550  [5.370 sec/step, loss=0.07553, avg_loss=0.07552]\n",
      "Generated 32 batches of size 32 in 2.325 sec\n",
      "Step 504551  [5.377 sec/step, loss=0.07757, avg_loss=0.07554]\n",
      "Step 504552  [5.393 sec/step, loss=0.07669, avg_loss=0.07555]\n",
      "Step 504553  [5.387 sec/step, loss=0.07673, avg_loss=0.07557]\n",
      "Step 504554  [5.373 sec/step, loss=0.07290, avg_loss=0.07553]\n",
      "Step 504555  [5.384 sec/step, loss=0.07835, avg_loss=0.07554]\n",
      "Step 504556  [5.376 sec/step, loss=0.07709, avg_loss=0.07555]\n",
      "Step 504557  [5.373 sec/step, loss=0.07508, avg_loss=0.07553]\n",
      "Step 504558  [5.357 sec/step, loss=0.06766, avg_loss=0.07544]\n",
      "Step 504559  [5.364 sec/step, loss=0.07584, avg_loss=0.07542]\n",
      "Step 504560  [5.347 sec/step, loss=0.07529, avg_loss=0.07541]\n",
      "Step 504561  [5.362 sec/step, loss=0.07819, avg_loss=0.07545]\n",
      "Step 504562  [5.339 sec/step, loss=0.07341, avg_loss=0.07541]\n",
      "Step 504563  [5.327 sec/step, loss=0.07698, avg_loss=0.07539]\n",
      "Step 504564  [5.350 sec/step, loss=0.07704, avg_loss=0.07544]\n",
      "Step 504565  [5.361 sec/step, loss=0.07548, avg_loss=0.07542]\n",
      "Step 504566  [5.362 sec/step, loss=0.07589, avg_loss=0.07542]\n",
      "Step 504567  [5.374 sec/step, loss=0.07677, avg_loss=0.07552]\n",
      "Step 504568  [5.361 sec/step, loss=0.06634, avg_loss=0.07545]\n",
      "Step 504569  [5.315 sec/step, loss=0.07675, avg_loss=0.07554]\n",
      "Step 504570  [5.307 sec/step, loss=0.07609, avg_loss=0.07552]\n",
      "Step 504571  [5.306 sec/step, loss=0.07651, avg_loss=0.07555]\n",
      "Step 504572  [5.358 sec/step, loss=0.06772, avg_loss=0.07546]\n",
      "Step 504573  [5.348 sec/step, loss=0.07604, avg_loss=0.07545]\n",
      "Step 504574  [5.358 sec/step, loss=0.07742, avg_loss=0.07546]\n",
      "Step 504575  [5.366 sec/step, loss=0.07662, avg_loss=0.07547]\n",
      "Step 504576  [5.369 sec/step, loss=0.07606, avg_loss=0.07546]\n",
      "Step 504577  [5.379 sec/step, loss=0.07553, avg_loss=0.07547]\n",
      "Step 504578  [5.366 sec/step, loss=0.07688, avg_loss=0.07547]\n",
      "Step 504579  [5.381 sec/step, loss=0.07702, avg_loss=0.07552]\n",
      "Step 504580  [5.370 sec/step, loss=0.07737, avg_loss=0.07551]\n",
      "Step 504581  [5.347 sec/step, loss=0.07273, avg_loss=0.07546]\n",
      "Step 504582  [5.340 sec/step, loss=0.07311, avg_loss=0.07543]\n",
      "Generated 32 batches of size 32 in 2.323 sec\n",
      "Step 504583  [5.380 sec/step, loss=0.07473, avg_loss=0.07545]\n",
      "Step 504584  [5.387 sec/step, loss=0.07758, avg_loss=0.07549]\n",
      "Step 504585  [5.394 sec/step, loss=0.07812, avg_loss=0.07551]\n",
      "Step 504586  [5.393 sec/step, loss=0.07580, avg_loss=0.07551]\n",
      "Step 504587  [5.394 sec/step, loss=0.07517, avg_loss=0.07551]\n",
      "Step 504588  [5.386 sec/step, loss=0.07523, avg_loss=0.07549]\n",
      "Step 504589  [5.392 sec/step, loss=0.07743, avg_loss=0.07551]\n",
      "Step 504590  [5.383 sec/step, loss=0.07322, avg_loss=0.07546]\n",
      "Step 504591  [5.372 sec/step, loss=0.07391, avg_loss=0.07542]\n",
      "Step 504592  [5.347 sec/step, loss=0.07740, avg_loss=0.07543]\n",
      "Step 504593  [5.318 sec/step, loss=0.07241, avg_loss=0.07539]\n",
      "Step 504594  [5.336 sec/step, loss=0.07546, avg_loss=0.07539]\n",
      "Step 504595  [5.347 sec/step, loss=0.07684, avg_loss=0.07541]\n",
      "Step 504596  [5.331 sec/step, loss=0.07558, avg_loss=0.07540]\n",
      "Step 504597  [5.332 sec/step, loss=0.07643, avg_loss=0.07541]\n",
      "Step 504598  [5.330 sec/step, loss=0.07554, avg_loss=0.07541]\n",
      "Step 504599  [5.331 sec/step, loss=0.07636, avg_loss=0.07541]\n",
      "Step 504600  [5.355 sec/step, loss=0.07696, avg_loss=0.07550]\n",
      "Writing summary at step: 504600\n",
      "Step 504601  [5.361 sec/step, loss=0.07796, avg_loss=0.07552]\n",
      "Step 504602  [5.412 sec/step, loss=0.06766, avg_loss=0.07542]\n",
      "Step 504603  [5.425 sec/step, loss=0.07756, avg_loss=0.07545]\n",
      "Step 504604  [5.406 sec/step, loss=0.07506, avg_loss=0.07542]\n",
      "Step 504605  [5.438 sec/step, loss=0.07503, avg_loss=0.07542]\n",
      "Step 504606  [5.455 sec/step, loss=0.07535, avg_loss=0.07540]\n",
      "Step 504607  [5.446 sec/step, loss=0.07685, avg_loss=0.07539]\n",
      "Step 504608  [5.434 sec/step, loss=0.07506, avg_loss=0.07538]\n",
      "Step 504609  [5.451 sec/step, loss=0.07685, avg_loss=0.07541]\n",
      "Step 504610  [5.462 sec/step, loss=0.07716, avg_loss=0.07545]\n",
      "Step 504611  [5.465 sec/step, loss=0.07673, avg_loss=0.07550]\n",
      "Step 504612  [5.461 sec/step, loss=0.07663, avg_loss=0.07550]\n",
      "Step 504613  [5.469 sec/step, loss=0.07642, avg_loss=0.07550]\n",
      "Generated 32 batches of size 32 in 2.366 sec\n",
      "Step 504614  [5.470 sec/step, loss=0.07843, avg_loss=0.07552]\n",
      "Step 504615  [5.471 sec/step, loss=0.07773, avg_loss=0.07551]\n",
      "Step 504616  [5.474 sec/step, loss=0.07543, avg_loss=0.07550]\n",
      "Step 504617  [5.484 sec/step, loss=0.07450, avg_loss=0.07551]\n",
      "Step 504618  [5.451 sec/step, loss=0.07502, avg_loss=0.07551]\n",
      "Step 504619  [5.445 sec/step, loss=0.07616, avg_loss=0.07552]\n",
      "Step 504620  [5.430 sec/step, loss=0.06783, avg_loss=0.07544]\n",
      "Step 504621  [5.416 sec/step, loss=0.07334, avg_loss=0.07541]\n",
      "Step 504622  [5.411 sec/step, loss=0.07564, avg_loss=0.07542]\n",
      "Step 504623  [5.361 sec/step, loss=0.07577, avg_loss=0.07550]\n",
      "Step 504624  [5.331 sec/step, loss=0.07364, avg_loss=0.07547]\n",
      "Step 504625  [5.324 sec/step, loss=0.07543, avg_loss=0.07545]\n",
      "Step 504626  [5.308 sec/step, loss=0.07581, avg_loss=0.07543]\n",
      "Step 504627  [5.299 sec/step, loss=0.07553, avg_loss=0.07546]\n",
      "Step 504628  [5.296 sec/step, loss=0.07592, avg_loss=0.07545]\n",
      "Step 504629  [5.308 sec/step, loss=0.07752, avg_loss=0.07550]\n",
      "Step 504630  [5.304 sec/step, loss=0.07797, avg_loss=0.07551]\n",
      "Step 504631  [5.321 sec/step, loss=0.07575, avg_loss=0.07552]\n",
      "Step 504632  [5.328 sec/step, loss=0.07775, avg_loss=0.07553]\n",
      "Step 504633  [5.339 sec/step, loss=0.07593, avg_loss=0.07553]\n",
      "Step 504634  [5.353 sec/step, loss=0.07775, avg_loss=0.07556]\n",
      "Step 504635  [5.355 sec/step, loss=0.07628, avg_loss=0.07556]\n",
      "Step 504636  [5.375 sec/step, loss=0.07746, avg_loss=0.07559]\n",
      "Step 504637  [5.387 sec/step, loss=0.07666, avg_loss=0.07557]\n",
      "Step 504638  [5.379 sec/step, loss=0.07621, avg_loss=0.07555]\n",
      "Step 504639  [5.346 sec/step, loss=0.07218, avg_loss=0.07552]\n",
      "Step 504640  [5.384 sec/step, loss=0.06634, avg_loss=0.07541]\n",
      "Step 504641  [5.389 sec/step, loss=0.07734, avg_loss=0.07543]\n",
      "Step 504642  [5.404 sec/step, loss=0.07715, avg_loss=0.07547]\n",
      "Step 504643  [5.378 sec/step, loss=0.06759, avg_loss=0.07536]\n",
      "Step 504644  [5.378 sec/step, loss=0.07689, avg_loss=0.07538]\n",
      "Step 504645  [5.382 sec/step, loss=0.07742, avg_loss=0.07537]\n",
      "Generated 32 batches of size 32 in 2.606 sec\n",
      "Step 504646  [5.387 sec/step, loss=0.07469, avg_loss=0.07539]\n",
      "Step 504647  [5.376 sec/step, loss=0.07329, avg_loss=0.07535]\n",
      "Step 504648  [5.356 sec/step, loss=0.07589, avg_loss=0.07536]\n",
      "Step 504649  [5.300 sec/step, loss=0.07467, avg_loss=0.07542]\n",
      "Step 504650  [5.311 sec/step, loss=0.07735, avg_loss=0.07544]\n",
      "Step 504651  [5.299 sec/step, loss=0.07677, avg_loss=0.07543]\n",
      "Step 504652  [5.297 sec/step, loss=0.07682, avg_loss=0.07543]\n",
      "Step 504653  [5.302 sec/step, loss=0.07666, avg_loss=0.07543]\n",
      "Step 504654  [5.340 sec/step, loss=0.07669, avg_loss=0.07547]\n",
      "Step 504655  [5.327 sec/step, loss=0.07703, avg_loss=0.07546]\n",
      "Step 504656  [5.333 sec/step, loss=0.07611, avg_loss=0.07545]\n",
      "Step 504657  [5.361 sec/step, loss=0.07488, avg_loss=0.07544]\n",
      "Step 504658  [5.429 sec/step, loss=0.06861, avg_loss=0.07545]\n",
      "Step 504659  [5.428 sec/step, loss=0.07505, avg_loss=0.07545]\n",
      "Step 504660  [5.451 sec/step, loss=0.07423, avg_loss=0.07544]\n",
      "Step 504661  [5.446 sec/step, loss=0.07672, avg_loss=0.07542]\n",
      "Step 504662  [5.457 sec/step, loss=0.07307, avg_loss=0.07542]\n",
      "Step 504663  [5.467 sec/step, loss=0.07736, avg_loss=0.07542]\n",
      "Step 504664  [5.455 sec/step, loss=0.07341, avg_loss=0.07538]\n",
      "Step 504665  [5.448 sec/step, loss=0.07671, avg_loss=0.07540]\n",
      "Step 504666  [5.457 sec/step, loss=0.07868, avg_loss=0.07542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 504667  [5.445 sec/step, loss=0.06760, avg_loss=0.07533]\n",
      "Step 504668  [5.453 sec/step, loss=0.07562, avg_loss=0.07543]\n",
      "Step 504669  [5.456 sec/step, loss=0.07622, avg_loss=0.07542]\n",
      "Step 504670  [5.451 sec/step, loss=0.07522, avg_loss=0.07541]\n",
      "Step 504671  [5.465 sec/step, loss=0.07526, avg_loss=0.07540]\n",
      "Step 504672  [5.420 sec/step, loss=0.07550, avg_loss=0.07548]\n",
      "Step 504673  [5.420 sec/step, loss=0.07348, avg_loss=0.07545]\n",
      "Step 504674  [5.398 sec/step, loss=0.07542, avg_loss=0.07543]\n",
      "Step 504675  [5.410 sec/step, loss=0.07829, avg_loss=0.07545]\n",
      "Step 504676  [5.419 sec/step, loss=0.07810, avg_loss=0.07547]\n",
      "Step 504677  [5.404 sec/step, loss=0.07498, avg_loss=0.07546]\n",
      "Generated 32 batches of size 32 in 2.395 sec\n",
      "Step 504678  [5.410 sec/step, loss=0.07664, avg_loss=0.07546]\n",
      "Step 504679  [5.403 sec/step, loss=0.07625, avg_loss=0.07545]\n",
      "Step 504680  [5.402 sec/step, loss=0.07604, avg_loss=0.07544]\n",
      "Step 504681  [5.428 sec/step, loss=0.07678, avg_loss=0.07548]\n",
      "Step 504682  [5.432 sec/step, loss=0.07711, avg_loss=0.07552]\n",
      "Step 504683  [5.411 sec/step, loss=0.07808, avg_loss=0.07555]\n",
      "Step 504684  [5.396 sec/step, loss=0.07553, avg_loss=0.07553]\n",
      "Step 504685  [5.370 sec/step, loss=0.07281, avg_loss=0.07548]\n",
      "Step 504686  [5.374 sec/step, loss=0.07293, avg_loss=0.07545]\n",
      "Step 504687  [5.371 sec/step, loss=0.07728, avg_loss=0.07547]\n",
      "Step 504688  [5.388 sec/step, loss=0.07685, avg_loss=0.07549]\n",
      "Step 504689  [5.377 sec/step, loss=0.07746, avg_loss=0.07549]\n",
      "Step 504690  [5.366 sec/step, loss=0.07557, avg_loss=0.07551]\n",
      "Step 504691  [5.371 sec/step, loss=0.07702, avg_loss=0.07554]\n",
      "Step 504692  [5.380 sec/step, loss=0.07751, avg_loss=0.07554]\n",
      "Step 504693  [5.395 sec/step, loss=0.07356, avg_loss=0.07556]\n",
      "Step 504694  [5.378 sec/step, loss=0.07540, avg_loss=0.07556]\n",
      "Step 504695  [5.388 sec/step, loss=0.07727, avg_loss=0.07556]\n",
      "Step 504696  [5.400 sec/step, loss=0.07399, avg_loss=0.07554]\n",
      "Step 504697  [5.385 sec/step, loss=0.07661, avg_loss=0.07555]\n",
      "Step 504698  [5.380 sec/step, loss=0.07545, avg_loss=0.07555]\n",
      "Step 504699  [5.377 sec/step, loss=0.07296, avg_loss=0.07551]\n",
      "Step 504700  [5.355 sec/step, loss=0.06673, avg_loss=0.07541]\n",
      "Writing summary at step: 504700\n",
      "Step 504701  [5.342 sec/step, loss=0.07679, avg_loss=0.07540]\n",
      "Step 504702  [5.343 sec/step, loss=0.06863, avg_loss=0.07541]\n",
      "Step 504703  [5.330 sec/step, loss=0.07640, avg_loss=0.07540]\n",
      "Step 504704  [5.350 sec/step, loss=0.07774, avg_loss=0.07542]\n",
      "Step 504705  [5.310 sec/step, loss=0.07237, avg_loss=0.07540]\n",
      "Step 504706  [5.300 sec/step, loss=0.07516, avg_loss=0.07539]\n",
      "Step 504707  [5.294 sec/step, loss=0.07483, avg_loss=0.07537]\n",
      "Step 504708  [5.317 sec/step, loss=0.07464, avg_loss=0.07537]\n",
      "Generated 32 batches of size 32 in 2.452 sec\n",
      "Step 504709  [5.320 sec/step, loss=0.07766, avg_loss=0.07538]\n",
      "Step 504710  [5.327 sec/step, loss=0.07348, avg_loss=0.07534]\n",
      "Step 504711  [5.335 sec/step, loss=0.07708, avg_loss=0.07534]\n",
      "Step 504712  [5.334 sec/step, loss=0.07592, avg_loss=0.07534]\n",
      "Step 504713  [5.327 sec/step, loss=0.07394, avg_loss=0.07531]\n",
      "Step 504714  [5.341 sec/step, loss=0.07437, avg_loss=0.07527]\n",
      "Step 504715  [5.322 sec/step, loss=0.07204, avg_loss=0.07521]\n",
      "Step 504716  [5.322 sec/step, loss=0.07641, avg_loss=0.07522]\n",
      "Step 504717  [5.335 sec/step, loss=0.07815, avg_loss=0.07526]\n",
      "Step 504718  [5.332 sec/step, loss=0.07398, avg_loss=0.07525]\n",
      "Step 504719  [5.351 sec/step, loss=0.07684, avg_loss=0.07526]\n",
      "Step 504720  [5.365 sec/step, loss=0.07497, avg_loss=0.07533]\n",
      "Step 504721  [5.387 sec/step, loss=0.07613, avg_loss=0.07536]\n",
      "Step 504722  [5.361 sec/step, loss=0.07534, avg_loss=0.07535]\n",
      "Step 504723  [5.366 sec/step, loss=0.07652, avg_loss=0.07536]\n",
      "Step 504724  [5.389 sec/step, loss=0.07799, avg_loss=0.07540]\n",
      "Step 504725  [5.394 sec/step, loss=0.07690, avg_loss=0.07542]\n",
      "Step 504726  [5.399 sec/step, loss=0.07482, avg_loss=0.07541]\n",
      "Step 504727  [5.423 sec/step, loss=0.07716, avg_loss=0.07543]\n",
      "Step 504728  [5.433 sec/step, loss=0.07805, avg_loss=0.07545]\n",
      "Step 504729  [5.439 sec/step, loss=0.07822, avg_loss=0.07545]\n",
      "Step 504730  [5.432 sec/step, loss=0.07700, avg_loss=0.07544]\n",
      "Step 504731  [5.422 sec/step, loss=0.07572, avg_loss=0.07544]\n",
      "Step 504732  [5.409 sec/step, loss=0.07672, avg_loss=0.07543]\n",
      "Step 504733  [5.408 sec/step, loss=0.07849, avg_loss=0.07546]\n",
      "Step 504734  [5.407 sec/step, loss=0.07553, avg_loss=0.07544]\n",
      "Step 504735  [5.456 sec/step, loss=0.06679, avg_loss=0.07534]\n",
      "Step 504736  [5.449 sec/step, loss=0.07646, avg_loss=0.07533]\n",
      "Step 504737  [5.432 sec/step, loss=0.07643, avg_loss=0.07533]\n",
      "Step 504738  [5.439 sec/step, loss=0.07587, avg_loss=0.07533]\n",
      "Step 504739  [5.434 sec/step, loss=0.06748, avg_loss=0.07528]\n",
      "Step 504740  [5.377 sec/step, loss=0.07570, avg_loss=0.07537]\n",
      "Generated 32 batches of size 32 in 2.563 sec\n",
      "Step 504741  [5.367 sec/step, loss=0.07550, avg_loss=0.07536]\n",
      "Step 504742  [5.348 sec/step, loss=0.07662, avg_loss=0.07535]\n",
      "Step 504743  [5.365 sec/step, loss=0.07631, avg_loss=0.07544]\n",
      "Step 504744  [5.378 sec/step, loss=0.07816, avg_loss=0.07545]\n",
      "Step 504745  [5.360 sec/step, loss=0.07503, avg_loss=0.07543]\n",
      "Step 504746  [5.356 sec/step, loss=0.07434, avg_loss=0.07542]\n",
      "Step 504747  [5.376 sec/step, loss=0.07697, avg_loss=0.07546]\n",
      "Step 504748  [5.357 sec/step, loss=0.07429, avg_loss=0.07544]\n",
      "Step 504749  [5.390 sec/step, loss=0.07480, avg_loss=0.07544]\n",
      "Step 504750  [5.403 sec/step, loss=0.07692, avg_loss=0.07544]\n",
      "Step 504751  [5.404 sec/step, loss=0.07682, avg_loss=0.07544]\n",
      "Step 504752  [5.411 sec/step, loss=0.07552, avg_loss=0.07543]\n",
      "Step 504753  [5.402 sec/step, loss=0.07476, avg_loss=0.07541]\n",
      "Step 504754  [5.368 sec/step, loss=0.07534, avg_loss=0.07540]\n",
      "Step 504755  [5.359 sec/step, loss=0.07272, avg_loss=0.07535]\n",
      "Step 504756  [5.347 sec/step, loss=0.07492, avg_loss=0.07534]\n",
      "Step 504757  [5.326 sec/step, loss=0.07529, avg_loss=0.07534]\n",
      "Step 504758  [5.286 sec/step, loss=0.07717, avg_loss=0.07543]\n",
      "Step 504759  [5.328 sec/step, loss=0.06694, avg_loss=0.07535]\n",
      "Step 504760  [5.306 sec/step, loss=0.07536, avg_loss=0.07536]\n",
      "Step 504761  [5.310 sec/step, loss=0.07671, avg_loss=0.07536]\n",
      "Step 504762  [5.310 sec/step, loss=0.07609, avg_loss=0.07539]\n",
      "Step 504763  [5.295 sec/step, loss=0.07288, avg_loss=0.07535]\n",
      "Step 504764  [5.298 sec/step, loss=0.07660, avg_loss=0.07538]\n",
      "Step 504765  [5.276 sec/step, loss=0.06796, avg_loss=0.07529]\n",
      "Step 504766  [5.257 sec/step, loss=0.07229, avg_loss=0.07523]\n",
      "Step 504767  [5.281 sec/step, loss=0.07804, avg_loss=0.07533]\n",
      "Step 504768  [5.288 sec/step, loss=0.07602, avg_loss=0.07533]\n",
      "Step 504769  [5.283 sec/step, loss=0.07779, avg_loss=0.07535]\n",
      "Step 504770  [5.284 sec/step, loss=0.07597, avg_loss=0.07536]\n",
      "Step 504771  [5.271 sec/step, loss=0.07460, avg_loss=0.07535]\n",
      "Step 504772  [5.268 sec/step, loss=0.07623, avg_loss=0.07536]\n",
      "Generated 32 batches of size 32 in 2.323 sec\n",
      "Step 504773  [5.287 sec/step, loss=0.07660, avg_loss=0.07539]\n",
      "Step 504774  [5.319 sec/step, loss=0.07436, avg_loss=0.07538]\n",
      "Step 504775  [5.319 sec/step, loss=0.07901, avg_loss=0.07539]\n",
      "Step 504776  [5.315 sec/step, loss=0.07784, avg_loss=0.07538]\n",
      "Step 504777  [5.321 sec/step, loss=0.07622, avg_loss=0.07540]\n",
      "Step 504778  [5.329 sec/step, loss=0.07745, avg_loss=0.07540]\n",
      "Step 504779  [5.349 sec/step, loss=0.07724, avg_loss=0.07541]\n",
      "Step 504780  [5.342 sec/step, loss=0.07531, avg_loss=0.07541]\n",
      "Step 504781  [5.338 sec/step, loss=0.07766, avg_loss=0.07542]\n",
      "Step 504782  [5.345 sec/step, loss=0.07743, avg_loss=0.07542]\n",
      "Step 504783  [5.353 sec/step, loss=0.07830, avg_loss=0.07542]\n",
      "Step 504784  [5.376 sec/step, loss=0.07755, avg_loss=0.07544]\n",
      "Step 504785  [5.394 sec/step, loss=0.07636, avg_loss=0.07548]\n",
      "Step 504786  [5.399 sec/step, loss=0.07394, avg_loss=0.07549]\n",
      "Step 504787  [5.383 sec/step, loss=0.07504, avg_loss=0.07546]\n",
      "Step 504788  [5.375 sec/step, loss=0.07685, avg_loss=0.07546]\n",
      "Step 504789  [5.380 sec/step, loss=0.07827, avg_loss=0.07547]\n",
      "Step 504790  [5.377 sec/step, loss=0.07274, avg_loss=0.07544]\n",
      "Step 504791  [5.372 sec/step, loss=0.07452, avg_loss=0.07542]\n",
      "Step 504792  [5.374 sec/step, loss=0.07493, avg_loss=0.07539]\n",
      "Step 504793  [5.364 sec/step, loss=0.07567, avg_loss=0.07541]\n",
      "Step 504794  [5.368 sec/step, loss=0.07666, avg_loss=0.07543]\n",
      "Step 504795  [5.356 sec/step, loss=0.07712, avg_loss=0.07543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 504796  [5.344 sec/step, loss=0.07592, avg_loss=0.07544]\n",
      "Step 504797  [5.399 sec/step, loss=0.06846, avg_loss=0.07536]\n",
      "Step 504798  [5.417 sec/step, loss=0.07890, avg_loss=0.07540]\n",
      "Step 504799  [5.428 sec/step, loss=0.07700, avg_loss=0.07544]\n",
      "Step 504800  [5.444 sec/step, loss=0.07709, avg_loss=0.07554]\n",
      "Writing summary at step: 504800\n",
      "Step 504801  [5.454 sec/step, loss=0.07775, avg_loss=0.07555]\n",
      "Step 504802  [5.405 sec/step, loss=0.07703, avg_loss=0.07564]\n",
      "Step 504803  [5.402 sec/step, loss=0.07543, avg_loss=0.07563]\n",
      "Generated 32 batches of size 32 in 2.341 sec\n",
      "Step 504804  [5.397 sec/step, loss=0.07612, avg_loss=0.07561]\n",
      "Step 504805  [5.424 sec/step, loss=0.07611, avg_loss=0.07565]\n",
      "Step 504806  [5.431 sec/step, loss=0.07513, avg_loss=0.07565]\n",
      "Step 504807  [5.435 sec/step, loss=0.07693, avg_loss=0.07567]\n",
      "Step 504808  [5.407 sec/step, loss=0.07190, avg_loss=0.07564]\n",
      "Step 504809  [5.421 sec/step, loss=0.07496, avg_loss=0.07561]\n",
      "Step 504810  [5.418 sec/step, loss=0.07574, avg_loss=0.07564]\n",
      "Step 504811  [5.414 sec/step, loss=0.07641, avg_loss=0.07563]\n",
      "Step 504812  [5.398 sec/step, loss=0.06633, avg_loss=0.07553]\n",
      "Step 504813  [5.398 sec/step, loss=0.07578, avg_loss=0.07555]\n",
      "Step 504814  [5.397 sec/step, loss=0.07383, avg_loss=0.07555]\n",
      "Step 504815  [5.408 sec/step, loss=0.07776, avg_loss=0.07560]\n",
      "Step 504816  [5.404 sec/step, loss=0.07385, avg_loss=0.07558]\n",
      "Step 504817  [5.401 sec/step, loss=0.07838, avg_loss=0.07558]\n",
      "Step 504818  [5.407 sec/step, loss=0.07693, avg_loss=0.07561]\n",
      "Step 504819  [5.377 sec/step, loss=0.06711, avg_loss=0.07551]\n",
      "Step 504820  [5.374 sec/step, loss=0.07600, avg_loss=0.07552]\n",
      "Step 504821  [5.382 sec/step, loss=0.07713, avg_loss=0.07553]\n",
      "Step 504822  [5.392 sec/step, loss=0.07629, avg_loss=0.07554]\n",
      "Step 504823  [5.437 sec/step, loss=0.06720, avg_loss=0.07545]\n",
      "Step 504824  [5.434 sec/step, loss=0.07583, avg_loss=0.07543]\n",
      "Step 504825  [5.429 sec/step, loss=0.07265, avg_loss=0.07538]\n",
      "Step 504826  [5.420 sec/step, loss=0.07554, avg_loss=0.07539]\n",
      "Step 504827  [5.407 sec/step, loss=0.07733, avg_loss=0.07539]\n",
      "Step 504828  [5.398 sec/step, loss=0.07586, avg_loss=0.07537]\n",
      "Step 504829  [5.385 sec/step, loss=0.07415, avg_loss=0.07533]\n",
      "Step 504830  [5.389 sec/step, loss=0.07601, avg_loss=0.07532]\n",
      "Step 504831  [5.386 sec/step, loss=0.07717, avg_loss=0.07534]\n",
      "Step 504832  [5.399 sec/step, loss=0.07563, avg_loss=0.07532]\n",
      "Step 504833  [5.399 sec/step, loss=0.07829, avg_loss=0.07532]\n",
      "Step 504834  [5.408 sec/step, loss=0.07603, avg_loss=0.07533]\n",
      "Step 504835  [5.366 sec/step, loss=0.07772, avg_loss=0.07544]\n",
      "Generated 32 batches of size 32 in 2.529 sec\n",
      "Step 504836  [5.357 sec/step, loss=0.07545, avg_loss=0.07543]\n",
      "Step 504837  [5.346 sec/step, loss=0.07299, avg_loss=0.07539]\n",
      "Step 504838  [5.352 sec/step, loss=0.07790, avg_loss=0.07541]\n",
      "Step 504839  [5.379 sec/step, loss=0.07699, avg_loss=0.07551]\n",
      "Step 504840  [5.391 sec/step, loss=0.07684, avg_loss=0.07552]\n",
      "Step 504841  [5.402 sec/step, loss=0.07634, avg_loss=0.07553]\n",
      "Step 504842  [5.406 sec/step, loss=0.07659, avg_loss=0.07553]\n",
      "Step 504843  [5.391 sec/step, loss=0.07306, avg_loss=0.07549]\n",
      "Step 504844  [5.385 sec/step, loss=0.07705, avg_loss=0.07548]\n",
      "Step 504845  [5.379 sec/step, loss=0.07396, avg_loss=0.07547]\n",
      "Step 504846  [5.379 sec/step, loss=0.07693, avg_loss=0.07550]\n",
      "Step 504847  [5.353 sec/step, loss=0.07260, avg_loss=0.07545]\n",
      "Step 504848  [5.379 sec/step, loss=0.07585, avg_loss=0.07547]\n",
      "Step 504849  [5.359 sec/step, loss=0.07852, avg_loss=0.07551]\n",
      "Step 504850  [5.350 sec/step, loss=0.07600, avg_loss=0.07550]\n",
      "Step 504851  [5.357 sec/step, loss=0.07677, avg_loss=0.07550]\n",
      "Step 504852  [5.354 sec/step, loss=0.07709, avg_loss=0.07551]\n",
      "Step 504853  [5.369 sec/step, loss=0.07530, avg_loss=0.07552]\n",
      "Step 504854  [5.379 sec/step, loss=0.07636, avg_loss=0.07553]\n",
      "Step 504855  [5.407 sec/step, loss=0.07690, avg_loss=0.07557]\n",
      "Step 504856  [5.420 sec/step, loss=0.07658, avg_loss=0.07559]\n",
      "Step 504857  [5.433 sec/step, loss=0.07703, avg_loss=0.07560]\n",
      "Step 504858  [5.407 sec/step, loss=0.06873, avg_loss=0.07552]\n",
      "Step 504859  [5.342 sec/step, loss=0.07300, avg_loss=0.07558]\n",
      "Step 504860  [5.341 sec/step, loss=0.07436, avg_loss=0.07557]\n",
      "Step 504861  [5.336 sec/step, loss=0.07761, avg_loss=0.07558]\n",
      "Step 504862  [5.339 sec/step, loss=0.07661, avg_loss=0.07559]\n",
      "Step 504863  [5.336 sec/step, loss=0.07541, avg_loss=0.07561]\n",
      "Step 504864  [5.356 sec/step, loss=0.07455, avg_loss=0.07559]\n",
      "Step 504865  [5.373 sec/step, loss=0.07623, avg_loss=0.07567]\n",
      "Step 504866  [5.392 sec/step, loss=0.07762, avg_loss=0.07573]\n",
      "Step 504867  [5.379 sec/step, loss=0.07090, avg_loss=0.07565]\n",
      "Generated 32 batches of size 32 in 2.757 sec\n",
      "Step 504868  [5.433 sec/step, loss=0.06755, avg_loss=0.07557]\n",
      "Step 504869  [5.446 sec/step, loss=0.07822, avg_loss=0.07557]\n",
      "Step 504870  [5.444 sec/step, loss=0.07541, avg_loss=0.07557]\n",
      "Step 504871  [5.443 sec/step, loss=0.07672, avg_loss=0.07559]\n",
      "Step 504872  [5.444 sec/step, loss=0.07295, avg_loss=0.07556]\n",
      "Step 504873  [5.433 sec/step, loss=0.07482, avg_loss=0.07554]\n",
      "Step 504874  [5.407 sec/step, loss=0.07706, avg_loss=0.07557]\n",
      "Step 504875  [5.407 sec/step, loss=0.07767, avg_loss=0.07555]\n",
      "Step 504876  [5.406 sec/step, loss=0.07727, avg_loss=0.07555]\n",
      "Step 504877  [5.416 sec/step, loss=0.07761, avg_loss=0.07556]\n",
      "Step 504878  [5.392 sec/step, loss=0.07552, avg_loss=0.07554]\n",
      "Step 504879  [5.373 sec/step, loss=0.07483, avg_loss=0.07552]\n",
      "Step 504880  [5.387 sec/step, loss=0.07649, avg_loss=0.07553]\n",
      "Step 504881  [5.359 sec/step, loss=0.06607, avg_loss=0.07541]\n",
      "Step 504882  [5.369 sec/step, loss=0.07518, avg_loss=0.07539]\n",
      "Step 504883  [5.367 sec/step, loss=0.07584, avg_loss=0.07537]\n",
      "Step 504884  [5.359 sec/step, loss=0.07559, avg_loss=0.07535]\n",
      "Step 504885  [5.353 sec/step, loss=0.07608, avg_loss=0.07534]\n",
      "Step 504886  [5.359 sec/step, loss=0.07793, avg_loss=0.07538]\n",
      "Step 504887  [5.392 sec/step, loss=0.07456, avg_loss=0.07538]\n",
      "Step 504888  [5.442 sec/step, loss=0.06701, avg_loss=0.07528]\n",
      "Step 504889  [5.431 sec/step, loss=0.07530, avg_loss=0.07525]\n",
      "Step 504890  [5.431 sec/step, loss=0.07303, avg_loss=0.07525]\n",
      "Step 504891  [5.436 sec/step, loss=0.07757, avg_loss=0.07528]\n",
      "Step 504892  [5.427 sec/step, loss=0.07662, avg_loss=0.07530]\n",
      "Step 504893  [5.443 sec/step, loss=0.07834, avg_loss=0.07533]\n",
      "Step 504894  [5.440 sec/step, loss=0.07601, avg_loss=0.07532]\n",
      "Step 504895  [5.445 sec/step, loss=0.07699, avg_loss=0.07532]\n",
      "Step 504896  [5.456 sec/step, loss=0.07627, avg_loss=0.07532]\n",
      "Step 504897  [5.400 sec/step, loss=0.07349, avg_loss=0.07537]\n",
      "Step 504898  [5.391 sec/step, loss=0.07589, avg_loss=0.07534]\n",
      "Step 504899  [5.376 sec/step, loss=0.07562, avg_loss=0.07533]\n",
      "Generated 32 batches of size 32 in 2.344 sec\n",
      "Step 504900  [5.383 sec/step, loss=0.07578, avg_loss=0.07532]\n",
      "Writing summary at step: 504900\n",
      "Step 504901  [5.375 sec/step, loss=0.07743, avg_loss=0.07531]\n",
      "Step 504902  [5.362 sec/step, loss=0.07234, avg_loss=0.07527]\n",
      "Step 504903  [5.378 sec/step, loss=0.07746, avg_loss=0.07529]\n",
      "Step 504904  [5.367 sec/step, loss=0.07274, avg_loss=0.07525]\n",
      "Step 504905  [5.367 sec/step, loss=0.07833, avg_loss=0.07528]\n",
      "Step 504906  [5.357 sec/step, loss=0.07623, avg_loss=0.07529]\n",
      "Step 504907  [5.364 sec/step, loss=0.07686, avg_loss=0.07529]\n",
      "Step 504908  [5.364 sec/step, loss=0.07559, avg_loss=0.07532]\n",
      "Step 504909  [5.335 sec/step, loss=0.07609, avg_loss=0.07533]\n",
      "Step 504910  [5.329 sec/step, loss=0.07525, avg_loss=0.07533]\n",
      "Step 504911  [5.353 sec/step, loss=0.07451, avg_loss=0.07531]\n",
      "Step 504912  [5.361 sec/step, loss=0.07570, avg_loss=0.07540]\n",
      "Step 504913  [5.372 sec/step, loss=0.07529, avg_loss=0.07540]\n",
      "Step 504914  [5.351 sec/step, loss=0.07698, avg_loss=0.07543]\n",
      "Step 504915  [5.358 sec/step, loss=0.07780, avg_loss=0.07543]\n",
      "Step 504916  [5.354 sec/step, loss=0.07259, avg_loss=0.07542]\n",
      "Step 504917  [5.353 sec/step, loss=0.07838, avg_loss=0.07542]\n",
      "Step 504918  [5.365 sec/step, loss=0.07578, avg_loss=0.07541]\n",
      "Step 504919  [5.384 sec/step, loss=0.07688, avg_loss=0.07550]\n",
      "Step 504920  [5.404 sec/step, loss=0.07754, avg_loss=0.07552]\n",
      "Step 504921  [5.370 sec/step, loss=0.06892, avg_loss=0.07544]\n",
      "Step 504922  [5.357 sec/step, loss=0.07291, avg_loss=0.07540]\n",
      "Step 504923  [5.357 sec/step, loss=0.06817, avg_loss=0.07541]\n",
      "Step 504924  [5.363 sec/step, loss=0.07819, avg_loss=0.07544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 504925  [5.363 sec/step, loss=0.07668, avg_loss=0.07548]\n",
      "Step 504926  [5.387 sec/step, loss=0.07687, avg_loss=0.07549]\n",
      "Step 504927  [5.382 sec/step, loss=0.07282, avg_loss=0.07545]\n",
      "Step 504928  [5.389 sec/step, loss=0.07613, avg_loss=0.07545]\n",
      "Step 504929  [5.393 sec/step, loss=0.07606, avg_loss=0.07547]\n",
      "Step 504930  [5.393 sec/step, loss=0.07575, avg_loss=0.07547]\n",
      "Generated 32 batches of size 32 in 2.336 sec\n",
      "Step 504931  [5.403 sec/step, loss=0.07732, avg_loss=0.07547]\n",
      "Step 504932  [5.395 sec/step, loss=0.07671, avg_loss=0.07548]\n",
      "Step 504933  [5.384 sec/step, loss=0.07477, avg_loss=0.07544]\n",
      "Step 504934  [5.387 sec/step, loss=0.07634, avg_loss=0.07545]\n",
      "Step 504935  [5.387 sec/step, loss=0.07542, avg_loss=0.07542]\n",
      "Step 504936  [5.384 sec/step, loss=0.07508, avg_loss=0.07542]\n",
      "Step 504937  [5.405 sec/step, loss=0.07755, avg_loss=0.07546]\n",
      "Step 504938  [5.384 sec/step, loss=0.07251, avg_loss=0.07541]\n",
      "Step 504939  [5.369 sec/step, loss=0.07697, avg_loss=0.07541]\n",
      "Step 504940  [5.369 sec/step, loss=0.07647, avg_loss=0.07541]\n",
      "Step 504941  [5.372 sec/step, loss=0.07910, avg_loss=0.07543]\n",
      "Step 504942  [5.377 sec/step, loss=0.07719, avg_loss=0.07544]\n",
      "Step 504943  [5.384 sec/step, loss=0.07548, avg_loss=0.07546]\n",
      "Step 504944  [5.390 sec/step, loss=0.07769, avg_loss=0.07547]\n",
      "Step 504945  [5.416 sec/step, loss=0.07674, avg_loss=0.07550]\n",
      "Step 504946  [5.410 sec/step, loss=0.07326, avg_loss=0.07546]\n",
      "Step 504947  [5.422 sec/step, loss=0.07649, avg_loss=0.07550]\n",
      "Step 504948  [5.418 sec/step, loss=0.07777, avg_loss=0.07552]\n",
      "Step 504949  [5.411 sec/step, loss=0.07803, avg_loss=0.07552]\n",
      "Step 504950  [5.406 sec/step, loss=0.07579, avg_loss=0.07551]\n",
      "Step 504951  [5.405 sec/step, loss=0.07651, avg_loss=0.07551]\n",
      "Step 504952  [5.424 sec/step, loss=0.07543, avg_loss=0.07549]\n",
      "Step 504953  [5.409 sec/step, loss=0.07723, avg_loss=0.07551]\n",
      "Step 504954  [5.406 sec/step, loss=0.07525, avg_loss=0.07550]\n",
      "Step 504955  [5.400 sec/step, loss=0.07759, avg_loss=0.07551]\n",
      "Step 504956  [5.404 sec/step, loss=0.07777, avg_loss=0.07552]\n",
      "Step 504957  [5.381 sec/step, loss=0.07485, avg_loss=0.07550]\n",
      "Step 504958  [5.447 sec/step, loss=0.06717, avg_loss=0.07548]\n",
      "Step 504959  [5.469 sec/step, loss=0.07810, avg_loss=0.07553]\n",
      "Step 504960  [5.457 sec/step, loss=0.06705, avg_loss=0.07546]\n",
      "Step 504961  [5.454 sec/step, loss=0.07573, avg_loss=0.07544]\n",
      "Step 504962  [5.472 sec/step, loss=0.07470, avg_loss=0.07542]\n",
      "Generated 32 batches of size 32 in 2.537 sec\n",
      "Step 504963  [5.482 sec/step, loss=0.07361, avg_loss=0.07541]\n",
      "Step 504964  [5.459 sec/step, loss=0.07707, avg_loss=0.07543]\n",
      "Step 504965  [5.450 sec/step, loss=0.07570, avg_loss=0.07543]\n",
      "Step 504966  [5.444 sec/step, loss=0.07648, avg_loss=0.07541]\n",
      "Step 504967  [5.447 sec/step, loss=0.07629, avg_loss=0.07547]\n",
      "Step 504968  [5.407 sec/step, loss=0.07758, avg_loss=0.07557]\n",
      "Step 504969  [5.395 sec/step, loss=0.07375, avg_loss=0.07552]\n",
      "Step 504970  [5.391 sec/step, loss=0.07536, avg_loss=0.07552]\n",
      "Step 504971  [5.380 sec/step, loss=0.07296, avg_loss=0.07549]\n",
      "Step 504972  [5.378 sec/step, loss=0.07641, avg_loss=0.07552]\n",
      "Step 504973  [5.378 sec/step, loss=0.07646, avg_loss=0.07554]\n",
      "Step 504974  [5.371 sec/step, loss=0.07531, avg_loss=0.07552]\n",
      "Step 504975  [5.358 sec/step, loss=0.07368, avg_loss=0.07548]\n",
      "Step 504976  [5.344 sec/step, loss=0.07561, avg_loss=0.07546]\n",
      "Step 504977  [5.345 sec/step, loss=0.07581, avg_loss=0.07544]\n",
      "Step 504978  [5.355 sec/step, loss=0.07719, avg_loss=0.07546]\n",
      "Step 504979  [5.356 sec/step, loss=0.07528, avg_loss=0.07547]\n",
      "Step 504980  [5.365 sec/step, loss=0.07658, avg_loss=0.07547]\n",
      "Step 504981  [5.372 sec/step, loss=0.07592, avg_loss=0.07556]\n",
      "Step 504982  [5.365 sec/step, loss=0.07703, avg_loss=0.07558]\n",
      "Step 504983  [5.356 sec/step, loss=0.07614, avg_loss=0.07559]\n",
      "Step 504984  [5.350 sec/step, loss=0.07626, avg_loss=0.07559]\n",
      "Step 504985  [5.365 sec/step, loss=0.07759, avg_loss=0.07561]\n",
      "Step 504986  [5.368 sec/step, loss=0.07788, avg_loss=0.07561]\n",
      "Step 504987  [5.326 sec/step, loss=0.06898, avg_loss=0.07555]\n",
      "Step 504988  [5.283 sec/step, loss=0.07761, avg_loss=0.07566]\n",
      "Step 504989  [5.312 sec/step, loss=0.07451, avg_loss=0.07565]\n",
      "Step 504990  [5.334 sec/step, loss=0.07844, avg_loss=0.07570]\n",
      "Step 504991  [5.328 sec/step, loss=0.07452, avg_loss=0.07567]\n",
      "Step 504992  [5.339 sec/step, loss=0.07561, avg_loss=0.07566]\n",
      "Step 504993  [5.320 sec/step, loss=0.07507, avg_loss=0.07563]\n",
      "Step 504994  [5.324 sec/step, loss=0.07699, avg_loss=0.07564]\n",
      "Generated 32 batches of size 32 in 2.701 sec\n",
      "Step 504995  [5.307 sec/step, loss=0.07338, avg_loss=0.07560]\n",
      "Step 504996  [5.308 sec/step, loss=0.07716, avg_loss=0.07561]\n",
      "Step 504997  [5.363 sec/step, loss=0.06787, avg_loss=0.07556]\n",
      "Step 504998  [5.363 sec/step, loss=0.07594, avg_loss=0.07556]\n",
      "Step 504999  [5.377 sec/step, loss=0.07530, avg_loss=0.07555]\n",
      "Step 505000  [5.378 sec/step, loss=0.07471, avg_loss=0.07554]\n",
      "Writing summary at step: 505000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-505000\n",
      "Saving audio and alignment...\n",
      "Input: tdaadzvar musddiq nay sarhadd miin unaasiivaan naeshnal murggbaanii maenidzmant paraadzaekt munaqidd kajjaa~________________________\n",
      "Step 505001  [5.384 sec/step, loss=0.07580, avg_loss=0.07553]\n",
      "Step 505002  [5.392 sec/step, loss=0.07439, avg_loss=0.07555]\n",
      "Step 505003  [5.375 sec/step, loss=0.07241, avg_loss=0.07550]\n",
      "Step 505004  [5.382 sec/step, loss=0.07596, avg_loss=0.07553]\n",
      "Step 505005  [5.364 sec/step, loss=0.07661, avg_loss=0.07551]\n",
      "Step 505006  [5.370 sec/step, loss=0.07712, avg_loss=0.07552]\n",
      "Step 505007  [5.375 sec/step, loss=0.07781, avg_loss=0.07553]\n",
      "Step 505008  [5.384 sec/step, loss=0.07626, avg_loss=0.07554]\n",
      "Step 505009  [5.374 sec/step, loss=0.07164, avg_loss=0.07549]\n",
      "Step 505010  [5.387 sec/step, loss=0.07750, avg_loss=0.07552]\n",
      "Step 505011  [5.366 sec/step, loss=0.07722, avg_loss=0.07554]\n",
      "Step 505012  [5.368 sec/step, loss=0.07561, avg_loss=0.07554]\n",
      "Step 505013  [5.363 sec/step, loss=0.07665, avg_loss=0.07556]\n",
      "Step 505014  [5.349 sec/step, loss=0.07419, avg_loss=0.07553]\n",
      "Step 505015  [5.354 sec/step, loss=0.07731, avg_loss=0.07552]\n",
      "Step 505016  [5.360 sec/step, loss=0.07684, avg_loss=0.07556]\n",
      "Step 505017  [5.363 sec/step, loss=0.07631, avg_loss=0.07554]\n",
      "Step 505018  [5.354 sec/step, loss=0.07686, avg_loss=0.07555]\n",
      "Step 505019  [5.378 sec/step, loss=0.07411, avg_loss=0.07553]\n",
      "Step 505020  [5.415 sec/step, loss=0.06747, avg_loss=0.07543]\n",
      "Step 505021  [5.418 sec/step, loss=0.07210, avg_loss=0.07546]\n",
      "Step 505022  [5.443 sec/step, loss=0.07803, avg_loss=0.07551]\n",
      "Step 505023  [5.394 sec/step, loss=0.07687, avg_loss=0.07560]\n",
      "Step 505024  [5.378 sec/step, loss=0.07487, avg_loss=0.07556]\n",
      "Generated 32 batches of size 32 in 2.911 sec\n",
      "Step 505025  [5.387 sec/step, loss=0.07607, avg_loss=0.07556]\n",
      "Step 505026  [5.375 sec/step, loss=0.07638, avg_loss=0.07555]\n",
      "Step 505027  [5.376 sec/step, loss=0.07636, avg_loss=0.07559]\n",
      "Step 505028  [5.383 sec/step, loss=0.07843, avg_loss=0.07561]\n",
      "Step 505029  [5.373 sec/step, loss=0.07562, avg_loss=0.07561]\n",
      "Step 505030  [5.377 sec/step, loss=0.07748, avg_loss=0.07562]\n",
      "Step 505031  [5.363 sec/step, loss=0.07590, avg_loss=0.07561]\n",
      "Step 505032  [5.346 sec/step, loss=0.06600, avg_loss=0.07550]\n",
      "Step 505033  [5.355 sec/step, loss=0.07799, avg_loss=0.07553]\n",
      "Step 505034  [5.332 sec/step, loss=0.07555, avg_loss=0.07553]\n",
      "Step 505035  [5.327 sec/step, loss=0.07669, avg_loss=0.07554]\n",
      "Step 505036  [5.344 sec/step, loss=0.07770, avg_loss=0.07557]\n",
      "Step 505037  [5.347 sec/step, loss=0.07672, avg_loss=0.07556]\n",
      "Step 505038  [5.356 sec/step, loss=0.07623, avg_loss=0.07559]\n",
      "Step 505039  [5.356 sec/step, loss=0.07619, avg_loss=0.07559]\n",
      "Step 505040  [5.341 sec/step, loss=0.07463, avg_loss=0.07557]\n",
      "Step 505041  [5.342 sec/step, loss=0.07736, avg_loss=0.07555]\n",
      "Step 505042  [5.325 sec/step, loss=0.07561, avg_loss=0.07554]\n",
      "Step 505043  [5.332 sec/step, loss=0.07655, avg_loss=0.07555]\n",
      "Step 505044  [5.324 sec/step, loss=0.07549, avg_loss=0.07552]\n",
      "Step 505045  [5.314 sec/step, loss=0.07528, avg_loss=0.07551]\n",
      "Step 505046  [5.332 sec/step, loss=0.07825, avg_loss=0.07556]\n",
      "Step 505047  [5.344 sec/step, loss=0.07678, avg_loss=0.07556]\n",
      "Step 505048  [5.387 sec/step, loss=0.06702, avg_loss=0.07545]\n",
      "Step 505049  [5.396 sec/step, loss=0.07580, avg_loss=0.07543]\n",
      "Step 505050  [5.379 sec/step, loss=0.06827, avg_loss=0.07536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 505051  [5.369 sec/step, loss=0.07610, avg_loss=0.07535]\n",
      "Step 505052  [5.370 sec/step, loss=0.07397, avg_loss=0.07534]\n",
      "Step 505053  [5.360 sec/step, loss=0.07249, avg_loss=0.07529]\n",
      "Step 505054  [5.363 sec/step, loss=0.07625, avg_loss=0.07530]\n",
      "Step 505055  [5.352 sec/step, loss=0.07304, avg_loss=0.07526]\n",
      "Step 505056  [5.349 sec/step, loss=0.07604, avg_loss=0.07524]\n",
      "Generated 32 batches of size 32 in 2.444 sec\n",
      "Step 505057  [5.357 sec/step, loss=0.07595, avg_loss=0.07525]\n",
      "Step 505058  [5.303 sec/step, loss=0.07329, avg_loss=0.07531]\n",
      "Step 505059  [5.305 sec/step, loss=0.07568, avg_loss=0.07529]\n",
      "Step 505060  [5.322 sec/step, loss=0.07743, avg_loss=0.07539]\n",
      "Step 505061  [5.321 sec/step, loss=0.07744, avg_loss=0.07541]\n",
      "Step 505062  [5.299 sec/step, loss=0.07468, avg_loss=0.07541]\n",
      "Step 505063  [5.286 sec/step, loss=0.07296, avg_loss=0.07540]\n",
      "Step 505064  [5.292 sec/step, loss=0.07787, avg_loss=0.07541]\n",
      "Step 505065  [5.312 sec/step, loss=0.07798, avg_loss=0.07543]\n",
      "Step 505066  [5.313 sec/step, loss=0.07628, avg_loss=0.07543]\n",
      "Step 505067  [5.313 sec/step, loss=0.07663, avg_loss=0.07543]\n",
      "Step 505068  [5.308 sec/step, loss=0.07794, avg_loss=0.07544]\n",
      "Step 505069  [5.310 sec/step, loss=0.07662, avg_loss=0.07546]\n",
      "Step 505070  [5.300 sec/step, loss=0.06670, avg_loss=0.07538]\n",
      "Step 505071  [5.355 sec/step, loss=0.07092, avg_loss=0.07536]\n",
      "Step 505072  [5.361 sec/step, loss=0.07721, avg_loss=0.07537]\n",
      "Step 505073  [5.373 sec/step, loss=0.07822, avg_loss=0.07538]\n",
      "Step 505074  [5.394 sec/step, loss=0.07751, avg_loss=0.07541]\n",
      "Step 505075  [5.383 sec/step, loss=0.07416, avg_loss=0.07541]\n",
      "Step 505076  [5.401 sec/step, loss=0.07742, avg_loss=0.07543]\n",
      "Step 505077  [5.387 sec/step, loss=0.07672, avg_loss=0.07544]\n",
      "Step 505078  [5.403 sec/step, loss=0.07692, avg_loss=0.07544]\n",
      "Step 505079  [5.394 sec/step, loss=0.07614, avg_loss=0.07544]\n",
      "Step 505080  [5.378 sec/step, loss=0.07755, avg_loss=0.07545]\n",
      "Step 505081  [5.383 sec/step, loss=0.07526, avg_loss=0.07545]\n",
      "Step 505082  [5.367 sec/step, loss=0.07631, avg_loss=0.07544]\n",
      "Step 505083  [5.359 sec/step, loss=0.07521, avg_loss=0.07543]\n",
      "Step 505084  [5.349 sec/step, loss=0.07340, avg_loss=0.07540]\n",
      "Step 505085  [5.331 sec/step, loss=0.07320, avg_loss=0.07536]\n",
      "Step 505086  [5.320 sec/step, loss=0.07577, avg_loss=0.07534]\n",
      "Step 505087  [5.358 sec/step, loss=0.07508, avg_loss=0.07540]\n",
      "Step 505088  [5.361 sec/step, loss=0.07731, avg_loss=0.07539]\n",
      "Generated 32 batches of size 32 in 2.319 sec\n",
      "Step 505089  [5.348 sec/step, loss=0.07481, avg_loss=0.07540]\n",
      "Step 505090  [5.341 sec/step, loss=0.07375, avg_loss=0.07535]\n",
      "Step 505091  [5.338 sec/step, loss=0.07513, avg_loss=0.07536]\n",
      "Step 505092  [5.333 sec/step, loss=0.07781, avg_loss=0.07538]\n",
      "Step 505093  [5.341 sec/step, loss=0.07711, avg_loss=0.07540]\n",
      "Step 505094  [5.338 sec/step, loss=0.07575, avg_loss=0.07539]\n",
      "Step 505095  [5.354 sec/step, loss=0.07845, avg_loss=0.07544]\n",
      "Step 505096  [5.354 sec/step, loss=0.07675, avg_loss=0.07543]\n",
      "Step 505097  [5.308 sec/step, loss=0.07618, avg_loss=0.07552]\n",
      "Step 505098  [5.315 sec/step, loss=0.07686, avg_loss=0.07553]\n",
      "Step 505099  [5.309 sec/step, loss=0.07619, avg_loss=0.07553]\n",
      "Step 505100  [5.310 sec/step, loss=0.07811, avg_loss=0.07557]\n",
      "Writing summary at step: 505100\n",
      "Step 505101  [5.306 sec/step, loss=0.07370, avg_loss=0.07555]\n",
      "Step 505102  [5.308 sec/step, loss=0.07576, avg_loss=0.07556]\n",
      "Step 505103  [5.327 sec/step, loss=0.07672, avg_loss=0.07560]\n",
      "Step 505104  [5.319 sec/step, loss=0.07314, avg_loss=0.07558]\n",
      "Step 505105  [5.318 sec/step, loss=0.07507, avg_loss=0.07556]\n",
      "Step 505106  [5.308 sec/step, loss=0.07773, avg_loss=0.07557]\n",
      "Step 505107  [5.306 sec/step, loss=0.07779, avg_loss=0.07557]\n",
      "Step 505108  [5.307 sec/step, loss=0.07592, avg_loss=0.07556]\n",
      "Step 505109  [5.300 sec/step, loss=0.06684, avg_loss=0.07552]\n",
      "Step 505110  [5.293 sec/step, loss=0.07705, avg_loss=0.07551]\n",
      "Step 505111  [5.305 sec/step, loss=0.07711, avg_loss=0.07551]\n",
      "Step 505112  [5.305 sec/step, loss=0.07442, avg_loss=0.07550]\n",
      "Step 505113  [5.303 sec/step, loss=0.07784, avg_loss=0.07551]\n",
      "Step 505114  [5.297 sec/step, loss=0.07145, avg_loss=0.07548]\n",
      "Step 505115  [5.273 sec/step, loss=0.07209, avg_loss=0.07543]\n",
      "Step 505116  [5.318 sec/step, loss=0.06812, avg_loss=0.07534]\n",
      "Step 505117  [5.321 sec/step, loss=0.07763, avg_loss=0.07536]\n",
      "Step 505118  [5.324 sec/step, loss=0.07656, avg_loss=0.07535]\n",
      "Step 505119  [5.323 sec/step, loss=0.07644, avg_loss=0.07538]\n",
      "Generated 32 batches of size 32 in 2.400 sec\n",
      "Step 505120  [5.282 sec/step, loss=0.07654, avg_loss=0.07547]\n",
      "Step 505121  [5.288 sec/step, loss=0.07546, avg_loss=0.07550]\n",
      "Step 505122  [5.283 sec/step, loss=0.07360, avg_loss=0.07546]\n",
      "Step 505123  [5.291 sec/step, loss=0.07579, avg_loss=0.07545]\n",
      "Step 505124  [5.305 sec/step, loss=0.07823, avg_loss=0.07548]\n",
      "Step 505125  [5.308 sec/step, loss=0.07760, avg_loss=0.07549]\n",
      "Step 505126  [5.303 sec/step, loss=0.07735, avg_loss=0.07550]\n",
      "Step 505127  [5.299 sec/step, loss=0.07532, avg_loss=0.07549]\n",
      "Step 505128  [5.278 sec/step, loss=0.07581, avg_loss=0.07547]\n",
      "Step 505129  [5.280 sec/step, loss=0.07555, avg_loss=0.07547]\n",
      "Step 505130  [5.273 sec/step, loss=0.07742, avg_loss=0.07547]\n",
      "Step 505131  [5.308 sec/step, loss=0.07395, avg_loss=0.07545]\n",
      "Step 505132  [5.308 sec/step, loss=0.06784, avg_loss=0.07546]\n",
      "Step 505133  [5.288 sec/step, loss=0.07327, avg_loss=0.07542]\n",
      "Step 505134  [5.290 sec/step, loss=0.07361, avg_loss=0.07540]\n",
      "Step 505135  [5.286 sec/step, loss=0.07646, avg_loss=0.07540]\n",
      "Step 505136  [5.287 sec/step, loss=0.07625, avg_loss=0.07538]\n",
      "Step 505137  [5.282 sec/step, loss=0.07635, avg_loss=0.07538]\n",
      "Step 505138  [5.278 sec/step, loss=0.07647, avg_loss=0.07538]\n",
      "Step 505139  [5.280 sec/step, loss=0.07649, avg_loss=0.07538]\n",
      "Step 505140  [5.297 sec/step, loss=0.07702, avg_loss=0.07541]\n",
      "Step 505141  [5.302 sec/step, loss=0.07721, avg_loss=0.07541]\n",
      "Step 505142  [5.309 sec/step, loss=0.07666, avg_loss=0.07542]\n",
      "Step 505143  [5.319 sec/step, loss=0.07825, avg_loss=0.07543]\n",
      "Step 505144  [5.309 sec/step, loss=0.07158, avg_loss=0.07539]\n",
      "Step 505145  [5.311 sec/step, loss=0.07814, avg_loss=0.07542]\n",
      "Step 505146  [5.307 sec/step, loss=0.07664, avg_loss=0.07541]\n",
      "Step 505147  [5.285 sec/step, loss=0.07519, avg_loss=0.07539]\n",
      "Step 505148  [5.236 sec/step, loss=0.07702, avg_loss=0.07549]\n",
      "Step 505149  [5.212 sec/step, loss=0.07431, avg_loss=0.07548]\n",
      "Step 505150  [5.232 sec/step, loss=0.07670, avg_loss=0.07556]\n",
      "Step 505151  [5.288 sec/step, loss=0.06833, avg_loss=0.07548]\n",
      "Generated 32 batches of size 32 in 2.377 sec\n",
      "Step 505152  [5.287 sec/step, loss=0.07587, avg_loss=0.07550]\n",
      "Step 505153  [5.300 sec/step, loss=0.07584, avg_loss=0.07553]\n",
      "Step 505154  [5.305 sec/step, loss=0.07699, avg_loss=0.07554]\n",
      "Step 505155  [5.301 sec/step, loss=0.07556, avg_loss=0.07557]\n",
      "Step 505156  [5.312 sec/step, loss=0.07547, avg_loss=0.07556]\n",
      "Step 505157  [5.300 sec/step, loss=0.07241, avg_loss=0.07553]\n",
      "Step 505158  [5.303 sec/step, loss=0.07505, avg_loss=0.07554]\n",
      "Step 505159  [5.301 sec/step, loss=0.07787, avg_loss=0.07557]\n",
      "Step 505160  [5.304 sec/step, loss=0.07675, avg_loss=0.07556]\n",
      "Step 505161  [5.307 sec/step, loss=0.07634, avg_loss=0.07555]\n",
      "Step 505162  [5.313 sec/step, loss=0.07359, avg_loss=0.07554]\n",
      "Step 505163  [5.334 sec/step, loss=0.07796, avg_loss=0.07559]\n",
      "Step 505164  [5.331 sec/step, loss=0.07762, avg_loss=0.07558]\n",
      "Step 505165  [5.315 sec/step, loss=0.07405, avg_loss=0.07555]\n",
      "Step 505166  [5.313 sec/step, loss=0.07771, avg_loss=0.07556]\n",
      "Step 505167  [5.304 sec/step, loss=0.07109, avg_loss=0.07550]\n",
      "Step 505168  [5.290 sec/step, loss=0.07347, avg_loss=0.07546]\n",
      "Step 505169  [5.279 sec/step, loss=0.07551, avg_loss=0.07545]\n",
      "Step 505170  [5.306 sec/step, loss=0.07775, avg_loss=0.07556]\n",
      "Step 505171  [5.269 sec/step, loss=0.07623, avg_loss=0.07561]\n",
      "Step 505172  [5.273 sec/step, loss=0.07607, avg_loss=0.07560]\n",
      "Step 505173  [5.245 sec/step, loss=0.06742, avg_loss=0.07549]\n",
      "Step 505174  [5.238 sec/step, loss=0.07776, avg_loss=0.07550]\n",
      "Step 505175  [5.254 sec/step, loss=0.07675, avg_loss=0.07552]\n",
      "Step 505176  [5.240 sec/step, loss=0.07601, avg_loss=0.07551]\n",
      "Step 505177  [5.251 sec/step, loss=0.07613, avg_loss=0.07550]\n",
      "Step 505178  [5.229 sec/step, loss=0.07719, avg_loss=0.07550]\n",
      "Step 505179  [5.235 sec/step, loss=0.07535, avg_loss=0.07550]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 505180  [5.223 sec/step, loss=0.07591, avg_loss=0.07548]\n",
      "Step 505181  [5.276 sec/step, loss=0.06697, avg_loss=0.07540]\n",
      "Step 505182  [5.291 sec/step, loss=0.07727, avg_loss=0.07541]\n",
      "Step 505183  [5.310 sec/step, loss=0.07653, avg_loss=0.07542]\n",
      "Generated 32 batches of size 32 in 2.307 sec\n",
      "Step 505184  [5.328 sec/step, loss=0.07615, avg_loss=0.07545]\n",
      "Step 505185  [5.350 sec/step, loss=0.07660, avg_loss=0.07548]\n",
      "Step 505186  [5.352 sec/step, loss=0.07365, avg_loss=0.07546]\n",
      "Step 505187  [5.339 sec/step, loss=0.07739, avg_loss=0.07548]\n",
      "Step 505188  [5.321 sec/step, loss=0.07447, avg_loss=0.07545]\n",
      "Step 505189  [5.307 sec/step, loss=0.07636, avg_loss=0.07547]\n",
      "Step 505190  [5.292 sec/step, loss=0.07243, avg_loss=0.07546]\n",
      "Step 505191  [5.296 sec/step, loss=0.07656, avg_loss=0.07547]\n",
      "Step 505192  [5.313 sec/step, loss=0.07638, avg_loss=0.07546]\n",
      "Step 505193  [5.313 sec/step, loss=0.07677, avg_loss=0.07545]\n",
      "Step 505194  [5.309 sec/step, loss=0.07391, avg_loss=0.07543]\n",
      "Step 505195  [5.307 sec/step, loss=0.07611, avg_loss=0.07541]\n",
      "Step 505196  [5.304 sec/step, loss=0.07648, avg_loss=0.07541]\n",
      "Step 505197  [5.311 sec/step, loss=0.07739, avg_loss=0.07542]\n",
      "Step 505198  [5.309 sec/step, loss=0.07609, avg_loss=0.07541]\n",
      "Step 505199  [5.313 sec/step, loss=0.07601, avg_loss=0.07541]\n",
      "Step 505200  [5.298 sec/step, loss=0.07461, avg_loss=0.07538]\n",
      "Writing summary at step: 505200\n",
      "Step 505201  [5.294 sec/step, loss=0.07675, avg_loss=0.07541]\n",
      "Step 505202  [5.281 sec/step, loss=0.07280, avg_loss=0.07538]\n",
      "Step 505203  [5.267 sec/step, loss=0.07447, avg_loss=0.07535]\n",
      "Step 505204  [5.282 sec/step, loss=0.07576, avg_loss=0.07538]\n",
      "Step 505205  [5.287 sec/step, loss=0.07683, avg_loss=0.07540]\n",
      "Step 505206  [5.265 sec/step, loss=0.06630, avg_loss=0.07528]\n",
      "Step 505207  [5.251 sec/step, loss=0.07358, avg_loss=0.07524]\n",
      "Step 505208  [5.247 sec/step, loss=0.07668, avg_loss=0.07525]\n",
      "Step 505209  [5.290 sec/step, loss=0.07472, avg_loss=0.07533]\n",
      "Step 505210  [5.286 sec/step, loss=0.07528, avg_loss=0.07531]\n",
      "Step 505211  [5.271 sec/step, loss=0.07631, avg_loss=0.07530]\n",
      "Step 505212  [5.265 sec/step, loss=0.07554, avg_loss=0.07531]\n",
      "Step 505213  [5.256 sec/step, loss=0.07541, avg_loss=0.07529]\n",
      "Step 505214  [5.280 sec/step, loss=0.07833, avg_loss=0.07536]\n",
      "Generated 32 batches of size 32 in 2.366 sec\n",
      "Step 505215  [5.302 sec/step, loss=0.07837, avg_loss=0.07542]\n",
      "Step 505216  [5.256 sec/step, loss=0.07621, avg_loss=0.07550]\n",
      "Step 505217  [5.248 sec/step, loss=0.07807, avg_loss=0.07551]\n",
      "Step 505218  [5.293 sec/step, loss=0.06823, avg_loss=0.07542]\n",
      "Step 505219  [5.262 sec/step, loss=0.07594, avg_loss=0.07542]\n",
      "Step 505220  [5.267 sec/step, loss=0.07770, avg_loss=0.07543]\n",
      "Step 505221  [5.290 sec/step, loss=0.07749, avg_loss=0.07545]\n",
      "Step 505222  [5.293 sec/step, loss=0.07684, avg_loss=0.07548]\n",
      "Step 505223  [5.300 sec/step, loss=0.07682, avg_loss=0.07549]\n",
      "Step 505224  [5.342 sec/step, loss=0.06694, avg_loss=0.07538]\n",
      "Step 505225  [5.334 sec/step, loss=0.07724, avg_loss=0.07538]\n",
      "Step 505226  [5.346 sec/step, loss=0.07601, avg_loss=0.07536]\n",
      "Step 505227  [5.352 sec/step, loss=0.07578, avg_loss=0.07537]\n",
      "Step 505228  [5.352 sec/step, loss=0.07247, avg_loss=0.07533]\n",
      "Step 505229  [5.373 sec/step, loss=0.07714, avg_loss=0.07535]\n",
      "Step 505230  [5.382 sec/step, loss=0.07738, avg_loss=0.07535]\n",
      "Step 505231  [5.353 sec/step, loss=0.07304, avg_loss=0.07534]\n",
      "Step 505232  [5.367 sec/step, loss=0.07670, avg_loss=0.07543]\n",
      "Step 505233  [5.387 sec/step, loss=0.07804, avg_loss=0.07548]\n",
      "Step 505234  [5.384 sec/step, loss=0.07417, avg_loss=0.07548]\n",
      "Step 505235  [5.367 sec/step, loss=0.06700, avg_loss=0.07539]\n",
      "Step 505236  [5.371 sec/step, loss=0.07747, avg_loss=0.07540]\n",
      "Step 505237  [5.370 sec/step, loss=0.07561, avg_loss=0.07539]\n",
      "Step 505238  [5.377 sec/step, loss=0.07718, avg_loss=0.07540]\n",
      "Step 505239  [5.375 sec/step, loss=0.07396, avg_loss=0.07537]\n",
      "Step 505240  [5.373 sec/step, loss=0.07610, avg_loss=0.07537]\n",
      "Step 505241  [5.355 sec/step, loss=0.07492, avg_loss=0.07534]\n",
      "Step 505242  [5.361 sec/step, loss=0.07692, avg_loss=0.07535]\n",
      "Step 505243  [5.348 sec/step, loss=0.07500, avg_loss=0.07531]\n",
      "Step 505244  [5.347 sec/step, loss=0.07595, avg_loss=0.07536]\n",
      "Step 505245  [5.336 sec/step, loss=0.07672, avg_loss=0.07534]\n",
      "Step 505246  [5.332 sec/step, loss=0.07621, avg_loss=0.07534]\n",
      "Generated 32 batches of size 32 in 2.434 sec\n",
      "Step 505247  [5.366 sec/step, loss=0.07674, avg_loss=0.07535]\n",
      "Step 505248  [5.378 sec/step, loss=0.07573, avg_loss=0.07534]\n",
      "Step 505249  [5.382 sec/step, loss=0.07552, avg_loss=0.07535]\n",
      "Step 505250  [5.391 sec/step, loss=0.07727, avg_loss=0.07536]\n",
      "Step 505251  [5.350 sec/step, loss=0.07835, avg_loss=0.07546]\n",
      "Step 505252  [5.342 sec/step, loss=0.07683, avg_loss=0.07547]\n",
      "Step 505253  [5.347 sec/step, loss=0.07670, avg_loss=0.07548]\n",
      "Step 505254  [5.330 sec/step, loss=0.07395, avg_loss=0.07545]\n",
      "Step 505255  [5.341 sec/step, loss=0.07606, avg_loss=0.07545]\n",
      "Step 505256  [5.336 sec/step, loss=0.07494, avg_loss=0.07545]\n",
      "Step 505257  [5.356 sec/step, loss=0.07600, avg_loss=0.07548]\n",
      "Step 505258  [5.359 sec/step, loss=0.07701, avg_loss=0.07550]\n",
      "Step 505259  [5.353 sec/step, loss=0.07569, avg_loss=0.07548]\n",
      "Step 505260  [5.335 sec/step, loss=0.07393, avg_loss=0.07545]\n",
      "Step 505261  [5.340 sec/step, loss=0.07605, avg_loss=0.07545]\n",
      "Step 505262  [5.337 sec/step, loss=0.07327, avg_loss=0.07545]\n",
      "Step 505263  [5.324 sec/step, loss=0.07541, avg_loss=0.07542]\n",
      "Step 505264  [5.303 sec/step, loss=0.06863, avg_loss=0.07533]\n",
      "Step 505265  [5.318 sec/step, loss=0.07668, avg_loss=0.07536]\n",
      "Step 505266  [5.311 sec/step, loss=0.07528, avg_loss=0.07533]\n",
      "Step 505267  [5.319 sec/step, loss=0.07656, avg_loss=0.07539]\n",
      "Step 505268  [5.315 sec/step, loss=0.07585, avg_loss=0.07541]\n",
      "Step 505269  [5.317 sec/step, loss=0.07445, avg_loss=0.07540]\n",
      "Step 505270  [5.311 sec/step, loss=0.07638, avg_loss=0.07539]\n",
      "Step 505271  [5.358 sec/step, loss=0.06804, avg_loss=0.07530]\n",
      "Step 505272  [5.358 sec/step, loss=0.07837, avg_loss=0.07533]\n",
      "Step 505273  [5.379 sec/step, loss=0.07777, avg_loss=0.07543]\n",
      "Step 505274  [5.380 sec/step, loss=0.07815, avg_loss=0.07544]\n",
      "Step 505275  [5.374 sec/step, loss=0.07686, avg_loss=0.07544]\n",
      "Step 505276  [5.375 sec/step, loss=0.07618, avg_loss=0.07544]\n",
      "Step 505277  [5.381 sec/step, loss=0.07680, avg_loss=0.07544]\n",
      "Step 505278  [5.392 sec/step, loss=0.07754, avg_loss=0.07545]\n",
      "Generated 32 batches of size 32 in 2.576 sec\n",
      "Step 505279  [5.401 sec/step, loss=0.07335, avg_loss=0.07543]\n",
      "Step 505280  [5.427 sec/step, loss=0.07699, avg_loss=0.07544]\n",
      "Step 505281  [5.390 sec/step, loss=0.07736, avg_loss=0.07554]\n",
      "Step 505282  [5.369 sec/step, loss=0.07316, avg_loss=0.07550]\n",
      "Step 505283  [5.374 sec/step, loss=0.07416, avg_loss=0.07548]\n",
      "Step 505284  [5.371 sec/step, loss=0.07660, avg_loss=0.07548]\n",
      "Step 505285  [5.380 sec/step, loss=0.07516, avg_loss=0.07547]\n",
      "Step 505286  [5.371 sec/step, loss=0.07237, avg_loss=0.07546]\n",
      "Step 505287  [5.364 sec/step, loss=0.07687, avg_loss=0.07545]\n",
      "Step 505288  [5.381 sec/step, loss=0.07661, avg_loss=0.07547]\n",
      "Step 505289  [5.386 sec/step, loss=0.07544, avg_loss=0.07546]\n",
      "Step 505290  [5.396 sec/step, loss=0.07696, avg_loss=0.07551]\n",
      "Step 505291  [5.397 sec/step, loss=0.07408, avg_loss=0.07548]\n",
      "Step 505292  [5.422 sec/step, loss=0.06681, avg_loss=0.07539]\n",
      "Step 505293  [5.415 sec/step, loss=0.07323, avg_loss=0.07535]\n",
      "Step 505294  [5.437 sec/step, loss=0.07849, avg_loss=0.07540]\n",
      "Step 505295  [5.445 sec/step, loss=0.07758, avg_loss=0.07541]\n",
      "Step 505296  [5.455 sec/step, loss=0.07774, avg_loss=0.07542]\n",
      "Step 505297  [5.452 sec/step, loss=0.07730, avg_loss=0.07542]\n",
      "Step 505298  [5.452 sec/step, loss=0.07798, avg_loss=0.07544]\n",
      "Step 505299  [5.472 sec/step, loss=0.07661, avg_loss=0.07545]\n",
      "Step 505300  [5.478 sec/step, loss=0.07728, avg_loss=0.07548]\n",
      "Writing summary at step: 505300\n",
      "Step 505301  [5.478 sec/step, loss=0.07677, avg_loss=0.07548]\n",
      "Step 505302  [5.494 sec/step, loss=0.07623, avg_loss=0.07551]\n",
      "Step 505303  [5.494 sec/step, loss=0.07617, avg_loss=0.07553]\n",
      "Step 505304  [5.501 sec/step, loss=0.07675, avg_loss=0.07554]\n",
      "Step 505305  [5.513 sec/step, loss=0.07544, avg_loss=0.07552]\n",
      "Step 505306  [5.515 sec/step, loss=0.07247, avg_loss=0.07558]\n",
      "Step 505307  [5.531 sec/step, loss=0.07680, avg_loss=0.07562]\n",
      "Step 505308  [5.539 sec/step, loss=0.07656, avg_loss=0.07562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 505309  [5.511 sec/step, loss=0.07551, avg_loss=0.07562]\n",
      "Generated 32 batches of size 32 in 2.596 sec\n",
      "Step 505310  [5.508 sec/step, loss=0.07210, avg_loss=0.07559]\n",
      "Step 505311  [5.522 sec/step, loss=0.07746, avg_loss=0.07560]\n",
      "Step 505312  [5.539 sec/step, loss=0.07679, avg_loss=0.07562]\n",
      "Step 505313  [5.545 sec/step, loss=0.07202, avg_loss=0.07558]\n",
      "Step 505314  [5.520 sec/step, loss=0.06644, avg_loss=0.07546]\n",
      "Step 505315  [5.499 sec/step, loss=0.07540, avg_loss=0.07543]\n",
      "Step 505316  [5.496 sec/step, loss=0.07598, avg_loss=0.07543]\n",
      "Step 505317  [5.485 sec/step, loss=0.07594, avg_loss=0.07541]\n",
      "Step 505318  [5.440 sec/step, loss=0.07582, avg_loss=0.07549]\n",
      "Step 505319  [5.453 sec/step, loss=0.07727, avg_loss=0.07550]\n",
      "Step 505320  [5.464 sec/step, loss=0.07487, avg_loss=0.07547]\n",
      "Step 505321  [5.447 sec/step, loss=0.07569, avg_loss=0.07545]\n",
      "Step 505322  [5.451 sec/step, loss=0.07760, avg_loss=0.07546]\n",
      "Step 505323  [5.438 sec/step, loss=0.07639, avg_loss=0.07546]\n",
      "Step 505324  [5.385 sec/step, loss=0.07311, avg_loss=0.07552]\n",
      "Step 505325  [5.380 sec/step, loss=0.07323, avg_loss=0.07548]\n",
      "Step 505326  [5.387 sec/step, loss=0.07771, avg_loss=0.07549]\n",
      "Step 505327  [5.384 sec/step, loss=0.07687, avg_loss=0.07551]\n",
      "Step 505328  [5.403 sec/step, loss=0.07655, avg_loss=0.07555]\n",
      "Step 505329  [5.400 sec/step, loss=0.07626, avg_loss=0.07554]\n",
      "Step 505330  [5.384 sec/step, loss=0.07555, avg_loss=0.07552]\n",
      "Step 505331  [5.375 sec/step, loss=0.07542, avg_loss=0.07554]\n",
      "Step 505332  [5.389 sec/step, loss=0.07583, avg_loss=0.07553]\n",
      "Step 505333  [5.382 sec/step, loss=0.07705, avg_loss=0.07552]\n",
      "Step 505334  [5.400 sec/step, loss=0.07689, avg_loss=0.07555]\n",
      "Step 505335  [5.402 sec/step, loss=0.07381, avg_loss=0.07562]\n",
      "Step 505336  [5.388 sec/step, loss=0.07696, avg_loss=0.07561]\n",
      "Step 505337  [5.430 sec/step, loss=0.06735, avg_loss=0.07553]\n",
      "Step 505338  [5.433 sec/step, loss=0.07647, avg_loss=0.07552]\n",
      "Step 505339  [5.440 sec/step, loss=0.07648, avg_loss=0.07555]\n",
      "Step 505340  [5.428 sec/step, loss=0.07545, avg_loss=0.07554]\n",
      "Step 505341  [5.438 sec/step, loss=0.07789, avg_loss=0.07557]\n",
      "Generated 32 batches of size 32 in 2.442 sec\n",
      "Step 505342  [5.442 sec/step, loss=0.07585, avg_loss=0.07556]\n",
      "Step 505343  [5.448 sec/step, loss=0.07310, avg_loss=0.07554]\n",
      "Step 505344  [5.478 sec/step, loss=0.07499, avg_loss=0.07553]\n",
      "Step 505345  [5.493 sec/step, loss=0.07801, avg_loss=0.07555]\n",
      "Step 505346  [5.477 sec/step, loss=0.06877, avg_loss=0.07547]\n",
      "Step 505347  [5.445 sec/step, loss=0.07560, avg_loss=0.07546]\n",
      "Step 505348  [5.423 sec/step, loss=0.07335, avg_loss=0.07544]\n",
      "Step 505349  [5.435 sec/step, loss=0.07711, avg_loss=0.07545]\n",
      "Step 505350  [5.421 sec/step, loss=0.07705, avg_loss=0.07545]\n",
      "Step 505351  [5.412 sec/step, loss=0.07385, avg_loss=0.07541]\n",
      "Step 505352  [5.402 sec/step, loss=0.07640, avg_loss=0.07540]\n",
      "Step 505353  [5.413 sec/step, loss=0.07748, avg_loss=0.07541]\n",
      "Step 505354  [5.423 sec/step, loss=0.07487, avg_loss=0.07542]\n",
      "Step 505355  [5.416 sec/step, loss=0.07698, avg_loss=0.07543]\n",
      "Step 505356  [5.395 sec/step, loss=0.07550, avg_loss=0.07543]\n",
      "Step 505357  [5.383 sec/step, loss=0.07590, avg_loss=0.07543]\n",
      "Step 505358  [5.392 sec/step, loss=0.07566, avg_loss=0.07542]\n",
      "Step 505359  [5.384 sec/step, loss=0.07566, avg_loss=0.07542]\n",
      "Step 505360  [5.402 sec/step, loss=0.07404, avg_loss=0.07542]\n",
      "Step 505361  [5.408 sec/step, loss=0.07704, avg_loss=0.07543]\n",
      "Step 505362  [5.400 sec/step, loss=0.07545, avg_loss=0.07545]\n",
      "Step 505363  [5.399 sec/step, loss=0.07507, avg_loss=0.07545]\n",
      "Step 505364  [5.419 sec/step, loss=0.07639, avg_loss=0.07553]\n",
      "Step 505365  [5.436 sec/step, loss=0.07450, avg_loss=0.07550]\n",
      "Step 505366  [5.439 sec/step, loss=0.07677, avg_loss=0.07552]\n",
      "Step 505367  [5.452 sec/step, loss=0.07520, avg_loss=0.07550]\n",
      "Step 505368  [5.446 sec/step, loss=0.07282, avg_loss=0.07547]\n",
      "Step 505369  [5.469 sec/step, loss=0.07676, avg_loss=0.07550]\n",
      "Step 505370  [5.516 sec/step, loss=0.06771, avg_loss=0.07541]\n",
      "Step 505371  [5.465 sec/step, loss=0.07578, avg_loss=0.07549]\n",
      "Step 505372  [5.458 sec/step, loss=0.07588, avg_loss=0.07546]\n",
      "Step 505373  [5.465 sec/step, loss=0.07726, avg_loss=0.07546]\n",
      "Generated 32 batches of size 32 in 2.396 sec\n",
      "Step 505374  [5.470 sec/step, loss=0.07779, avg_loss=0.07545]\n",
      "Step 505375  [5.455 sec/step, loss=0.06667, avg_loss=0.07535]\n",
      "Step 505376  [5.459 sec/step, loss=0.07682, avg_loss=0.07536]\n",
      "Step 505377  [5.449 sec/step, loss=0.07613, avg_loss=0.07535]\n",
      "Step 505378  [5.454 sec/step, loss=0.07756, avg_loss=0.07535]\n",
      "Step 505379  [5.453 sec/step, loss=0.07733, avg_loss=0.07539]\n",
      "Step 505380  [5.445 sec/step, loss=0.07757, avg_loss=0.07540]\n",
      "Step 505381  [5.423 sec/step, loss=0.07288, avg_loss=0.07535]\n",
      "Step 505382  [5.434 sec/step, loss=0.07375, avg_loss=0.07536]\n",
      "Step 505383  [5.416 sec/step, loss=0.07584, avg_loss=0.07538]\n",
      "Step 505384  [5.407 sec/step, loss=0.07479, avg_loss=0.07536]\n",
      "Step 505385  [5.381 sec/step, loss=0.07649, avg_loss=0.07537]\n",
      "Step 505386  [5.439 sec/step, loss=0.06684, avg_loss=0.07532]\n",
      "Step 505387  [5.427 sec/step, loss=0.07557, avg_loss=0.07530]\n",
      "Step 505388  [5.426 sec/step, loss=0.07782, avg_loss=0.07531]\n",
      "Step 505389  [5.436 sec/step, loss=0.07668, avg_loss=0.07533]\n",
      "Step 505390  [5.426 sec/step, loss=0.07209, avg_loss=0.07528]\n",
      "Step 505391  [5.423 sec/step, loss=0.07580, avg_loss=0.07530]\n",
      "Step 505392  [5.356 sec/step, loss=0.06649, avg_loss=0.07529]\n",
      "Step 505393  [5.363 sec/step, loss=0.07626, avg_loss=0.07532]\n",
      "Step 505394  [5.364 sec/step, loss=0.07776, avg_loss=0.07532]\n",
      "Step 505395  [5.354 sec/step, loss=0.07399, avg_loss=0.07528]\n",
      "Step 505396  [5.338 sec/step, loss=0.07350, avg_loss=0.07524]\n",
      "Step 505397  [5.328 sec/step, loss=0.07577, avg_loss=0.07522]\n",
      "Step 505398  [5.320 sec/step, loss=0.07455, avg_loss=0.07519]\n",
      "Step 505399  [5.299 sec/step, loss=0.07702, avg_loss=0.07519]\n",
      "Step 505400  [5.293 sec/step, loss=0.07234, avg_loss=0.07514]\n",
      "Writing summary at step: 505400\n",
      "Step 505401  [5.303 sec/step, loss=0.07562, avg_loss=0.07513]\n",
      "Step 505402  [5.309 sec/step, loss=0.07778, avg_loss=0.07515]\n",
      "Step 505403  [5.314 sec/step, loss=0.07569, avg_loss=0.07514]\n",
      "Step 505404  [5.296 sec/step, loss=0.07634, avg_loss=0.07514]\n",
      "Generated 32 batches of size 32 in 2.479 sec\n",
      "Step 505405  [5.277 sec/step, loss=0.07184, avg_loss=0.07510]\n",
      "Step 505406  [5.317 sec/step, loss=0.07427, avg_loss=0.07512]\n",
      "Step 505407  [5.315 sec/step, loss=0.07780, avg_loss=0.07513]\n",
      "Step 505408  [5.308 sec/step, loss=0.07515, avg_loss=0.07512]\n",
      "Step 505409  [5.329 sec/step, loss=0.07667, avg_loss=0.07513]\n",
      "Step 505410  [5.350 sec/step, loss=0.07650, avg_loss=0.07517]\n",
      "Step 505411  [5.332 sec/step, loss=0.07725, avg_loss=0.07517]\n",
      "Step 505412  [5.336 sec/step, loss=0.07617, avg_loss=0.07516]\n",
      "Step 505413  [5.351 sec/step, loss=0.07764, avg_loss=0.07522]\n",
      "Step 505414  [5.360 sec/step, loss=0.07496, avg_loss=0.07530]\n",
      "Step 505415  [5.354 sec/step, loss=0.06781, avg_loss=0.07523]\n",
      "Step 505416  [5.359 sec/step, loss=0.07608, avg_loss=0.07523]\n",
      "Step 505417  [5.356 sec/step, loss=0.07551, avg_loss=0.07522]\n",
      "Step 505418  [5.348 sec/step, loss=0.07672, avg_loss=0.07523]\n",
      "Step 505419  [5.332 sec/step, loss=0.07225, avg_loss=0.07518]\n",
      "Step 505420  [5.308 sec/step, loss=0.07595, avg_loss=0.07519]\n",
      "Step 505421  [5.316 sec/step, loss=0.07776, avg_loss=0.07522]\n",
      "Step 505422  [5.314 sec/step, loss=0.07641, avg_loss=0.07520]\n",
      "Step 505423  [5.324 sec/step, loss=0.07483, avg_loss=0.07519]\n",
      "Step 505424  [5.337 sec/step, loss=0.07674, avg_loss=0.07522]\n",
      "Step 505425  [5.344 sec/step, loss=0.07756, avg_loss=0.07527]\n",
      "Step 505426  [5.330 sec/step, loss=0.07391, avg_loss=0.07523]\n",
      "Step 505427  [5.330 sec/step, loss=0.07582, avg_loss=0.07522]\n",
      "Step 505428  [5.318 sec/step, loss=0.07621, avg_loss=0.07522]\n",
      "Step 505429  [5.312 sec/step, loss=0.07614, avg_loss=0.07521]\n",
      "Step 505430  [5.327 sec/step, loss=0.07812, avg_loss=0.07524]\n",
      "Step 505431  [5.339 sec/step, loss=0.07690, avg_loss=0.07525]\n",
      "Step 505432  [5.329 sec/step, loss=0.07736, avg_loss=0.07527]\n",
      "Step 505433  [5.324 sec/step, loss=0.07542, avg_loss=0.07525]\n",
      "Step 505434  [5.308 sec/step, loss=0.07113, avg_loss=0.07520]\n",
      "Step 505435  [5.333 sec/step, loss=0.07649, avg_loss=0.07522]\n",
      "Step 505436  [5.335 sec/step, loss=0.07626, avg_loss=0.07522]\n",
      "Generated 32 batches of size 32 in 2.418 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 505437  [5.286 sec/step, loss=0.07308, avg_loss=0.07527]\n",
      "Step 505438  [5.295 sec/step, loss=0.07821, avg_loss=0.07529]\n",
      "Step 505439  [5.277 sec/step, loss=0.07415, avg_loss=0.07527]\n",
      "Step 505440  [5.302 sec/step, loss=0.07462, avg_loss=0.07526]\n",
      "Step 505441  [5.293 sec/step, loss=0.07507, avg_loss=0.07523]\n",
      "Step 505442  [5.296 sec/step, loss=0.07611, avg_loss=0.07523]\n",
      "Step 505443  [5.321 sec/step, loss=0.07489, avg_loss=0.07525]\n",
      "Step 505444  [5.349 sec/step, loss=0.06790, avg_loss=0.07518]\n",
      "Step 505445  [5.350 sec/step, loss=0.07811, avg_loss=0.07518]\n",
      "Step 505446  [5.365 sec/step, loss=0.07571, avg_loss=0.07525]\n",
      "Step 505447  [5.360 sec/step, loss=0.07539, avg_loss=0.07525]\n",
      "Step 505448  [5.379 sec/step, loss=0.07674, avg_loss=0.07528]\n",
      "Step 505449  [5.366 sec/step, loss=0.07259, avg_loss=0.07524]\n",
      "Step 505450  [5.354 sec/step, loss=0.07303, avg_loss=0.07520]\n",
      "Step 505451  [5.353 sec/step, loss=0.07693, avg_loss=0.07523]\n",
      "Step 505452  [5.345 sec/step, loss=0.07617, avg_loss=0.07523]\n",
      "Step 505453  [5.343 sec/step, loss=0.07722, avg_loss=0.07522]\n",
      "Step 505454  [5.348 sec/step, loss=0.07656, avg_loss=0.07524]\n",
      "Step 505455  [5.359 sec/step, loss=0.07756, avg_loss=0.07525]\n",
      "Step 505456  [5.368 sec/step, loss=0.07524, avg_loss=0.07524]\n",
      "Step 505457  [5.384 sec/step, loss=0.07697, avg_loss=0.07525]\n",
      "Step 505458  [5.383 sec/step, loss=0.07753, avg_loss=0.07527]\n",
      "Step 505459  [5.373 sec/step, loss=0.06651, avg_loss=0.07518]\n",
      "Step 505460  [5.368 sec/step, loss=0.07655, avg_loss=0.07521]\n",
      "Step 505461  [5.352 sec/step, loss=0.07642, avg_loss=0.07520]\n",
      "Step 505462  [5.365 sec/step, loss=0.07735, avg_loss=0.07522]\n",
      "Step 505463  [5.395 sec/step, loss=0.07470, avg_loss=0.07522]\n",
      "Step 505464  [5.385 sec/step, loss=0.07400, avg_loss=0.07519]\n",
      "Step 505465  [5.363 sec/step, loss=0.07685, avg_loss=0.07522]\n",
      "Step 505466  [5.371 sec/step, loss=0.07612, avg_loss=0.07521]\n",
      "Step 505467  [5.378 sec/step, loss=0.07666, avg_loss=0.07522]\n",
      "Step 505468  [5.442 sec/step, loss=0.06793, avg_loss=0.07517]\n",
      "Generated 32 batches of size 32 in 2.368 sec\n",
      "Step 505469  [5.436 sec/step, loss=0.07802, avg_loss=0.07519]\n",
      "Step 505470  [5.397 sec/step, loss=0.07544, avg_loss=0.07526]\n",
      "Step 505471  [5.402 sec/step, loss=0.07436, avg_loss=0.07525]\n",
      "Step 505472  [5.394 sec/step, loss=0.07531, avg_loss=0.07524]\n",
      "Step 505473  [5.392 sec/step, loss=0.07400, avg_loss=0.07521]\n",
      "Step 505474  [5.377 sec/step, loss=0.07285, avg_loss=0.07516]\n",
      "Step 505475  [5.388 sec/step, loss=0.07566, avg_loss=0.07525]\n",
      "Step 505476  [5.376 sec/step, loss=0.07617, avg_loss=0.07525]\n",
      "Step 505477  [5.384 sec/step, loss=0.07792, avg_loss=0.07526]\n",
      "Step 505478  [5.381 sec/step, loss=0.07444, avg_loss=0.07523]\n",
      "Step 505479  [5.378 sec/step, loss=0.07636, avg_loss=0.07522]\n",
      "Step 505480  [5.385 sec/step, loss=0.07700, avg_loss=0.07522]\n",
      "Step 505481  [5.396 sec/step, loss=0.07535, avg_loss=0.07524]\n",
      "Step 505482  [5.390 sec/step, loss=0.07324, avg_loss=0.07524]\n",
      "Step 505483  [5.379 sec/step, loss=0.07156, avg_loss=0.07519]\n",
      "Step 505484  [5.369 sec/step, loss=0.07313, avg_loss=0.07518]\n",
      "Step 505485  [5.361 sec/step, loss=0.07415, avg_loss=0.07515]\n",
      "Step 505486  [5.315 sec/step, loss=0.07638, avg_loss=0.07525]\n",
      "Step 505487  [5.334 sec/step, loss=0.07723, avg_loss=0.07527]\n",
      "Step 505488  [5.326 sec/step, loss=0.07712, avg_loss=0.07526]\n",
      "Step 505489  [5.317 sec/step, loss=0.07610, avg_loss=0.07525]\n",
      "Step 505490  [5.331 sec/step, loss=0.07722, avg_loss=0.07530]\n",
      "Step 505491  [5.335 sec/step, loss=0.07678, avg_loss=0.07531]\n",
      "Step 505492  [5.350 sec/step, loss=0.07182, avg_loss=0.07537]\n",
      "Step 505493  [5.340 sec/step, loss=0.07504, avg_loss=0.07536]\n",
      "Step 505494  [5.339 sec/step, loss=0.07794, avg_loss=0.07536]\n",
      "Step 505495  [5.341 sec/step, loss=0.07799, avg_loss=0.07540]\n",
      "Step 505496  [5.356 sec/step, loss=0.07859, avg_loss=0.07545]\n",
      "Step 505497  [5.375 sec/step, loss=0.07679, avg_loss=0.07546]\n",
      "Step 505498  [5.371 sec/step, loss=0.07537, avg_loss=0.07547]\n",
      "Step 505499  [5.366 sec/step, loss=0.07670, avg_loss=0.07546]\n",
      "Step 505500  [5.370 sec/step, loss=0.07558, avg_loss=0.07550]\n",
      "Writing summary at step: 505500\n",
      "Generated 32 batches of size 32 in 2.329 sec\n",
      "Step 505501  [5.372 sec/step, loss=0.07747, avg_loss=0.07551]\n",
      "Step 505502  [5.380 sec/step, loss=0.07575, avg_loss=0.07549]\n",
      "Step 505503  [5.380 sec/step, loss=0.07449, avg_loss=0.07548]\n",
      "Step 505504  [5.430 sec/step, loss=0.06678, avg_loss=0.07539]\n",
      "Step 505505  [5.426 sec/step, loss=0.07605, avg_loss=0.07543]\n",
      "Step 505506  [5.427 sec/step, loss=0.07342, avg_loss=0.07542]\n",
      "Step 505507  [5.419 sec/step, loss=0.07702, avg_loss=0.07541]\n",
      "Step 505508  [5.405 sec/step, loss=0.06755, avg_loss=0.07534]\n",
      "Step 505509  [5.392 sec/step, loss=0.07586, avg_loss=0.07533]\n",
      "Step 505510  [5.372 sec/step, loss=0.07541, avg_loss=0.07532]\n",
      "Step 505511  [5.365 sec/step, loss=0.07557, avg_loss=0.07530]\n",
      "Step 505512  [5.379 sec/step, loss=0.07467, avg_loss=0.07528]\n",
      "Step 505513  [5.381 sec/step, loss=0.07850, avg_loss=0.07529]\n",
      "Step 505514  [5.370 sec/step, loss=0.06672, avg_loss=0.07521]\n",
      "Step 505515  [5.392 sec/step, loss=0.07464, avg_loss=0.07528]\n",
      "Step 505516  [5.384 sec/step, loss=0.07486, avg_loss=0.07527]\n",
      "Step 505517  [5.402 sec/step, loss=0.07837, avg_loss=0.07530]\n",
      "Step 505518  [5.416 sec/step, loss=0.07698, avg_loss=0.07530]\n",
      "Step 505519  [5.423 sec/step, loss=0.07668, avg_loss=0.07534]\n",
      "Step 505520  [5.422 sec/step, loss=0.07594, avg_loss=0.07534]\n",
      "Step 505521  [5.424 sec/step, loss=0.07788, avg_loss=0.07534]\n",
      "Step 505522  [5.417 sec/step, loss=0.07641, avg_loss=0.07534]\n",
      "Step 505523  [5.402 sec/step, loss=0.07266, avg_loss=0.07532]\n",
      "Step 505524  [5.385 sec/step, loss=0.07085, avg_loss=0.07526]\n",
      "Step 505525  [5.394 sec/step, loss=0.07771, avg_loss=0.07526]\n",
      "Step 505526  [5.407 sec/step, loss=0.07516, avg_loss=0.07528]\n",
      "Step 505527  [5.400 sec/step, loss=0.07389, avg_loss=0.07526]\n",
      "Step 505528  [5.404 sec/step, loss=0.07707, avg_loss=0.07527]\n",
      "Step 505529  [5.403 sec/step, loss=0.07369, avg_loss=0.07524]\n",
      "Step 505530  [5.445 sec/step, loss=0.06778, avg_loss=0.07514]\n",
      "Step 505531  [5.444 sec/step, loss=0.07360, avg_loss=0.07511]\n",
      "Generated 32 batches of size 32 in 2.460 sec\n",
      "Step 505532  [5.449 sec/step, loss=0.07759, avg_loss=0.07511]\n",
      "Step 505533  [5.461 sec/step, loss=0.07747, avg_loss=0.07513]\n",
      "Step 505534  [5.466 sec/step, loss=0.07537, avg_loss=0.07517]\n",
      "Step 505535  [5.441 sec/step, loss=0.07350, avg_loss=0.07514]\n",
      "Step 505536  [5.430 sec/step, loss=0.07642, avg_loss=0.07514]\n",
      "Step 505537  [5.436 sec/step, loss=0.07542, avg_loss=0.07517]\n",
      "Step 505538  [5.418 sec/step, loss=0.07511, avg_loss=0.07513]\n",
      "Step 505539  [5.457 sec/step, loss=0.07479, avg_loss=0.07514]\n",
      "Step 505540  [5.452 sec/step, loss=0.07665, avg_loss=0.07516]\n",
      "Step 505541  [5.459 sec/step, loss=0.07505, avg_loss=0.07516]\n",
      "Step 505542  [5.459 sec/step, loss=0.07747, avg_loss=0.07517]\n",
      "Step 505543  [5.442 sec/step, loss=0.07766, avg_loss=0.07520]\n",
      "Step 505544  [5.402 sec/step, loss=0.07739, avg_loss=0.07530]\n",
      "Step 505545  [5.387 sec/step, loss=0.07719, avg_loss=0.07529]\n",
      "Step 505546  [5.403 sec/step, loss=0.07658, avg_loss=0.07530]\n",
      "Step 505547  [5.412 sec/step, loss=0.07654, avg_loss=0.07531]\n",
      "Step 505548  [5.397 sec/step, loss=0.07472, avg_loss=0.07529]\n",
      "Step 505549  [5.404 sec/step, loss=0.07395, avg_loss=0.07530]\n",
      "Step 505550  [5.407 sec/step, loss=0.07507, avg_loss=0.07532]\n",
      "Step 505551  [5.403 sec/step, loss=0.07559, avg_loss=0.07531]\n",
      "Step 505552  [5.402 sec/step, loss=0.07467, avg_loss=0.07529]\n",
      "Step 505553  [5.394 sec/step, loss=0.07617, avg_loss=0.07528]\n",
      "Step 505554  [5.405 sec/step, loss=0.07714, avg_loss=0.07529]\n",
      "Step 505555  [5.392 sec/step, loss=0.07559, avg_loss=0.07527]\n",
      "Step 505556  [5.394 sec/step, loss=0.07710, avg_loss=0.07529]\n",
      "Step 505557  [5.389 sec/step, loss=0.07737, avg_loss=0.07529]\n",
      "Step 505558  [5.395 sec/step, loss=0.07718, avg_loss=0.07529]\n",
      "Step 505559  [5.412 sec/step, loss=0.07587, avg_loss=0.07538]\n",
      "Step 505560  [5.411 sec/step, loss=0.07272, avg_loss=0.07534]\n",
      "Step 505561  [5.403 sec/step, loss=0.07206, avg_loss=0.07530]\n",
      "Step 505562  [5.408 sec/step, loss=0.07561, avg_loss=0.07528]\n",
      "Step 505563  [5.382 sec/step, loss=0.07642, avg_loss=0.07530]\n",
      "Generated 32 batches of size 32 in 2.388 sec\n",
      "Step 505564  [5.417 sec/step, loss=0.07660, avg_loss=0.07533]\n",
      "Step 505565  [5.418 sec/step, loss=0.07471, avg_loss=0.07530]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 505566  [5.397 sec/step, loss=0.06739, avg_loss=0.07522]\n",
      "Step 505567  [5.386 sec/step, loss=0.07623, avg_loss=0.07521]\n",
      "Step 505568  [5.322 sec/step, loss=0.07361, avg_loss=0.07527]\n",
      "Step 505569  [5.361 sec/step, loss=0.06701, avg_loss=0.07516]\n",
      "Step 505570  [5.353 sec/step, loss=0.07717, avg_loss=0.07518]\n",
      "Step 505571  [5.347 sec/step, loss=0.07659, avg_loss=0.07520]\n",
      "Step 505572  [5.361 sec/step, loss=0.07826, avg_loss=0.07523]\n",
      "Step 505573  [5.344 sec/step, loss=0.07546, avg_loss=0.07524]\n",
      "Step 505574  [5.341 sec/step, loss=0.07473, avg_loss=0.07526]\n",
      "Step 505575  [5.338 sec/step, loss=0.07262, avg_loss=0.07523]\n",
      "Step 505576  [5.353 sec/step, loss=0.07681, avg_loss=0.07524]\n",
      "Step 505577  [5.349 sec/step, loss=0.07797, avg_loss=0.07524]\n",
      "Step 505578  [5.335 sec/step, loss=0.07406, avg_loss=0.07524]\n",
      "Step 505579  [5.336 sec/step, loss=0.07619, avg_loss=0.07523]\n",
      "Step 505580  [5.332 sec/step, loss=0.07793, avg_loss=0.07524]\n",
      "Step 505581  [5.348 sec/step, loss=0.07665, avg_loss=0.07526]\n",
      "Step 505582  [5.359 sec/step, loss=0.07283, avg_loss=0.07525]\n",
      "Step 505583  [5.367 sec/step, loss=0.07535, avg_loss=0.07529]\n",
      "Step 505584  [5.391 sec/step, loss=0.07773, avg_loss=0.07534]\n",
      "Step 505585  [5.408 sec/step, loss=0.07540, avg_loss=0.07535]\n",
      "Step 505586  [5.419 sec/step, loss=0.07671, avg_loss=0.07535]\n",
      "Step 505587  [5.394 sec/step, loss=0.06734, avg_loss=0.07525]\n",
      "Step 505588  [5.405 sec/step, loss=0.07788, avg_loss=0.07526]\n",
      "Step 505589  [5.410 sec/step, loss=0.07715, avg_loss=0.07527]\n",
      "Step 505590  [5.413 sec/step, loss=0.07683, avg_loss=0.07527]\n",
      "Step 505591  [5.419 sec/step, loss=0.07341, avg_loss=0.07523]\n",
      "Step 505592  [5.429 sec/step, loss=0.07757, avg_loss=0.07529]\n",
      "Step 505593  [5.463 sec/step, loss=0.07487, avg_loss=0.07529]\n",
      "Step 505594  [5.450 sec/step, loss=0.07355, avg_loss=0.07524]\n",
      "Step 505595  [5.437 sec/step, loss=0.07548, avg_loss=0.07522]\n",
      "Generated 32 batches of size 32 in 2.673 sec\n",
      "Step 505596  [5.421 sec/step, loss=0.07274, avg_loss=0.07516]\n",
      "Step 505597  [5.454 sec/step, loss=0.06780, avg_loss=0.07507]\n",
      "Step 505598  [5.464 sec/step, loss=0.07769, avg_loss=0.07509]\n",
      "Step 505599  [5.467 sec/step, loss=0.07331, avg_loss=0.07506]\n",
      "Step 505600  [5.466 sec/step, loss=0.07655, avg_loss=0.07507]\n",
      "Writing summary at step: 505600\n",
      "Step 505601  [5.458 sec/step, loss=0.07749, avg_loss=0.07507]\n",
      "Step 505602  [5.444 sec/step, loss=0.07673, avg_loss=0.07508]\n",
      "Step 505603  [5.437 sec/step, loss=0.07669, avg_loss=0.07510]\n",
      "Step 505604  [5.386 sec/step, loss=0.07453, avg_loss=0.07518]\n",
      "Step 505605  [5.409 sec/step, loss=0.07745, avg_loss=0.07519]\n",
      "Step 505606  [5.383 sec/step, loss=0.07669, avg_loss=0.07523]\n",
      "Step 505607  [5.364 sec/step, loss=0.06658, avg_loss=0.07512]\n",
      "Step 505608  [5.391 sec/step, loss=0.07539, avg_loss=0.07520]\n",
      "Step 505609  [5.397 sec/step, loss=0.07796, avg_loss=0.07522]\n",
      "Step 505610  [5.400 sec/step, loss=0.07617, avg_loss=0.07523]\n",
      "Step 505611  [5.406 sec/step, loss=0.07368, avg_loss=0.07521]\n",
      "Step 505612  [5.391 sec/step, loss=0.07798, avg_loss=0.07524]\n",
      "Step 505613  [5.381 sec/step, loss=0.07420, avg_loss=0.07520]\n",
      "Step 505614  [5.426 sec/step, loss=0.07464, avg_loss=0.07528]\n",
      "Step 505615  [5.432 sec/step, loss=0.07504, avg_loss=0.07528]\n",
      "Step 505616  [5.486 sec/step, loss=0.06742, avg_loss=0.07521]\n",
      "Step 505617  [5.478 sec/step, loss=0.07589, avg_loss=0.07518]\n",
      "Step 505618  [5.456 sec/step, loss=0.07578, avg_loss=0.07517]\n",
      "Step 505619  [5.470 sec/step, loss=0.07817, avg_loss=0.07519]\n",
      "Step 505620  [5.457 sec/step, loss=0.07240, avg_loss=0.07515]\n",
      "Step 505621  [5.454 sec/step, loss=0.07626, avg_loss=0.07514]\n",
      "Step 505622  [5.442 sec/step, loss=0.07524, avg_loss=0.07512]\n",
      "Step 505623  [5.454 sec/step, loss=0.07709, avg_loss=0.07517]\n",
      "Step 505624  [5.456 sec/step, loss=0.07515, avg_loss=0.07521]\n",
      "Step 505625  [5.436 sec/step, loss=0.07280, avg_loss=0.07516]\n",
      "Step 505626  [5.427 sec/step, loss=0.07695, avg_loss=0.07518]\n",
      "Generated 32 batches of size 32 in 2.523 sec\n",
      "Step 505627  [5.434 sec/step, loss=0.07513, avg_loss=0.07519]\n",
      "Step 505628  [5.447 sec/step, loss=0.07725, avg_loss=0.07519]\n",
      "Step 505629  [5.448 sec/step, loss=0.07611, avg_loss=0.07522]\n",
      "Step 505630  [5.399 sec/step, loss=0.07667, avg_loss=0.07531]\n",
      "Step 505631  [5.416 sec/step, loss=0.07673, avg_loss=0.07534]\n",
      "Step 505632  [5.406 sec/step, loss=0.07679, avg_loss=0.07533]\n",
      "Step 505633  [5.395 sec/step, loss=0.07677, avg_loss=0.07532]\n",
      "Step 505634  [5.399 sec/step, loss=0.07620, avg_loss=0.07533]\n",
      "Step 505635  [5.409 sec/step, loss=0.07406, avg_loss=0.07534]\n",
      "Step 505636  [5.401 sec/step, loss=0.06730, avg_loss=0.07525]\n",
      "Step 505637  [5.400 sec/step, loss=0.07477, avg_loss=0.07524]\n",
      "Step 505638  [5.401 sec/step, loss=0.07261, avg_loss=0.07522]\n",
      "Step 505639  [5.390 sec/step, loss=0.07665, avg_loss=0.07523]\n",
      "Step 505640  [5.387 sec/step, loss=0.07523, avg_loss=0.07522]\n",
      "Step 505641  [5.398 sec/step, loss=0.07623, avg_loss=0.07523]\n",
      "Step 505642  [5.375 sec/step, loss=0.07287, avg_loss=0.07519]\n",
      "Step 505643  [5.367 sec/step, loss=0.07667, avg_loss=0.07518]\n",
      "Step 505644  [5.352 sec/step, loss=0.07638, avg_loss=0.07517]\n",
      "Step 505645  [5.397 sec/step, loss=0.07004, avg_loss=0.07509]\n",
      "Step 505646  [5.386 sec/step, loss=0.07562, avg_loss=0.07508]\n",
      "Step 505647  [5.386 sec/step, loss=0.07750, avg_loss=0.07509]\n",
      "Step 505648  [5.383 sec/step, loss=0.07235, avg_loss=0.07507]\n",
      "Step 505649  [5.387 sec/step, loss=0.07296, avg_loss=0.07506]\n",
      "Step 505650  [5.422 sec/step, loss=0.07717, avg_loss=0.07508]\n",
      "Step 505651  [5.422 sec/step, loss=0.07525, avg_loss=0.07508]\n",
      "Step 505652  [5.431 sec/step, loss=0.07541, avg_loss=0.07509]\n",
      "Step 505653  [5.425 sec/step, loss=0.07612, avg_loss=0.07509]\n",
      "Step 505654  [5.423 sec/step, loss=0.07867, avg_loss=0.07510]\n",
      "Step 505655  [5.433 sec/step, loss=0.07570, avg_loss=0.07510]\n",
      "Step 505656  [5.440 sec/step, loss=0.07749, avg_loss=0.07511]\n",
      "Step 505657  [5.429 sec/step, loss=0.07702, avg_loss=0.07510]\n",
      "Step 505658  [5.413 sec/step, loss=0.07635, avg_loss=0.07509]\n",
      "Generated 32 batches of size 32 in 2.424 sec\n",
      "Step 505659  [5.423 sec/step, loss=0.07683, avg_loss=0.07510]\n",
      "Step 505660  [5.421 sec/step, loss=0.07544, avg_loss=0.07513]\n",
      "Step 505661  [5.425 sec/step, loss=0.07561, avg_loss=0.07517]\n",
      "Step 505662  [5.424 sec/step, loss=0.07787, avg_loss=0.07519]\n",
      "Step 505663  [5.437 sec/step, loss=0.07755, avg_loss=0.07520]\n",
      "Step 505664  [5.408 sec/step, loss=0.07597, avg_loss=0.07519]\n",
      "Step 505665  [5.398 sec/step, loss=0.07371, avg_loss=0.07518]\n",
      "Step 505666  [5.404 sec/step, loss=0.07543, avg_loss=0.07526]\n",
      "Step 505667  [5.410 sec/step, loss=0.07778, avg_loss=0.07528]\n",
      "Step 505668  [5.450 sec/step, loss=0.07440, avg_loss=0.07529]\n",
      "Step 505669  [5.408 sec/step, loss=0.07742, avg_loss=0.07539]\n",
      "Step 505670  [5.401 sec/step, loss=0.07477, avg_loss=0.07537]\n",
      "Step 505671  [5.395 sec/step, loss=0.07561, avg_loss=0.07536]\n",
      "Step 505672  [5.397 sec/step, loss=0.07746, avg_loss=0.07535]\n",
      "Step 505673  [5.393 sec/step, loss=0.07289, avg_loss=0.07532]\n",
      "Step 505674  [5.402 sec/step, loss=0.07364, avg_loss=0.07531]\n",
      "Step 505675  [5.431 sec/step, loss=0.07524, avg_loss=0.07534]\n",
      "Step 505676  [5.438 sec/step, loss=0.07749, avg_loss=0.07535]\n",
      "Step 505677  [5.427 sec/step, loss=0.07351, avg_loss=0.07530]\n",
      "Step 505678  [5.433 sec/step, loss=0.07438, avg_loss=0.07530]\n",
      "Step 505679  [5.438 sec/step, loss=0.07678, avg_loss=0.07531]\n",
      "Step 505680  [5.424 sec/step, loss=0.07698, avg_loss=0.07530]\n",
      "Step 505681  [5.420 sec/step, loss=0.07585, avg_loss=0.07529]\n",
      "Step 505682  [5.417 sec/step, loss=0.07704, avg_loss=0.07534]\n",
      "Step 505683  [5.417 sec/step, loss=0.07624, avg_loss=0.07534]\n",
      "Step 505684  [5.412 sec/step, loss=0.07604, avg_loss=0.07533]\n",
      "Step 505685  [5.402 sec/step, loss=0.07695, avg_loss=0.07534]\n",
      "Step 505686  [5.398 sec/step, loss=0.07765, avg_loss=0.07535]\n",
      "Step 505687  [5.421 sec/step, loss=0.07398, avg_loss=0.07542]\n",
      "Step 505688  [5.409 sec/step, loss=0.07614, avg_loss=0.07540]\n",
      "Step 505689  [5.390 sec/step, loss=0.07528, avg_loss=0.07538]\n",
      "Step 505690  [5.396 sec/step, loss=0.07637, avg_loss=0.07538]\n",
      "Generated 32 batches of size 32 in 2.885 sec\n",
      "Step 505691  [5.382 sec/step, loss=0.06818, avg_loss=0.07533]\n",
      "Step 505692  [5.384 sec/step, loss=0.07768, avg_loss=0.07533]\n",
      "Step 505693  [5.356 sec/step, loss=0.07579, avg_loss=0.07534]\n",
      "Step 505694  [5.355 sec/step, loss=0.07522, avg_loss=0.07535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 505695  [5.378 sec/step, loss=0.07766, avg_loss=0.07537]\n",
      "Step 505696  [5.437 sec/step, loss=0.06792, avg_loss=0.07533]\n",
      "Step 505697  [5.376 sec/step, loss=0.07251, avg_loss=0.07537]\n",
      "Step 505698  [5.360 sec/step, loss=0.07544, avg_loss=0.07535]\n",
      "Step 505699  [5.364 sec/step, loss=0.07772, avg_loss=0.07539]\n",
      "Step 505700  [5.380 sec/step, loss=0.07748, avg_loss=0.07540]\n",
      "Writing summary at step: 505700\n",
      "Step 505701  [5.376 sec/step, loss=0.07669, avg_loss=0.07540]\n",
      "Step 505702  [5.367 sec/step, loss=0.07485, avg_loss=0.07538]\n",
      "Step 505703  [5.362 sec/step, loss=0.07587, avg_loss=0.07537]\n",
      "Step 505704  [5.376 sec/step, loss=0.07723, avg_loss=0.07540]\n",
      "Step 505705  [5.390 sec/step, loss=0.07426, avg_loss=0.07536]\n",
      "Step 505706  [5.397 sec/step, loss=0.07788, avg_loss=0.07538]\n",
      "Step 505707  [5.418 sec/step, loss=0.07507, avg_loss=0.07546]\n",
      "Step 505708  [5.398 sec/step, loss=0.07355, avg_loss=0.07544]\n",
      "Step 505709  [5.387 sec/step, loss=0.07662, avg_loss=0.07543]\n",
      "Step 505710  [5.400 sec/step, loss=0.07801, avg_loss=0.07545]\n",
      "Step 505711  [5.454 sec/step, loss=0.06930, avg_loss=0.07540]\n",
      "Step 505712  [5.437 sec/step, loss=0.07550, avg_loss=0.07538]\n",
      "Step 505713  [5.431 sec/step, loss=0.07407, avg_loss=0.07538]\n",
      "Step 505714  [5.403 sec/step, loss=0.07577, avg_loss=0.07539]\n",
      "Step 505715  [5.398 sec/step, loss=0.07522, avg_loss=0.07539]\n",
      "Step 505716  [5.345 sec/step, loss=0.07671, avg_loss=0.07548]\n",
      "Step 505717  [5.329 sec/step, loss=0.06801, avg_loss=0.07540]\n",
      "Step 505718  [5.344 sec/step, loss=0.07653, avg_loss=0.07541]\n",
      "Step 505719  [5.330 sec/step, loss=0.07327, avg_loss=0.07536]\n",
      "Step 505720  [5.345 sec/step, loss=0.07773, avg_loss=0.07542]\n",
      "Step 505721  [5.345 sec/step, loss=0.07566, avg_loss=0.07541]\n",
      "Generated 32 batches of size 32 in 2.417 sec\n",
      "Step 505722  [5.367 sec/step, loss=0.07481, avg_loss=0.07541]\n",
      "Step 505723  [5.380 sec/step, loss=0.07480, avg_loss=0.07538]\n",
      "Step 505724  [5.390 sec/step, loss=0.07664, avg_loss=0.07540]\n",
      "Step 505725  [5.409 sec/step, loss=0.07807, avg_loss=0.07545]\n",
      "Step 505726  [5.405 sec/step, loss=0.07742, avg_loss=0.07546]\n",
      "Step 505727  [5.405 sec/step, loss=0.07579, avg_loss=0.07546]\n",
      "Step 505728  [5.393 sec/step, loss=0.07641, avg_loss=0.07545]\n",
      "Step 505729  [5.375 sec/step, loss=0.07255, avg_loss=0.07542]\n",
      "Step 505730  [5.372 sec/step, loss=0.07220, avg_loss=0.07537]\n",
      "Step 505731  [5.363 sec/step, loss=0.07805, avg_loss=0.07539]\n",
      "Step 505732  [5.375 sec/step, loss=0.07795, avg_loss=0.07540]\n",
      "Step 505733  [5.366 sec/step, loss=0.07585, avg_loss=0.07539]\n",
      "Step 505734  [5.376 sec/step, loss=0.07693, avg_loss=0.07540]\n",
      "Step 505735  [5.376 sec/step, loss=0.07665, avg_loss=0.07542]\n",
      "Step 505736  [5.395 sec/step, loss=0.07691, avg_loss=0.07552]\n",
      "Step 505737  [5.399 sec/step, loss=0.07757, avg_loss=0.07555]\n",
      "Step 505738  [5.406 sec/step, loss=0.07624, avg_loss=0.07558]\n",
      "Step 505739  [5.390 sec/step, loss=0.07606, avg_loss=0.07558]\n",
      "Step 505740  [5.388 sec/step, loss=0.07517, avg_loss=0.07558]\n",
      "Step 505741  [5.382 sec/step, loss=0.07862, avg_loss=0.07560]\n",
      "Step 505742  [5.413 sec/step, loss=0.07629, avg_loss=0.07563]\n",
      "Step 505743  [5.413 sec/step, loss=0.07636, avg_loss=0.07563]\n",
      "Step 505744  [5.412 sec/step, loss=0.07308, avg_loss=0.07560]\n",
      "Step 505745  [5.382 sec/step, loss=0.07778, avg_loss=0.07568]\n",
      "Step 505746  [5.381 sec/step, loss=0.07640, avg_loss=0.07568]\n",
      "Step 505747  [5.362 sec/step, loss=0.06715, avg_loss=0.07558]\n",
      "Step 505748  [5.371 sec/step, loss=0.07531, avg_loss=0.07561]\n",
      "Step 505749  [5.385 sec/step, loss=0.07578, avg_loss=0.07564]\n",
      "Step 505750  [5.358 sec/step, loss=0.07657, avg_loss=0.07563]\n",
      "Step 505751  [5.352 sec/step, loss=0.07347, avg_loss=0.07561]\n",
      "Step 505752  [5.345 sec/step, loss=0.07677, avg_loss=0.07563]\n",
      "Step 505753  [5.351 sec/step, loss=0.07569, avg_loss=0.07562]\n",
      "Generated 32 batches of size 32 in 2.710 sec\n",
      "Step 505754  [5.396 sec/step, loss=0.06609, avg_loss=0.07550]\n",
      "Step 505755  [5.384 sec/step, loss=0.07524, avg_loss=0.07549]\n",
      "Step 505756  [5.374 sec/step, loss=0.07587, avg_loss=0.07548]\n",
      "Step 505757  [5.366 sec/step, loss=0.07251, avg_loss=0.07543]\n",
      "Step 505758  [5.361 sec/step, loss=0.07498, avg_loss=0.07542]\n",
      "Step 505759  [5.345 sec/step, loss=0.07554, avg_loss=0.07540]\n",
      "Step 505760  [5.350 sec/step, loss=0.07677, avg_loss=0.07542]\n",
      "Step 505761  [5.383 sec/step, loss=0.07626, avg_loss=0.07542]\n",
      "Step 505762  [5.369 sec/step, loss=0.07504, avg_loss=0.07540]\n",
      "Step 505763  [5.407 sec/step, loss=0.06693, avg_loss=0.07529]\n",
      "Step 505764  [5.404 sec/step, loss=0.07629, avg_loss=0.07529]\n",
      "Step 505765  [5.426 sec/step, loss=0.07703, avg_loss=0.07533]\n",
      "Step 505766  [5.436 sec/step, loss=0.07377, avg_loss=0.07531]\n",
      "Step 505767  [5.407 sec/step, loss=0.06685, avg_loss=0.07520]\n",
      "Step 505768  [5.377 sec/step, loss=0.07696, avg_loss=0.07523]\n",
      "Step 505769  [5.380 sec/step, loss=0.07817, avg_loss=0.07523]\n",
      "Step 505770  [5.386 sec/step, loss=0.07603, avg_loss=0.07525]\n",
      "Step 505771  [5.397 sec/step, loss=0.07548, avg_loss=0.07524]\n",
      "Step 505772  [5.397 sec/step, loss=0.07478, avg_loss=0.07522]\n",
      "Step 505773  [5.419 sec/step, loss=0.07600, avg_loss=0.07525]\n",
      "Step 505774  [5.416 sec/step, loss=0.07713, avg_loss=0.07528]\n",
      "Step 505775  [5.395 sec/step, loss=0.07555, avg_loss=0.07529]\n",
      "Step 505776  [5.374 sec/step, loss=0.07247, avg_loss=0.07524]\n",
      "Step 505777  [5.367 sec/step, loss=0.07557, avg_loss=0.07526]\n",
      "Step 505778  [5.367 sec/step, loss=0.07645, avg_loss=0.07528]\n",
      "Step 505779  [5.372 sec/step, loss=0.07639, avg_loss=0.07527]\n",
      "Step 505780  [5.401 sec/step, loss=0.07427, avg_loss=0.07525]\n",
      "Step 505781  [5.375 sec/step, loss=0.07285, avg_loss=0.07522]\n",
      "Step 505782  [5.373 sec/step, loss=0.07641, avg_loss=0.07521]\n",
      "Step 505783  [5.385 sec/step, loss=0.07475, avg_loss=0.07520]\n",
      "Step 505784  [5.395 sec/step, loss=0.07702, avg_loss=0.07521]\n",
      "Step 505785  [5.392 sec/step, loss=0.07256, avg_loss=0.07516]\n",
      "Generated 32 batches of size 32 in 2.369 sec\n",
      "Step 505786  [5.393 sec/step, loss=0.07839, avg_loss=0.07517]\n",
      "Step 505787  [5.379 sec/step, loss=0.07465, avg_loss=0.07518]\n",
      "Step 505788  [5.375 sec/step, loss=0.07517, avg_loss=0.07517]\n",
      "Step 505789  [5.377 sec/step, loss=0.07554, avg_loss=0.07517]\n",
      "Step 505790  [5.363 sec/step, loss=0.07406, avg_loss=0.07515]\n",
      "Step 505791  [5.379 sec/step, loss=0.07618, avg_loss=0.07523]\n",
      "Step 505792  [5.374 sec/step, loss=0.07673, avg_loss=0.07522]\n",
      "Step 505793  [5.389 sec/step, loss=0.07782, avg_loss=0.07524]\n",
      "Step 505794  [5.398 sec/step, loss=0.07791, avg_loss=0.07526]\n",
      "Step 505795  [5.366 sec/step, loss=0.06655, avg_loss=0.07515]\n",
      "Step 505796  [5.317 sec/step, loss=0.07660, avg_loss=0.07524]\n",
      "Step 505797  [5.314 sec/step, loss=0.07238, avg_loss=0.07524]\n",
      "Step 505798  [5.333 sec/step, loss=0.07759, avg_loss=0.07526]\n",
      "Step 505799  [5.320 sec/step, loss=0.07521, avg_loss=0.07523]\n",
      "Step 505800  [5.310 sec/step, loss=0.07632, avg_loss=0.07522]\n",
      "Writing summary at step: 505800\n",
      "Step 505801  [5.320 sec/step, loss=0.07366, avg_loss=0.07519]\n",
      "Step 505802  [5.325 sec/step, loss=0.07639, avg_loss=0.07521]\n",
      "Step 505803  [5.341 sec/step, loss=0.07734, avg_loss=0.07522]\n",
      "Step 505804  [5.355 sec/step, loss=0.07499, avg_loss=0.07520]\n",
      "Step 505805  [5.318 sec/step, loss=0.07312, avg_loss=0.07519]\n",
      "Step 505806  [5.308 sec/step, loss=0.07321, avg_loss=0.07514]\n",
      "Step 505807  [5.306 sec/step, loss=0.07526, avg_loss=0.07514]\n",
      "Step 505808  [5.317 sec/step, loss=0.07725, avg_loss=0.07518]\n",
      "Step 505809  [5.322 sec/step, loss=0.07559, avg_loss=0.07517]\n",
      "Step 505810  [5.333 sec/step, loss=0.07510, avg_loss=0.07514]\n",
      "Step 505811  [5.282 sec/step, loss=0.07623, avg_loss=0.07521]\n",
      "Step 505812  [5.290 sec/step, loss=0.07564, avg_loss=0.07521]\n",
      "Step 505813  [5.305 sec/step, loss=0.07773, avg_loss=0.07525]\n",
      "Step 505814  [5.313 sec/step, loss=0.07770, avg_loss=0.07527]\n",
      "Step 505815  [5.297 sec/step, loss=0.07540, avg_loss=0.07527]\n",
      "Step 505816  [5.304 sec/step, loss=0.07375, avg_loss=0.07524]\n",
      "Generated 32 batches of size 32 in 2.386 sec\n",
      "Step 505817  [5.335 sec/step, loss=0.07655, avg_loss=0.07533]\n",
      "Step 505818  [5.381 sec/step, loss=0.06839, avg_loss=0.07524]\n",
      "Step 505819  [5.399 sec/step, loss=0.07647, avg_loss=0.07528]\n",
      "Step 505820  [5.393 sec/step, loss=0.07400, avg_loss=0.07524]\n",
      "Step 505821  [5.378 sec/step, loss=0.07561, avg_loss=0.07524]\n",
      "Step 505822  [5.375 sec/step, loss=0.07773, avg_loss=0.07527]\n",
      "Step 505823  [5.360 sec/step, loss=0.07823, avg_loss=0.07530]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 505824  [5.353 sec/step, loss=0.07231, avg_loss=0.07526]\n",
      "Step 505825  [5.359 sec/step, loss=0.07768, avg_loss=0.07525]\n",
      "Step 505826  [5.362 sec/step, loss=0.07667, avg_loss=0.07525]\n",
      "Step 505827  [5.361 sec/step, loss=0.07524, avg_loss=0.07524]\n",
      "Step 505828  [5.349 sec/step, loss=0.07325, avg_loss=0.07521]\n",
      "Step 505829  [5.355 sec/step, loss=0.07518, avg_loss=0.07524]\n",
      "Step 505830  [5.371 sec/step, loss=0.07766, avg_loss=0.07529]\n",
      "Step 505831  [5.361 sec/step, loss=0.07568, avg_loss=0.07527]\n",
      "Step 505832  [5.362 sec/step, loss=0.07672, avg_loss=0.07526]\n",
      "Step 505833  [5.384 sec/step, loss=0.07754, avg_loss=0.07527]\n",
      "Step 505834  [5.370 sec/step, loss=0.07617, avg_loss=0.07526]\n",
      "Step 505835  [5.369 sec/step, loss=0.07115, avg_loss=0.07521]\n",
      "Step 505836  [5.370 sec/step, loss=0.07692, avg_loss=0.07521]\n",
      "Step 505837  [5.363 sec/step, loss=0.07633, avg_loss=0.07520]\n",
      "Step 505838  [5.354 sec/step, loss=0.07411, avg_loss=0.07518]\n",
      "Step 505839  [5.362 sec/step, loss=0.07747, avg_loss=0.07519]\n",
      "Step 505840  [5.341 sec/step, loss=0.07405, avg_loss=0.07518]\n",
      "Step 505841  [5.342 sec/step, loss=0.07573, avg_loss=0.07515]\n",
      "Step 505842  [5.323 sec/step, loss=0.07295, avg_loss=0.07512]\n",
      "Step 505843  [5.325 sec/step, loss=0.07335, avg_loss=0.07509]\n",
      "Step 505844  [5.354 sec/step, loss=0.07479, avg_loss=0.07510]\n",
      "Step 505845  [5.354 sec/step, loss=0.07793, avg_loss=0.07511]\n",
      "Step 505846  [5.362 sec/step, loss=0.07798, avg_loss=0.07512]\n",
      "Step 505847  [5.377 sec/step, loss=0.07746, avg_loss=0.07522]\n",
      "Step 505848  [5.395 sec/step, loss=0.07742, avg_loss=0.07525]\n",
      "Generated 32 batches of size 32 in 2.406 sec\n",
      "Step 505849  [5.395 sec/step, loss=0.07598, avg_loss=0.07525]\n",
      "Step 505850  [5.402 sec/step, loss=0.07825, avg_loss=0.07526]\n",
      "Step 505851  [5.400 sec/step, loss=0.06921, avg_loss=0.07522]\n",
      "Step 505852  [5.394 sec/step, loss=0.07597, avg_loss=0.07521]\n",
      "Step 505853  [5.390 sec/step, loss=0.07533, avg_loss=0.07521]\n",
      "Step 505854  [5.337 sec/step, loss=0.07688, avg_loss=0.07532]\n",
      "Step 505855  [5.345 sec/step, loss=0.07765, avg_loss=0.07534]\n",
      "Step 505856  [5.336 sec/step, loss=0.07641, avg_loss=0.07535]\n",
      "Step 505857  [5.396 sec/step, loss=0.06766, avg_loss=0.07530]\n",
      "Step 505858  [5.400 sec/step, loss=0.07560, avg_loss=0.07530]\n",
      "Step 505859  [5.407 sec/step, loss=0.07758, avg_loss=0.07533]\n",
      "Step 505860  [5.422 sec/step, loss=0.07670, avg_loss=0.07532]\n",
      "Step 505861  [5.421 sec/step, loss=0.07473, avg_loss=0.07531]\n",
      "Step 505862  [5.438 sec/step, loss=0.07696, avg_loss=0.07533]\n",
      "Step 505863  [5.383 sec/step, loss=0.07423, avg_loss=0.07540]\n",
      "Step 505864  [5.372 sec/step, loss=0.07299, avg_loss=0.07537]\n",
      "Step 505865  [5.359 sec/step, loss=0.07693, avg_loss=0.07537]\n",
      "Step 505866  [5.408 sec/step, loss=0.06777, avg_loss=0.07531]\n",
      "Step 505867  [5.439 sec/step, loss=0.07547, avg_loss=0.07539]\n",
      "Step 505868  [5.434 sec/step, loss=0.07541, avg_loss=0.07538]\n",
      "Step 505869  [5.437 sec/step, loss=0.07751, avg_loss=0.07537]\n",
      "Step 505870  [5.435 sec/step, loss=0.07661, avg_loss=0.07538]\n",
      "Step 505871  [5.436 sec/step, loss=0.07643, avg_loss=0.07539]\n",
      "Step 505872  [5.430 sec/step, loss=0.07770, avg_loss=0.07542]\n",
      "Step 505873  [5.412 sec/step, loss=0.07577, avg_loss=0.07541]\n",
      "Step 505874  [5.418 sec/step, loss=0.07788, avg_loss=0.07542]\n",
      "Step 505875  [5.410 sec/step, loss=0.07166, avg_loss=0.07538]\n",
      "Step 505876  [5.427 sec/step, loss=0.07588, avg_loss=0.07542]\n",
      "Step 505877  [5.434 sec/step, loss=0.07508, avg_loss=0.07541]\n",
      "Step 505878  [5.434 sec/step, loss=0.07703, avg_loss=0.07542]\n",
      "Step 505879  [5.436 sec/step, loss=0.07598, avg_loss=0.07541]\n",
      "Step 505880  [5.420 sec/step, loss=0.07485, avg_loss=0.07542]\n",
      "Generated 32 batches of size 32 in 2.406 sec\n",
      "Step 505881  [5.449 sec/step, loss=0.07807, avg_loss=0.07547]\n",
      "Step 505882  [5.447 sec/step, loss=0.07684, avg_loss=0.07548]\n",
      "Step 505883  [5.421 sec/step, loss=0.06713, avg_loss=0.07540]\n",
      "Step 505884  [5.411 sec/step, loss=0.07505, avg_loss=0.07538]\n",
      "Step 505885  [5.413 sec/step, loss=0.07604, avg_loss=0.07541]\n",
      "Step 505886  [5.398 sec/step, loss=0.07296, avg_loss=0.07536]\n",
      "Step 505887  [5.402 sec/step, loss=0.07678, avg_loss=0.07538]\n",
      "Step 505888  [5.401 sec/step, loss=0.07553, avg_loss=0.07538]\n",
      "Step 505889  [5.407 sec/step, loss=0.07403, avg_loss=0.07537]\n",
      "Step 505890  [5.407 sec/step, loss=0.07669, avg_loss=0.07540]\n",
      "Step 505891  [5.422 sec/step, loss=0.07446, avg_loss=0.07538]\n",
      "Step 505892  [5.414 sec/step, loss=0.07444, avg_loss=0.07536]\n",
      "Step 505893  [5.406 sec/step, loss=0.07643, avg_loss=0.07534]\n",
      "Step 505894  [5.395 sec/step, loss=0.07452, avg_loss=0.07531]\n",
      "Step 505895  [5.425 sec/step, loss=0.07740, avg_loss=0.07542]\n",
      "Step 505896  [5.432 sec/step, loss=0.07678, avg_loss=0.07542]\n",
      "Step 505897  [5.442 sec/step, loss=0.07678, avg_loss=0.07546]\n",
      "Step 505898  [5.433 sec/step, loss=0.07593, avg_loss=0.07545]\n",
      "Step 505899  [5.447 sec/step, loss=0.07765, avg_loss=0.07547]\n",
      "Step 505900  [5.443 sec/step, loss=0.07701, avg_loss=0.07548]\n",
      "Writing summary at step: 505900\n",
      "Step 505901  [5.448 sec/step, loss=0.07750, avg_loss=0.07552]\n",
      "Step 505902  [5.502 sec/step, loss=0.06725, avg_loss=0.07542]\n",
      "Step 505903  [5.483 sec/step, loss=0.07579, avg_loss=0.07541]\n",
      "Step 505904  [5.468 sec/step, loss=0.07779, avg_loss=0.07544]\n",
      "Step 505905  [5.480 sec/step, loss=0.07648, avg_loss=0.07547]\n",
      "Step 505906  [5.473 sec/step, loss=0.07517, avg_loss=0.07549]\n",
      "Step 505907  [5.471 sec/step, loss=0.07503, avg_loss=0.07549]\n",
      "Step 505908  [5.459 sec/step, loss=0.07236, avg_loss=0.07544]\n",
      "Step 505909  [5.481 sec/step, loss=0.07485, avg_loss=0.07543]\n",
      "Step 505910  [5.446 sec/step, loss=0.07347, avg_loss=0.07541]\n",
      "Step 505911  [5.451 sec/step, loss=0.07626, avg_loss=0.07542]\n",
      "Generated 32 batches of size 32 in 2.444 sec\n",
      "Step 505912  [5.457 sec/step, loss=0.07526, avg_loss=0.07541]\n",
      "Step 505913  [5.455 sec/step, loss=0.07658, avg_loss=0.07540]\n",
      "Step 505914  [5.455 sec/step, loss=0.07793, avg_loss=0.07540]\n",
      "Step 505915  [5.462 sec/step, loss=0.07229, avg_loss=0.07537]\n",
      "Step 505916  [5.443 sec/step, loss=0.06834, avg_loss=0.07532]\n",
      "Step 505917  [5.430 sec/step, loss=0.07690, avg_loss=0.07532]\n",
      "Step 505918  [5.391 sec/step, loss=0.07656, avg_loss=0.07540]\n",
      "Step 505919  [5.393 sec/step, loss=0.07715, avg_loss=0.07541]\n",
      "Step 505920  [5.402 sec/step, loss=0.07622, avg_loss=0.07543]\n",
      "Step 505921  [5.422 sec/step, loss=0.07483, avg_loss=0.07542]\n",
      "Step 505922  [5.411 sec/step, loss=0.07442, avg_loss=0.07539]\n",
      "Step 505923  [5.401 sec/step, loss=0.07572, avg_loss=0.07536]\n",
      "Step 505924  [5.412 sec/step, loss=0.07654, avg_loss=0.07541]\n",
      "Step 505925  [5.401 sec/step, loss=0.07606, avg_loss=0.07539]\n",
      "Step 505926  [5.404 sec/step, loss=0.07773, avg_loss=0.07540]\n",
      "Step 505927  [5.411 sec/step, loss=0.07601, avg_loss=0.07541]\n",
      "Step 505928  [5.422 sec/step, loss=0.07668, avg_loss=0.07544]\n",
      "Step 505929  [5.443 sec/step, loss=0.07748, avg_loss=0.07547]\n",
      "Step 505930  [5.445 sec/step, loss=0.07732, avg_loss=0.07546]\n",
      "Step 505931  [5.450 sec/step, loss=0.07631, avg_loss=0.07547]\n",
      "Step 505932  [5.491 sec/step, loss=0.06801, avg_loss=0.07538]\n",
      "Step 505933  [5.475 sec/step, loss=0.07438, avg_loss=0.07535]\n",
      "Step 505934  [5.468 sec/step, loss=0.07601, avg_loss=0.07535]\n",
      "Step 505935  [5.473 sec/step, loss=0.07553, avg_loss=0.07539]\n",
      "Step 505936  [5.462 sec/step, loss=0.07525, avg_loss=0.07538]\n",
      "Step 505937  [5.472 sec/step, loss=0.07827, avg_loss=0.07540]\n",
      "Step 505938  [5.479 sec/step, loss=0.07406, avg_loss=0.07540]\n",
      "Step 505939  [5.478 sec/step, loss=0.07740, avg_loss=0.07539]\n",
      "Step 505940  [5.506 sec/step, loss=0.07745, avg_loss=0.07543]\n",
      "Step 505941  [5.491 sec/step, loss=0.07508, avg_loss=0.07542]\n",
      "Step 505942  [5.476 sec/step, loss=0.06588, avg_loss=0.07535]\n",
      "Step 505943  [5.470 sec/step, loss=0.07656, avg_loss=0.07538]\n",
      "Generated 32 batches of size 32 in 2.955 sec\n",
      "Step 505944  [5.435 sec/step, loss=0.07223, avg_loss=0.07536]\n",
      "Step 505945  [5.426 sec/step, loss=0.07697, avg_loss=0.07535]\n",
      "Step 505946  [5.410 sec/step, loss=0.07443, avg_loss=0.07531]\n",
      "Step 505947  [5.403 sec/step, loss=0.07593, avg_loss=0.07530]\n",
      "Step 505948  [5.382 sec/step, loss=0.07661, avg_loss=0.07529]\n",
      "Step 505949  [5.381 sec/step, loss=0.07540, avg_loss=0.07528]\n",
      "Step 505950  [5.381 sec/step, loss=0.07723, avg_loss=0.07527]\n",
      "Step 505951  [5.386 sec/step, loss=0.07286, avg_loss=0.07531]\n",
      "Step 505952  [5.417 sec/step, loss=0.07369, avg_loss=0.07529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 505953  [5.431 sec/step, loss=0.07545, avg_loss=0.07529]\n",
      "Step 505954  [5.442 sec/step, loss=0.07765, avg_loss=0.07530]\n",
      "Step 505955  [5.439 sec/step, loss=0.07698, avg_loss=0.07529]\n",
      "Step 505956  [5.433 sec/step, loss=0.06626, avg_loss=0.07519]\n",
      "Step 505957  [5.379 sec/step, loss=0.07331, avg_loss=0.07524]\n",
      "Step 505958  [5.387 sec/step, loss=0.07799, avg_loss=0.07527]\n",
      "Step 505959  [5.377 sec/step, loss=0.07525, avg_loss=0.07524]\n",
      "Step 505960  [5.349 sec/step, loss=0.07190, avg_loss=0.07520]\n",
      "Step 505961  [5.325 sec/step, loss=0.07689, avg_loss=0.07522]\n",
      "Step 505962  [5.317 sec/step, loss=0.07649, avg_loss=0.07521]\n",
      "Step 505963  [5.318 sec/step, loss=0.07548, avg_loss=0.07523]\n",
      "Step 505964  [5.381 sec/step, loss=0.06729, avg_loss=0.07517]\n",
      "Step 505965  [5.391 sec/step, loss=0.07750, avg_loss=0.07517]\n",
      "Step 505966  [5.341 sec/step, loss=0.07535, avg_loss=0.07525]\n",
      "Step 505967  [5.327 sec/step, loss=0.07581, avg_loss=0.07525]\n",
      "Step 505968  [5.346 sec/step, loss=0.07619, avg_loss=0.07526]\n",
      "Step 505969  [5.342 sec/step, loss=0.07768, avg_loss=0.07526]\n",
      "Step 505970  [5.346 sec/step, loss=0.07553, avg_loss=0.07525]\n",
      "Step 505971  [5.346 sec/step, loss=0.07706, avg_loss=0.07526]\n",
      "Step 505972  [5.346 sec/step, loss=0.07636, avg_loss=0.07525]\n",
      "Step 505973  [5.364 sec/step, loss=0.07705, avg_loss=0.07526]\n",
      "Step 505974  [5.342 sec/step, loss=0.07298, avg_loss=0.07521]\n",
      "Step 505975  [5.347 sec/step, loss=0.07232, avg_loss=0.07522]\n",
      "Generated 32 batches of size 32 in 2.537 sec\n",
      "Step 505976  [5.337 sec/step, loss=0.07528, avg_loss=0.07521]\n",
      "Step 505977  [5.342 sec/step, loss=0.07660, avg_loss=0.07523]\n",
      "Step 505978  [5.350 sec/step, loss=0.07761, avg_loss=0.07523]\n",
      "Step 505979  [5.344 sec/step, loss=0.07613, avg_loss=0.07523]\n",
      "Step 505980  [5.361 sec/step, loss=0.07475, avg_loss=0.07523]\n",
      "Step 505981  [5.365 sec/step, loss=0.07498, avg_loss=0.07520]\n",
      "Step 505982  [5.371 sec/step, loss=0.07657, avg_loss=0.07520]\n",
      "Step 505983  [5.380 sec/step, loss=0.07502, avg_loss=0.07528]\n",
      "Step 505984  [5.372 sec/step, loss=0.07661, avg_loss=0.07529]\n",
      "Step 505985  [5.377 sec/step, loss=0.07629, avg_loss=0.07530]\n",
      "Step 505986  [5.366 sec/step, loss=0.07370, avg_loss=0.07530]\n",
      "Step 505987  [5.361 sec/step, loss=0.07512, avg_loss=0.07529]\n",
      "Step 505988  [5.377 sec/step, loss=0.07737, avg_loss=0.07530]\n",
      "Step 505989  [5.379 sec/step, loss=0.07615, avg_loss=0.07533]\n",
      "Step 505990  [5.371 sec/step, loss=0.07290, avg_loss=0.07529]\n",
      "Step 505991  [5.369 sec/step, loss=0.07704, avg_loss=0.07531]\n",
      "Step 505992  [5.375 sec/step, loss=0.07714, avg_loss=0.07534]\n",
      "Step 505993  [5.397 sec/step, loss=0.07381, avg_loss=0.07531]\n",
      "Step 505994  [5.407 sec/step, loss=0.07579, avg_loss=0.07533]\n",
      "Step 505995  [5.392 sec/step, loss=0.07647, avg_loss=0.07532]\n",
      "Step 505996  [5.368 sec/step, loss=0.06758, avg_loss=0.07523]\n",
      "Step 505997  [5.372 sec/step, loss=0.07469, avg_loss=0.07521]\n",
      "Step 505998  [5.380 sec/step, loss=0.07748, avg_loss=0.07522]\n",
      "Step 505999  [5.382 sec/step, loss=0.07776, avg_loss=0.07522]\n",
      "Step 506000  [5.382 sec/step, loss=0.07726, avg_loss=0.07522]\n",
      "Writing summary at step: 506000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-506000\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tvenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Saving audio and alignment...\n",
      "Input: aediishnal dzidzuun kii tdaviil addaaiigiioon sae suuis baenk munaafoon kay uglay addaaddooshumaar kay dzaddval riiliiz kur ddiiay giay~______\n",
      "Step 506001  [5.380 sec/step, loss=0.07712, avg_loss=0.07522]\n",
      "Step 506002  [5.318 sec/step, loss=0.07571, avg_loss=0.07531]\n",
      "Step 506003  [5.324 sec/step, loss=0.07695, avg_loss=0.07532]\n",
      "Step 506004  [5.362 sec/step, loss=0.06724, avg_loss=0.07521]\n",
      "Step 506005  [5.358 sec/step, loss=0.07650, avg_loss=0.07521]\n",
      "Generated 32 batches of size 32 in 2.406 sec\n",
      "Step 506006  [5.370 sec/step, loss=0.07594, avg_loss=0.07522]\n",
      "Step 506007  [5.381 sec/step, loss=0.07767, avg_loss=0.07525]\n",
      "Step 506008  [5.387 sec/step, loss=0.07561, avg_loss=0.07528]\n",
      "Step 506009  [5.356 sec/step, loss=0.07467, avg_loss=0.07528]\n",
      "Step 506010  [5.381 sec/step, loss=0.07708, avg_loss=0.07531]\n",
      "Step 506011  [5.384 sec/step, loss=0.07513, avg_loss=0.07530]\n",
      "Step 506012  [5.374 sec/step, loss=0.07142, avg_loss=0.07526]\n",
      "Step 506013  [5.371 sec/step, loss=0.07779, avg_loss=0.07527]\n",
      "Step 506014  [5.361 sec/step, loss=0.07372, avg_loss=0.07523]\n",
      "Step 506015  [5.374 sec/step, loss=0.07734, avg_loss=0.07528]\n",
      "Step 506016  [5.380 sec/step, loss=0.07516, avg_loss=0.07535]\n",
      "Step 506017  [5.379 sec/step, loss=0.07692, avg_loss=0.07535]\n",
      "Step 506018  [5.352 sec/step, loss=0.06752, avg_loss=0.07526]\n",
      "Step 506019  [5.332 sec/step, loss=0.07660, avg_loss=0.07526]\n",
      "Step 506020  [5.330 sec/step, loss=0.07681, avg_loss=0.07526]\n",
      "Step 506021  [5.326 sec/step, loss=0.07757, avg_loss=0.07529]\n",
      "Step 506022  [5.337 sec/step, loss=0.07626, avg_loss=0.07531]\n",
      "Step 506023  [5.350 sec/step, loss=0.07783, avg_loss=0.07533]\n",
      "Step 506024  [5.344 sec/step, loss=0.07425, avg_loss=0.07531]\n",
      "Step 506025  [5.340 sec/step, loss=0.07303, avg_loss=0.07528]\n",
      "Step 506026  [5.325 sec/step, loss=0.07510, avg_loss=0.07525]\n",
      "Step 506027  [5.310 sec/step, loss=0.07251, avg_loss=0.07521]\n",
      "Step 506028  [5.309 sec/step, loss=0.07577, avg_loss=0.07520]\n",
      "Step 506029  [5.282 sec/step, loss=0.07312, avg_loss=0.07516]\n",
      "Step 506030  [5.262 sec/step, loss=0.07233, avg_loss=0.07511]\n",
      "Step 506031  [5.266 sec/step, loss=0.07773, avg_loss=0.07513]\n",
      "Step 506032  [5.241 sec/step, loss=0.07426, avg_loss=0.07519]\n",
      "Step 506033  [5.241 sec/step, loss=0.07668, avg_loss=0.07521]\n",
      "Step 506034  [5.241 sec/step, loss=0.07523, avg_loss=0.07520]\n",
      "Step 506035  [5.234 sec/step, loss=0.07368, avg_loss=0.07518]\n",
      "Step 506036  [5.256 sec/step, loss=0.07614, avg_loss=0.07519]\n",
      "Step 506037  [5.253 sec/step, loss=0.07664, avg_loss=0.07518]\n",
      "Generated 32 batches of size 32 in 2.326 sec\n",
      "Step 506038  [5.260 sec/step, loss=0.07810, avg_loss=0.07522]\n",
      "Step 506039  [5.259 sec/step, loss=0.07292, avg_loss=0.07517]\n",
      "Step 506040  [5.251 sec/step, loss=0.07764, avg_loss=0.07517]\n",
      "Step 506041  [5.253 sec/step, loss=0.07546, avg_loss=0.07518]\n",
      "Step 506042  [5.278 sec/step, loss=0.07536, avg_loss=0.07527]\n",
      "Step 506043  [5.284 sec/step, loss=0.07601, avg_loss=0.07527]\n",
      "Step 506044  [5.283 sec/step, loss=0.07525, avg_loss=0.07530]\n",
      "Step 506045  [5.331 sec/step, loss=0.06741, avg_loss=0.07520]\n",
      "Step 506046  [5.336 sec/step, loss=0.07685, avg_loss=0.07523]\n",
      "Step 506047  [5.342 sec/step, loss=0.07481, avg_loss=0.07522]\n",
      "Step 506048  [5.354 sec/step, loss=0.07570, avg_loss=0.07521]\n",
      "Step 506049  [5.356 sec/step, loss=0.07768, avg_loss=0.07523]\n",
      "Step 506050  [5.348 sec/step, loss=0.07653, avg_loss=0.07522]\n",
      "Step 506051  [5.371 sec/step, loss=0.07733, avg_loss=0.07527]\n",
      "Step 506052  [5.340 sec/step, loss=0.07317, avg_loss=0.07526]\n",
      "Step 506053  [5.375 sec/step, loss=0.06835, avg_loss=0.07519]\n",
      "Step 506054  [5.365 sec/step, loss=0.07549, avg_loss=0.07517]\n",
      "Step 506055  [5.352 sec/step, loss=0.06831, avg_loss=0.07508]\n",
      "Step 506056  [5.369 sec/step, loss=0.07652, avg_loss=0.07518]\n",
      "Step 506057  [5.360 sec/step, loss=0.07266, avg_loss=0.07518]\n",
      "Step 506058  [5.343 sec/step, loss=0.07512, avg_loss=0.07515]\n",
      "Step 506059  [5.361 sec/step, loss=0.07607, avg_loss=0.07516]\n",
      "Step 506060  [5.398 sec/step, loss=0.07535, avg_loss=0.07519]\n",
      "Step 506061  [5.395 sec/step, loss=0.07665, avg_loss=0.07519]\n",
      "Step 506062  [5.390 sec/step, loss=0.07389, avg_loss=0.07516]\n",
      "Step 506063  [5.395 sec/step, loss=0.07722, avg_loss=0.07518]\n",
      "Step 506064  [5.343 sec/step, loss=0.07179, avg_loss=0.07523]\n",
      "Step 506065  [5.340 sec/step, loss=0.07493, avg_loss=0.07520]\n",
      "Step 506066  [5.344 sec/step, loss=0.07685, avg_loss=0.07522]\n",
      "Step 506067  [5.364 sec/step, loss=0.07479, avg_loss=0.07521]\n",
      "Step 506068  [5.358 sec/step, loss=0.07645, avg_loss=0.07521]\n",
      "Step 506069  [5.337 sec/step, loss=0.07562, avg_loss=0.07519]\n",
      "Generated 32 batches of size 32 in 2.481 sec\n",
      "Step 506070  [5.338 sec/step, loss=0.07624, avg_loss=0.07519]\n",
      "Step 506071  [5.344 sec/step, loss=0.07816, avg_loss=0.07521]\n",
      "Step 506072  [5.338 sec/step, loss=0.07545, avg_loss=0.07520]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 506073  [5.336 sec/step, loss=0.07780, avg_loss=0.07520]\n",
      "Step 506074  [5.342 sec/step, loss=0.07543, avg_loss=0.07523]\n",
      "Step 506075  [5.355 sec/step, loss=0.07737, avg_loss=0.07528]\n",
      "Step 506076  [5.370 sec/step, loss=0.07829, avg_loss=0.07531]\n",
      "Step 506077  [5.370 sec/step, loss=0.07666, avg_loss=0.07531]\n",
      "Step 506078  [5.351 sec/step, loss=0.07210, avg_loss=0.07525]\n",
      "Step 506079  [5.333 sec/step, loss=0.07397, avg_loss=0.07523]\n",
      "Step 506080  [5.312 sec/step, loss=0.07671, avg_loss=0.07525]\n",
      "Step 506081  [5.319 sec/step, loss=0.07364, avg_loss=0.07524]\n",
      "Step 506082  [5.327 sec/step, loss=0.07539, avg_loss=0.07523]\n",
      "Step 506083  [5.333 sec/step, loss=0.07579, avg_loss=0.07523]\n",
      "Step 506084  [5.346 sec/step, loss=0.07520, avg_loss=0.07522]\n",
      "Step 506085  [5.335 sec/step, loss=0.07522, avg_loss=0.07521]\n",
      "Step 506086  [5.362 sec/step, loss=0.07797, avg_loss=0.07525]\n",
      "Step 506087  [5.360 sec/step, loss=0.07242, avg_loss=0.07523]\n",
      "Step 506088  [5.348 sec/step, loss=0.07406, avg_loss=0.07519]\n",
      "Step 506089  [5.397 sec/step, loss=0.06711, avg_loss=0.07510]\n",
      "Step 506090  [5.426 sec/step, loss=0.07728, avg_loss=0.07515]\n",
      "Step 506091  [5.404 sec/step, loss=0.07572, avg_loss=0.07513]\n",
      "Step 506092  [5.404 sec/step, loss=0.07734, avg_loss=0.07513]\n",
      "Step 506093  [5.382 sec/step, loss=0.07592, avg_loss=0.07516]\n",
      "Step 506094  [5.379 sec/step, loss=0.07650, avg_loss=0.07516]\n",
      "Step 506095  [5.378 sec/step, loss=0.07688, avg_loss=0.07517]\n",
      "Step 506096  [5.376 sec/step, loss=0.06740, avg_loss=0.07517]\n",
      "Step 506097  [5.373 sec/step, loss=0.07571, avg_loss=0.07518]\n",
      "Step 506098  [5.369 sec/step, loss=0.07707, avg_loss=0.07517]\n",
      "Step 506099  [5.360 sec/step, loss=0.07603, avg_loss=0.07515]\n",
      "Step 506100  [5.367 sec/step, loss=0.07793, avg_loss=0.07516]\n",
      "Writing summary at step: 506100\n",
      "Generated 32 batches of size 32 in 2.357 sec\n",
      "Step 506101  [5.371 sec/step, loss=0.07805, avg_loss=0.07517]\n",
      "Step 506102  [5.397 sec/step, loss=0.07757, avg_loss=0.07519]\n",
      "Step 506103  [5.399 sec/step, loss=0.07651, avg_loss=0.07518]\n",
      "Step 506104  [5.339 sec/step, loss=0.07355, avg_loss=0.07525]\n",
      "Step 506105  [5.350 sec/step, loss=0.07801, avg_loss=0.07526]\n",
      "Step 506106  [5.339 sec/step, loss=0.07586, avg_loss=0.07526]\n",
      "Step 506107  [5.331 sec/step, loss=0.07483, avg_loss=0.07523]\n",
      "Step 506108  [5.347 sec/step, loss=0.07825, avg_loss=0.07526]\n",
      "Step 506109  [5.355 sec/step, loss=0.07658, avg_loss=0.07528]\n",
      "Step 506110  [5.337 sec/step, loss=0.07561, avg_loss=0.07526]\n",
      "Step 506111  [5.356 sec/step, loss=0.07460, avg_loss=0.07526]\n",
      "Step 506112  [5.372 sec/step, loss=0.07767, avg_loss=0.07532]\n",
      "Step 506113  [5.372 sec/step, loss=0.07824, avg_loss=0.07533]\n",
      "Step 506114  [5.423 sec/step, loss=0.06740, avg_loss=0.07526]\n",
      "Step 506115  [5.431 sec/step, loss=0.07494, avg_loss=0.07524]\n",
      "Step 506116  [5.439 sec/step, loss=0.07680, avg_loss=0.07526]\n",
      "Step 506117  [5.453 sec/step, loss=0.07621, avg_loss=0.07525]\n",
      "Step 506118  [5.480 sec/step, loss=0.07586, avg_loss=0.07533]\n",
      "Step 506119  [5.489 sec/step, loss=0.07717, avg_loss=0.07534]\n",
      "Step 506120  [5.483 sec/step, loss=0.07380, avg_loss=0.07531]\n",
      "Step 506121  [5.475 sec/step, loss=0.07691, avg_loss=0.07530]\n",
      "Step 506122  [5.462 sec/step, loss=0.07293, avg_loss=0.07527]\n",
      "Step 506123  [5.436 sec/step, loss=0.07238, avg_loss=0.07521]\n",
      "Step 506124  [5.440 sec/step, loss=0.07711, avg_loss=0.07524]\n",
      "Step 506125  [5.442 sec/step, loss=0.07623, avg_loss=0.07527]\n",
      "Step 506126  [5.458 sec/step, loss=0.07776, avg_loss=0.07530]\n",
      "Step 506127  [5.480 sec/step, loss=0.07778, avg_loss=0.07535]\n",
      "Step 506128  [5.471 sec/step, loss=0.07242, avg_loss=0.07532]\n",
      "Step 506129  [5.491 sec/step, loss=0.07610, avg_loss=0.07535]\n",
      "Step 506130  [5.497 sec/step, loss=0.07762, avg_loss=0.07540]\n",
      "Step 506131  [5.480 sec/step, loss=0.07512, avg_loss=0.07538]\n",
      "Step 506132  [5.456 sec/step, loss=0.07580, avg_loss=0.07539]\n",
      "Generated 32 batches of size 32 in 2.371 sec\n",
      "Step 506133  [5.469 sec/step, loss=0.07615, avg_loss=0.07539]\n",
      "Step 506134  [5.485 sec/step, loss=0.07551, avg_loss=0.07539]\n",
      "Step 506135  [5.492 sec/step, loss=0.07632, avg_loss=0.07541]\n",
      "Step 506136  [5.468 sec/step, loss=0.07555, avg_loss=0.07541]\n",
      "Step 506137  [5.453 sec/step, loss=0.07538, avg_loss=0.07540]\n",
      "Step 506138  [5.448 sec/step, loss=0.07708, avg_loss=0.07539]\n",
      "Step 506139  [5.456 sec/step, loss=0.07771, avg_loss=0.07543]\n",
      "Step 506140  [5.434 sec/step, loss=0.06611, avg_loss=0.07532]\n",
      "Step 506141  [5.434 sec/step, loss=0.07702, avg_loss=0.07533]\n",
      "Step 506142  [5.434 sec/step, loss=0.07773, avg_loss=0.07536]\n",
      "Step 506143  [5.428 sec/step, loss=0.07368, avg_loss=0.07533]\n",
      "Step 506144  [5.452 sec/step, loss=0.07599, avg_loss=0.07534]\n",
      "Step 506145  [5.404 sec/step, loss=0.07781, avg_loss=0.07545]\n",
      "Step 506146  [5.398 sec/step, loss=0.07449, avg_loss=0.07542]\n",
      "Step 506147  [5.414 sec/step, loss=0.07731, avg_loss=0.07545]\n",
      "Step 506148  [5.402 sec/step, loss=0.07532, avg_loss=0.07544]\n",
      "Step 506149  [5.396 sec/step, loss=0.07773, avg_loss=0.07544]\n",
      "Step 506150  [5.403 sec/step, loss=0.07603, avg_loss=0.07544]\n",
      "Step 506151  [5.377 sec/step, loss=0.07377, avg_loss=0.07540]\n",
      "Step 506152  [5.390 sec/step, loss=0.07681, avg_loss=0.07544]\n",
      "Step 506153  [5.355 sec/step, loss=0.07814, avg_loss=0.07554]\n",
      "Step 506154  [5.342 sec/step, loss=0.07554, avg_loss=0.07554]\n",
      "Step 506155  [5.348 sec/step, loss=0.07362, avg_loss=0.07559]\n",
      "Step 506156  [5.336 sec/step, loss=0.07546, avg_loss=0.07558]\n",
      "Step 506157  [5.333 sec/step, loss=0.06829, avg_loss=0.07554]\n",
      "Step 506158  [5.347 sec/step, loss=0.07785, avg_loss=0.07556]\n",
      "Step 506159  [5.387 sec/step, loss=0.06870, avg_loss=0.07549]\n",
      "Step 506160  [5.381 sec/step, loss=0.07428, avg_loss=0.07548]\n",
      "Step 506161  [5.410 sec/step, loss=0.07475, avg_loss=0.07546]\n",
      "Step 506162  [5.416 sec/step, loss=0.07770, avg_loss=0.07550]\n",
      "Step 506163  [5.407 sec/step, loss=0.07586, avg_loss=0.07549]\n",
      "Step 506164  [5.407 sec/step, loss=0.07686, avg_loss=0.07554]\n",
      "Generated 32 batches of size 32 in 2.360 sec\n",
      "Step 506165  [5.404 sec/step, loss=0.07713, avg_loss=0.07556]\n",
      "Step 506166  [5.402 sec/step, loss=0.07703, avg_loss=0.07556]\n",
      "Step 506167  [5.393 sec/step, loss=0.07609, avg_loss=0.07557]\n",
      "Step 506168  [5.400 sec/step, loss=0.07790, avg_loss=0.07559]\n",
      "Step 506169  [5.410 sec/step, loss=0.07479, avg_loss=0.07558]\n",
      "Step 506170  [5.408 sec/step, loss=0.07320, avg_loss=0.07555]\n",
      "Step 506171  [5.397 sec/step, loss=0.07625, avg_loss=0.07553]\n",
      "Step 506172  [5.401 sec/step, loss=0.07618, avg_loss=0.07554]\n",
      "Step 506173  [5.392 sec/step, loss=0.07711, avg_loss=0.07553]\n",
      "Step 506174  [5.397 sec/step, loss=0.07685, avg_loss=0.07554]\n",
      "Step 506175  [5.389 sec/step, loss=0.07587, avg_loss=0.07553]\n",
      "Step 506176  [5.383 sec/step, loss=0.07654, avg_loss=0.07551]\n",
      "Step 506177  [5.365 sec/step, loss=0.06698, avg_loss=0.07542]\n",
      "Step 506178  [5.389 sec/step, loss=0.07796, avg_loss=0.07547]\n",
      "Step 506179  [5.390 sec/step, loss=0.07246, avg_loss=0.07546]\n",
      "Step 506180  [5.392 sec/step, loss=0.07810, avg_loss=0.07547]\n",
      "Step 506181  [5.373 sec/step, loss=0.07825, avg_loss=0.07552]\n",
      "Step 506182  [5.361 sec/step, loss=0.07517, avg_loss=0.07552]\n",
      "Step 506183  [5.363 sec/step, loss=0.07694, avg_loss=0.07553]\n",
      "Step 506184  [5.343 sec/step, loss=0.07539, avg_loss=0.07553]\n",
      "Step 506185  [5.340 sec/step, loss=0.07212, avg_loss=0.07550]\n",
      "Step 506186  [5.319 sec/step, loss=0.07515, avg_loss=0.07547]\n",
      "Step 506187  [5.341 sec/step, loss=0.07591, avg_loss=0.07551]\n",
      "Step 506188  [5.345 sec/step, loss=0.07721, avg_loss=0.07554]\n",
      "Step 506189  [5.299 sec/step, loss=0.07640, avg_loss=0.07563]\n",
      "Step 506190  [5.330 sec/step, loss=0.06743, avg_loss=0.07553]\n",
      "Step 506191  [5.329 sec/step, loss=0.07541, avg_loss=0.07553]\n",
      "Step 506192  [5.340 sec/step, loss=0.07701, avg_loss=0.07552]\n",
      "Step 506193  [5.342 sec/step, loss=0.07668, avg_loss=0.07553]\n",
      "Step 506194  [5.333 sec/step, loss=0.07472, avg_loss=0.07551]\n",
      "Step 506195  [5.333 sec/step, loss=0.07625, avg_loss=0.07551]\n",
      "Step 506196  [5.361 sec/step, loss=0.07571, avg_loss=0.07559]\n",
      "Generated 32 batches of size 32 in 2.560 sec\n",
      "Step 506197  [5.366 sec/step, loss=0.07538, avg_loss=0.07559]\n",
      "Step 506198  [5.373 sec/step, loss=0.07800, avg_loss=0.07560]\n",
      "Step 506199  [5.401 sec/step, loss=0.07482, avg_loss=0.07559]\n",
      "Step 506200  [5.410 sec/step, loss=0.07662, avg_loss=0.07557]\n",
      "Writing summary at step: 506200\n",
      "Step 506201  [5.408 sec/step, loss=0.07706, avg_loss=0.07556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 506202  [5.395 sec/step, loss=0.07421, avg_loss=0.07553]\n",
      "Step 506203  [5.395 sec/step, loss=0.07494, avg_loss=0.07551]\n",
      "Step 506204  [5.409 sec/step, loss=0.07366, avg_loss=0.07551]\n",
      "Step 506205  [5.399 sec/step, loss=0.07139, avg_loss=0.07545]\n",
      "Step 506206  [5.394 sec/step, loss=0.07388, avg_loss=0.07543]\n",
      "Step 506207  [5.400 sec/step, loss=0.07758, avg_loss=0.07546]\n",
      "Step 506208  [5.401 sec/step, loss=0.07787, avg_loss=0.07545]\n",
      "Step 506209  [5.389 sec/step, loss=0.07569, avg_loss=0.07544]\n",
      "Step 506210  [5.419 sec/step, loss=0.07444, avg_loss=0.07543]\n",
      "Step 506211  [5.392 sec/step, loss=0.07526, avg_loss=0.07544]\n",
      "Step 506212  [5.371 sec/step, loss=0.07370, avg_loss=0.07540]\n",
      "Step 506213  [5.368 sec/step, loss=0.07610, avg_loss=0.07538]\n",
      "Step 506214  [5.324 sec/step, loss=0.07725, avg_loss=0.07548]\n",
      "Step 506215  [5.310 sec/step, loss=0.07698, avg_loss=0.07550]\n",
      "Step 506216  [5.307 sec/step, loss=0.07654, avg_loss=0.07549]\n",
      "Step 506217  [5.300 sec/step, loss=0.07615, avg_loss=0.07549]\n",
      "Step 506218  [5.339 sec/step, loss=0.06738, avg_loss=0.07541]\n",
      "Step 506219  [5.331 sec/step, loss=0.07669, avg_loss=0.07540]\n",
      "Step 506220  [5.343 sec/step, loss=0.07794, avg_loss=0.07544]\n",
      "Step 506221  [5.345 sec/step, loss=0.07593, avg_loss=0.07543]\n",
      "Step 506222  [5.345 sec/step, loss=0.07643, avg_loss=0.07547]\n",
      "Step 506223  [5.354 sec/step, loss=0.07434, avg_loss=0.07549]\n",
      "Step 506224  [5.348 sec/step, loss=0.07705, avg_loss=0.07549]\n",
      "Step 506225  [5.347 sec/step, loss=0.07713, avg_loss=0.07550]\n",
      "Step 506226  [5.352 sec/step, loss=0.07758, avg_loss=0.07550]\n",
      "Step 506227  [5.352 sec/step, loss=0.07744, avg_loss=0.07549]\n",
      "Generated 32 batches of size 32 in 2.587 sec\n",
      "Step 506228  [5.362 sec/step, loss=0.07553, avg_loss=0.07552]\n",
      "Step 506229  [5.361 sec/step, loss=0.07612, avg_loss=0.07552]\n",
      "Step 506230  [5.374 sec/step, loss=0.07619, avg_loss=0.07551]\n",
      "Step 506231  [5.383 sec/step, loss=0.07665, avg_loss=0.07552]\n",
      "Step 506232  [5.394 sec/step, loss=0.07418, avg_loss=0.07551]\n",
      "Step 506233  [5.380 sec/step, loss=0.07592, avg_loss=0.07551]\n",
      "Step 506234  [5.389 sec/step, loss=0.07720, avg_loss=0.07552]\n",
      "Step 506235  [5.372 sec/step, loss=0.06787, avg_loss=0.07544]\n",
      "Step 506236  [5.372 sec/step, loss=0.07548, avg_loss=0.07544]\n",
      "Step 506237  [5.364 sec/step, loss=0.07323, avg_loss=0.07542]\n",
      "Step 506238  [5.361 sec/step, loss=0.07639, avg_loss=0.07541]\n",
      "Step 506239  [5.336 sec/step, loss=0.07325, avg_loss=0.07536]\n",
      "Step 506240  [5.356 sec/step, loss=0.07599, avg_loss=0.07546]\n",
      "Step 506241  [5.358 sec/step, loss=0.07591, avg_loss=0.07545]\n",
      "Step 506242  [5.361 sec/step, loss=0.07706, avg_loss=0.07545]\n",
      "Step 506243  [5.370 sec/step, loss=0.07747, avg_loss=0.07548]\n",
      "Step 506244  [5.379 sec/step, loss=0.07588, avg_loss=0.07548]\n",
      "Step 506245  [5.379 sec/step, loss=0.07707, avg_loss=0.07548]\n",
      "Step 506246  [5.381 sec/step, loss=0.07500, avg_loss=0.07548]\n",
      "Step 506247  [5.364 sec/step, loss=0.07467, avg_loss=0.07545]\n",
      "Step 506248  [5.375 sec/step, loss=0.07640, avg_loss=0.07546]\n",
      "Step 506249  [5.378 sec/step, loss=0.07595, avg_loss=0.07545]\n",
      "Step 506250  [5.372 sec/step, loss=0.07612, avg_loss=0.07545]\n",
      "Step 506251  [5.378 sec/step, loss=0.07495, avg_loss=0.07546]\n",
      "Step 506252  [5.418 sec/step, loss=0.06699, avg_loss=0.07536]\n",
      "Step 506253  [5.400 sec/step, loss=0.07254, avg_loss=0.07531]\n",
      "Step 506254  [5.419 sec/step, loss=0.07571, avg_loss=0.07531]\n",
      "Step 506255  [5.427 sec/step, loss=0.07626, avg_loss=0.07533]\n",
      "Step 506256  [5.445 sec/step, loss=0.07746, avg_loss=0.07535]\n",
      "Step 506257  [5.469 sec/step, loss=0.07619, avg_loss=0.07543]\n",
      "Step 506258  [5.490 sec/step, loss=0.07494, avg_loss=0.07540]\n",
      "Step 506259  [5.458 sec/step, loss=0.07589, avg_loss=0.07548]\n",
      "Generated 32 batches of size 32 in 2.397 sec\n",
      "Step 506260  [5.448 sec/step, loss=0.07641, avg_loss=0.07550]\n",
      "Step 506261  [5.424 sec/step, loss=0.07633, avg_loss=0.07551]\n",
      "Step 506262  [5.431 sec/step, loss=0.07804, avg_loss=0.07552]\n",
      "Step 506263  [5.428 sec/step, loss=0.07562, avg_loss=0.07551]\n",
      "Step 506264  [5.423 sec/step, loss=0.07509, avg_loss=0.07550]\n",
      "Step 506265  [5.420 sec/step, loss=0.07571, avg_loss=0.07548]\n",
      "Step 506266  [5.416 sec/step, loss=0.07437, avg_loss=0.07545]\n",
      "Step 506267  [5.403 sec/step, loss=0.07744, avg_loss=0.07547]\n",
      "Step 506268  [5.374 sec/step, loss=0.06724, avg_loss=0.07536]\n",
      "Step 506269  [5.386 sec/step, loss=0.07534, avg_loss=0.07537]\n",
      "Step 506270  [5.401 sec/step, loss=0.07656, avg_loss=0.07540]\n",
      "Step 506271  [5.414 sec/step, loss=0.07818, avg_loss=0.07542]\n",
      "Step 506272  [5.397 sec/step, loss=0.07300, avg_loss=0.07539]\n",
      "Step 506273  [5.397 sec/step, loss=0.07417, avg_loss=0.07536]\n",
      "Step 506274  [5.403 sec/step, loss=0.07663, avg_loss=0.07536]\n",
      "Step 506275  [5.395 sec/step, loss=0.07551, avg_loss=0.07535]\n",
      "Step 506276  [5.395 sec/step, loss=0.07801, avg_loss=0.07537]\n",
      "Step 506277  [5.422 sec/step, loss=0.07579, avg_loss=0.07546]\n",
      "Step 506278  [5.458 sec/step, loss=0.06845, avg_loss=0.07536]\n",
      "Step 506279  [5.486 sec/step, loss=0.07779, avg_loss=0.07541]\n",
      "Step 506280  [5.476 sec/step, loss=0.07279, avg_loss=0.07536]\n",
      "Step 506281  [5.469 sec/step, loss=0.07463, avg_loss=0.07532]\n",
      "Step 506282  [5.459 sec/step, loss=0.07520, avg_loss=0.07533]\n",
      "Step 506283  [5.459 sec/step, loss=0.07728, avg_loss=0.07533]\n",
      "Step 506284  [5.478 sec/step, loss=0.07801, avg_loss=0.07535]\n",
      "Step 506285  [5.488 sec/step, loss=0.07605, avg_loss=0.07539]\n",
      "Step 506286  [5.519 sec/step, loss=0.07566, avg_loss=0.07540]\n",
      "Step 506287  [5.490 sec/step, loss=0.06816, avg_loss=0.07532]\n",
      "Step 506288  [5.490 sec/step, loss=0.07664, avg_loss=0.07532]\n",
      "Step 506289  [5.487 sec/step, loss=0.07739, avg_loss=0.07533]\n",
      "Step 506290  [5.434 sec/step, loss=0.07687, avg_loss=0.07542]\n",
      "Step 506291  [5.435 sec/step, loss=0.07463, avg_loss=0.07541]\n",
      "Generated 32 batches of size 32 in 2.439 sec\n",
      "Step 506292  [5.426 sec/step, loss=0.07561, avg_loss=0.07540]\n",
      "Step 506293  [5.424 sec/step, loss=0.07591, avg_loss=0.07539]\n",
      "Step 506294  [5.428 sec/step, loss=0.07521, avg_loss=0.07540]\n",
      "Step 506295  [5.440 sec/step, loss=0.07803, avg_loss=0.07541]\n",
      "Step 506296  [5.433 sec/step, loss=0.07662, avg_loss=0.07542]\n",
      "Step 506297  [5.425 sec/step, loss=0.07549, avg_loss=0.07542]\n",
      "Step 506298  [5.425 sec/step, loss=0.07738, avg_loss=0.07542]\n",
      "Step 506299  [5.406 sec/step, loss=0.07640, avg_loss=0.07543]\n",
      "Step 506300  [5.375 sec/step, loss=0.07283, avg_loss=0.07540]\n",
      "Writing summary at step: 506300\n",
      "Step 506301  [5.378 sec/step, loss=0.07731, avg_loss=0.07540]\n",
      "Step 506302  [5.371 sec/step, loss=0.07451, avg_loss=0.07540]\n",
      "Step 506303  [5.423 sec/step, loss=0.06859, avg_loss=0.07534]\n",
      "Step 506304  [5.445 sec/step, loss=0.07473, avg_loss=0.07535]\n",
      "Step 506305  [5.450 sec/step, loss=0.07381, avg_loss=0.07537]\n",
      "Step 506306  [5.467 sec/step, loss=0.07679, avg_loss=0.07540]\n",
      "Step 506307  [5.443 sec/step, loss=0.06709, avg_loss=0.07530]\n",
      "Step 506308  [5.450 sec/step, loss=0.07420, avg_loss=0.07526]\n",
      "Step 506309  [5.469 sec/step, loss=0.07714, avg_loss=0.07527]\n",
      "Step 506310  [5.456 sec/step, loss=0.07763, avg_loss=0.07531]\n",
      "Step 506311  [5.471 sec/step, loss=0.07701, avg_loss=0.07532]\n",
      "Step 506312  [5.481 sec/step, loss=0.07649, avg_loss=0.07535]\n",
      "Step 506313  [5.482 sec/step, loss=0.07673, avg_loss=0.07536]\n",
      "Step 506314  [5.476 sec/step, loss=0.07606, avg_loss=0.07535]\n",
      "Step 506315  [5.476 sec/step, loss=0.07572, avg_loss=0.07533]\n",
      "Step 506316  [5.470 sec/step, loss=0.07547, avg_loss=0.07532]\n",
      "Step 506317  [5.471 sec/step, loss=0.07748, avg_loss=0.07534]\n",
      "Step 506318  [5.418 sec/step, loss=0.07686, avg_loss=0.07543]\n",
      "Step 506319  [5.431 sec/step, loss=0.07524, avg_loss=0.07542]\n",
      "Step 506320  [5.432 sec/step, loss=0.07798, avg_loss=0.07542]\n",
      "Step 506321  [5.431 sec/step, loss=0.07320, avg_loss=0.07539]\n",
      "Step 506322  [5.434 sec/step, loss=0.07694, avg_loss=0.07539]\n",
      "Generated 32 batches of size 32 in 2.410 sec\n",
      "Step 506323  [5.454 sec/step, loss=0.07562, avg_loss=0.07541]\n",
      "Step 506324  [5.449 sec/step, loss=0.07410, avg_loss=0.07538]\n",
      "Step 506325  [5.438 sec/step, loss=0.07286, avg_loss=0.07533]\n",
      "Step 506326  [5.417 sec/step, loss=0.07581, avg_loss=0.07532]\n",
      "Step 506327  [5.407 sec/step, loss=0.07596, avg_loss=0.07530]\n",
      "Step 506328  [5.409 sec/step, loss=0.07264, avg_loss=0.07527]\n",
      "Step 506329  [5.402 sec/step, loss=0.07702, avg_loss=0.07528]\n",
      "Step 506330  [5.374 sec/step, loss=0.07400, avg_loss=0.07526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 506331  [5.381 sec/step, loss=0.07760, avg_loss=0.07527]\n",
      "Step 506332  [5.381 sec/step, loss=0.07775, avg_loss=0.07531]\n",
      "Step 506333  [5.377 sec/step, loss=0.07153, avg_loss=0.07526]\n",
      "Step 506334  [5.341 sec/step, loss=0.06549, avg_loss=0.07514]\n",
      "Step 506335  [5.347 sec/step, loss=0.07524, avg_loss=0.07522]\n",
      "Step 506336  [5.371 sec/step, loss=0.07483, avg_loss=0.07521]\n",
      "Step 506337  [5.409 sec/step, loss=0.07700, avg_loss=0.07525]\n",
      "Step 506338  [5.404 sec/step, loss=0.07509, avg_loss=0.07524]\n",
      "Step 506339  [5.405 sec/step, loss=0.07386, avg_loss=0.07524]\n",
      "Step 506340  [5.398 sec/step, loss=0.07687, avg_loss=0.07525]\n",
      "Step 506341  [5.403 sec/step, loss=0.07686, avg_loss=0.07526]\n",
      "Step 506342  [5.392 sec/step, loss=0.07758, avg_loss=0.07527]\n",
      "Step 506343  [5.387 sec/step, loss=0.07698, avg_loss=0.07526]\n",
      "Step 506344  [5.371 sec/step, loss=0.07539, avg_loss=0.07526]\n",
      "Step 506345  [5.367 sec/step, loss=0.07368, avg_loss=0.07522]\n",
      "Step 506346  [5.368 sec/step, loss=0.07617, avg_loss=0.07523]\n",
      "Step 506347  [5.382 sec/step, loss=0.07809, avg_loss=0.07527]\n",
      "Step 506348  [5.370 sec/step, loss=0.07208, avg_loss=0.07523]\n",
      "Step 506349  [5.361 sec/step, loss=0.07292, avg_loss=0.07519]\n",
      "Step 506350  [5.374 sec/step, loss=0.07795, avg_loss=0.07521]\n",
      "Step 506351  [5.386 sec/step, loss=0.07521, avg_loss=0.07522]\n",
      "Step 506352  [5.330 sec/step, loss=0.07595, avg_loss=0.07531]\n",
      "Step 506353  [5.341 sec/step, loss=0.07596, avg_loss=0.07534]\n",
      "Step 506354  [5.331 sec/step, loss=0.07616, avg_loss=0.07534]\n",
      "Generated 32 batches of size 32 in 2.475 sec\n",
      "Step 506355  [5.337 sec/step, loss=0.07500, avg_loss=0.07533]\n",
      "Step 506356  [5.345 sec/step, loss=0.07775, avg_loss=0.07533]\n",
      "Step 506357  [5.344 sec/step, loss=0.07757, avg_loss=0.07535]\n",
      "Step 506358  [5.335 sec/step, loss=0.07680, avg_loss=0.07537]\n",
      "Step 506359  [5.322 sec/step, loss=0.07695, avg_loss=0.07538]\n",
      "Step 506360  [5.309 sec/step, loss=0.07699, avg_loss=0.07538]\n",
      "Step 506361  [5.293 sec/step, loss=0.07223, avg_loss=0.07534]\n",
      "Step 506362  [5.293 sec/step, loss=0.07488, avg_loss=0.07531]\n",
      "Step 506363  [5.354 sec/step, loss=0.06594, avg_loss=0.07521]\n",
      "Step 506364  [5.357 sec/step, loss=0.07674, avg_loss=0.07523]\n",
      "Step 506365  [5.352 sec/step, loss=0.07559, avg_loss=0.07523]\n",
      "Step 506366  [5.363 sec/step, loss=0.07797, avg_loss=0.07527]\n",
      "Step 506367  [5.376 sec/step, loss=0.07788, avg_loss=0.07527]\n",
      "Step 506368  [5.404 sec/step, loss=0.07532, avg_loss=0.07535]\n",
      "Step 506369  [5.381 sec/step, loss=0.07188, avg_loss=0.07532]\n",
      "Step 506370  [5.375 sec/step, loss=0.07782, avg_loss=0.07533]\n",
      "Step 506371  [5.368 sec/step, loss=0.07734, avg_loss=0.07532]\n",
      "Step 506372  [5.370 sec/step, loss=0.07571, avg_loss=0.07535]\n",
      "Step 506373  [5.369 sec/step, loss=0.07415, avg_loss=0.07535]\n",
      "Step 506374  [5.361 sec/step, loss=0.07134, avg_loss=0.07529]\n",
      "Step 506375  [5.383 sec/step, loss=0.07598, avg_loss=0.07530]\n",
      "Step 506376  [5.385 sec/step, loss=0.07725, avg_loss=0.07529]\n",
      "Step 506377  [5.372 sec/step, loss=0.07625, avg_loss=0.07530]\n",
      "Step 506378  [5.314 sec/step, loss=0.07509, avg_loss=0.07536]\n",
      "Step 506379  [5.349 sec/step, loss=0.06749, avg_loss=0.07526]\n",
      "Step 506380  [5.364 sec/step, loss=0.07678, avg_loss=0.07530]\n",
      "Step 506381  [5.362 sec/step, loss=0.07625, avg_loss=0.07532]\n",
      "Step 506382  [5.369 sec/step, loss=0.07543, avg_loss=0.07532]\n",
      "Step 506383  [5.394 sec/step, loss=0.07410, avg_loss=0.07529]\n",
      "Step 506384  [5.372 sec/step, loss=0.07297, avg_loss=0.07524]\n",
      "Step 506385  [5.384 sec/step, loss=0.07534, avg_loss=0.07523]\n",
      "Step 506386  [5.364 sec/step, loss=0.07509, avg_loss=0.07522]\n",
      "Generated 32 batches of size 32 in 2.386 sec\n",
      "Step 506387  [5.402 sec/step, loss=0.07642, avg_loss=0.07531]\n",
      "Step 506388  [5.394 sec/step, loss=0.07558, avg_loss=0.07530]\n",
      "Step 506389  [5.396 sec/step, loss=0.07671, avg_loss=0.07529]\n",
      "Step 506390  [5.404 sec/step, loss=0.07642, avg_loss=0.07528]\n",
      "Step 506391  [5.409 sec/step, loss=0.07403, avg_loss=0.07528]\n",
      "Step 506392  [5.389 sec/step, loss=0.06763, avg_loss=0.07520]\n",
      "Step 506393  [5.386 sec/step, loss=0.07455, avg_loss=0.07518]\n",
      "Step 506394  [5.392 sec/step, loss=0.07676, avg_loss=0.07520]\n",
      "Step 506395  [5.388 sec/step, loss=0.07625, avg_loss=0.07518]\n",
      "Step 506396  [5.387 sec/step, loss=0.07327, avg_loss=0.07515]\n",
      "Step 506397  [5.404 sec/step, loss=0.07790, avg_loss=0.07517]\n",
      "Step 506398  [5.391 sec/step, loss=0.07652, avg_loss=0.07516]\n",
      "Step 506399  [5.384 sec/step, loss=0.07584, avg_loss=0.07516]\n",
      "Step 506400  [5.404 sec/step, loss=0.07640, avg_loss=0.07519]\n",
      "Writing summary at step: 506400\n",
      "Step 506401  [5.402 sec/step, loss=0.07569, avg_loss=0.07518]\n",
      "Step 506402  [5.409 sec/step, loss=0.07753, avg_loss=0.07521]\n",
      "Step 506403  [5.356 sec/step, loss=0.07496, avg_loss=0.07527]\n",
      "Step 506404  [5.315 sec/step, loss=0.06829, avg_loss=0.07521]\n",
      "Step 506405  [5.314 sec/step, loss=0.07408, avg_loss=0.07521]\n",
      "Step 506406  [5.337 sec/step, loss=0.07433, avg_loss=0.07519]\n",
      "Step 506407  [5.349 sec/step, loss=0.07400, avg_loss=0.07525]\n",
      "Step 506408  [5.323 sec/step, loss=0.07515, avg_loss=0.07526]\n",
      "Step 506409  [5.326 sec/step, loss=0.07741, avg_loss=0.07527]\n",
      "Step 506410  [5.313 sec/step, loss=0.07631, avg_loss=0.07525]\n",
      "Step 506411  [5.291 sec/step, loss=0.07560, avg_loss=0.07524]\n",
      "Step 506412  [5.276 sec/step, loss=0.07323, avg_loss=0.07521]\n",
      "Step 506413  [5.283 sec/step, loss=0.07744, avg_loss=0.07521]\n",
      "Step 506414  [5.332 sec/step, loss=0.06696, avg_loss=0.07512]\n",
      "Step 506415  [5.332 sec/step, loss=0.07626, avg_loss=0.07513]\n",
      "Step 506416  [5.347 sec/step, loss=0.07641, avg_loss=0.07514]\n",
      "Step 506417  [5.350 sec/step, loss=0.07481, avg_loss=0.07511]\n",
      "Generated 32 batches of size 32 in 2.406 sec\n",
      "Step 506418  [5.359 sec/step, loss=0.07621, avg_loss=0.07511]\n",
      "Step 506419  [5.359 sec/step, loss=0.07800, avg_loss=0.07513]\n",
      "Step 506420  [5.348 sec/step, loss=0.07676, avg_loss=0.07512]\n",
      "Step 506421  [5.357 sec/step, loss=0.07670, avg_loss=0.07516]\n",
      "Step 506422  [5.349 sec/step, loss=0.07552, avg_loss=0.07514]\n",
      "Step 506423  [5.330 sec/step, loss=0.07366, avg_loss=0.07512]\n",
      "Step 506424  [5.323 sec/step, loss=0.07272, avg_loss=0.07511]\n",
      "Step 506425  [5.351 sec/step, loss=0.07718, avg_loss=0.07515]\n",
      "Step 506426  [5.367 sec/step, loss=0.07840, avg_loss=0.07518]\n",
      "Step 506427  [5.378 sec/step, loss=0.07760, avg_loss=0.07519]\n",
      "Step 506428  [5.386 sec/step, loss=0.07597, avg_loss=0.07523]\n",
      "Step 506429  [5.400 sec/step, loss=0.07523, avg_loss=0.07521]\n",
      "Step 506430  [5.407 sec/step, loss=0.07558, avg_loss=0.07522]\n",
      "Step 506431  [5.406 sec/step, loss=0.07817, avg_loss=0.07523]\n",
      "Step 506432  [5.392 sec/step, loss=0.07569, avg_loss=0.07521]\n",
      "Step 506433  [5.405 sec/step, loss=0.07695, avg_loss=0.07526]\n",
      "Step 506434  [5.424 sec/step, loss=0.07304, avg_loss=0.07534]\n",
      "Step 506435  [5.439 sec/step, loss=0.07592, avg_loss=0.07535]\n",
      "Step 506436  [5.427 sec/step, loss=0.07652, avg_loss=0.07536]\n",
      "Step 506437  [5.413 sec/step, loss=0.07632, avg_loss=0.07536]\n",
      "Step 506438  [5.404 sec/step, loss=0.07331, avg_loss=0.07534]\n",
      "Step 506439  [5.427 sec/step, loss=0.07767, avg_loss=0.07538]\n",
      "Step 506440  [5.427 sec/step, loss=0.07194, avg_loss=0.07533]\n",
      "Step 506441  [5.407 sec/step, loss=0.06634, avg_loss=0.07522]\n",
      "Step 506442  [5.408 sec/step, loss=0.07707, avg_loss=0.07522]\n",
      "Step 506443  [5.412 sec/step, loss=0.07664, avg_loss=0.07521]\n",
      "Step 506444  [5.403 sec/step, loss=0.07519, avg_loss=0.07521]\n",
      "Step 506445  [5.396 sec/step, loss=0.07302, avg_loss=0.07520]\n",
      "Step 506446  [5.396 sec/step, loss=0.07567, avg_loss=0.07520]\n",
      "Step 506447  [5.377 sec/step, loss=0.07587, avg_loss=0.07518]\n",
      "Step 506448  [5.391 sec/step, loss=0.07458, avg_loss=0.07520]\n",
      "Step 506449  [5.385 sec/step, loss=0.07512, avg_loss=0.07522]\n",
      "Generated 32 batches of size 32 in 2.426 sec\n",
      "Step 506450  [5.374 sec/step, loss=0.07753, avg_loss=0.07522]\n",
      "Step 506451  [5.377 sec/step, loss=0.07727, avg_loss=0.07524]\n",
      "Step 506452  [5.433 sec/step, loss=0.06782, avg_loss=0.07516]\n",
      "Step 506453  [5.418 sec/step, loss=0.07548, avg_loss=0.07515]\n",
      "Step 506454  [5.429 sec/step, loss=0.07708, avg_loss=0.07516]\n",
      "Step 506455  [5.443 sec/step, loss=0.07663, avg_loss=0.07518]\n",
      "Step 506456  [5.424 sec/step, loss=0.07667, avg_loss=0.07517]\n",
      "Step 506457  [5.444 sec/step, loss=0.07413, avg_loss=0.07514]\n",
      "Step 506458  [5.431 sec/step, loss=0.07697, avg_loss=0.07514]\n",
      "Step 506459  [5.475 sec/step, loss=0.06755, avg_loss=0.07504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 506460  [5.494 sec/step, loss=0.07673, avg_loss=0.07504]\n",
      "Step 506461  [5.506 sec/step, loss=0.07546, avg_loss=0.07507]\n",
      "Step 506462  [5.521 sec/step, loss=0.07447, avg_loss=0.07507]\n",
      "Step 506463  [5.476 sec/step, loss=0.07670, avg_loss=0.07518]\n",
      "Step 506464  [5.487 sec/step, loss=0.07819, avg_loss=0.07519]\n",
      "Step 506465  [5.489 sec/step, loss=0.07670, avg_loss=0.07520]\n",
      "Step 506466  [5.491 sec/step, loss=0.07813, avg_loss=0.07520]\n",
      "Step 506467  [5.496 sec/step, loss=0.07640, avg_loss=0.07519]\n",
      "Step 506468  [5.492 sec/step, loss=0.07517, avg_loss=0.07519]\n",
      "Step 506469  [5.512 sec/step, loss=0.07801, avg_loss=0.07525]\n",
      "Step 506470  [5.502 sec/step, loss=0.07370, avg_loss=0.07521]\n",
      "Step 506471  [5.496 sec/step, loss=0.07625, avg_loss=0.07520]\n",
      "Step 506472  [5.512 sec/step, loss=0.07591, avg_loss=0.07520]\n",
      "Step 506473  [5.524 sec/step, loss=0.07612, avg_loss=0.07522]\n",
      "Step 506474  [5.527 sec/step, loss=0.07675, avg_loss=0.07527]\n",
      "Step 506475  [5.522 sec/step, loss=0.07788, avg_loss=0.07529]\n",
      "Step 506476  [5.504 sec/step, loss=0.07586, avg_loss=0.07528]\n",
      "Step 506477  [5.494 sec/step, loss=0.07329, avg_loss=0.07525]\n",
      "Step 506478  [5.499 sec/step, loss=0.07514, avg_loss=0.07525]\n",
      "Step 506479  [5.442 sec/step, loss=0.07533, avg_loss=0.07533]\n",
      "Step 506480  [5.425 sec/step, loss=0.07578, avg_loss=0.07532]\n",
      "Step 506481  [5.415 sec/step, loss=0.07146, avg_loss=0.07527]\n",
      "Generated 32 batches of size 32 in 2.373 sec\n",
      "Step 506482  [5.425 sec/step, loss=0.07683, avg_loss=0.07528]\n",
      "Step 506483  [5.391 sec/step, loss=0.07512, avg_loss=0.07529]\n",
      "Step 506484  [5.407 sec/step, loss=0.07700, avg_loss=0.07533]\n",
      "Step 506485  [5.379 sec/step, loss=0.06839, avg_loss=0.07526]\n",
      "Step 506486  [5.369 sec/step, loss=0.07275, avg_loss=0.07524]\n",
      "Step 506487  [5.352 sec/step, loss=0.07685, avg_loss=0.07524]\n",
      "Step 506488  [5.356 sec/step, loss=0.07644, avg_loss=0.07525]\n",
      "Step 506489  [5.367 sec/step, loss=0.07707, avg_loss=0.07526]\n",
      "Step 506490  [5.374 sec/step, loss=0.07472, avg_loss=0.07524]\n",
      "Step 506491  [5.387 sec/step, loss=0.07746, avg_loss=0.07527]\n",
      "Step 506492  [5.399 sec/step, loss=0.07493, avg_loss=0.07535]\n",
      "Step 506493  [5.387 sec/step, loss=0.07289, avg_loss=0.07533]\n",
      "Step 506494  [5.390 sec/step, loss=0.07572, avg_loss=0.07532]\n",
      "Step 506495  [5.396 sec/step, loss=0.07768, avg_loss=0.07533]\n",
      "Step 506496  [5.385 sec/step, loss=0.07538, avg_loss=0.07535]\n",
      "Step 506497  [5.367 sec/step, loss=0.07519, avg_loss=0.07533]\n",
      "Step 506498  [5.360 sec/step, loss=0.07532, avg_loss=0.07532]\n",
      "Step 506499  [5.368 sec/step, loss=0.07586, avg_loss=0.07532]\n",
      "Step 506500  [5.364 sec/step, loss=0.07682, avg_loss=0.07532]\n",
      "Writing summary at step: 506500\n",
      "Step 506501  [5.365 sec/step, loss=0.07780, avg_loss=0.07534]\n",
      "Step 506502  [5.360 sec/step, loss=0.07704, avg_loss=0.07534]\n",
      "Step 506503  [5.390 sec/step, loss=0.07456, avg_loss=0.07533]\n",
      "Step 506504  [5.407 sec/step, loss=0.07397, avg_loss=0.07539]\n",
      "Step 506505  [5.400 sec/step, loss=0.07489, avg_loss=0.07540]\n",
      "Step 506506  [5.372 sec/step, loss=0.07632, avg_loss=0.07542]\n",
      "Step 506507  [5.377 sec/step, loss=0.07676, avg_loss=0.07544]\n",
      "Step 506508  [5.370 sec/step, loss=0.07273, avg_loss=0.07542]\n",
      "Step 506509  [5.369 sec/step, loss=0.07577, avg_loss=0.07540]\n",
      "Step 506510  [5.371 sec/step, loss=0.07648, avg_loss=0.07541]\n",
      "Step 506511  [5.387 sec/step, loss=0.07729, avg_loss=0.07542]\n",
      "Step 506512  [5.410 sec/step, loss=0.07804, avg_loss=0.07547]\n",
      "Generated 32 batches of size 32 in 2.701 sec\n",
      "Step 506513  [5.453 sec/step, loss=0.06747, avg_loss=0.07537]\n",
      "Step 506514  [5.412 sec/step, loss=0.07692, avg_loss=0.07547]\n",
      "Step 506515  [5.405 sec/step, loss=0.07474, avg_loss=0.07546]\n",
      "Step 506516  [5.408 sec/step, loss=0.07631, avg_loss=0.07545]\n",
      "Step 506517  [5.396 sec/step, loss=0.07568, avg_loss=0.07546]\n",
      "Step 506518  [5.390 sec/step, loss=0.07354, avg_loss=0.07544]\n",
      "Step 506519  [5.363 sec/step, loss=0.06591, avg_loss=0.07532]\n",
      "Step 506520  [5.387 sec/step, loss=0.07487, avg_loss=0.07530]\n",
      "Step 506521  [5.384 sec/step, loss=0.07671, avg_loss=0.07530]\n",
      "Step 506522  [5.388 sec/step, loss=0.07236, avg_loss=0.07527]\n",
      "Step 506523  [5.389 sec/step, loss=0.07700, avg_loss=0.07530]\n",
      "Step 506524  [5.393 sec/step, loss=0.07579, avg_loss=0.07533]\n",
      "Step 506525  [5.388 sec/step, loss=0.07597, avg_loss=0.07532]\n",
      "Step 506526  [5.392 sec/step, loss=0.07782, avg_loss=0.07531]\n",
      "Step 506527  [5.371 sec/step, loss=0.07538, avg_loss=0.07529]\n",
      "Step 506528  [5.388 sec/step, loss=0.07440, avg_loss=0.07527]\n",
      "Step 506529  [5.371 sec/step, loss=0.07546, avg_loss=0.07528]\n",
      "Step 506530  [5.364 sec/step, loss=0.07375, avg_loss=0.07526]\n",
      "Step 506531  [5.358 sec/step, loss=0.07666, avg_loss=0.07524]\n",
      "Step 506532  [5.355 sec/step, loss=0.07391, avg_loss=0.07522]\n",
      "Step 506533  [5.358 sec/step, loss=0.07545, avg_loss=0.07521]\n",
      "Step 506534  [5.352 sec/step, loss=0.07683, avg_loss=0.07525]\n",
      "Step 506535  [5.356 sec/step, loss=0.07680, avg_loss=0.07526]\n",
      "Step 506536  [5.353 sec/step, loss=0.07576, avg_loss=0.07525]\n",
      "Step 506537  [5.351 sec/step, loss=0.07787, avg_loss=0.07526]\n",
      "Step 506538  [5.379 sec/step, loss=0.07706, avg_loss=0.07530]\n",
      "Step 506539  [5.357 sec/step, loss=0.07544, avg_loss=0.07528]\n",
      "Step 506540  [5.360 sec/step, loss=0.07680, avg_loss=0.07533]\n",
      "Step 506541  [5.392 sec/step, loss=0.07674, avg_loss=0.07543]\n",
      "Step 506542  [5.394 sec/step, loss=0.07678, avg_loss=0.07543]\n",
      "Step 506543  [5.401 sec/step, loss=0.07649, avg_loss=0.07543]\n",
      "Step 506544  [5.411 sec/step, loss=0.07853, avg_loss=0.07546]\n",
      "Generated 32 batches of size 32 in 2.342 sec\n",
      "Step 506545  [5.427 sec/step, loss=0.07683, avg_loss=0.07550]\n",
      "Step 506546  [5.412 sec/step, loss=0.06722, avg_loss=0.07541]\n",
      "Step 506547  [5.433 sec/step, loss=0.07801, avg_loss=0.07544]\n",
      "Step 506548  [5.424 sec/step, loss=0.07680, avg_loss=0.07546]\n",
      "Step 506549  [5.434 sec/step, loss=0.07720, avg_loss=0.07548]\n",
      "Step 506550  [5.483 sec/step, loss=0.06733, avg_loss=0.07538]\n",
      "Step 506551  [5.477 sec/step, loss=0.07470, avg_loss=0.07535]\n",
      "Step 506552  [5.436 sec/step, loss=0.07690, avg_loss=0.07544]\n",
      "Step 506553  [5.432 sec/step, loss=0.07256, avg_loss=0.07541]\n",
      "Step 506554  [5.413 sec/step, loss=0.07524, avg_loss=0.07539]\n",
      "Step 506555  [5.400 sec/step, loss=0.07686, avg_loss=0.07540]\n",
      "Step 506556  [5.429 sec/step, loss=0.07489, avg_loss=0.07538]\n",
      "Step 506557  [5.389 sec/step, loss=0.07327, avg_loss=0.07537]\n",
      "Step 506558  [5.403 sec/step, loss=0.07476, avg_loss=0.07535]\n",
      "Step 506559  [5.364 sec/step, loss=0.07693, avg_loss=0.07544]\n",
      "Step 506560  [5.362 sec/step, loss=0.07791, avg_loss=0.07545]\n",
      "Step 506561  [5.374 sec/step, loss=0.07768, avg_loss=0.07548]\n",
      "Step 506562  [5.341 sec/step, loss=0.07582, avg_loss=0.07549]\n",
      "Step 506563  [5.344 sec/step, loss=0.07814, avg_loss=0.07550]\n",
      "Step 506564  [5.348 sec/step, loss=0.07571, avg_loss=0.07548]\n",
      "Step 506565  [5.346 sec/step, loss=0.07513, avg_loss=0.07546]\n",
      "Step 506566  [5.336 sec/step, loss=0.07713, avg_loss=0.07545]\n",
      "Step 506567  [5.325 sec/step, loss=0.07398, avg_loss=0.07543]\n",
      "Step 506568  [5.325 sec/step, loss=0.07779, avg_loss=0.07546]\n",
      "Step 506569  [5.365 sec/step, loss=0.06819, avg_loss=0.07536]\n",
      "Step 506570  [5.361 sec/step, loss=0.07554, avg_loss=0.07538]\n",
      "Step 506571  [5.365 sec/step, loss=0.07642, avg_loss=0.07538]\n",
      "Step 506572  [5.366 sec/step, loss=0.07776, avg_loss=0.07540]\n",
      "Step 506573  [5.368 sec/step, loss=0.07536, avg_loss=0.07539]\n",
      "Step 506574  [5.368 sec/step, loss=0.07661, avg_loss=0.07539]\n",
      "Step 506575  [5.359 sec/step, loss=0.07514, avg_loss=0.07536]\n",
      "Step 506576  [5.358 sec/step, loss=0.07210, avg_loss=0.07532]\n",
      "Generated 32 batches of size 32 in 2.503 sec\n",
      "Step 506577  [5.379 sec/step, loss=0.07429, avg_loss=0.07533]\n",
      "Step 506578  [5.381 sec/step, loss=0.07678, avg_loss=0.07535]\n",
      "Step 506579  [5.378 sec/step, loss=0.07546, avg_loss=0.07535]\n",
      "Step 506580  [5.365 sec/step, loss=0.06762, avg_loss=0.07527]\n",
      "Step 506581  [5.378 sec/step, loss=0.07715, avg_loss=0.07533]\n",
      "Step 506582  [5.368 sec/step, loss=0.07352, avg_loss=0.07529]\n",
      "Step 506583  [5.384 sec/step, loss=0.07397, avg_loss=0.07528]\n",
      "Step 506584  [5.395 sec/step, loss=0.07790, avg_loss=0.07529]\n",
      "Step 506585  [5.408 sec/step, loss=0.07515, avg_loss=0.07536]\n",
      "Step 506586  [5.420 sec/step, loss=0.07539, avg_loss=0.07538]\n",
      "Step 506587  [5.420 sec/step, loss=0.07624, avg_loss=0.07538]\n",
      "Step 506588  [5.430 sec/step, loss=0.07751, avg_loss=0.07539]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 506589  [5.412 sec/step, loss=0.07464, avg_loss=0.07536]\n",
      "Step 506590  [5.397 sec/step, loss=0.07673, avg_loss=0.07538]\n",
      "Step 506591  [5.384 sec/step, loss=0.07666, avg_loss=0.07538]\n",
      "Step 506592  [5.398 sec/step, loss=0.07714, avg_loss=0.07540]\n",
      "Step 506593  [5.412 sec/step, loss=0.07627, avg_loss=0.07543]\n",
      "Step 506594  [5.407 sec/step, loss=0.07585, avg_loss=0.07543]\n",
      "Step 506595  [5.445 sec/step, loss=0.06766, avg_loss=0.07533]\n",
      "Step 506596  [5.468 sec/step, loss=0.07694, avg_loss=0.07535]\n",
      "Step 506597  [5.463 sec/step, loss=0.07372, avg_loss=0.07533]\n",
      "Step 506598  [5.480 sec/step, loss=0.07630, avg_loss=0.07534]\n",
      "Step 506599  [5.452 sec/step, loss=0.06643, avg_loss=0.07525]\n",
      "Step 506600  [5.443 sec/step, loss=0.07548, avg_loss=0.07524]\n",
      "Writing summary at step: 506600\n",
      "Step 506601  [5.437 sec/step, loss=0.07816, avg_loss=0.07524]\n",
      "Step 506602  [5.453 sec/step, loss=0.07813, avg_loss=0.07525]\n",
      "Step 506603  [5.418 sec/step, loss=0.07515, avg_loss=0.07526]\n",
      "Step 506604  [5.407 sec/step, loss=0.07410, avg_loss=0.07526]\n",
      "Step 506605  [5.415 sec/step, loss=0.07705, avg_loss=0.07528]\n",
      "Step 506606  [5.412 sec/step, loss=0.07543, avg_loss=0.07527]\n",
      "Step 506607  [5.402 sec/step, loss=0.07130, avg_loss=0.07522]\n",
      "Generated 32 batches of size 32 in 2.373 sec\n",
      "Step 506608  [5.422 sec/step, loss=0.07729, avg_loss=0.07526]\n",
      "Step 506609  [5.436 sec/step, loss=0.07483, avg_loss=0.07525]\n",
      "Step 506610  [5.431 sec/step, loss=0.07424, avg_loss=0.07523]\n",
      "Step 506611  [5.424 sec/step, loss=0.07336, avg_loss=0.07519]\n",
      "Step 506612  [5.435 sec/step, loss=0.07509, avg_loss=0.07516]\n",
      "Step 506613  [5.395 sec/step, loss=0.07764, avg_loss=0.07526]\n",
      "Step 506614  [5.386 sec/step, loss=0.07626, avg_loss=0.07526]\n",
      "Step 506615  [5.391 sec/step, loss=0.07409, avg_loss=0.07525]\n",
      "Step 506616  [5.385 sec/step, loss=0.07693, avg_loss=0.07526]\n",
      "Step 506617  [5.374 sec/step, loss=0.07255, avg_loss=0.07522]\n",
      "Step 506618  [5.358 sec/step, loss=0.06809, avg_loss=0.07517]\n",
      "Step 506619  [5.378 sec/step, loss=0.07590, avg_loss=0.07527]\n",
      "Step 506620  [5.353 sec/step, loss=0.07465, avg_loss=0.07527]\n",
      "Step 506621  [5.362 sec/step, loss=0.07717, avg_loss=0.07527]\n",
      "Step 506622  [5.351 sec/step, loss=0.07259, avg_loss=0.07527]\n",
      "Step 506623  [5.353 sec/step, loss=0.07570, avg_loss=0.07526]\n",
      "Step 506624  [5.353 sec/step, loss=0.07449, avg_loss=0.07525]\n",
      "Step 506625  [5.365 sec/step, loss=0.07439, avg_loss=0.07523]\n",
      "Step 506626  [5.342 sec/step, loss=0.07541, avg_loss=0.07521]\n",
      "Step 506627  [5.363 sec/step, loss=0.07733, avg_loss=0.07523]\n",
      "Step 506628  [5.338 sec/step, loss=0.07693, avg_loss=0.07525]\n",
      "Step 506629  [5.353 sec/step, loss=0.07630, avg_loss=0.07526]\n",
      "Step 506630  [5.386 sec/step, loss=0.07630, avg_loss=0.07529]\n",
      "Step 506631  [5.394 sec/step, loss=0.07783, avg_loss=0.07530]\n",
      "Step 506632  [5.405 sec/step, loss=0.07621, avg_loss=0.07532]\n",
      "Step 506633  [5.392 sec/step, loss=0.07567, avg_loss=0.07532]\n",
      "Step 506634  [5.396 sec/step, loss=0.07374, avg_loss=0.07529]\n",
      "Step 506635  [5.391 sec/step, loss=0.07544, avg_loss=0.07528]\n",
      "Step 506636  [5.401 sec/step, loss=0.07742, avg_loss=0.07530]\n",
      "Step 506637  [5.389 sec/step, loss=0.07592, avg_loss=0.07528]\n",
      "Step 506638  [5.370 sec/step, loss=0.07703, avg_loss=0.07528]\n",
      "Step 506639  [5.388 sec/step, loss=0.07809, avg_loss=0.07530]\n",
      "Generated 32 batches of size 32 in 2.451 sec\n",
      "Step 506640  [5.393 sec/step, loss=0.07541, avg_loss=0.07529]\n",
      "Step 506641  [5.380 sec/step, loss=0.07665, avg_loss=0.07529]\n",
      "Step 506642  [5.387 sec/step, loss=0.07794, avg_loss=0.07530]\n",
      "Step 506643  [5.373 sec/step, loss=0.07573, avg_loss=0.07529]\n",
      "Step 506644  [5.364 sec/step, loss=0.07642, avg_loss=0.07527]\n",
      "Step 506645  [5.366 sec/step, loss=0.07786, avg_loss=0.07528]\n",
      "Step 506646  [5.388 sec/step, loss=0.07697, avg_loss=0.07538]\n",
      "Step 506647  [5.427 sec/step, loss=0.06745, avg_loss=0.07527]\n",
      "Step 506648  [5.421 sec/step, loss=0.07399, avg_loss=0.07524]\n",
      "Step 506649  [5.416 sec/step, loss=0.07513, avg_loss=0.07522]\n",
      "Step 506650  [5.364 sec/step, loss=0.07605, avg_loss=0.07531]\n",
      "Step 506651  [5.375 sec/step, loss=0.07694, avg_loss=0.07533]\n",
      "Step 506652  [5.416 sec/step, loss=0.06669, avg_loss=0.07523]\n",
      "Step 506653  [5.426 sec/step, loss=0.07722, avg_loss=0.07528]\n",
      "Step 506654  [5.426 sec/step, loss=0.07467, avg_loss=0.07527]\n",
      "Step 506655  [5.428 sec/step, loss=0.07758, avg_loss=0.07528]\n",
      "Step 506656  [5.406 sec/step, loss=0.07590, avg_loss=0.07529]\n",
      "Step 506657  [5.445 sec/step, loss=0.07603, avg_loss=0.07532]\n",
      "Step 506658  [5.414 sec/step, loss=0.07186, avg_loss=0.07529]\n",
      "Step 506659  [5.422 sec/step, loss=0.07657, avg_loss=0.07528]\n",
      "Step 506660  [5.420 sec/step, loss=0.07505, avg_loss=0.07526]\n",
      "Step 506661  [5.424 sec/step, loss=0.07736, avg_loss=0.07525]\n",
      "Step 506662  [5.426 sec/step, loss=0.07400, avg_loss=0.07523]\n",
      "Step 506663  [5.402 sec/step, loss=0.06770, avg_loss=0.07513]\n",
      "Step 506664  [5.398 sec/step, loss=0.07616, avg_loss=0.07513]\n",
      "Step 506665  [5.402 sec/step, loss=0.07651, avg_loss=0.07515]\n",
      "Step 506666  [5.391 sec/step, loss=0.07519, avg_loss=0.07513]\n",
      "Step 506667  [5.397 sec/step, loss=0.07738, avg_loss=0.07516]\n",
      "Step 506668  [5.389 sec/step, loss=0.07337, avg_loss=0.07512]\n",
      "Step 506669  [5.344 sec/step, loss=0.07683, avg_loss=0.07521]\n",
      "Step 506670  [5.334 sec/step, loss=0.07327, avg_loss=0.07518]\n",
      "Step 506671  [5.325 sec/step, loss=0.07646, avg_loss=0.07518]\n",
      "Generated 32 batches of size 32 in 2.543 sec\n",
      "Step 506672  [5.317 sec/step, loss=0.07608, avg_loss=0.07517]\n",
      "Step 506673  [5.319 sec/step, loss=0.07754, avg_loss=0.07519]\n",
      "Step 506674  [5.320 sec/step, loss=0.07621, avg_loss=0.07518]\n",
      "Step 506675  [5.322 sec/step, loss=0.07705, avg_loss=0.07520]\n",
      "Step 506676  [5.328 sec/step, loss=0.07509, avg_loss=0.07523]\n",
      "Step 506677  [5.329 sec/step, loss=0.07774, avg_loss=0.07527]\n",
      "Step 506678  [5.328 sec/step, loss=0.07701, avg_loss=0.07527]\n",
      "Step 506679  [5.342 sec/step, loss=0.07604, avg_loss=0.07528]\n",
      "Step 506680  [5.369 sec/step, loss=0.07825, avg_loss=0.07538]\n",
      "Step 506681  [5.370 sec/step, loss=0.07689, avg_loss=0.07538]\n",
      "Step 506682  [5.381 sec/step, loss=0.07808, avg_loss=0.07542]\n",
      "Step 506683  [5.362 sec/step, loss=0.07344, avg_loss=0.07542]\n",
      "Step 506684  [5.351 sec/step, loss=0.07614, avg_loss=0.07540]\n",
      "Step 506685  [5.354 sec/step, loss=0.07696, avg_loss=0.07542]\n",
      "Step 506686  [5.339 sec/step, loss=0.07544, avg_loss=0.07542]\n",
      "Step 506687  [5.333 sec/step, loss=0.07550, avg_loss=0.07541]\n",
      "Step 506688  [5.375 sec/step, loss=0.06791, avg_loss=0.07532]\n",
      "Step 506689  [5.390 sec/step, loss=0.07780, avg_loss=0.07535]\n",
      "Step 506690  [5.397 sec/step, loss=0.07570, avg_loss=0.07534]\n",
      "Step 506691  [5.394 sec/step, loss=0.07644, avg_loss=0.07534]\n",
      "Step 506692  [5.411 sec/step, loss=0.07471, avg_loss=0.07531]\n",
      "Step 506693  [5.409 sec/step, loss=0.07672, avg_loss=0.07532]\n",
      "Step 506694  [5.415 sec/step, loss=0.07635, avg_loss=0.07532]\n",
      "Step 506695  [5.366 sec/step, loss=0.07636, avg_loss=0.07541]\n",
      "Step 506696  [5.361 sec/step, loss=0.07486, avg_loss=0.07539]\n",
      "Step 506697  [5.360 sec/step, loss=0.07352, avg_loss=0.07539]\n",
      "Step 506698  [5.346 sec/step, loss=0.07519, avg_loss=0.07537]\n",
      "Step 506699  [5.354 sec/step, loss=0.07553, avg_loss=0.07547]\n",
      "Step 506700  [5.358 sec/step, loss=0.07352, avg_loss=0.07545]\n",
      "Writing summary at step: 506700\n",
      "Step 506701  [5.358 sec/step, loss=0.07649, avg_loss=0.07543]\n",
      "Step 506702  [5.344 sec/step, loss=0.07685, avg_loss=0.07542]\n",
      "Generated 32 batches of size 32 in 2.535 sec\n",
      "Step 506703  [5.353 sec/step, loss=0.07219, avg_loss=0.07539]\n",
      "Step 506704  [5.377 sec/step, loss=0.07722, avg_loss=0.07542]\n",
      "Step 506705  [5.356 sec/step, loss=0.06748, avg_loss=0.07532]\n",
      "Step 506706  [5.378 sec/step, loss=0.07763, avg_loss=0.07534]\n",
      "Step 506707  [5.399 sec/step, loss=0.07547, avg_loss=0.07539]\n",
      "Step 506708  [5.392 sec/step, loss=0.07538, avg_loss=0.07537]\n",
      "Step 506709  [5.377 sec/step, loss=0.07759, avg_loss=0.07539]\n",
      "Step 506710  [5.387 sec/step, loss=0.07639, avg_loss=0.07542]\n",
      "Step 506711  [5.402 sec/step, loss=0.07742, avg_loss=0.07546]\n",
      "Step 506712  [5.379 sec/step, loss=0.07402, avg_loss=0.07545]\n",
      "Step 506713  [5.392 sec/step, loss=0.07532, avg_loss=0.07542]\n",
      "Step 506714  [5.400 sec/step, loss=0.07675, avg_loss=0.07543]\n",
      "Step 506715  [5.399 sec/step, loss=0.07704, avg_loss=0.07546]\n",
      "Step 506716  [5.401 sec/step, loss=0.07625, avg_loss=0.07545]\n",
      "Step 506717  [5.420 sec/step, loss=0.07777, avg_loss=0.07550]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 506718  [5.429 sec/step, loss=0.07523, avg_loss=0.07557]\n",
      "Step 506719  [5.430 sec/step, loss=0.07664, avg_loss=0.07558]\n",
      "Step 506720  [5.445 sec/step, loss=0.07776, avg_loss=0.07561]\n",
      "Step 506721  [5.437 sec/step, loss=0.07327, avg_loss=0.07557]\n",
      "Step 506722  [5.437 sec/step, loss=0.07298, avg_loss=0.07558]\n",
      "Step 506723  [5.432 sec/step, loss=0.07470, avg_loss=0.07557]\n",
      "Step 506724  [5.452 sec/step, loss=0.07570, avg_loss=0.07558]\n",
      "Step 506725  [5.424 sec/step, loss=0.07543, avg_loss=0.07559]\n",
      "Step 506726  [5.433 sec/step, loss=0.07608, avg_loss=0.07560]\n",
      "Step 506727  [5.406 sec/step, loss=0.06642, avg_loss=0.07549]\n",
      "Step 506728  [5.425 sec/step, loss=0.07449, avg_loss=0.07546]\n",
      "Step 506729  [5.422 sec/step, loss=0.07752, avg_loss=0.07548]\n",
      "Step 506730  [5.398 sec/step, loss=0.07678, avg_loss=0.07548]\n",
      "Step 506731  [5.393 sec/step, loss=0.07599, avg_loss=0.07546]\n",
      "Step 506732  [5.381 sec/step, loss=0.07566, avg_loss=0.07546]\n",
      "Step 506733  [5.379 sec/step, loss=0.07523, avg_loss=0.07545]\n",
      "Step 506734  [5.392 sec/step, loss=0.07769, avg_loss=0.07549]\n",
      "Generated 32 batches of size 32 in 2.412 sec\n",
      "Step 506735  [5.391 sec/step, loss=0.07479, avg_loss=0.07548]\n",
      "Step 506736  [5.389 sec/step, loss=0.07687, avg_loss=0.07548]\n",
      "Step 506737  [5.388 sec/step, loss=0.07653, avg_loss=0.07549]\n",
      "Step 506738  [5.403 sec/step, loss=0.07624, avg_loss=0.07548]\n",
      "Step 506739  [5.394 sec/step, loss=0.07707, avg_loss=0.07547]\n",
      "Step 506740  [5.403 sec/step, loss=0.07786, avg_loss=0.07549]\n",
      "Step 506741  [5.400 sec/step, loss=0.07472, avg_loss=0.07547]\n",
      "Step 506742  [5.378 sec/step, loss=0.07287, avg_loss=0.07542]\n",
      "Step 506743  [5.433 sec/step, loss=0.06804, avg_loss=0.07534]\n",
      "Step 506744  [5.441 sec/step, loss=0.07762, avg_loss=0.07536]\n",
      "Step 506745  [5.440 sec/step, loss=0.07504, avg_loss=0.07533]\n",
      "Step 506746  [5.443 sec/step, loss=0.07786, avg_loss=0.07534]\n",
      "Step 506747  [5.382 sec/step, loss=0.07155, avg_loss=0.07538]\n",
      "Step 506748  [5.387 sec/step, loss=0.07335, avg_loss=0.07537]\n",
      "Step 506749  [5.380 sec/step, loss=0.07505, avg_loss=0.07537]\n",
      "Step 506750  [5.367 sec/step, loss=0.07250, avg_loss=0.07534]\n",
      "Step 506751  [5.361 sec/step, loss=0.07590, avg_loss=0.07533]\n",
      "Step 506752  [5.315 sec/step, loss=0.07603, avg_loss=0.07542]\n",
      "Step 506753  [5.312 sec/step, loss=0.07620, avg_loss=0.07541]\n",
      "Step 506754  [5.330 sec/step, loss=0.07459, avg_loss=0.07541]\n",
      "Step 506755  [5.321 sec/step, loss=0.07401, avg_loss=0.07537]\n",
      "Step 506756  [5.342 sec/step, loss=0.07591, avg_loss=0.07537]\n",
      "Step 506757  [5.319 sec/step, loss=0.07729, avg_loss=0.07538]\n",
      "Step 506758  [5.323 sec/step, loss=0.07517, avg_loss=0.07542]\n",
      "Step 506759  [5.307 sec/step, loss=0.07687, avg_loss=0.07542]\n",
      "Step 506760  [5.303 sec/step, loss=0.07524, avg_loss=0.07542]\n",
      "Step 506761  [5.284 sec/step, loss=0.07393, avg_loss=0.07539]\n",
      "Step 506762  [5.272 sec/step, loss=0.06649, avg_loss=0.07531]\n",
      "Step 506763  [5.286 sec/step, loss=0.07564, avg_loss=0.07539]\n",
      "Step 506764  [5.291 sec/step, loss=0.07663, avg_loss=0.07540]\n",
      "Step 506765  [5.299 sec/step, loss=0.07794, avg_loss=0.07541]\n",
      "Step 506766  [5.323 sec/step, loss=0.07635, avg_loss=0.07542]\n",
      "Generated 32 batches of size 32 in 2.519 sec\n",
      "Step 506767  [5.361 sec/step, loss=0.07015, avg_loss=0.07535]\n",
      "Step 506768  [5.364 sec/step, loss=0.07689, avg_loss=0.07539]\n",
      "Step 506769  [5.371 sec/step, loss=0.07736, avg_loss=0.07539]\n",
      "Step 506770  [5.374 sec/step, loss=0.07580, avg_loss=0.07542]\n",
      "Step 506771  [5.382 sec/step, loss=0.07680, avg_loss=0.07542]\n",
      "Step 506772  [5.402 sec/step, loss=0.07661, avg_loss=0.07543]\n",
      "Step 506773  [5.386 sec/step, loss=0.07628, avg_loss=0.07541]\n",
      "Step 506774  [5.387 sec/step, loss=0.07570, avg_loss=0.07541]\n",
      "Step 506775  [5.381 sec/step, loss=0.07500, avg_loss=0.07539]\n",
      "Step 506776  [5.399 sec/step, loss=0.07788, avg_loss=0.07541]\n",
      "Step 506777  [5.404 sec/step, loss=0.07676, avg_loss=0.07541]\n",
      "Step 506778  [5.405 sec/step, loss=0.07434, avg_loss=0.07538]\n",
      "Step 506779  [5.400 sec/step, loss=0.07739, avg_loss=0.07539]\n",
      "Step 506780  [5.416 sec/step, loss=0.07395, avg_loss=0.07535]\n",
      "Step 506781  [5.410 sec/step, loss=0.07657, avg_loss=0.07535]\n",
      "Step 506782  [5.422 sec/step, loss=0.07497, avg_loss=0.07531]\n",
      "Step 506783  [5.437 sec/step, loss=0.07563, avg_loss=0.07534]\n",
      "Step 506784  [5.444 sec/step, loss=0.07766, avg_loss=0.07535]\n",
      "Step 506785  [5.443 sec/step, loss=0.07645, avg_loss=0.07535]\n",
      "Step 506786  [5.452 sec/step, loss=0.07566, avg_loss=0.07535]\n",
      "Step 506787  [5.459 sec/step, loss=0.07685, avg_loss=0.07536]\n",
      "Step 506788  [5.411 sec/step, loss=0.07610, avg_loss=0.07544]\n",
      "Step 506789  [5.396 sec/step, loss=0.07406, avg_loss=0.07541]\n",
      "Step 506790  [5.388 sec/step, loss=0.07666, avg_loss=0.07542]\n",
      "Step 506791  [5.380 sec/step, loss=0.07278, avg_loss=0.07538]\n",
      "Step 506792  [5.357 sec/step, loss=0.07565, avg_loss=0.07539]\n",
      "Step 506793  [5.365 sec/step, loss=0.07699, avg_loss=0.07539]\n",
      "Step 506794  [5.367 sec/step, loss=0.07744, avg_loss=0.07540]\n",
      "Step 506795  [5.355 sec/step, loss=0.07527, avg_loss=0.07539]\n",
      "Step 506796  [5.331 sec/step, loss=0.07343, avg_loss=0.07538]\n",
      "Step 506797  [5.341 sec/step, loss=0.07171, avg_loss=0.07536]\n",
      "Step 506798  [5.398 sec/step, loss=0.06814, avg_loss=0.07529]\n",
      "Generated 32 batches of size 32 in 2.364 sec\n",
      "Step 506799  [5.424 sec/step, loss=0.07566, avg_loss=0.07529]\n",
      "Step 506800  [5.412 sec/step, loss=0.06743, avg_loss=0.07523]\n",
      "Writing summary at step: 506800\n",
      "Step 506801  [5.414 sec/step, loss=0.07553, avg_loss=0.07522]\n",
      "Step 506802  [5.429 sec/step, loss=0.07774, avg_loss=0.07523]\n",
      "Step 506803  [5.424 sec/step, loss=0.07525, avg_loss=0.07526]\n",
      "Step 506804  [5.403 sec/step, loss=0.07554, avg_loss=0.07524]\n",
      "Step 506805  [5.429 sec/step, loss=0.07758, avg_loss=0.07534]\n",
      "Step 506806  [5.412 sec/step, loss=0.07601, avg_loss=0.07533]\n",
      "Step 506807  [5.405 sec/step, loss=0.07532, avg_loss=0.07533]\n",
      "Step 506808  [5.392 sec/step, loss=0.07404, avg_loss=0.07531]\n",
      "Step 506809  [5.380 sec/step, loss=0.07558, avg_loss=0.07529]\n",
      "Step 506810  [5.400 sec/step, loss=0.07452, avg_loss=0.07527]\n",
      "Step 506811  [5.396 sec/step, loss=0.07778, avg_loss=0.07528]\n",
      "Step 506812  [5.392 sec/step, loss=0.07225, avg_loss=0.07526]\n",
      "Step 506813  [5.383 sec/step, loss=0.07662, avg_loss=0.07527]\n",
      "Step 506814  [5.364 sec/step, loss=0.07355, avg_loss=0.07524]\n",
      "Step 506815  [5.360 sec/step, loss=0.07393, avg_loss=0.07521]\n",
      "Step 506816  [5.355 sec/step, loss=0.07569, avg_loss=0.07520]\n",
      "Step 506817  [5.358 sec/step, loss=0.07552, avg_loss=0.07518]\n",
      "Step 506818  [5.360 sec/step, loss=0.07502, avg_loss=0.07518]\n",
      "Step 506819  [5.364 sec/step, loss=0.07635, avg_loss=0.07518]\n",
      "Step 506820  [5.346 sec/step, loss=0.07547, avg_loss=0.07515]\n",
      "Step 506821  [5.345 sec/step, loss=0.07654, avg_loss=0.07519]\n",
      "Step 506822  [5.372 sec/step, loss=0.07735, avg_loss=0.07523]\n",
      "Step 506823  [5.387 sec/step, loss=0.07523, avg_loss=0.07524]\n",
      "Step 506824  [5.375 sec/step, loss=0.07655, avg_loss=0.07524]\n",
      "Step 506825  [5.378 sec/step, loss=0.07430, avg_loss=0.07523]\n",
      "Step 506826  [5.376 sec/step, loss=0.07638, avg_loss=0.07524]\n",
      "Step 506827  [5.388 sec/step, loss=0.07679, avg_loss=0.07534]\n",
      "Step 506828  [5.375 sec/step, loss=0.07751, avg_loss=0.07537]\n",
      "Step 506829  [5.372 sec/step, loss=0.07678, avg_loss=0.07536]\n",
      "Generated 32 batches of size 32 in 2.291 sec\n",
      "Step 506830  [5.380 sec/step, loss=0.07686, avg_loss=0.07536]\n",
      "Step 506831  [5.390 sec/step, loss=0.07735, avg_loss=0.07538]\n",
      "Step 506832  [5.398 sec/step, loss=0.07655, avg_loss=0.07539]\n",
      "Step 506833  [5.417 sec/step, loss=0.07782, avg_loss=0.07541]\n",
      "Step 506834  [5.389 sec/step, loss=0.06797, avg_loss=0.07531]\n",
      "Step 506835  [5.377 sec/step, loss=0.07577, avg_loss=0.07532]\n",
      "Step 506836  [5.378 sec/step, loss=0.07596, avg_loss=0.07531]\n",
      "Step 506837  [5.432 sec/step, loss=0.06650, avg_loss=0.07521]\n",
      "Step 506838  [5.418 sec/step, loss=0.07510, avg_loss=0.07520]\n",
      "Step 506839  [5.425 sec/step, loss=0.07677, avg_loss=0.07520]\n",
      "Step 506840  [5.404 sec/step, loss=0.07438, avg_loss=0.07517]\n",
      "Step 506841  [5.402 sec/step, loss=0.07552, avg_loss=0.07517]\n",
      "Step 506842  [5.426 sec/step, loss=0.07613, avg_loss=0.07521]\n",
      "Step 506843  [5.425 sec/step, loss=0.06802, avg_loss=0.07521]\n",
      "Step 506844  [5.422 sec/step, loss=0.07321, avg_loss=0.07516]\n",
      "Step 506845  [5.405 sec/step, loss=0.07146, avg_loss=0.07513]\n",
      "Step 506846  [5.390 sec/step, loss=0.07531, avg_loss=0.07510]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 506847  [5.401 sec/step, loss=0.07641, avg_loss=0.07515]\n",
      "Step 506848  [5.390 sec/step, loss=0.07537, avg_loss=0.07517]\n",
      "Step 506849  [5.407 sec/step, loss=0.07757, avg_loss=0.07519]\n",
      "Step 506850  [5.418 sec/step, loss=0.07283, avg_loss=0.07520]\n",
      "Step 506851  [5.426 sec/step, loss=0.07724, avg_loss=0.07521]\n",
      "Step 506852  [5.418 sec/step, loss=0.07355, avg_loss=0.07519]\n",
      "Step 506853  [5.423 sec/step, loss=0.07729, avg_loss=0.07520]\n",
      "Step 506854  [5.429 sec/step, loss=0.07727, avg_loss=0.07522]\n",
      "Step 506855  [5.434 sec/step, loss=0.07683, avg_loss=0.07525]\n",
      "Step 506856  [5.407 sec/step, loss=0.07333, avg_loss=0.07523]\n",
      "Step 506857  [5.392 sec/step, loss=0.07227, avg_loss=0.07518]\n",
      "Step 506858  [5.393 sec/step, loss=0.07523, avg_loss=0.07518]\n",
      "Step 506859  [5.403 sec/step, loss=0.07812, avg_loss=0.07519]\n",
      "Step 506860  [5.406 sec/step, loss=0.07759, avg_loss=0.07521]\n",
      "Step 506861  [5.413 sec/step, loss=0.07652, avg_loss=0.07524]\n",
      "Generated 32 batches of size 32 in 2.347 sec\n",
      "Step 506862  [5.442 sec/step, loss=0.07558, avg_loss=0.07533]\n",
      "Step 506863  [5.468 sec/step, loss=0.07509, avg_loss=0.07532]\n",
      "Step 506864  [5.438 sec/step, loss=0.06654, avg_loss=0.07522]\n",
      "Step 506865  [5.437 sec/step, loss=0.07762, avg_loss=0.07522]\n",
      "Step 506866  [5.425 sec/step, loss=0.07425, avg_loss=0.07520]\n",
      "Step 506867  [5.375 sec/step, loss=0.07674, avg_loss=0.07526]\n",
      "Step 506868  [5.379 sec/step, loss=0.07549, avg_loss=0.07525]\n",
      "Step 506869  [5.372 sec/step, loss=0.07685, avg_loss=0.07525]\n",
      "Step 506870  [5.393 sec/step, loss=0.07505, avg_loss=0.07524]\n",
      "Step 506871  [5.381 sec/step, loss=0.07497, avg_loss=0.07522]\n",
      "Step 506872  [5.375 sec/step, loss=0.07594, avg_loss=0.07521]\n",
      "Step 506873  [5.382 sec/step, loss=0.07690, avg_loss=0.07522]\n",
      "Step 506874  [5.381 sec/step, loss=0.07535, avg_loss=0.07522]\n",
      "Step 506875  [5.403 sec/step, loss=0.07711, avg_loss=0.07524]\n",
      "Step 506876  [5.398 sec/step, loss=0.07634, avg_loss=0.07522]\n",
      "Step 506877  [5.386 sec/step, loss=0.07733, avg_loss=0.07523]\n",
      "Step 506878  [5.400 sec/step, loss=0.07723, avg_loss=0.07526]\n",
      "Step 506879  [5.389 sec/step, loss=0.07561, avg_loss=0.07524]\n",
      "Step 506880  [5.365 sec/step, loss=0.07763, avg_loss=0.07528]\n",
      "Step 506881  [5.372 sec/step, loss=0.07491, avg_loss=0.07526]\n",
      "Step 506882  [5.349 sec/step, loss=0.07491, avg_loss=0.07526]\n",
      "Step 506883  [5.345 sec/step, loss=0.07658, avg_loss=0.07527]\n",
      "Step 506884  [5.336 sec/step, loss=0.07588, avg_loss=0.07525]\n",
      "Step 506885  [5.344 sec/step, loss=0.07751, avg_loss=0.07526]\n",
      "Step 506886  [5.355 sec/step, loss=0.07810, avg_loss=0.07528]\n",
      "Step 506887  [5.358 sec/step, loss=0.07717, avg_loss=0.07529]\n",
      "Step 506888  [5.343 sec/step, loss=0.07370, avg_loss=0.07526]\n",
      "Step 506889  [5.361 sec/step, loss=0.07537, avg_loss=0.07528]\n",
      "Step 506890  [5.380 sec/step, loss=0.07698, avg_loss=0.07528]\n",
      "Step 506891  [5.383 sec/step, loss=0.07561, avg_loss=0.07531]\n",
      "Step 506892  [5.369 sec/step, loss=0.07337, avg_loss=0.07529]\n",
      "Step 506893  [5.354 sec/step, loss=0.07532, avg_loss=0.07527]\n",
      "Generated 32 batches of size 32 in 2.546 sec\n",
      "Step 506894  [5.350 sec/step, loss=0.07646, avg_loss=0.07526]\n",
      "Step 506895  [5.343 sec/step, loss=0.06938, avg_loss=0.07520]\n",
      "Step 506896  [5.361 sec/step, loss=0.07645, avg_loss=0.07523]\n",
      "Step 506897  [5.373 sec/step, loss=0.07740, avg_loss=0.07529]\n",
      "Step 506898  [5.350 sec/step, loss=0.07494, avg_loss=0.07536]\n",
      "Step 506899  [5.332 sec/step, loss=0.07388, avg_loss=0.07534]\n",
      "Step 506900  [5.345 sec/step, loss=0.07719, avg_loss=0.07544]\n",
      "Writing summary at step: 506900\n",
      "Step 506901  [5.333 sec/step, loss=0.07622, avg_loss=0.07544]\n",
      "Step 506902  [5.332 sec/step, loss=0.07767, avg_loss=0.07544]\n",
      "Step 506903  [5.353 sec/step, loss=0.07602, avg_loss=0.07545]\n",
      "Step 506904  [5.356 sec/step, loss=0.07485, avg_loss=0.07544]\n",
      "Step 506905  [5.361 sec/step, loss=0.07803, avg_loss=0.07545]\n",
      "Step 506906  [5.358 sec/step, loss=0.07582, avg_loss=0.07544]\n",
      "Step 506907  [5.342 sec/step, loss=0.07207, avg_loss=0.07541]\n",
      "Step 506908  [5.340 sec/step, loss=0.06560, avg_loss=0.07533]\n",
      "Step 506909  [5.334 sec/step, loss=0.07487, avg_loss=0.07532]\n",
      "Step 506910  [5.331 sec/step, loss=0.07446, avg_loss=0.07532]\n",
      "Step 506911  [5.324 sec/step, loss=0.07571, avg_loss=0.07530]\n",
      "Step 506912  [5.332 sec/step, loss=0.07559, avg_loss=0.07533]\n",
      "Step 506913  [5.322 sec/step, loss=0.07640, avg_loss=0.07533]\n",
      "Step 506914  [5.345 sec/step, loss=0.07737, avg_loss=0.07537]\n",
      "Step 506915  [5.352 sec/step, loss=0.07664, avg_loss=0.07540]\n",
      "Step 506916  [5.349 sec/step, loss=0.07641, avg_loss=0.07540]\n",
      "Step 506917  [5.346 sec/step, loss=0.07661, avg_loss=0.07541]\n",
      "Step 506918  [5.356 sec/step, loss=0.07692, avg_loss=0.07543]\n",
      "Step 506919  [5.350 sec/step, loss=0.07333, avg_loss=0.07540]\n",
      "Step 506920  [5.343 sec/step, loss=0.07225, avg_loss=0.07537]\n",
      "Step 506921  [5.341 sec/step, loss=0.07637, avg_loss=0.07537]\n",
      "Step 506922  [5.329 sec/step, loss=0.07752, avg_loss=0.07537]\n",
      "Step 506923  [5.317 sec/step, loss=0.07551, avg_loss=0.07537]\n",
      "Step 506924  [5.326 sec/step, loss=0.07543, avg_loss=0.07536]\n",
      "Generated 32 batches of size 32 in 2.364 sec\n",
      "Step 506925  [5.340 sec/step, loss=0.07785, avg_loss=0.07540]\n",
      "Step 506926  [5.335 sec/step, loss=0.07530, avg_loss=0.07539]\n",
      "Step 506927  [5.336 sec/step, loss=0.07488, avg_loss=0.07537]\n",
      "Step 506928  [5.330 sec/step, loss=0.07463, avg_loss=0.07534]\n",
      "Step 506929  [5.315 sec/step, loss=0.07507, avg_loss=0.07532]\n",
      "Step 506930  [5.307 sec/step, loss=0.07454, avg_loss=0.07530]\n",
      "Step 506931  [5.342 sec/step, loss=0.06783, avg_loss=0.07520]\n",
      "Step 506932  [5.348 sec/step, loss=0.07767, avg_loss=0.07521]\n",
      "Step 506933  [5.354 sec/step, loss=0.07712, avg_loss=0.07521]\n",
      "Step 506934  [5.371 sec/step, loss=0.07678, avg_loss=0.07530]\n",
      "Step 506935  [5.365 sec/step, loss=0.07304, avg_loss=0.07527]\n",
      "Step 506936  [5.383 sec/step, loss=0.07671, avg_loss=0.07528]\n",
      "Step 506937  [5.383 sec/step, loss=0.06676, avg_loss=0.07528]\n",
      "Step 506938  [5.385 sec/step, loss=0.07391, avg_loss=0.07527]\n",
      "Step 506939  [5.374 sec/step, loss=0.07616, avg_loss=0.07526]\n",
      "Step 506940  [5.382 sec/step, loss=0.07545, avg_loss=0.07527]\n",
      "Step 506941  [5.373 sec/step, loss=0.07573, avg_loss=0.07527]\n",
      "Step 506942  [5.364 sec/step, loss=0.07766, avg_loss=0.07529]\n",
      "Step 506943  [5.332 sec/step, loss=0.07688, avg_loss=0.07538]\n",
      "Step 506944  [5.324 sec/step, loss=0.07723, avg_loss=0.07542]\n",
      "Step 506945  [5.345 sec/step, loss=0.07792, avg_loss=0.07548]\n",
      "Step 506946  [5.347 sec/step, loss=0.07210, avg_loss=0.07545]\n",
      "Step 506947  [5.360 sec/step, loss=0.07728, avg_loss=0.07546]\n",
      "Step 506948  [5.375 sec/step, loss=0.07483, avg_loss=0.07545]\n",
      "Step 506949  [5.366 sec/step, loss=0.07605, avg_loss=0.07544]\n",
      "Step 506950  [5.380 sec/step, loss=0.07694, avg_loss=0.07548]\n",
      "Step 506951  [5.377 sec/step, loss=0.07590, avg_loss=0.07547]\n",
      "Step 506952  [5.385 sec/step, loss=0.07626, avg_loss=0.07549]\n",
      "Step 506953  [5.393 sec/step, loss=0.07587, avg_loss=0.07548]\n",
      "Step 506954  [5.381 sec/step, loss=0.07655, avg_loss=0.07547]\n",
      "Step 506955  [5.380 sec/step, loss=0.07592, avg_loss=0.07546]\n",
      "Step 506956  [5.380 sec/step, loss=0.07529, avg_loss=0.07548]\n",
      "Generated 32 batches of size 32 in 2.521 sec\n",
      "Step 506957  [5.391 sec/step, loss=0.07527, avg_loss=0.07551]\n",
      "Step 506958  [5.391 sec/step, loss=0.07508, avg_loss=0.07551]\n",
      "Step 506959  [5.373 sec/step, loss=0.07470, avg_loss=0.07548]\n",
      "Step 506960  [5.352 sec/step, loss=0.07148, avg_loss=0.07542]\n",
      "Step 506961  [5.359 sec/step, loss=0.07527, avg_loss=0.07540]\n",
      "Step 506962  [5.348 sec/step, loss=0.07686, avg_loss=0.07542]\n",
      "Step 506963  [5.336 sec/step, loss=0.07752, avg_loss=0.07544]\n",
      "Step 506964  [5.336 sec/step, loss=0.06676, avg_loss=0.07544]\n",
      "Step 506965  [5.335 sec/step, loss=0.07767, avg_loss=0.07544]\n",
      "Step 506966  [5.352 sec/step, loss=0.07425, avg_loss=0.07544]\n",
      "Step 506967  [5.342 sec/step, loss=0.07313, avg_loss=0.07541]\n",
      "Step 506968  [5.348 sec/step, loss=0.07788, avg_loss=0.07543]\n",
      "Step 506969  [5.344 sec/step, loss=0.07331, avg_loss=0.07540]\n",
      "Step 506970  [5.324 sec/step, loss=0.07610, avg_loss=0.07541]\n",
      "Step 506971  [5.333 sec/step, loss=0.07675, avg_loss=0.07542]\n",
      "Step 506972  [5.317 sec/step, loss=0.07380, avg_loss=0.07540]\n",
      "Step 506973  [5.309 sec/step, loss=0.07561, avg_loss=0.07539]\n",
      "Step 506974  [5.303 sec/step, loss=0.07475, avg_loss=0.07538]\n",
      "Step 506975  [5.277 sec/step, loss=0.07473, avg_loss=0.07536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 506976  [5.281 sec/step, loss=0.07624, avg_loss=0.07536]\n",
      "Step 506977  [5.284 sec/step, loss=0.07753, avg_loss=0.07536]\n",
      "Step 506978  [5.275 sec/step, loss=0.07611, avg_loss=0.07535]\n",
      "Step 506979  [5.313 sec/step, loss=0.07477, avg_loss=0.07534]\n",
      "Step 506980  [5.325 sec/step, loss=0.07812, avg_loss=0.07535]\n",
      "Step 506981  [5.320 sec/step, loss=0.07642, avg_loss=0.07536]\n",
      "Step 506982  [5.334 sec/step, loss=0.07549, avg_loss=0.07537]\n",
      "Step 506983  [5.341 sec/step, loss=0.07812, avg_loss=0.07538]\n",
      "Step 506984  [5.342 sec/step, loss=0.07715, avg_loss=0.07539]\n",
      "Step 506985  [5.319 sec/step, loss=0.06743, avg_loss=0.07529]\n",
      "Step 506986  [5.307 sec/step, loss=0.07486, avg_loss=0.07526]\n",
      "Step 506987  [5.295 sec/step, loss=0.07671, avg_loss=0.07526]\n",
      "Step 506988  [5.316 sec/step, loss=0.07768, avg_loss=0.07530]\n",
      "Generated 32 batches of size 32 in 2.773 sec\n",
      "Step 506989  [5.359 sec/step, loss=0.06814, avg_loss=0.07522]\n",
      "Step 506990  [5.351 sec/step, loss=0.07773, avg_loss=0.07523]\n",
      "Step 506991  [5.354 sec/step, loss=0.07705, avg_loss=0.07525]\n",
      "Step 506992  [5.377 sec/step, loss=0.07727, avg_loss=0.07529]\n",
      "Step 506993  [5.384 sec/step, loss=0.07711, avg_loss=0.07530]\n",
      "Step 506994  [5.372 sec/step, loss=0.07317, avg_loss=0.07527]\n",
      "Step 506995  [5.387 sec/step, loss=0.07264, avg_loss=0.07530]\n",
      "Step 506996  [5.386 sec/step, loss=0.07551, avg_loss=0.07529]\n",
      "Step 506997  [5.387 sec/step, loss=0.07672, avg_loss=0.07529]\n",
      "Step 506998  [5.364 sec/step, loss=0.07593, avg_loss=0.07530]\n",
      "Step 506999  [5.370 sec/step, loss=0.07645, avg_loss=0.07532]\n",
      "Step 507000  [5.388 sec/step, loss=0.07727, avg_loss=0.07532]\n",
      "Writing summary at step: 507000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-507000\n",
      "Saving audio and alignment...\n",
      "Input: muxaejar lijaaqatd tsaddhddharr nay kahaa miin tsil kur nanhjaal kay dayray tdak hooloon~_____________________\n",
      "Step 507001  [5.401 sec/step, loss=0.07680, avg_loss=0.07533]\n",
      "Step 507002  [5.386 sec/step, loss=0.07640, avg_loss=0.07532]\n",
      "Step 507003  [5.379 sec/step, loss=0.07737, avg_loss=0.07533]\n",
      "Step 507004  [5.379 sec/step, loss=0.07529, avg_loss=0.07533]\n",
      "Step 507005  [5.371 sec/step, loss=0.07760, avg_loss=0.07533]\n",
      "Step 507006  [5.366 sec/step, loss=0.07575, avg_loss=0.07533]\n",
      "Step 507007  [5.381 sec/step, loss=0.07573, avg_loss=0.07537]\n",
      "Step 507008  [5.393 sec/step, loss=0.07650, avg_loss=0.07547]\n",
      "Step 507009  [5.400 sec/step, loss=0.07666, avg_loss=0.07549]\n",
      "Step 507010  [5.378 sec/step, loss=0.07684, avg_loss=0.07552]\n",
      "Step 507011  [5.423 sec/step, loss=0.06737, avg_loss=0.07543]\n",
      "Step 507012  [5.421 sec/step, loss=0.07622, avg_loss=0.07544]\n",
      "Step 507013  [5.440 sec/step, loss=0.07414, avg_loss=0.07542]\n",
      "Step 507014  [5.425 sec/step, loss=0.07414, avg_loss=0.07538]\n",
      "Step 507015  [5.410 sec/step, loss=0.07563, avg_loss=0.07537]\n",
      "Step 507016  [5.407 sec/step, loss=0.07507, avg_loss=0.07536]\n",
      "Step 507017  [5.397 sec/step, loss=0.07592, avg_loss=0.07535]\n",
      "Step 507018  [5.401 sec/step, loss=0.07717, avg_loss=0.07536]\n",
      "Generated 32 batches of size 32 in 2.867 sec\n",
      "Step 507019  [5.389 sec/step, loss=0.07379, avg_loss=0.07536]\n",
      "Step 507020  [5.386 sec/step, loss=0.06766, avg_loss=0.07532]\n",
      "Step 507021  [5.390 sec/step, loss=0.07568, avg_loss=0.07531]\n",
      "Step 507022  [5.377 sec/step, loss=0.07227, avg_loss=0.07526]\n",
      "Step 507023  [5.394 sec/step, loss=0.07675, avg_loss=0.07527]\n",
      "Step 507024  [5.395 sec/step, loss=0.07796, avg_loss=0.07529]\n",
      "Step 507025  [5.390 sec/step, loss=0.07819, avg_loss=0.07530]\n",
      "Step 507026  [5.395 sec/step, loss=0.07472, avg_loss=0.07529]\n",
      "Step 507027  [5.409 sec/step, loss=0.07578, avg_loss=0.07530]\n",
      "Step 507028  [5.427 sec/step, loss=0.07417, avg_loss=0.07530]\n",
      "Step 507029  [5.448 sec/step, loss=0.07773, avg_loss=0.07532]\n",
      "Step 507030  [5.459 sec/step, loss=0.07695, avg_loss=0.07535]\n",
      "Step 507031  [5.396 sec/step, loss=0.07273, avg_loss=0.07539]\n",
      "Step 507032  [5.394 sec/step, loss=0.07716, avg_loss=0.07539]\n",
      "Step 507033  [5.393 sec/step, loss=0.07709, avg_loss=0.07539]\n",
      "Step 507034  [5.388 sec/step, loss=0.07430, avg_loss=0.07536]\n",
      "Step 507035  [5.412 sec/step, loss=0.07621, avg_loss=0.07540]\n",
      "Step 507036  [5.396 sec/step, loss=0.07778, avg_loss=0.07541]\n",
      "Step 507037  [5.345 sec/step, loss=0.07537, avg_loss=0.07549]\n",
      "Step 507038  [5.374 sec/step, loss=0.07455, avg_loss=0.07550]\n",
      "Step 507039  [5.367 sec/step, loss=0.07541, avg_loss=0.07549]\n",
      "Step 507040  [5.365 sec/step, loss=0.07618, avg_loss=0.07550]\n",
      "Step 507041  [5.376 sec/step, loss=0.07746, avg_loss=0.07552]\n",
      "Step 507042  [5.367 sec/step, loss=0.07395, avg_loss=0.07548]\n",
      "Step 507043  [5.342 sec/step, loss=0.07556, avg_loss=0.07547]\n",
      "Step 507044  [5.329 sec/step, loss=0.06640, avg_loss=0.07536]\n",
      "Step 507045  [5.324 sec/step, loss=0.07639, avg_loss=0.07534]\n",
      "Step 507046  [5.322 sec/step, loss=0.07567, avg_loss=0.07538]\n",
      "Step 507047  [5.311 sec/step, loss=0.07546, avg_loss=0.07536]\n",
      "Step 507048  [5.304 sec/step, loss=0.07625, avg_loss=0.07537]\n",
      "Step 507049  [5.354 sec/step, loss=0.06695, avg_loss=0.07528]\n",
      "Step 507050  [5.356 sec/step, loss=0.07782, avg_loss=0.07529]\n",
      "Generated 32 batches of size 32 in 2.631 sec\n",
      "Step 507051  [5.341 sec/step, loss=0.07286, avg_loss=0.07526]\n",
      "Step 507052  [5.341 sec/step, loss=0.07622, avg_loss=0.07526]\n",
      "Step 507053  [5.329 sec/step, loss=0.07660, avg_loss=0.07527]\n",
      "Step 507054  [5.324 sec/step, loss=0.07736, avg_loss=0.07528]\n",
      "Step 507055  [5.318 sec/step, loss=0.07503, avg_loss=0.07527]\n",
      "Step 507056  [5.322 sec/step, loss=0.07698, avg_loss=0.07528]\n",
      "Step 507057  [5.337 sec/step, loss=0.07514, avg_loss=0.07528]\n",
      "Step 507058  [5.342 sec/step, loss=0.07332, avg_loss=0.07527]\n",
      "Step 507059  [5.353 sec/step, loss=0.07601, avg_loss=0.07528]\n",
      "Step 507060  [5.364 sec/step, loss=0.07254, avg_loss=0.07529]\n",
      "Step 507061  [5.362 sec/step, loss=0.07770, avg_loss=0.07531]\n",
      "Step 507062  [5.373 sec/step, loss=0.07771, avg_loss=0.07532]\n",
      "Step 507063  [5.412 sec/step, loss=0.06751, avg_loss=0.07522]\n",
      "Step 507064  [5.429 sec/step, loss=0.07633, avg_loss=0.07532]\n",
      "Step 507065  [5.418 sec/step, loss=0.07616, avg_loss=0.07530]\n",
      "Step 507066  [5.385 sec/step, loss=0.07304, avg_loss=0.07529]\n",
      "Step 507067  [5.400 sec/step, loss=0.07668, avg_loss=0.07533]\n",
      "Step 507068  [5.405 sec/step, loss=0.07625, avg_loss=0.07531]\n",
      "Step 507069  [5.386 sec/step, loss=0.06776, avg_loss=0.07525]\n",
      "Step 507070  [5.400 sec/step, loss=0.07561, avg_loss=0.07525]\n",
      "Step 507071  [5.409 sec/step, loss=0.07805, avg_loss=0.07526]\n",
      "Step 507072  [5.404 sec/step, loss=0.07233, avg_loss=0.07525]\n",
      "Step 507073  [5.415 sec/step, loss=0.07742, avg_loss=0.07527]\n",
      "Step 507074  [5.436 sec/step, loss=0.07708, avg_loss=0.07529]\n",
      "Step 507075  [5.437 sec/step, loss=0.07581, avg_loss=0.07530]\n",
      "Step 507076  [5.425 sec/step, loss=0.07720, avg_loss=0.07531]\n",
      "Step 507077  [5.430 sec/step, loss=0.07773, avg_loss=0.07531]\n",
      "Step 507078  [5.421 sec/step, loss=0.07430, avg_loss=0.07529]\n",
      "Step 507079  [5.384 sec/step, loss=0.07537, avg_loss=0.07530]\n",
      "Step 507080  [5.396 sec/step, loss=0.07423, avg_loss=0.07526]\n",
      "Step 507081  [5.398 sec/step, loss=0.07690, avg_loss=0.07527]\n",
      "Step 507082  [5.392 sec/step, loss=0.07652, avg_loss=0.07528]\n",
      "Generated 32 batches of size 32 in 2.389 sec\n",
      "Step 507083  [5.393 sec/step, loss=0.07698, avg_loss=0.07526]\n",
      "Step 507084  [5.389 sec/step, loss=0.07673, avg_loss=0.07526]\n",
      "Step 507085  [5.404 sec/step, loss=0.07479, avg_loss=0.07533]\n",
      "Step 507086  [5.415 sec/step, loss=0.07598, avg_loss=0.07534]\n",
      "Step 507087  [5.427 sec/step, loss=0.07692, avg_loss=0.07535]\n",
      "Step 507088  [5.418 sec/step, loss=0.07245, avg_loss=0.07529]\n",
      "Step 507089  [5.374 sec/step, loss=0.07763, avg_loss=0.07539]\n",
      "Step 507090  [5.363 sec/step, loss=0.07545, avg_loss=0.07537]\n",
      "Step 507091  [5.360 sec/step, loss=0.07555, avg_loss=0.07535]\n",
      "Step 507092  [5.348 sec/step, loss=0.07713, avg_loss=0.07535]\n",
      "Step 507093  [5.335 sec/step, loss=0.07236, avg_loss=0.07530]\n",
      "Step 507094  [5.346 sec/step, loss=0.07675, avg_loss=0.07534]\n",
      "Step 507095  [5.350 sec/step, loss=0.07628, avg_loss=0.07538]\n",
      "Step 507096  [5.339 sec/step, loss=0.07477, avg_loss=0.07537]\n",
      "Step 507097  [5.354 sec/step, loss=0.07600, avg_loss=0.07536]\n",
      "Step 507098  [5.363 sec/step, loss=0.07703, avg_loss=0.07537]\n",
      "Step 507099  [5.355 sec/step, loss=0.07501, avg_loss=0.07536]\n",
      "Step 507100  [5.334 sec/step, loss=0.07548, avg_loss=0.07534]\n",
      "Writing summary at step: 507100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 507101  [5.327 sec/step, loss=0.07635, avg_loss=0.07533]\n",
      "Step 507102  [5.335 sec/step, loss=0.07592, avg_loss=0.07533]\n",
      "Step 507103  [5.314 sec/step, loss=0.07530, avg_loss=0.07531]\n",
      "Step 507104  [5.318 sec/step, loss=0.07522, avg_loss=0.07531]\n",
      "Step 507105  [5.326 sec/step, loss=0.07627, avg_loss=0.07530]\n",
      "Step 507106  [5.332 sec/step, loss=0.07561, avg_loss=0.07529]\n",
      "Step 507107  [5.333 sec/step, loss=0.07610, avg_loss=0.07530]\n",
      "Step 507108  [5.336 sec/step, loss=0.07642, avg_loss=0.07530]\n",
      "Step 507109  [5.322 sec/step, loss=0.07181, avg_loss=0.07525]\n",
      "Step 507110  [5.332 sec/step, loss=0.07760, avg_loss=0.07526]\n",
      "Step 507111  [5.290 sec/step, loss=0.07778, avg_loss=0.07536]\n",
      "Step 507112  [5.288 sec/step, loss=0.07682, avg_loss=0.07537]\n",
      "Step 507113  [5.265 sec/step, loss=0.07618, avg_loss=0.07539]\n",
      "Generated 32 batches of size 32 in 2.722 sec\n",
      "Step 507114  [5.323 sec/step, loss=0.06683, avg_loss=0.07531]\n",
      "Step 507115  [5.329 sec/step, loss=0.07441, avg_loss=0.07530]\n",
      "Step 507116  [5.333 sec/step, loss=0.07679, avg_loss=0.07532]\n",
      "Step 507117  [5.316 sec/step, loss=0.06772, avg_loss=0.07524]\n",
      "Step 507118  [5.318 sec/step, loss=0.07495, avg_loss=0.07521]\n",
      "Step 507119  [5.345 sec/step, loss=0.07656, avg_loss=0.07524]\n",
      "Step 507120  [5.368 sec/step, loss=0.07558, avg_loss=0.07532]\n",
      "Step 507121  [5.371 sec/step, loss=0.07808, avg_loss=0.07535]\n",
      "Step 507122  [5.381 sec/step, loss=0.07308, avg_loss=0.07535]\n",
      "Step 507123  [5.348 sec/step, loss=0.06697, avg_loss=0.07526]\n",
      "Step 507124  [5.342 sec/step, loss=0.07653, avg_loss=0.07524]\n",
      "Step 507125  [5.343 sec/step, loss=0.07770, avg_loss=0.07524]\n",
      "Step 507126  [5.341 sec/step, loss=0.07663, avg_loss=0.07526]\n",
      "Step 507127  [5.344 sec/step, loss=0.07596, avg_loss=0.07526]\n",
      "Step 507128  [5.334 sec/step, loss=0.07661, avg_loss=0.07528]\n",
      "Step 507129  [5.328 sec/step, loss=0.07609, avg_loss=0.07527]\n",
      "Step 507130  [5.306 sec/step, loss=0.07450, avg_loss=0.07524]\n",
      "Step 507131  [5.313 sec/step, loss=0.07414, avg_loss=0.07525]\n",
      "Step 507132  [5.306 sec/step, loss=0.07651, avg_loss=0.07525]\n",
      "Step 507133  [5.283 sec/step, loss=0.07308, avg_loss=0.07521]\n",
      "Step 507134  [5.278 sec/step, loss=0.07562, avg_loss=0.07522]\n",
      "Step 507135  [5.277 sec/step, loss=0.07632, avg_loss=0.07522]\n",
      "Step 507136  [5.269 sec/step, loss=0.07470, avg_loss=0.07519]\n",
      "Step 507137  [5.274 sec/step, loss=0.07674, avg_loss=0.07521]\n",
      "Step 507138  [5.295 sec/step, loss=0.06867, avg_loss=0.07515]\n",
      "Step 507139  [5.328 sec/step, loss=0.07566, avg_loss=0.07515]\n",
      "Step 507140  [5.335 sec/step, loss=0.07777, avg_loss=0.07516]\n",
      "Step 507141  [5.324 sec/step, loss=0.07524, avg_loss=0.07514]\n",
      "Step 507142  [5.333 sec/step, loss=0.07760, avg_loss=0.07518]\n",
      "Step 507143  [5.341 sec/step, loss=0.07650, avg_loss=0.07519]\n",
      "Step 507144  [5.372 sec/step, loss=0.07599, avg_loss=0.07528]\n",
      "Step 507145  [5.359 sec/step, loss=0.07512, avg_loss=0.07527]\n",
      "Generated 32 batches of size 32 in 2.355 sec\n",
      "Step 507146  [5.370 sec/step, loss=0.07589, avg_loss=0.07527]\n",
      "Step 507147  [5.377 sec/step, loss=0.07750, avg_loss=0.07529]\n",
      "Step 507148  [5.381 sec/step, loss=0.07695, avg_loss=0.07530]\n",
      "Step 507149  [5.328 sec/step, loss=0.07323, avg_loss=0.07536]\n",
      "Step 507150  [5.341 sec/step, loss=0.07468, avg_loss=0.07533]\n",
      "Step 507151  [5.347 sec/step, loss=0.07559, avg_loss=0.07536]\n",
      "Step 507152  [5.338 sec/step, loss=0.07636, avg_loss=0.07536]\n",
      "Step 507153  [5.338 sec/step, loss=0.07528, avg_loss=0.07535]\n",
      "Step 507154  [5.354 sec/step, loss=0.07492, avg_loss=0.07532]\n",
      "Step 507155  [5.369 sec/step, loss=0.07538, avg_loss=0.07533]\n",
      "Step 507156  [5.391 sec/step, loss=0.07518, avg_loss=0.07531]\n",
      "Step 507157  [5.368 sec/step, loss=0.07144, avg_loss=0.07527]\n",
      "Step 507158  [5.380 sec/step, loss=0.07743, avg_loss=0.07531]\n",
      "Step 507159  [5.358 sec/step, loss=0.06697, avg_loss=0.07522]\n",
      "Step 507160  [5.357 sec/step, loss=0.07645, avg_loss=0.07526]\n",
      "Step 507161  [5.344 sec/step, loss=0.07678, avg_loss=0.07525]\n",
      "Step 507162  [5.324 sec/step, loss=0.07498, avg_loss=0.07523]\n",
      "Step 507163  [5.284 sec/step, loss=0.07527, avg_loss=0.07530]\n",
      "Step 507164  [5.278 sec/step, loss=0.07377, avg_loss=0.07528]\n",
      "Step 507165  [5.285 sec/step, loss=0.07530, avg_loss=0.07527]\n",
      "Step 507166  [5.348 sec/step, loss=0.06687, avg_loss=0.07521]\n",
      "Step 507167  [5.342 sec/step, loss=0.07508, avg_loss=0.07519]\n",
      "Step 507168  [5.335 sec/step, loss=0.07757, avg_loss=0.07521]\n",
      "Step 507169  [5.352 sec/step, loss=0.07458, avg_loss=0.07527]\n",
      "Step 507170  [5.360 sec/step, loss=0.07569, avg_loss=0.07527]\n",
      "Step 507171  [5.346 sec/step, loss=0.07650, avg_loss=0.07526]\n",
      "Step 507172  [5.366 sec/step, loss=0.07740, avg_loss=0.07531]\n",
      "Step 507173  [5.365 sec/step, loss=0.07674, avg_loss=0.07530]\n",
      "Step 507174  [5.365 sec/step, loss=0.07766, avg_loss=0.07531]\n",
      "Step 507175  [5.382 sec/step, loss=0.07778, avg_loss=0.07533]\n",
      "Step 507176  [5.379 sec/step, loss=0.07264, avg_loss=0.07528]\n",
      "Step 507177  [5.371 sec/step, loss=0.07543, avg_loss=0.07526]\n",
      "Generated 32 batches of size 32 in 2.363 sec\n",
      "Step 507178  [5.382 sec/step, loss=0.07753, avg_loss=0.07529]\n",
      "Step 507179  [5.382 sec/step, loss=0.07578, avg_loss=0.07530]\n",
      "Step 507180  [5.360 sec/step, loss=0.07610, avg_loss=0.07531]\n",
      "Step 507181  [5.359 sec/step, loss=0.07565, avg_loss=0.07530]\n",
      "Step 507182  [5.359 sec/step, loss=0.07639, avg_loss=0.07530]\n",
      "Step 507183  [5.351 sec/step, loss=0.07657, avg_loss=0.07530]\n",
      "Step 507184  [5.379 sec/step, loss=0.07422, avg_loss=0.07527]\n",
      "Step 507185  [5.373 sec/step, loss=0.07519, avg_loss=0.07528]\n",
      "Step 507186  [5.350 sec/step, loss=0.07304, avg_loss=0.07525]\n",
      "Step 507187  [5.367 sec/step, loss=0.07376, avg_loss=0.07521]\n",
      "Step 507188  [5.365 sec/step, loss=0.07624, avg_loss=0.07525]\n",
      "Step 507189  [5.364 sec/step, loss=0.07769, avg_loss=0.07525]\n",
      "Step 507190  [5.352 sec/step, loss=0.06791, avg_loss=0.07518]\n",
      "Step 507191  [5.363 sec/step, loss=0.07575, avg_loss=0.07518]\n",
      "Step 507192  [5.368 sec/step, loss=0.07783, avg_loss=0.07519]\n",
      "Step 507193  [5.390 sec/step, loss=0.07626, avg_loss=0.07523]\n",
      "Step 507194  [5.393 sec/step, loss=0.07648, avg_loss=0.07522]\n",
      "Step 507195  [5.382 sec/step, loss=0.07548, avg_loss=0.07522]\n",
      "Step 507196  [5.382 sec/step, loss=0.07562, avg_loss=0.07522]\n",
      "Step 507197  [5.370 sec/step, loss=0.07744, avg_loss=0.07524]\n",
      "Step 507198  [5.355 sec/step, loss=0.07610, avg_loss=0.07523]\n",
      "Step 507199  [5.348 sec/step, loss=0.07210, avg_loss=0.07520]\n",
      "Step 507200  [5.351 sec/step, loss=0.07410, avg_loss=0.07519]\n",
      "Writing summary at step: 507200\n",
      "Step 507201  [5.393 sec/step, loss=0.06789, avg_loss=0.07510]\n",
      "Step 507202  [5.374 sec/step, loss=0.07380, avg_loss=0.07508]\n",
      "Step 507203  [5.398 sec/step, loss=0.07627, avg_loss=0.07509]\n",
      "Step 507204  [5.403 sec/step, loss=0.07599, avg_loss=0.07510]\n",
      "Step 507205  [5.401 sec/step, loss=0.07750, avg_loss=0.07511]\n",
      "Step 507206  [5.412 sec/step, loss=0.07485, avg_loss=0.07510]\n",
      "Step 507207  [5.414 sec/step, loss=0.07656, avg_loss=0.07511]\n",
      "Step 507208  [5.412 sec/step, loss=0.07259, avg_loss=0.07507]\n",
      "Generated 32 batches of size 32 in 2.369 sec\n",
      "Step 507209  [5.448 sec/step, loss=0.07595, avg_loss=0.07511]\n",
      "Step 507210  [5.434 sec/step, loss=0.07659, avg_loss=0.07510]\n",
      "Step 507211  [5.430 sec/step, loss=0.07707, avg_loss=0.07509]\n",
      "Step 507212  [5.434 sec/step, loss=0.07579, avg_loss=0.07508]\n",
      "Step 507213  [5.430 sec/step, loss=0.07567, avg_loss=0.07508]\n",
      "Step 507214  [5.372 sec/step, loss=0.07493, avg_loss=0.07516]\n",
      "Step 507215  [5.369 sec/step, loss=0.07527, avg_loss=0.07517]\n",
      "Step 507216  [5.373 sec/step, loss=0.07717, avg_loss=0.07517]\n",
      "Step 507217  [5.394 sec/step, loss=0.07754, avg_loss=0.07527]\n",
      "Step 507218  [5.395 sec/step, loss=0.07467, avg_loss=0.07527]\n",
      "Step 507219  [5.379 sec/step, loss=0.07698, avg_loss=0.07527]\n",
      "Step 507220  [5.365 sec/step, loss=0.07541, avg_loss=0.07527]\n",
      "Step 507221  [5.337 sec/step, loss=0.06630, avg_loss=0.07515]\n",
      "Step 507222  [5.329 sec/step, loss=0.07256, avg_loss=0.07515]\n",
      "Step 507223  [5.352 sec/step, loss=0.07739, avg_loss=0.07525]\n",
      "Step 507224  [5.357 sec/step, loss=0.07812, avg_loss=0.07527]\n",
      "Step 507225  [5.342 sec/step, loss=0.07452, avg_loss=0.07523]\n",
      "Step 507226  [5.334 sec/step, loss=0.07519, avg_loss=0.07522]\n",
      "Step 507227  [5.320 sec/step, loss=0.07370, avg_loss=0.07520]\n",
      "Step 507228  [5.324 sec/step, loss=0.07686, avg_loss=0.07520]\n",
      "Step 507229  [5.306 sec/step, loss=0.07278, avg_loss=0.07517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 507230  [5.319 sec/step, loss=0.07590, avg_loss=0.07518]\n",
      "Step 507231  [5.334 sec/step, loss=0.07714, avg_loss=0.07521]\n",
      "Step 507232  [5.331 sec/step, loss=0.07665, avg_loss=0.07521]\n",
      "Step 507233  [5.366 sec/step, loss=0.07655, avg_loss=0.07525]\n",
      "Step 507234  [5.391 sec/step, loss=0.07640, avg_loss=0.07525]\n",
      "Step 507235  [5.377 sec/step, loss=0.07673, avg_loss=0.07526]\n",
      "Step 507236  [5.378 sec/step, loss=0.07688, avg_loss=0.07528]\n",
      "Step 507237  [5.377 sec/step, loss=0.07627, avg_loss=0.07528]\n",
      "Step 507238  [5.328 sec/step, loss=0.07529, avg_loss=0.07534]\n",
      "Step 507239  [5.304 sec/step, loss=0.07554, avg_loss=0.07534]\n",
      "Step 507240  [5.305 sec/step, loss=0.07649, avg_loss=0.07533]\n",
      "Generated 32 batches of size 32 in 2.358 sec\n",
      "Step 507241  [5.322 sec/step, loss=0.07577, avg_loss=0.07533]\n",
      "Step 507242  [5.311 sec/step, loss=0.07285, avg_loss=0.07529]\n",
      "Step 507243  [5.362 sec/step, loss=0.06762, avg_loss=0.07520]\n",
      "Step 507244  [5.358 sec/step, loss=0.07750, avg_loss=0.07521]\n",
      "Step 507245  [5.381 sec/step, loss=0.07625, avg_loss=0.07522]\n",
      "Step 507246  [5.374 sec/step, loss=0.07482, avg_loss=0.07521]\n",
      "Step 507247  [5.361 sec/step, loss=0.07246, avg_loss=0.07516]\n",
      "Step 507248  [5.368 sec/step, loss=0.07794, avg_loss=0.07517]\n",
      "Step 507249  [5.376 sec/step, loss=0.07681, avg_loss=0.07521]\n",
      "Step 507250  [5.353 sec/step, loss=0.07626, avg_loss=0.07522]\n",
      "Step 507251  [5.363 sec/step, loss=0.07608, avg_loss=0.07523]\n",
      "Step 507252  [5.360 sec/step, loss=0.07569, avg_loss=0.07522]\n",
      "Step 507253  [5.382 sec/step, loss=0.07452, avg_loss=0.07521]\n",
      "Step 507254  [5.366 sec/step, loss=0.07629, avg_loss=0.07523]\n",
      "Step 507255  [5.347 sec/step, loss=0.07144, avg_loss=0.07519]\n",
      "Step 507256  [5.350 sec/step, loss=0.07427, avg_loss=0.07518]\n",
      "Step 507257  [5.357 sec/step, loss=0.07673, avg_loss=0.07523]\n",
      "Step 507258  [5.334 sec/step, loss=0.07276, avg_loss=0.07519]\n",
      "Step 507259  [5.352 sec/step, loss=0.07442, avg_loss=0.07526]\n",
      "Step 507260  [5.350 sec/step, loss=0.07526, avg_loss=0.07525]\n",
      "Step 507261  [5.355 sec/step, loss=0.07617, avg_loss=0.07524]\n",
      "Step 507262  [5.376 sec/step, loss=0.07780, avg_loss=0.07527]\n",
      "Step 507263  [5.361 sec/step, loss=0.07497, avg_loss=0.07527]\n",
      "Step 507264  [5.409 sec/step, loss=0.07030, avg_loss=0.07523]\n",
      "Step 507265  [5.409 sec/step, loss=0.07588, avg_loss=0.07524]\n",
      "Step 507266  [5.368 sec/step, loss=0.07817, avg_loss=0.07535]\n",
      "Step 507267  [5.372 sec/step, loss=0.07298, avg_loss=0.07533]\n",
      "Step 507268  [5.357 sec/step, loss=0.07210, avg_loss=0.07528]\n",
      "Step 507269  [5.361 sec/step, loss=0.07333, avg_loss=0.07526]\n",
      "Step 507270  [5.360 sec/step, loss=0.07784, avg_loss=0.07528]\n",
      "Step 507271  [5.365 sec/step, loss=0.07661, avg_loss=0.07529]\n",
      "Step 507272  [5.341 sec/step, loss=0.07324, avg_loss=0.07524]\n",
      "Generated 32 batches of size 32 in 2.381 sec\n",
      "Step 507273  [5.348 sec/step, loss=0.07738, avg_loss=0.07525]\n",
      "Step 507274  [5.338 sec/step, loss=0.07650, avg_loss=0.07524]\n",
      "Step 507275  [5.341 sec/step, loss=0.07779, avg_loss=0.07524]\n",
      "Step 507276  [5.344 sec/step, loss=0.07565, avg_loss=0.07527]\n",
      "Step 507277  [5.352 sec/step, loss=0.07750, avg_loss=0.07529]\n",
      "Step 507278  [5.358 sec/step, loss=0.07531, avg_loss=0.07527]\n",
      "Step 507279  [5.365 sec/step, loss=0.07507, avg_loss=0.07526]\n",
      "Step 507280  [5.345 sec/step, loss=0.06697, avg_loss=0.07517]\n",
      "Step 507281  [5.354 sec/step, loss=0.07603, avg_loss=0.07517]\n",
      "Step 507282  [5.355 sec/step, loss=0.07465, avg_loss=0.07516]\n",
      "Step 507283  [5.355 sec/step, loss=0.07708, avg_loss=0.07516]\n",
      "Step 507284  [5.320 sec/step, loss=0.07177, avg_loss=0.07514]\n",
      "Step 507285  [5.333 sec/step, loss=0.07735, avg_loss=0.07516]\n",
      "Step 507286  [5.339 sec/step, loss=0.07455, avg_loss=0.07517]\n",
      "Step 507287  [5.327 sec/step, loss=0.07739, avg_loss=0.07521]\n",
      "Step 507288  [5.334 sec/step, loss=0.07583, avg_loss=0.07521]\n",
      "Step 507289  [5.318 sec/step, loss=0.07349, avg_loss=0.07516]\n",
      "Step 507290  [5.360 sec/step, loss=0.07448, avg_loss=0.07523]\n",
      "Step 507291  [5.364 sec/step, loss=0.07761, avg_loss=0.07525]\n",
      "Step 507292  [5.347 sec/step, loss=0.07223, avg_loss=0.07519]\n",
      "Step 507293  [5.337 sec/step, loss=0.07591, avg_loss=0.07519]\n",
      "Step 507294  [5.337 sec/step, loss=0.07525, avg_loss=0.07518]\n",
      "Step 507295  [5.364 sec/step, loss=0.07651, avg_loss=0.07519]\n",
      "Step 507296  [5.371 sec/step, loss=0.07430, avg_loss=0.07517]\n",
      "Step 507297  [5.367 sec/step, loss=0.07775, avg_loss=0.07518]\n",
      "Step 507298  [5.370 sec/step, loss=0.07594, avg_loss=0.07517]\n",
      "Step 507299  [5.363 sec/step, loss=0.06717, avg_loss=0.07512]\n",
      "Step 507300  [5.365 sec/step, loss=0.07618, avg_loss=0.07515]\n",
      "Writing summary at step: 507300\n",
      "Step 507301  [5.317 sec/step, loss=0.07688, avg_loss=0.07524]\n",
      "Step 507302  [5.381 sec/step, loss=0.06738, avg_loss=0.07517]\n",
      "Step 507303  [5.379 sec/step, loss=0.07535, avg_loss=0.07516]\n",
      "Generated 32 batches of size 32 in 2.442 sec\n",
      "Step 507304  [5.369 sec/step, loss=0.07580, avg_loss=0.07516]\n",
      "Step 507305  [5.350 sec/step, loss=0.07519, avg_loss=0.07514]\n",
      "Step 507306  [5.344 sec/step, loss=0.07628, avg_loss=0.07515]\n",
      "Step 507307  [5.337 sec/step, loss=0.07566, avg_loss=0.07514]\n",
      "Step 507308  [5.336 sec/step, loss=0.07467, avg_loss=0.07516]\n",
      "Step 507309  [5.308 sec/step, loss=0.07547, avg_loss=0.07516]\n",
      "Step 507310  [5.324 sec/step, loss=0.07746, avg_loss=0.07517]\n",
      "Step 507311  [5.332 sec/step, loss=0.07772, avg_loss=0.07517]\n",
      "Step 507312  [5.327 sec/step, loss=0.07639, avg_loss=0.07518]\n",
      "Step 507313  [5.328 sec/step, loss=0.07730, avg_loss=0.07520]\n",
      "Step 507314  [5.337 sec/step, loss=0.07666, avg_loss=0.07521]\n",
      "Step 507315  [5.334 sec/step, loss=0.07522, avg_loss=0.07521]\n",
      "Step 507316  [5.340 sec/step, loss=0.07723, avg_loss=0.07521]\n",
      "Step 507317  [5.350 sec/step, loss=0.07476, avg_loss=0.07518]\n",
      "Step 507318  [5.347 sec/step, loss=0.07619, avg_loss=0.07520]\n",
      "Step 507319  [5.349 sec/step, loss=0.07643, avg_loss=0.07519]\n",
      "Step 507320  [5.356 sec/step, loss=0.07432, avg_loss=0.07518]\n",
      "Step 507321  [5.384 sec/step, loss=0.07768, avg_loss=0.07530]\n",
      "Step 507322  [5.398 sec/step, loss=0.07525, avg_loss=0.07532]\n",
      "Step 507323  [5.403 sec/step, loss=0.07460, avg_loss=0.07530]\n",
      "Step 507324  [5.393 sec/step, loss=0.07614, avg_loss=0.07528]\n",
      "Step 507325  [5.395 sec/step, loss=0.07330, avg_loss=0.07526]\n",
      "Step 507326  [5.407 sec/step, loss=0.07703, avg_loss=0.07528]\n",
      "Step 507327  [5.435 sec/step, loss=0.07449, avg_loss=0.07529]\n",
      "Step 507328  [5.415 sec/step, loss=0.07478, avg_loss=0.07527]\n",
      "Step 507329  [5.442 sec/step, loss=0.07786, avg_loss=0.07532]\n",
      "Step 507330  [5.444 sec/step, loss=0.07337, avg_loss=0.07530]\n",
      "Step 507331  [5.428 sec/step, loss=0.07575, avg_loss=0.07528]\n",
      "Step 507332  [5.421 sec/step, loss=0.07309, avg_loss=0.07525]\n",
      "Step 507333  [5.392 sec/step, loss=0.07500, avg_loss=0.07523]\n",
      "Step 507334  [5.372 sec/step, loss=0.07660, avg_loss=0.07523]\n",
      "Step 507335  [5.373 sec/step, loss=0.07671, avg_loss=0.07523]\n",
      "Generated 32 batches of size 32 in 2.541 sec\n",
      "Step 507336  [5.373 sec/step, loss=0.07297, avg_loss=0.07519]\n",
      "Step 507337  [5.379 sec/step, loss=0.07648, avg_loss=0.07520]\n",
      "Step 507338  [5.429 sec/step, loss=0.06706, avg_loss=0.07511]\n",
      "Step 507339  [5.448 sec/step, loss=0.07672, avg_loss=0.07512]\n",
      "Step 507340  [5.458 sec/step, loss=0.07683, avg_loss=0.07513]\n",
      "Step 507341  [5.458 sec/step, loss=0.07781, avg_loss=0.07515]\n",
      "Step 507342  [5.450 sec/step, loss=0.07313, avg_loss=0.07515]\n",
      "Step 507343  [5.402 sec/step, loss=0.07648, avg_loss=0.07524]\n",
      "Step 507344  [5.373 sec/step, loss=0.06706, avg_loss=0.07514]\n",
      "Step 507345  [5.347 sec/step, loss=0.07274, avg_loss=0.07510]\n",
      "Step 507346  [5.337 sec/step, loss=0.07331, avg_loss=0.07509]\n",
      "Step 507347  [5.337 sec/step, loss=0.07589, avg_loss=0.07512]\n",
      "Step 507348  [5.337 sec/step, loss=0.07678, avg_loss=0.07511]\n",
      "Step 507349  [5.324 sec/step, loss=0.07519, avg_loss=0.07509]\n",
      "Step 507350  [5.320 sec/step, loss=0.07323, avg_loss=0.07506]\n",
      "Step 507351  [5.360 sec/step, loss=0.06799, avg_loss=0.07498]\n",
      "Step 507352  [5.375 sec/step, loss=0.07781, avg_loss=0.07500]\n",
      "Step 507353  [5.360 sec/step, loss=0.07567, avg_loss=0.07501]\n",
      "Step 507354  [5.377 sec/step, loss=0.07678, avg_loss=0.07502]\n",
      "Step 507355  [5.388 sec/step, loss=0.07678, avg_loss=0.07507]\n",
      "Step 507356  [5.372 sec/step, loss=0.07813, avg_loss=0.07511]\n",
      "Step 507357  [5.380 sec/step, loss=0.07420, avg_loss=0.07508]\n",
      "Step 507358  [5.383 sec/step, loss=0.07511, avg_loss=0.07511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 507359  [5.407 sec/step, loss=0.07453, avg_loss=0.07511]\n",
      "Step 507360  [5.419 sec/step, loss=0.07710, avg_loss=0.07513]\n",
      "Step 507361  [5.419 sec/step, loss=0.07610, avg_loss=0.07513]\n",
      "Step 507362  [5.419 sec/step, loss=0.07716, avg_loss=0.07512]\n",
      "Step 507363  [5.409 sec/step, loss=0.06725, avg_loss=0.07504]\n",
      "Step 507364  [5.379 sec/step, loss=0.07526, avg_loss=0.07509]\n",
      "Step 507365  [5.378 sec/step, loss=0.07690, avg_loss=0.07510]\n",
      "Step 507366  [5.366 sec/step, loss=0.07503, avg_loss=0.07507]\n",
      "Step 507367  [5.363 sec/step, loss=0.07664, avg_loss=0.07511]\n",
      "Generated 32 batches of size 32 in 2.349 sec\n",
      "Step 507368  [5.381 sec/step, loss=0.07573, avg_loss=0.07514]\n",
      "Step 507369  [5.387 sec/step, loss=0.07827, avg_loss=0.07519]\n",
      "Step 507370  [5.390 sec/step, loss=0.07757, avg_loss=0.07519]\n",
      "Step 507371  [5.386 sec/step, loss=0.07668, avg_loss=0.07519]\n",
      "Step 507372  [5.401 sec/step, loss=0.07644, avg_loss=0.07522]\n",
      "Step 507373  [5.386 sec/step, loss=0.07419, avg_loss=0.07519]\n",
      "Step 507374  [5.371 sec/step, loss=0.07572, avg_loss=0.07518]\n",
      "Step 507375  [5.354 sec/step, loss=0.07529, avg_loss=0.07516]\n",
      "Step 507376  [5.359 sec/step, loss=0.07682, avg_loss=0.07517]\n",
      "Step 507377  [5.349 sec/step, loss=0.07585, avg_loss=0.07515]\n",
      "Step 507378  [5.340 sec/step, loss=0.07604, avg_loss=0.07516]\n",
      "Step 507379  [5.353 sec/step, loss=0.07779, avg_loss=0.07519]\n",
      "Step 507380  [5.370 sec/step, loss=0.07795, avg_loss=0.07530]\n",
      "Step 507381  [5.364 sec/step, loss=0.07590, avg_loss=0.07530]\n",
      "Step 507382  [5.354 sec/step, loss=0.07379, avg_loss=0.07529]\n",
      "Step 507383  [5.349 sec/step, loss=0.07689, avg_loss=0.07529]\n",
      "Step 507384  [5.362 sec/step, loss=0.07713, avg_loss=0.07534]\n",
      "Step 507385  [5.364 sec/step, loss=0.07925, avg_loss=0.07536]\n",
      "Step 507386  [5.372 sec/step, loss=0.07617, avg_loss=0.07538]\n",
      "Step 507387  [5.384 sec/step, loss=0.07534, avg_loss=0.07536]\n",
      "Step 507388  [5.378 sec/step, loss=0.07606, avg_loss=0.07536]\n",
      "Step 507389  [5.395 sec/step, loss=0.07749, avg_loss=0.07540]\n",
      "Step 507390  [5.418 sec/step, loss=0.06752, avg_loss=0.07533]\n",
      "Step 507391  [5.423 sec/step, loss=0.07550, avg_loss=0.07531]\n",
      "Step 507392  [5.421 sec/step, loss=0.07401, avg_loss=0.07533]\n",
      "Step 507393  [5.409 sec/step, loss=0.07386, avg_loss=0.07530]\n",
      "Step 507394  [5.389 sec/step, loss=0.06838, avg_loss=0.07524]\n",
      "Step 507395  [5.391 sec/step, loss=0.07448, avg_loss=0.07522]\n",
      "Step 507396  [5.389 sec/step, loss=0.07662, avg_loss=0.07524]\n",
      "Step 507397  [5.391 sec/step, loss=0.07584, avg_loss=0.07522]\n",
      "Step 507398  [5.382 sec/step, loss=0.07530, avg_loss=0.07521]\n",
      "Step 507399  [5.401 sec/step, loss=0.07372, avg_loss=0.07528]\n",
      "Generated 32 batches of size 32 in 2.396 sec\n",
      "Step 507400  [5.419 sec/step, loss=0.07706, avg_loss=0.07529]\n",
      "Writing summary at step: 507400\n",
      "Step 507401  [5.415 sec/step, loss=0.07192, avg_loss=0.07524]\n",
      "Step 507402  [5.361 sec/step, loss=0.07521, avg_loss=0.07532]\n",
      "Step 507403  [5.351 sec/step, loss=0.07659, avg_loss=0.07533]\n",
      "Step 507404  [5.364 sec/step, loss=0.07759, avg_loss=0.07535]\n",
      "Step 507405  [5.385 sec/step, loss=0.07794, avg_loss=0.07537]\n",
      "Step 507406  [5.376 sec/step, loss=0.07523, avg_loss=0.07536]\n",
      "Step 507407  [5.388 sec/step, loss=0.07771, avg_loss=0.07538]\n",
      "Step 507408  [5.393 sec/step, loss=0.07725, avg_loss=0.07541]\n",
      "Step 507409  [5.397 sec/step, loss=0.07536, avg_loss=0.07541]\n",
      "Step 507410  [5.380 sec/step, loss=0.07670, avg_loss=0.07540]\n",
      "Step 507411  [5.380 sec/step, loss=0.07783, avg_loss=0.07540]\n",
      "Step 507412  [5.379 sec/step, loss=0.07334, avg_loss=0.07537]\n",
      "Step 507413  [5.374 sec/step, loss=0.07300, avg_loss=0.07533]\n",
      "Step 507414  [5.376 sec/step, loss=0.07568, avg_loss=0.07532]\n",
      "Step 507415  [5.387 sec/step, loss=0.07714, avg_loss=0.07534]\n",
      "Step 507416  [5.372 sec/step, loss=0.07492, avg_loss=0.07532]\n",
      "Step 507417  [5.408 sec/step, loss=0.06796, avg_loss=0.07525]\n",
      "Step 507418  [5.396 sec/step, loss=0.07525, avg_loss=0.07524]\n",
      "Step 507419  [5.396 sec/step, loss=0.07564, avg_loss=0.07523]\n",
      "Step 507420  [5.409 sec/step, loss=0.07541, avg_loss=0.07524]\n",
      "Step 507421  [5.385 sec/step, loss=0.07213, avg_loss=0.07519]\n",
      "Step 507422  [5.377 sec/step, loss=0.07683, avg_loss=0.07520]\n",
      "Step 507423  [5.393 sec/step, loss=0.07445, avg_loss=0.07520]\n",
      "Step 507424  [5.386 sec/step, loss=0.07564, avg_loss=0.07519]\n",
      "Step 507425  [5.397 sec/step, loss=0.07798, avg_loss=0.07524]\n",
      "Step 507426  [5.394 sec/step, loss=0.07659, avg_loss=0.07524]\n",
      "Step 507427  [5.357 sec/step, loss=0.07565, avg_loss=0.07525]\n",
      "Step 507428  [5.353 sec/step, loss=0.07185, avg_loss=0.07522]\n",
      "Step 507429  [5.348 sec/step, loss=0.07801, avg_loss=0.07522]\n",
      "Step 507430  [5.345 sec/step, loss=0.07533, avg_loss=0.07524]\n",
      "Generated 32 batches of size 32 in 2.400 sec\n",
      "Step 507431  [5.369 sec/step, loss=0.07828, avg_loss=0.07527]\n",
      "Step 507432  [5.383 sec/step, loss=0.07607, avg_loss=0.07530]\n",
      "Step 507433  [5.399 sec/step, loss=0.07620, avg_loss=0.07531]\n",
      "Step 507434  [5.425 sec/step, loss=0.07484, avg_loss=0.07529]\n",
      "Step 507435  [5.439 sec/step, loss=0.07578, avg_loss=0.07528]\n",
      "Step 507436  [5.453 sec/step, loss=0.07706, avg_loss=0.07532]\n",
      "Step 507437  [5.449 sec/step, loss=0.07615, avg_loss=0.07532]\n",
      "Step 507438  [5.382 sec/step, loss=0.06621, avg_loss=0.07531]\n",
      "Step 507439  [5.365 sec/step, loss=0.07728, avg_loss=0.07532]\n",
      "Step 507440  [5.351 sec/step, loss=0.07695, avg_loss=0.07532]\n",
      "Step 507441  [5.370 sec/step, loss=0.07439, avg_loss=0.07528]\n",
      "Step 507442  [5.381 sec/step, loss=0.07476, avg_loss=0.07530]\n",
      "Step 507443  [5.370 sec/step, loss=0.07455, avg_loss=0.07528]\n",
      "Step 507444  [5.399 sec/step, loss=0.07760, avg_loss=0.07538]\n",
      "Step 507445  [5.414 sec/step, loss=0.07365, avg_loss=0.07539]\n",
      "Step 507446  [5.430 sec/step, loss=0.07692, avg_loss=0.07543]\n",
      "Step 507447  [5.449 sec/step, loss=0.07649, avg_loss=0.07544]\n",
      "Step 507448  [5.437 sec/step, loss=0.07496, avg_loss=0.07542]\n",
      "Step 507449  [5.438 sec/step, loss=0.07545, avg_loss=0.07542]\n",
      "Step 507450  [5.439 sec/step, loss=0.07692, avg_loss=0.07546]\n",
      "Step 507451  [5.393 sec/step, loss=0.07482, avg_loss=0.07553]\n",
      "Step 507452  [5.395 sec/step, loss=0.07534, avg_loss=0.07550]\n",
      "Step 507453  [5.400 sec/step, loss=0.07721, avg_loss=0.07552]\n",
      "Step 507454  [5.380 sec/step, loss=0.07673, avg_loss=0.07552]\n",
      "Step 507455  [5.376 sec/step, loss=0.07596, avg_loss=0.07551]\n",
      "Step 507456  [5.356 sec/step, loss=0.07544, avg_loss=0.07548]\n",
      "Step 507457  [5.401 sec/step, loss=0.06745, avg_loss=0.07541]\n",
      "Step 507458  [5.418 sec/step, loss=0.07774, avg_loss=0.07544]\n",
      "Step 507459  [5.376 sec/step, loss=0.06810, avg_loss=0.07538]\n",
      "Step 507460  [5.382 sec/step, loss=0.07625, avg_loss=0.07537]\n",
      "Step 507461  [5.386 sec/step, loss=0.07542, avg_loss=0.07536]\n",
      "Step 507462  [5.373 sec/step, loss=0.07582, avg_loss=0.07535]\n",
      "Generated 32 batches of size 32 in 2.449 sec\n",
      "Step 507463  [5.404 sec/step, loss=0.07690, avg_loss=0.07544]\n",
      "Step 507464  [5.398 sec/step, loss=0.07756, avg_loss=0.07547]\n",
      "Step 507465  [5.391 sec/step, loss=0.07655, avg_loss=0.07546]\n",
      "Step 507466  [5.385 sec/step, loss=0.07310, avg_loss=0.07544]\n",
      "Step 507467  [5.384 sec/step, loss=0.07520, avg_loss=0.07543]\n",
      "Step 507468  [5.372 sec/step, loss=0.07483, avg_loss=0.07542]\n",
      "Step 507469  [5.375 sec/step, loss=0.07787, avg_loss=0.07542]\n",
      "Step 507470  [5.375 sec/step, loss=0.07728, avg_loss=0.07541]\n",
      "Step 507471  [5.363 sec/step, loss=0.07340, avg_loss=0.07538]\n",
      "Step 507472  [5.367 sec/step, loss=0.07428, avg_loss=0.07536]\n",
      "Step 507473  [5.368 sec/step, loss=0.07588, avg_loss=0.07538]\n",
      "Step 507474  [5.370 sec/step, loss=0.07598, avg_loss=0.07538]\n",
      "Step 507475  [5.381 sec/step, loss=0.07592, avg_loss=0.07538]\n",
      "Step 507476  [5.372 sec/step, loss=0.07550, avg_loss=0.07537]\n",
      "Step 507477  [5.369 sec/step, loss=0.07609, avg_loss=0.07537]\n",
      "Step 507478  [5.369 sec/step, loss=0.07700, avg_loss=0.07538]\n",
      "Step 507479  [5.356 sec/step, loss=0.07467, avg_loss=0.07535]\n",
      "Step 507480  [5.344 sec/step, loss=0.07174, avg_loss=0.07529]\n",
      "Step 507481  [5.340 sec/step, loss=0.07361, avg_loss=0.07527]\n",
      "Step 507482  [5.355 sec/step, loss=0.07451, avg_loss=0.07527]\n",
      "Step 507483  [5.368 sec/step, loss=0.07754, avg_loss=0.07528]\n",
      "Step 507484  [5.372 sec/step, loss=0.07764, avg_loss=0.07529]\n",
      "Step 507485  [5.351 sec/step, loss=0.07239, avg_loss=0.07522]\n",
      "Step 507486  [5.376 sec/step, loss=0.07447, avg_loss=0.07520]\n",
      "Step 507487  [5.357 sec/step, loss=0.07514, avg_loss=0.07520]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 507488  [5.360 sec/step, loss=0.07686, avg_loss=0.07521]\n",
      "Step 507489  [5.360 sec/step, loss=0.07511, avg_loss=0.07518]\n",
      "Step 507490  [5.312 sec/step, loss=0.07388, avg_loss=0.07525]\n",
      "Step 507491  [5.306 sec/step, loss=0.07779, avg_loss=0.07527]\n",
      "Step 507492  [5.317 sec/step, loss=0.07534, avg_loss=0.07528]\n",
      "Step 507493  [5.377 sec/step, loss=0.06713, avg_loss=0.07521]\n",
      "Step 507494  [5.407 sec/step, loss=0.07790, avg_loss=0.07531]\n",
      "Generated 32 batches of size 32 in 2.423 sec\n",
      "Step 507495  [5.412 sec/step, loss=0.07476, avg_loss=0.07531]\n",
      "Step 507496  [5.429 sec/step, loss=0.07725, avg_loss=0.07532]\n",
      "Step 507497  [5.414 sec/step, loss=0.07250, avg_loss=0.07529]\n",
      "Step 507498  [5.404 sec/step, loss=0.06709, avg_loss=0.07520]\n",
      "Step 507499  [5.393 sec/step, loss=0.07591, avg_loss=0.07523]\n",
      "Step 507500  [5.371 sec/step, loss=0.07446, avg_loss=0.07520]\n",
      "Writing summary at step: 507500\n",
      "Step 507501  [5.375 sec/step, loss=0.07673, avg_loss=0.07525]\n",
      "Step 507502  [5.380 sec/step, loss=0.07581, avg_loss=0.07525]\n",
      "Step 507503  [5.391 sec/step, loss=0.07734, avg_loss=0.07526]\n",
      "Step 507504  [5.371 sec/step, loss=0.07322, avg_loss=0.07522]\n",
      "Step 507505  [5.364 sec/step, loss=0.07544, avg_loss=0.07519]\n",
      "Step 507506  [5.373 sec/step, loss=0.07687, avg_loss=0.07521]\n",
      "Step 507507  [5.366 sec/step, loss=0.07651, avg_loss=0.07520]\n",
      "Step 507508  [5.364 sec/step, loss=0.07571, avg_loss=0.07518]\n",
      "Step 507509  [5.358 sec/step, loss=0.07505, avg_loss=0.07518]\n",
      "Step 507510  [5.376 sec/step, loss=0.07513, avg_loss=0.07516]\n",
      "Step 507511  [5.365 sec/step, loss=0.07544, avg_loss=0.07514]\n",
      "Step 507512  [5.370 sec/step, loss=0.07358, avg_loss=0.07514]\n",
      "Step 507513  [5.382 sec/step, loss=0.07741, avg_loss=0.07519]\n",
      "Step 507514  [5.400 sec/step, loss=0.07668, avg_loss=0.07520]\n",
      "Step 507515  [5.381 sec/step, loss=0.06557, avg_loss=0.07508]\n",
      "Step 507516  [5.386 sec/step, loss=0.07344, avg_loss=0.07506]\n",
      "Step 507517  [5.339 sec/step, loss=0.07755, avg_loss=0.07516]\n",
      "Step 507518  [5.344 sec/step, loss=0.07701, avg_loss=0.07518]\n",
      "Step 507519  [5.329 sec/step, loss=0.07534, avg_loss=0.07518]\n",
      "Step 507520  [5.328 sec/step, loss=0.07772, avg_loss=0.07520]\n",
      "Step 507521  [5.346 sec/step, loss=0.07645, avg_loss=0.07524]\n",
      "Step 507522  [5.351 sec/step, loss=0.07642, avg_loss=0.07524]\n",
      "Step 507523  [5.321 sec/step, loss=0.07652, avg_loss=0.07526]\n",
      "Step 507524  [5.344 sec/step, loss=0.07759, avg_loss=0.07528]\n",
      "Step 507525  [5.342 sec/step, loss=0.07497, avg_loss=0.07525]\n",
      "Generated 32 batches of size 32 in 2.585 sec\n",
      "Step 507526  [5.344 sec/step, loss=0.07422, avg_loss=0.07522]\n",
      "Step 507527  [5.344 sec/step, loss=0.07248, avg_loss=0.07519]\n",
      "Step 507528  [5.372 sec/step, loss=0.07620, avg_loss=0.07524]\n",
      "Step 507529  [5.360 sec/step, loss=0.07661, avg_loss=0.07522]\n",
      "Step 507530  [5.356 sec/step, loss=0.07493, avg_loss=0.07522]\n",
      "Step 507531  [5.349 sec/step, loss=0.07723, avg_loss=0.07521]\n",
      "Step 507532  [5.355 sec/step, loss=0.07574, avg_loss=0.07520]\n",
      "Step 507533  [5.337 sec/step, loss=0.07517, avg_loss=0.07519]\n",
      "Step 507534  [5.365 sec/step, loss=0.06671, avg_loss=0.07511]\n",
      "Step 507535  [5.357 sec/step, loss=0.07356, avg_loss=0.07509]\n",
      "Step 507536  [5.335 sec/step, loss=0.07538, avg_loss=0.07507]\n",
      "Step 507537  [5.337 sec/step, loss=0.07746, avg_loss=0.07509]\n",
      "Step 507538  [5.362 sec/step, loss=0.07790, avg_loss=0.07520]\n",
      "Step 507539  [5.377 sec/step, loss=0.07665, avg_loss=0.07520]\n",
      "Step 507540  [5.374 sec/step, loss=0.07710, avg_loss=0.07520]\n",
      "Step 507541  [5.333 sec/step, loss=0.07408, avg_loss=0.07520]\n",
      "Step 507542  [5.335 sec/step, loss=0.07774, avg_loss=0.07523]\n",
      "Step 507543  [5.347 sec/step, loss=0.07637, avg_loss=0.07524]\n",
      "Step 507544  [5.333 sec/step, loss=0.07661, avg_loss=0.07523]\n",
      "Step 507545  [5.333 sec/step, loss=0.07586, avg_loss=0.07526]\n",
      "Step 507546  [5.331 sec/step, loss=0.07372, avg_loss=0.07522]\n",
      "Step 507547  [5.363 sec/step, loss=0.06853, avg_loss=0.07514]\n",
      "Step 507548  [5.363 sec/step, loss=0.07264, avg_loss=0.07512]\n",
      "Step 507549  [5.378 sec/step, loss=0.07528, avg_loss=0.07512]\n",
      "Step 507550  [5.389 sec/step, loss=0.07738, avg_loss=0.07512]\n",
      "Step 507551  [5.395 sec/step, loss=0.07747, avg_loss=0.07515]\n",
      "Step 507552  [5.395 sec/step, loss=0.07754, avg_loss=0.07517]\n",
      "Step 507553  [5.382 sec/step, loss=0.07602, avg_loss=0.07516]\n",
      "Step 507554  [5.369 sec/step, loss=0.06792, avg_loss=0.07507]\n",
      "Step 507555  [5.397 sec/step, loss=0.07432, avg_loss=0.07506]\n",
      "Step 507556  [5.404 sec/step, loss=0.07530, avg_loss=0.07505]\n",
      "Step 507557  [5.370 sec/step, loss=0.07665, avg_loss=0.07515]\n",
      "Generated 32 batches of size 32 in 2.539 sec\n",
      "Step 507558  [5.356 sec/step, loss=0.07378, avg_loss=0.07511]\n",
      "Step 507559  [5.376 sec/step, loss=0.07627, avg_loss=0.07519]\n",
      "Step 507560  [5.372 sec/step, loss=0.07516, avg_loss=0.07518]\n",
      "Step 507561  [5.362 sec/step, loss=0.07498, avg_loss=0.07517]\n",
      "Step 507562  [5.365 sec/step, loss=0.07617, avg_loss=0.07518]\n",
      "Step 507563  [5.342 sec/step, loss=0.07123, avg_loss=0.07512]\n",
      "Step 507564  [5.328 sec/step, loss=0.07501, avg_loss=0.07509]\n",
      "Step 507565  [5.329 sec/step, loss=0.07544, avg_loss=0.07508]\n",
      "Step 507566  [5.339 sec/step, loss=0.07620, avg_loss=0.07511]\n",
      "Step 507567  [5.345 sec/step, loss=0.07490, avg_loss=0.07511]\n",
      "Step 507568  [5.342 sec/step, loss=0.07636, avg_loss=0.07513]\n",
      "Step 507569  [5.338 sec/step, loss=0.07776, avg_loss=0.07513]\n",
      "Step 507570  [5.375 sec/step, loss=0.06689, avg_loss=0.07502]\n",
      "Step 507571  [5.387 sec/step, loss=0.07211, avg_loss=0.07501]\n",
      "Step 507572  [5.366 sec/step, loss=0.06754, avg_loss=0.07494]\n",
      "Step 507573  [5.378 sec/step, loss=0.07449, avg_loss=0.07493]\n",
      "Step 507574  [5.376 sec/step, loss=0.07565, avg_loss=0.07492]\n",
      "Step 507575  [5.366 sec/step, loss=0.07724, avg_loss=0.07494]\n",
      "Step 507576  [5.368 sec/step, loss=0.07632, avg_loss=0.07495]\n",
      "Step 507577  [5.368 sec/step, loss=0.07393, avg_loss=0.07492]\n",
      "Step 507578  [5.364 sec/step, loss=0.07538, avg_loss=0.07491]\n",
      "Step 507579  [5.355 sec/step, loss=0.07301, avg_loss=0.07489]\n",
      "Step 507580  [5.363 sec/step, loss=0.07422, avg_loss=0.07492]\n",
      "Step 507581  [5.373 sec/step, loss=0.07705, avg_loss=0.07495]\n",
      "Step 507582  [5.355 sec/step, loss=0.07580, avg_loss=0.07496]\n",
      "Step 507583  [5.350 sec/step, loss=0.07604, avg_loss=0.07495]\n",
      "Step 507584  [5.331 sec/step, loss=0.07196, avg_loss=0.07489]\n",
      "Step 507585  [5.346 sec/step, loss=0.07645, avg_loss=0.07493]\n",
      "Step 507586  [5.312 sec/step, loss=0.07453, avg_loss=0.07493]\n",
      "Step 507587  [5.304 sec/step, loss=0.07700, avg_loss=0.07495]\n",
      "Step 507588  [5.319 sec/step, loss=0.07723, avg_loss=0.07496]\n",
      "Step 507589  [5.321 sec/step, loss=0.07729, avg_loss=0.07498]\n",
      "Generated 32 batches of size 32 in 2.450 sec\n",
      "Step 507590  [5.324 sec/step, loss=0.07473, avg_loss=0.07499]\n",
      "Step 507591  [5.328 sec/step, loss=0.07518, avg_loss=0.07496]\n",
      "Step 507592  [5.356 sec/step, loss=0.07379, avg_loss=0.07494]\n",
      "Step 507593  [5.302 sec/step, loss=0.07502, avg_loss=0.07502]\n",
      "Step 507594  [5.296 sec/step, loss=0.07542, avg_loss=0.07500]\n",
      "Step 507595  [5.275 sec/step, loss=0.07640, avg_loss=0.07501]\n",
      "Step 507596  [5.274 sec/step, loss=0.07709, avg_loss=0.07501]\n",
      "Step 507597  [5.288 sec/step, loss=0.07786, avg_loss=0.07507]\n",
      "Step 507598  [5.312 sec/step, loss=0.07782, avg_loss=0.07517]\n",
      "Step 507599  [5.313 sec/step, loss=0.07494, avg_loss=0.07516]\n",
      "Step 507600  [5.334 sec/step, loss=0.07791, avg_loss=0.07520]\n",
      "Writing summary at step: 507600\n",
      "Step 507601  [5.337 sec/step, loss=0.07712, avg_loss=0.07520]\n",
      "Step 507602  [5.347 sec/step, loss=0.07749, avg_loss=0.07522]\n",
      "Step 507603  [5.343 sec/step, loss=0.07621, avg_loss=0.07521]\n",
      "Step 507604  [5.353 sec/step, loss=0.07677, avg_loss=0.07524]\n",
      "Step 507605  [5.350 sec/step, loss=0.07624, avg_loss=0.07525]\n",
      "Step 507606  [5.357 sec/step, loss=0.07721, avg_loss=0.07525]\n",
      "Step 507607  [5.345 sec/step, loss=0.07484, avg_loss=0.07524]\n",
      "Step 507608  [5.396 sec/step, loss=0.06805, avg_loss=0.07516]\n",
      "Step 507609  [5.404 sec/step, loss=0.07714, avg_loss=0.07518]\n",
      "Step 507610  [5.384 sec/step, loss=0.07564, avg_loss=0.07519]\n",
      "Step 507611  [5.386 sec/step, loss=0.07358, avg_loss=0.07517]\n",
      "Step 507612  [5.396 sec/step, loss=0.07506, avg_loss=0.07518]\n",
      "Step 507613  [5.391 sec/step, loss=0.07639, avg_loss=0.07517]\n",
      "Step 507614  [5.366 sec/step, loss=0.07635, avg_loss=0.07517]\n",
      "Step 507615  [5.392 sec/step, loss=0.07712, avg_loss=0.07529]\n",
      "Step 507616  [5.386 sec/step, loss=0.07289, avg_loss=0.07528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 507617  [5.367 sec/step, loss=0.06730, avg_loss=0.07518]\n",
      "Step 507618  [5.352 sec/step, loss=0.07248, avg_loss=0.07513]\n",
      "Step 507619  [5.365 sec/step, loss=0.07350, avg_loss=0.07511]\n",
      "Step 507620  [5.371 sec/step, loss=0.07463, avg_loss=0.07508]\n",
      "Generated 32 batches of size 32 in 2.460 sec\n",
      "Step 507621  [5.398 sec/step, loss=0.07553, avg_loss=0.07507]\n",
      "Step 507622  [5.395 sec/step, loss=0.07338, avg_loss=0.07504]\n",
      "Step 507623  [5.399 sec/step, loss=0.07650, avg_loss=0.07504]\n",
      "Step 507624  [5.392 sec/step, loss=0.07702, avg_loss=0.07504]\n",
      "Step 507625  [5.395 sec/step, loss=0.07784, avg_loss=0.07507]\n",
      "Step 507626  [5.395 sec/step, loss=0.07483, avg_loss=0.07507]\n",
      "Step 507627  [5.398 sec/step, loss=0.07150, avg_loss=0.07506]\n",
      "Step 507628  [5.379 sec/step, loss=0.07680, avg_loss=0.07507]\n",
      "Step 507629  [5.379 sec/step, loss=0.07517, avg_loss=0.07505]\n",
      "Step 507630  [5.393 sec/step, loss=0.07753, avg_loss=0.07508]\n",
      "Step 507631  [5.369 sec/step, loss=0.06608, avg_loss=0.07497]\n",
      "Step 507632  [5.370 sec/step, loss=0.07746, avg_loss=0.07499]\n",
      "Step 507633  [5.374 sec/step, loss=0.07264, avg_loss=0.07496]\n",
      "Step 507634  [5.316 sec/step, loss=0.07496, avg_loss=0.07504]\n",
      "Step 507635  [5.308 sec/step, loss=0.07504, avg_loss=0.07506]\n",
      "Step 507636  [5.310 sec/step, loss=0.07359, avg_loss=0.07504]\n",
      "Step 507637  [5.306 sec/step, loss=0.07537, avg_loss=0.07502]\n",
      "Step 507638  [5.315 sec/step, loss=0.07698, avg_loss=0.07501]\n",
      "Step 507639  [5.307 sec/step, loss=0.07746, avg_loss=0.07502]\n",
      "Step 507640  [5.312 sec/step, loss=0.07681, avg_loss=0.07501]\n",
      "Step 507641  [5.331 sec/step, loss=0.07614, avg_loss=0.07504]\n",
      "Step 507642  [5.331 sec/step, loss=0.07640, avg_loss=0.07502]\n",
      "Step 507643  [5.327 sec/step, loss=0.07297, avg_loss=0.07499]\n",
      "Step 507644  [5.333 sec/step, loss=0.07651, avg_loss=0.07499]\n",
      "Step 507645  [5.376 sec/step, loss=0.06685, avg_loss=0.07490]\n",
      "Step 507646  [5.372 sec/step, loss=0.07660, avg_loss=0.07493]\n",
      "Step 507647  [5.331 sec/step, loss=0.07779, avg_loss=0.07502]\n",
      "Step 507648  [5.333 sec/step, loss=0.07628, avg_loss=0.07505]\n",
      "Step 507649  [5.349 sec/step, loss=0.07587, avg_loss=0.07506]\n",
      "Step 507650  [5.349 sec/step, loss=0.07689, avg_loss=0.07506]\n",
      "Step 507651  [5.353 sec/step, loss=0.07641, avg_loss=0.07505]\n",
      "Step 507652  [5.343 sec/step, loss=0.07613, avg_loss=0.07503]\n",
      "Generated 32 batches of size 32 in 2.383 sec\n",
      "Step 507653  [5.361 sec/step, loss=0.07579, avg_loss=0.07503]\n",
      "Step 507654  [5.369 sec/step, loss=0.07467, avg_loss=0.07510]\n",
      "Step 507655  [5.333 sec/step, loss=0.07549, avg_loss=0.07511]\n",
      "Step 507656  [5.326 sec/step, loss=0.07153, avg_loss=0.07507]\n",
      "Step 507657  [5.310 sec/step, loss=0.07503, avg_loss=0.07505]\n",
      "Step 507658  [5.328 sec/step, loss=0.07755, avg_loss=0.07509]\n",
      "Step 507659  [5.329 sec/step, loss=0.07564, avg_loss=0.07509]\n",
      "Step 507660  [5.304 sec/step, loss=0.07381, avg_loss=0.07507]\n",
      "Step 507661  [5.312 sec/step, loss=0.07686, avg_loss=0.07509]\n",
      "Step 507662  [5.312 sec/step, loss=0.07591, avg_loss=0.07509]\n",
      "Step 507663  [5.333 sec/step, loss=0.07770, avg_loss=0.07515]\n",
      "Step 507664  [5.338 sec/step, loss=0.07487, avg_loss=0.07515]\n",
      "Step 507665  [5.329 sec/step, loss=0.07189, avg_loss=0.07512]\n",
      "Step 507666  [5.327 sec/step, loss=0.07663, avg_loss=0.07512]\n",
      "Step 507667  [5.310 sec/step, loss=0.06714, avg_loss=0.07504]\n",
      "Step 507668  [5.310 sec/step, loss=0.07547, avg_loss=0.07503]\n",
      "Step 507669  [5.312 sec/step, loss=0.07449, avg_loss=0.07500]\n",
      "Step 507670  [5.263 sec/step, loss=0.07665, avg_loss=0.07510]\n",
      "Step 507671  [5.255 sec/step, loss=0.07520, avg_loss=0.07513]\n",
      "Step 507672  [5.268 sec/step, loss=0.07369, avg_loss=0.07519]\n",
      "Step 507673  [5.274 sec/step, loss=0.07671, avg_loss=0.07521]\n",
      "Step 507674  [5.293 sec/step, loss=0.07776, avg_loss=0.07523]\n",
      "Step 507675  [5.288 sec/step, loss=0.07524, avg_loss=0.07521]\n",
      "Step 507676  [5.304 sec/step, loss=0.07701, avg_loss=0.07522]\n",
      "Step 507677  [5.305 sec/step, loss=0.07616, avg_loss=0.07524]\n",
      "Step 507678  [5.305 sec/step, loss=0.07657, avg_loss=0.07526]\n",
      "Step 507679  [5.326 sec/step, loss=0.07558, avg_loss=0.07528]\n",
      "Step 507680  [5.329 sec/step, loss=0.07659, avg_loss=0.07530]\n",
      "Step 507681  [5.328 sec/step, loss=0.07783, avg_loss=0.07531]\n",
      "Step 507682  [5.342 sec/step, loss=0.07597, avg_loss=0.07531]\n",
      "Step 507683  [5.349 sec/step, loss=0.07733, avg_loss=0.07533]\n",
      "Step 507684  [5.346 sec/step, loss=0.07256, avg_loss=0.07533]\n",
      "Generated 32 batches of size 32 in 2.383 sec\n",
      "Step 507685  [5.361 sec/step, loss=0.07742, avg_loss=0.07534]\n",
      "Step 507686  [5.361 sec/step, loss=0.07437, avg_loss=0.07534]\n",
      "Step 507687  [5.361 sec/step, loss=0.07561, avg_loss=0.07533]\n",
      "Step 507688  [5.350 sec/step, loss=0.07683, avg_loss=0.07532]\n",
      "Step 507689  [5.331 sec/step, loss=0.07470, avg_loss=0.07530]\n",
      "Step 507690  [5.325 sec/step, loss=0.07564, avg_loss=0.07531]\n",
      "Step 507691  [5.317 sec/step, loss=0.07553, avg_loss=0.07531]\n",
      "Step 507692  [5.316 sec/step, loss=0.07366, avg_loss=0.07531]\n",
      "Step 507693  [5.370 sec/step, loss=0.06773, avg_loss=0.07524]\n",
      "Step 507694  [5.358 sec/step, loss=0.07249, avg_loss=0.07521]\n",
      "Step 507695  [5.336 sec/step, loss=0.06730, avg_loss=0.07512]\n",
      "Step 507696  [5.345 sec/step, loss=0.07390, avg_loss=0.07508]\n",
      "Step 507697  [5.346 sec/step, loss=0.07741, avg_loss=0.07508]\n",
      "Step 507698  [5.342 sec/step, loss=0.07663, avg_loss=0.07507]\n",
      "Step 507699  [5.346 sec/step, loss=0.07650, avg_loss=0.07508]\n",
      "Step 507700  [5.334 sec/step, loss=0.07678, avg_loss=0.07507]\n",
      "Writing summary at step: 507700\n",
      "Step 507701  [5.328 sec/step, loss=0.07531, avg_loss=0.07505]\n",
      "Step 507702  [5.322 sec/step, loss=0.07547, avg_loss=0.07503]\n",
      "Step 507703  [5.323 sec/step, loss=0.07441, avg_loss=0.07502]\n",
      "Step 507704  [5.325 sec/step, loss=0.07464, avg_loss=0.07499]\n",
      "Step 507705  [5.311 sec/step, loss=0.07550, avg_loss=0.07499]\n",
      "Step 507706  [5.300 sec/step, loss=0.07694, avg_loss=0.07498]\n",
      "Step 507707  [5.356 sec/step, loss=0.06722, avg_loss=0.07491]\n",
      "Step 507708  [5.319 sec/step, loss=0.07777, avg_loss=0.07500]\n",
      "Step 507709  [5.326 sec/step, loss=0.07546, avg_loss=0.07499]\n",
      "Step 507710  [5.347 sec/step, loss=0.07787, avg_loss=0.07501]\n",
      "Step 507711  [5.339 sec/step, loss=0.07508, avg_loss=0.07503]\n",
      "Step 507712  [5.335 sec/step, loss=0.07725, avg_loss=0.07505]\n",
      "Step 507713  [5.328 sec/step, loss=0.07470, avg_loss=0.07503]\n",
      "Step 507714  [5.315 sec/step, loss=0.07319, avg_loss=0.07500]\n",
      "Step 507715  [5.332 sec/step, loss=0.07468, avg_loss=0.07497]\n",
      "Generated 32 batches of size 32 in 2.370 sec\n",
      "Step 507716  [5.359 sec/step, loss=0.07513, avg_loss=0.07500]\n",
      "Step 507717  [5.365 sec/step, loss=0.07260, avg_loss=0.07505]\n",
      "Step 507718  [5.379 sec/step, loss=0.07639, avg_loss=0.07509]\n",
      "Step 507719  [5.368 sec/step, loss=0.07518, avg_loss=0.07511]\n",
      "Step 507720  [5.356 sec/step, loss=0.07605, avg_loss=0.07512]\n",
      "Step 507721  [5.335 sec/step, loss=0.07762, avg_loss=0.07514]\n",
      "Step 507722  [5.334 sec/step, loss=0.07687, avg_loss=0.07518]\n",
      "Step 507723  [5.328 sec/step, loss=0.07424, avg_loss=0.07515]\n",
      "Step 507724  [5.324 sec/step, loss=0.07660, avg_loss=0.07515]\n",
      "Step 507725  [5.306 sec/step, loss=0.07284, avg_loss=0.07510]\n",
      "Step 507726  [5.310 sec/step, loss=0.07661, avg_loss=0.07512]\n",
      "Step 507727  [5.308 sec/step, loss=0.07538, avg_loss=0.07516]\n",
      "Step 507728  [5.323 sec/step, loss=0.07740, avg_loss=0.07516]\n",
      "Step 507729  [5.339 sec/step, loss=0.07509, avg_loss=0.07516]\n",
      "Step 507730  [5.313 sec/step, loss=0.06689, avg_loss=0.07505]\n",
      "Step 507731  [5.335 sec/step, loss=0.07713, avg_loss=0.07516]\n",
      "Step 507732  [5.373 sec/step, loss=0.06751, avg_loss=0.07507]\n",
      "Step 507733  [5.378 sec/step, loss=0.07620, avg_loss=0.07510]\n",
      "Step 507734  [5.387 sec/step, loss=0.07723, avg_loss=0.07512]\n",
      "Step 507735  [5.387 sec/step, loss=0.07543, avg_loss=0.07513]\n",
      "Step 507736  [5.396 sec/step, loss=0.07395, avg_loss=0.07513]\n",
      "Step 507737  [5.404 sec/step, loss=0.07524, avg_loss=0.07513]\n",
      "Step 507738  [5.386 sec/step, loss=0.07264, avg_loss=0.07509]\n",
      "Step 507739  [5.374 sec/step, loss=0.07424, avg_loss=0.07505]\n",
      "Step 507740  [5.363 sec/step, loss=0.07528, avg_loss=0.07504]\n",
      "Step 507741  [5.357 sec/step, loss=0.07618, avg_loss=0.07504]\n",
      "Step 507742  [5.364 sec/step, loss=0.07749, avg_loss=0.07505]\n",
      "Step 507743  [5.361 sec/step, loss=0.07695, avg_loss=0.07509]\n",
      "Step 507744  [5.360 sec/step, loss=0.07395, avg_loss=0.07506]\n",
      "Step 507745  [5.313 sec/step, loss=0.07679, avg_loss=0.07516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 507746  [5.326 sec/step, loss=0.07647, avg_loss=0.07516]\n",
      "Step 507747  [5.311 sec/step, loss=0.07200, avg_loss=0.07510]\n",
      "Generated 32 batches of size 32 in 2.405 sec\n",
      "Step 507748  [5.332 sec/step, loss=0.07766, avg_loss=0.07512]\n",
      "Step 507749  [5.305 sec/step, loss=0.07613, avg_loss=0.07512]\n",
      "Step 507750  [5.310 sec/step, loss=0.07691, avg_loss=0.07512]\n",
      "Step 507751  [5.296 sec/step, loss=0.07552, avg_loss=0.07511]\n",
      "Step 507752  [5.302 sec/step, loss=0.07579, avg_loss=0.07511]\n",
      "Step 507753  [5.275 sec/step, loss=0.07255, avg_loss=0.07508]\n",
      "Step 507754  [5.309 sec/step, loss=0.07430, avg_loss=0.07507]\n",
      "Step 507755  [5.327 sec/step, loss=0.07694, avg_loss=0.07509]\n",
      "Step 507756  [5.350 sec/step, loss=0.07758, avg_loss=0.07515]\n",
      "Step 507757  [5.361 sec/step, loss=0.07503, avg_loss=0.07515]\n",
      "Step 507758  [5.349 sec/step, loss=0.07555, avg_loss=0.07513]\n",
      "Step 507759  [5.339 sec/step, loss=0.07406, avg_loss=0.07511]\n",
      "Step 507760  [5.338 sec/step, loss=0.07219, avg_loss=0.07510]\n",
      "Step 507761  [5.325 sec/step, loss=0.07162, avg_loss=0.07504]\n",
      "Step 507762  [5.330 sec/step, loss=0.07789, avg_loss=0.07506]\n",
      "Step 507763  [5.312 sec/step, loss=0.07493, avg_loss=0.07504]\n",
      "Step 507764  [5.307 sec/step, loss=0.07424, avg_loss=0.07503]\n",
      "Step 507765  [5.317 sec/step, loss=0.07699, avg_loss=0.07508]\n",
      "Step 507766  [5.317 sec/step, loss=0.07500, avg_loss=0.07506]\n",
      "Step 507767  [5.339 sec/step, loss=0.07811, avg_loss=0.07517]\n",
      "Step 507768  [5.389 sec/step, loss=0.06753, avg_loss=0.07509]\n",
      "Step 507769  [5.374 sec/step, loss=0.07621, avg_loss=0.07511]\n",
      "Step 507770  [5.397 sec/step, loss=0.07407, avg_loss=0.07509]\n",
      "Step 507771  [5.415 sec/step, loss=0.07735, avg_loss=0.07511]\n",
      "Step 507772  [5.440 sec/step, loss=0.07477, avg_loss=0.07512]\n",
      "Step 507773  [5.431 sec/step, loss=0.07669, avg_loss=0.07512]\n",
      "Step 507774  [5.431 sec/step, loss=0.07578, avg_loss=0.07510]\n",
      "Step 507775  [5.438 sec/step, loss=0.07578, avg_loss=0.07510]\n",
      "Step 507776  [5.439 sec/step, loss=0.07699, avg_loss=0.07510]\n",
      "Step 507777  [5.443 sec/step, loss=0.07689, avg_loss=0.07511]\n",
      "Step 507778  [5.455 sec/step, loss=0.07488, avg_loss=0.07509]\n",
      "Step 507779  [5.440 sec/step, loss=0.07494, avg_loss=0.07509]\n",
      "Generated 32 batches of size 32 in 2.338 sec\n",
      "Step 507780  [5.446 sec/step, loss=0.07599, avg_loss=0.07508]\n",
      "Step 507781  [5.434 sec/step, loss=0.07655, avg_loss=0.07507]\n",
      "Step 507782  [5.442 sec/step, loss=0.07723, avg_loss=0.07508]\n",
      "Step 507783  [5.413 sec/step, loss=0.06711, avg_loss=0.07498]\n",
      "Step 507784  [5.431 sec/step, loss=0.07573, avg_loss=0.07501]\n",
      "Step 507785  [5.413 sec/step, loss=0.07422, avg_loss=0.07498]\n",
      "Step 507786  [5.425 sec/step, loss=0.07565, avg_loss=0.07499]\n",
      "Step 507787  [5.416 sec/step, loss=0.07513, avg_loss=0.07499]\n",
      "Step 507788  [5.415 sec/step, loss=0.07635, avg_loss=0.07498]\n",
      "Step 507789  [5.429 sec/step, loss=0.07777, avg_loss=0.07501]\n",
      "Step 507790  [5.432 sec/step, loss=0.07683, avg_loss=0.07502]\n",
      "Step 507791  [5.440 sec/step, loss=0.07775, avg_loss=0.07505]\n",
      "Step 507792  [5.414 sec/step, loss=0.07573, avg_loss=0.07507]\n",
      "Step 507793  [5.364 sec/step, loss=0.07579, avg_loss=0.07515]\n",
      "Step 507794  [5.361 sec/step, loss=0.07506, avg_loss=0.07517]\n",
      "Step 507795  [5.385 sec/step, loss=0.07649, avg_loss=0.07527]\n",
      "Step 507796  [5.367 sec/step, loss=0.07628, avg_loss=0.07529]\n",
      "Step 507797  [5.348 sec/step, loss=0.07495, avg_loss=0.07526]\n",
      "Step 507798  [5.351 sec/step, loss=0.07538, avg_loss=0.07525]\n",
      "Step 507799  [5.354 sec/step, loss=0.07480, avg_loss=0.07523]\n",
      "Step 507800  [5.357 sec/step, loss=0.07627, avg_loss=0.07523]\n",
      "Writing summary at step: 507800\n",
      "Step 507801  [5.369 sec/step, loss=0.07754, avg_loss=0.07525]\n",
      "Step 507802  [5.383 sec/step, loss=0.07669, avg_loss=0.07526]\n",
      "Step 507803  [5.360 sec/step, loss=0.07404, avg_loss=0.07526]\n",
      "Step 507804  [5.358 sec/step, loss=0.07483, avg_loss=0.07526]\n",
      "Step 507805  [5.382 sec/step, loss=0.07691, avg_loss=0.07528]\n",
      "Step 507806  [5.371 sec/step, loss=0.07559, avg_loss=0.07526]\n",
      "Step 507807  [5.307 sec/step, loss=0.06781, avg_loss=0.07527]\n",
      "Step 507808  [5.301 sec/step, loss=0.07536, avg_loss=0.07525]\n",
      "Step 507809  [5.282 sec/step, loss=0.07304, avg_loss=0.07522]\n",
      "Step 507810  [5.265 sec/step, loss=0.07242, avg_loss=0.07517]\n",
      "Generated 32 batches of size 32 in 2.397 sec\n",
      "Step 507811  [5.272 sec/step, loss=0.07642, avg_loss=0.07518]\n",
      "Step 507812  [5.277 sec/step, loss=0.07654, avg_loss=0.07517]\n",
      "Step 507813  [5.276 sec/step, loss=0.07668, avg_loss=0.07519]\n",
      "Step 507814  [5.339 sec/step, loss=0.06647, avg_loss=0.07513]\n",
      "Step 507815  [5.308 sec/step, loss=0.07304, avg_loss=0.07511]\n",
      "Step 507816  [5.292 sec/step, loss=0.07648, avg_loss=0.07512]\n",
      "Step 507817  [5.328 sec/step, loss=0.07450, avg_loss=0.07514]\n",
      "Step 507818  [5.338 sec/step, loss=0.07453, avg_loss=0.07512]\n",
      "Step 507819  [5.354 sec/step, loss=0.07728, avg_loss=0.07514]\n",
      "Step 507820  [5.333 sec/step, loss=0.06755, avg_loss=0.07506]\n",
      "Step 507821  [5.333 sec/step, loss=0.07475, avg_loss=0.07503]\n",
      "Step 507822  [5.331 sec/step, loss=0.07665, avg_loss=0.07503]\n",
      "Step 507823  [5.358 sec/step, loss=0.07434, avg_loss=0.07503]\n",
      "Step 507824  [5.348 sec/step, loss=0.07456, avg_loss=0.07501]\n",
      "Step 507825  [5.365 sec/step, loss=0.07668, avg_loss=0.07505]\n",
      "Step 507826  [5.373 sec/step, loss=0.07493, avg_loss=0.07503]\n",
      "Step 507827  [5.380 sec/step, loss=0.07537, avg_loss=0.07503]\n",
      "Step 507828  [5.368 sec/step, loss=0.07682, avg_loss=0.07502]\n",
      "Step 507829  [5.364 sec/step, loss=0.07759, avg_loss=0.07505]\n",
      "Step 507830  [5.376 sec/step, loss=0.07668, avg_loss=0.07515]\n",
      "Step 507831  [5.419 sec/step, loss=0.06757, avg_loss=0.07505]\n",
      "Step 507832  [5.391 sec/step, loss=0.07464, avg_loss=0.07512]\n",
      "Step 507833  [5.391 sec/step, loss=0.07377, avg_loss=0.07510]\n",
      "Step 507834  [5.379 sec/step, loss=0.07317, avg_loss=0.07506]\n",
      "Step 507835  [5.388 sec/step, loss=0.07635, avg_loss=0.07507]\n",
      "Step 507836  [5.389 sec/step, loss=0.07656, avg_loss=0.07509]\n",
      "Step 507837  [5.368 sec/step, loss=0.07535, avg_loss=0.07509]\n",
      "Step 507838  [5.373 sec/step, loss=0.07367, avg_loss=0.07510]\n",
      "Step 507839  [5.373 sec/step, loss=0.07389, avg_loss=0.07510]\n",
      "Step 507840  [5.375 sec/step, loss=0.07091, avg_loss=0.07506]\n",
      "Step 507841  [5.389 sec/step, loss=0.07681, avg_loss=0.07506]\n",
      "Step 507842  [5.377 sec/step, loss=0.07497, avg_loss=0.07504]\n",
      "Generated 32 batches of size 32 in 2.774 sec\n",
      "Step 507843  [5.371 sec/step, loss=0.07339, avg_loss=0.07500]\n",
      "Step 507844  [5.383 sec/step, loss=0.07721, avg_loss=0.07504]\n",
      "Step 507845  [5.385 sec/step, loss=0.07616, avg_loss=0.07503]\n",
      "Step 507846  [5.374 sec/step, loss=0.07573, avg_loss=0.07502]\n",
      "Step 507847  [5.389 sec/step, loss=0.07734, avg_loss=0.07508]\n",
      "Step 507848  [5.363 sec/step, loss=0.07210, avg_loss=0.07502]\n",
      "Step 507849  [5.367 sec/step, loss=0.07664, avg_loss=0.07502]\n",
      "Step 507850  [5.361 sec/step, loss=0.07733, avg_loss=0.07503]\n",
      "Step 507851  [5.370 sec/step, loss=0.07760, avg_loss=0.07505]\n",
      "Step 507852  [5.364 sec/step, loss=0.07488, avg_loss=0.07504]\n",
      "Step 507853  [5.391 sec/step, loss=0.07777, avg_loss=0.07509]\n",
      "Step 507854  [5.373 sec/step, loss=0.07716, avg_loss=0.07512]\n",
      "Step 507855  [5.369 sec/step, loss=0.07592, avg_loss=0.07511]\n",
      "Step 507856  [5.362 sec/step, loss=0.07634, avg_loss=0.07510]\n",
      "Step 507857  [5.361 sec/step, loss=0.07774, avg_loss=0.07513]\n",
      "Step 507858  [5.348 sec/step, loss=0.07198, avg_loss=0.07509]\n",
      "Step 507859  [5.348 sec/step, loss=0.07476, avg_loss=0.07510]\n",
      "Step 507860  [5.371 sec/step, loss=0.07469, avg_loss=0.07512]\n",
      "Step 507861  [5.387 sec/step, loss=0.07435, avg_loss=0.07515]\n",
      "Step 507862  [5.379 sec/step, loss=0.07686, avg_loss=0.07514]\n",
      "Step 507863  [5.394 sec/step, loss=0.07682, avg_loss=0.07516]\n",
      "Step 507864  [5.406 sec/step, loss=0.07391, avg_loss=0.07515]\n",
      "Step 507865  [5.405 sec/step, loss=0.07620, avg_loss=0.07515]\n",
      "Step 507866  [5.396 sec/step, loss=0.07214, avg_loss=0.07512]\n",
      "Step 507867  [5.440 sec/step, loss=0.06657, avg_loss=0.07500]\n",
      "Step 507868  [5.383 sec/step, loss=0.07420, avg_loss=0.07507]\n",
      "Step 507869  [5.405 sec/step, loss=0.07708, avg_loss=0.07508]\n",
      "Step 507870  [5.362 sec/step, loss=0.06649, avg_loss=0.07500]\n",
      "Step 507871  [5.351 sec/step, loss=0.07682, avg_loss=0.07500]\n",
      "Step 507872  [5.325 sec/step, loss=0.07512, avg_loss=0.07500]\n",
      "Step 507873  [5.306 sec/step, loss=0.07512, avg_loss=0.07499]\n",
      "Step 507874  [5.299 sec/step, loss=0.07653, avg_loss=0.07499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 32 batches of size 32 in 2.351 sec\n",
      "Step 507875  [5.307 sec/step, loss=0.07560, avg_loss=0.07499]\n",
      "Step 507876  [5.303 sec/step, loss=0.07770, avg_loss=0.07500]\n",
      "Step 507877  [5.295 sec/step, loss=0.07509, avg_loss=0.07498]\n",
      "Step 507878  [5.290 sec/step, loss=0.07545, avg_loss=0.07499]\n",
      "Step 507879  [5.296 sec/step, loss=0.07385, avg_loss=0.07497]\n",
      "Step 507880  [5.315 sec/step, loss=0.07342, avg_loss=0.07495]\n",
      "Step 507881  [5.316 sec/step, loss=0.07581, avg_loss=0.07494]\n",
      "Step 507882  [5.297 sec/step, loss=0.07281, avg_loss=0.07490]\n",
      "Step 507883  [5.329 sec/step, loss=0.07595, avg_loss=0.07499]\n",
      "Step 507884  [5.333 sec/step, loss=0.07731, avg_loss=0.07500]\n",
      "Step 507885  [5.328 sec/step, loss=0.07126, avg_loss=0.07497]\n",
      "Step 507886  [5.375 sec/step, loss=0.06806, avg_loss=0.07490]\n",
      "Step 507887  [5.392 sec/step, loss=0.07465, avg_loss=0.07489]\n",
      "Step 507888  [5.388 sec/step, loss=0.07686, avg_loss=0.07490]\n",
      "Step 507889  [5.372 sec/step, loss=0.07268, avg_loss=0.07485]\n",
      "Step 507890  [5.370 sec/step, loss=0.07611, avg_loss=0.07484]\n",
      "Step 507891  [5.352 sec/step, loss=0.07523, avg_loss=0.07481]\n",
      "Step 507892  [5.357 sec/step, loss=0.07687, avg_loss=0.07482]\n",
      "Step 507893  [5.369 sec/step, loss=0.07694, avg_loss=0.07484]\n",
      "Step 507894  [5.385 sec/step, loss=0.07755, avg_loss=0.07486]\n",
      "Step 507895  [5.368 sec/step, loss=0.07320, avg_loss=0.07483]\n",
      "Step 507896  [5.372 sec/step, loss=0.07827, avg_loss=0.07485]\n",
      "Step 507897  [5.374 sec/step, loss=0.07514, avg_loss=0.07485]\n",
      "Step 507898  [5.396 sec/step, loss=0.07446, avg_loss=0.07484]\n",
      "Step 507899  [5.397 sec/step, loss=0.07722, avg_loss=0.07486]\n",
      "Step 507900  [5.408 sec/step, loss=0.07827, avg_loss=0.07488]\n",
      "Writing summary at step: 507900\n",
      "Step 507901  [5.405 sec/step, loss=0.07678, avg_loss=0.07488]\n",
      "Step 507902  [5.391 sec/step, loss=0.07371, avg_loss=0.07485]\n",
      "Step 507903  [5.389 sec/step, loss=0.06666, avg_loss=0.07477]\n",
      "Step 507904  [5.379 sec/step, loss=0.07414, avg_loss=0.07477]\n",
      "Step 507905  [5.364 sec/step, loss=0.07558, avg_loss=0.07475]\n",
      "Generated 32 batches of size 32 in 2.364 sec\n",
      "Step 507906  [5.390 sec/step, loss=0.07510, avg_loss=0.07475]\n",
      "Step 507907  [5.402 sec/step, loss=0.07510, avg_loss=0.07482]\n",
      "Step 507908  [5.392 sec/step, loss=0.07647, avg_loss=0.07483]\n",
      "Step 507909  [5.406 sec/step, loss=0.07588, avg_loss=0.07486]\n",
      "Step 507910  [5.412 sec/step, loss=0.07443, avg_loss=0.07488]\n",
      "Step 507911  [5.429 sec/step, loss=0.07635, avg_loss=0.07488]\n",
      "Step 507912  [5.416 sec/step, loss=0.07674, avg_loss=0.07488]\n",
      "Step 507913  [5.432 sec/step, loss=0.07492, avg_loss=0.07486]\n",
      "Step 507914  [5.381 sec/step, loss=0.07315, avg_loss=0.07493]\n",
      "Step 507915  [5.386 sec/step, loss=0.07538, avg_loss=0.07495]\n",
      "Step 507916  [5.393 sec/step, loss=0.07742, avg_loss=0.07496]\n",
      "Step 507917  [5.372 sec/step, loss=0.07580, avg_loss=0.07498]\n",
      "Step 507918  [5.353 sec/step, loss=0.07483, avg_loss=0.07498]\n",
      "Step 507919  [5.328 sec/step, loss=0.06741, avg_loss=0.07488]\n",
      "Step 507920  [5.348 sec/step, loss=0.07542, avg_loss=0.07496]\n",
      "Step 507921  [5.335 sec/step, loss=0.07675, avg_loss=0.07498]\n",
      "Step 507922  [5.342 sec/step, loss=0.07687, avg_loss=0.07498]\n",
      "Step 507923  [5.306 sec/step, loss=0.07210, avg_loss=0.07496]\n",
      "Step 507924  [5.325 sec/step, loss=0.07526, avg_loss=0.07497]\n",
      "Step 507925  [5.344 sec/step, loss=0.07469, avg_loss=0.07495]\n",
      "Step 507926  [5.332 sec/step, loss=0.07259, avg_loss=0.07492]\n",
      "Step 507927  [5.347 sec/step, loss=0.07742, avg_loss=0.07494]\n",
      "Step 507928  [5.347 sec/step, loss=0.07649, avg_loss=0.07494]\n",
      "Step 507929  [5.334 sec/step, loss=0.07622, avg_loss=0.07493]\n",
      "Step 507930  [5.328 sec/step, loss=0.07200, avg_loss=0.07488]\n",
      "Step 507931  [5.270 sec/step, loss=0.07542, avg_loss=0.07496]\n",
      "Step 507932  [5.257 sec/step, loss=0.07588, avg_loss=0.07497]\n",
      "Step 507933  [5.266 sec/step, loss=0.07497, avg_loss=0.07498]\n",
      "Step 507934  [5.290 sec/step, loss=0.07736, avg_loss=0.07503]\n",
      "Step 507935  [5.282 sec/step, loss=0.07383, avg_loss=0.07500]\n",
      "Step 507936  [5.327 sec/step, loss=0.06767, avg_loss=0.07491]\n",
      "Step 507937  [5.342 sec/step, loss=0.07755, avg_loss=0.07493]\n",
      "Generated 32 batches of size 32 in 2.388 sec\n",
      "Step 507938  [5.363 sec/step, loss=0.07503, avg_loss=0.07495]\n",
      "Step 507939  [5.361 sec/step, loss=0.07469, avg_loss=0.07495]\n",
      "Step 507940  [5.373 sec/step, loss=0.07599, avg_loss=0.07501]\n",
      "Step 507941  [5.365 sec/step, loss=0.07667, avg_loss=0.07500]\n",
      "Step 507942  [5.366 sec/step, loss=0.07519, avg_loss=0.07501]\n",
      "Step 507943  [5.364 sec/step, loss=0.07541, avg_loss=0.07503]\n",
      "Step 507944  [5.353 sec/step, loss=0.07676, avg_loss=0.07502]\n",
      "Step 507945  [5.349 sec/step, loss=0.07627, avg_loss=0.07502]\n",
      "Step 507946  [5.361 sec/step, loss=0.07837, avg_loss=0.07505]\n",
      "Step 507947  [5.350 sec/step, loss=0.07255, avg_loss=0.07500]\n",
      "Step 507948  [5.341 sec/step, loss=0.06595, avg_loss=0.07494]\n",
      "Step 507949  [5.347 sec/step, loss=0.07695, avg_loss=0.07494]\n",
      "Step 507950  [5.346 sec/step, loss=0.07775, avg_loss=0.07495]\n",
      "Step 507951  [5.333 sec/step, loss=0.07661, avg_loss=0.07494]\n",
      "Step 507952  [5.328 sec/step, loss=0.07521, avg_loss=0.07494]\n",
      "Step 507953  [5.305 sec/step, loss=0.07130, avg_loss=0.07488]\n",
      "Step 507954  [5.292 sec/step, loss=0.07631, avg_loss=0.07487]\n",
      "Step 507955  [5.292 sec/step, loss=0.07642, avg_loss=0.07487]\n",
      "Step 507956  [5.313 sec/step, loss=0.07468, avg_loss=0.07486]\n",
      "Step 507957  [5.309 sec/step, loss=0.07618, avg_loss=0.07484]\n",
      "Step 507958  [5.329 sec/step, loss=0.07602, avg_loss=0.07488]\n",
      "Step 507959  [5.338 sec/step, loss=0.07695, avg_loss=0.07490]\n",
      "Step 507960  [5.341 sec/step, loss=0.07668, avg_loss=0.07492]\n",
      "Step 507961  [5.353 sec/step, loss=0.07666, avg_loss=0.07495]\n",
      "Step 507962  [5.353 sec/step, loss=0.07634, avg_loss=0.07494]\n",
      "Step 507963  [5.337 sec/step, loss=0.07509, avg_loss=0.07492]\n",
      "Step 507964  [5.322 sec/step, loss=0.07538, avg_loss=0.07494]\n",
      "Step 507965  [5.335 sec/step, loss=0.07754, avg_loss=0.07495]\n",
      "Step 507966  [5.353 sec/step, loss=0.07678, avg_loss=0.07500]\n",
      "Step 507967  [5.304 sec/step, loss=0.07660, avg_loss=0.07510]\n",
      "Step 507968  [5.310 sec/step, loss=0.07444, avg_loss=0.07510]\n",
      "Step 507969  [5.302 sec/step, loss=0.07521, avg_loss=0.07508]\n",
      "Generated 32 batches of size 32 in 2.675 sec\n",
      "Step 507970  [5.311 sec/step, loss=0.07297, avg_loss=0.07515]\n",
      "Step 507971  [5.309 sec/step, loss=0.07340, avg_loss=0.07511]\n",
      "Step 507972  [5.322 sec/step, loss=0.07729, avg_loss=0.07513]\n",
      "Step 507973  [5.346 sec/step, loss=0.07717, avg_loss=0.07515]\n",
      "Step 507974  [5.336 sec/step, loss=0.07428, avg_loss=0.07513]\n",
      "Step 507975  [5.330 sec/step, loss=0.07295, avg_loss=0.07511]\n",
      "Step 507976  [5.325 sec/step, loss=0.07706, avg_loss=0.07510]\n",
      "Step 507977  [5.382 sec/step, loss=0.06798, avg_loss=0.07503]\n",
      "Step 507978  [5.374 sec/step, loss=0.07593, avg_loss=0.07503]\n",
      "Step 507979  [5.371 sec/step, loss=0.07485, avg_loss=0.07504]\n",
      "Step 507980  [5.329 sec/step, loss=0.06749, avg_loss=0.07498]\n",
      "Step 507981  [5.326 sec/step, loss=0.07400, avg_loss=0.07497]\n",
      "Step 507982  [5.327 sec/step, loss=0.07241, avg_loss=0.07496]\n",
      "Step 507983  [5.316 sec/step, loss=0.07666, avg_loss=0.07497]\n",
      "Step 507984  [5.307 sec/step, loss=0.07611, avg_loss=0.07496]\n",
      "Step 507985  [5.314 sec/step, loss=0.07722, avg_loss=0.07502]\n",
      "Step 507986  [5.267 sec/step, loss=0.07590, avg_loss=0.07509]\n",
      "Step 507987  [5.268 sec/step, loss=0.07734, avg_loss=0.07512]\n",
      "Step 507988  [5.283 sec/step, loss=0.07656, avg_loss=0.07512]\n",
      "Step 507989  [5.292 sec/step, loss=0.07748, avg_loss=0.07517]\n",
      "Step 507990  [5.297 sec/step, loss=0.07500, avg_loss=0.07516]\n",
      "Step 507991  [5.318 sec/step, loss=0.07741, avg_loss=0.07518]\n",
      "Step 507992  [5.327 sec/step, loss=0.07733, avg_loss=0.07518]\n",
      "Step 507993  [5.307 sec/step, loss=0.07548, avg_loss=0.07517]\n",
      "Step 507994  [5.290 sec/step, loss=0.07216, avg_loss=0.07511]\n",
      "Step 507995  [5.299 sec/step, loss=0.07571, avg_loss=0.07514]\n",
      "Step 507996  [5.295 sec/step, loss=0.07410, avg_loss=0.07510]\n",
      "Step 507997  [5.311 sec/step, loss=0.07779, avg_loss=0.07512]\n",
      "Step 507998  [5.334 sec/step, loss=0.06624, avg_loss=0.07504]\n",
      "Step 507999  [5.342 sec/step, loss=0.07776, avg_loss=0.07505]\n",
      "Step 508000  [5.326 sec/step, loss=0.07477, avg_loss=0.07501]\n",
      "Writing summary at step: 508000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-508000\n",
      "Saving audio and alignment...\n",
      "Generated 32 batches of size 32 in 1.852 sec\n",
      "Input: aauut aaf vay taraansfar kii laagatd laend salaaiiding kii marhuun hoo gii~________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 508001  [5.328 sec/step, loss=0.07742, avg_loss=0.07502]\n",
      "Step 508002  [5.331 sec/step, loss=0.07755, avg_loss=0.07506]\n",
      "Step 508003  [5.356 sec/step, loss=0.07465, avg_loss=0.07514]\n",
      "Step 508004  [5.356 sec/step, loss=0.07358, avg_loss=0.07513]\n",
      "Step 508005  [5.350 sec/step, loss=0.07637, avg_loss=0.07514]\n",
      "Step 508006  [5.326 sec/step, loss=0.07565, avg_loss=0.07514]\n",
      "Step 508007  [5.329 sec/step, loss=0.07623, avg_loss=0.07515]\n",
      "Step 508008  [5.355 sec/step, loss=0.07627, avg_loss=0.07515]\n",
      "Step 508009  [5.344 sec/step, loss=0.07522, avg_loss=0.07515]\n",
      "Step 508010  [5.340 sec/step, loss=0.07566, avg_loss=0.07516]\n",
      "Step 508011  [5.322 sec/step, loss=0.07725, avg_loss=0.07517]\n",
      "Step 508012  [5.330 sec/step, loss=0.07760, avg_loss=0.07518]\n",
      "Step 508013  [5.336 sec/step, loss=0.07652, avg_loss=0.07519]\n",
      "Step 508014  [5.347 sec/step, loss=0.07741, avg_loss=0.07523]\n",
      "Step 508015  [5.347 sec/step, loss=0.07615, avg_loss=0.07524]\n",
      "Step 508016  [5.363 sec/step, loss=0.07672, avg_loss=0.07524]\n",
      "Step 508017  [5.367 sec/step, loss=0.07719, avg_loss=0.07525]\n",
      "Step 508018  [5.372 sec/step, loss=0.07484, avg_loss=0.07525]\n",
      "Step 508019  [5.388 sec/step, loss=0.07324, avg_loss=0.07531]\n",
      "Step 508020  [5.397 sec/step, loss=0.07680, avg_loss=0.07532]\n",
      "Step 508021  [5.396 sec/step, loss=0.07633, avg_loss=0.07532]\n",
      "Step 508022  [5.405 sec/step, loss=0.07444, avg_loss=0.07529]\n",
      "Step 508023  [5.415 sec/step, loss=0.07451, avg_loss=0.07532]\n",
      "Step 508024  [5.395 sec/step, loss=0.07562, avg_loss=0.07532]\n",
      "Step 508025  [5.365 sec/step, loss=0.07612, avg_loss=0.07534]\n",
      "Step 508026  [5.357 sec/step, loss=0.07419, avg_loss=0.07535]\n",
      "Step 508027  [5.326 sec/step, loss=0.06719, avg_loss=0.07525]\n",
      "Step 508028  [5.328 sec/step, loss=0.07632, avg_loss=0.07525]\n",
      "Step 508029  [5.342 sec/step, loss=0.07513, avg_loss=0.07524]\n",
      "Step 508030  [5.357 sec/step, loss=0.07502, avg_loss=0.07527]\n",
      "Step 508031  [5.355 sec/step, loss=0.07210, avg_loss=0.07523]\n",
      "Generated 32 batches of size 32 in 2.359 sec\n",
      "Step 508032  [5.362 sec/step, loss=0.07733, avg_loss=0.07525]\n",
      "Step 508033  [5.342 sec/step, loss=0.07541, avg_loss=0.07525]\n",
      "Step 508034  [5.339 sec/step, loss=0.07767, avg_loss=0.07526]\n",
      "Step 508035  [5.343 sec/step, loss=0.07601, avg_loss=0.07528]\n",
      "Step 508036  [5.298 sec/step, loss=0.07662, avg_loss=0.07537]\n",
      "Step 508037  [5.293 sec/step, loss=0.07652, avg_loss=0.07536]\n",
      "Step 508038  [5.255 sec/step, loss=0.07340, avg_loss=0.07534]\n",
      "Step 508039  [5.265 sec/step, loss=0.07650, avg_loss=0.07536]\n",
      "Step 508040  [5.308 sec/step, loss=0.06691, avg_loss=0.07527]\n",
      "Step 508041  [5.309 sec/step, loss=0.07756, avg_loss=0.07528]\n",
      "Step 508042  [5.308 sec/step, loss=0.07610, avg_loss=0.07529]\n",
      "Step 508043  [5.304 sec/step, loss=0.07295, avg_loss=0.07526]\n",
      "Step 508044  [5.304 sec/step, loss=0.07310, avg_loss=0.07522]\n",
      "Step 508045  [5.300 sec/step, loss=0.07210, avg_loss=0.07518]\n",
      "Step 508046  [5.294 sec/step, loss=0.07688, avg_loss=0.07517]\n",
      "Step 508047  [5.290 sec/step, loss=0.07500, avg_loss=0.07519]\n",
      "Step 508048  [5.307 sec/step, loss=0.07432, avg_loss=0.07528]\n",
      "Step 508049  [5.311 sec/step, loss=0.07583, avg_loss=0.07526]\n",
      "Step 508050  [5.290 sec/step, loss=0.07531, avg_loss=0.07524]\n",
      "Step 508051  [5.302 sec/step, loss=0.07774, avg_loss=0.07525]\n",
      "Step 508052  [5.318 sec/step, loss=0.07717, avg_loss=0.07527]\n",
      "Step 508053  [5.312 sec/step, loss=0.06602, avg_loss=0.07522]\n",
      "Step 508054  [5.320 sec/step, loss=0.07397, avg_loss=0.07519]\n",
      "Step 508055  [5.323 sec/step, loss=0.07504, avg_loss=0.07518]\n",
      "Step 508056  [5.290 sec/step, loss=0.07415, avg_loss=0.07518]\n",
      "Step 508057  [5.291 sec/step, loss=0.07750, avg_loss=0.07519]\n",
      "Step 508058  [5.285 sec/step, loss=0.07627, avg_loss=0.07519]\n",
      "Step 508059  [5.305 sec/step, loss=0.07548, avg_loss=0.07518]\n",
      "Step 508060  [5.309 sec/step, loss=0.07468, avg_loss=0.07516]\n",
      "Step 508061  [5.305 sec/step, loss=0.07548, avg_loss=0.07514]\n",
      "Step 508062  [5.353 sec/step, loss=0.06743, avg_loss=0.07506]\n",
      "Step 508063  [5.355 sec/step, loss=0.07381, avg_loss=0.07504]\n",
      "Generated 32 batches of size 32 in 2.411 sec\n",
      "Step 508064  [5.366 sec/step, loss=0.07677, avg_loss=0.07506]\n",
      "Step 508065  [5.364 sec/step, loss=0.07786, avg_loss=0.07506]\n",
      "Step 508066  [5.360 sec/step, loss=0.07380, avg_loss=0.07503]\n",
      "Step 508067  [5.362 sec/step, loss=0.07578, avg_loss=0.07502]\n",
      "Step 508068  [5.370 sec/step, loss=0.07815, avg_loss=0.07506]\n",
      "Step 508069  [5.362 sec/step, loss=0.07682, avg_loss=0.07508]\n",
      "Step 508070  [5.361 sec/step, loss=0.07235, avg_loss=0.07507]\n",
      "Step 508071  [5.365 sec/step, loss=0.07553, avg_loss=0.07509]\n",
      "Step 508072  [5.346 sec/step, loss=0.07534, avg_loss=0.07507]\n",
      "Step 508073  [5.339 sec/step, loss=0.07745, avg_loss=0.07507]\n",
      "Step 508074  [5.360 sec/step, loss=0.07709, avg_loss=0.07510]\n",
      "Step 508075  [5.359 sec/step, loss=0.07312, avg_loss=0.07510]\n",
      "Step 508076  [5.362 sec/step, loss=0.07611, avg_loss=0.07509]\n",
      "Step 508077  [5.297 sec/step, loss=0.07282, avg_loss=0.07514]\n",
      "Step 508078  [5.305 sec/step, loss=0.07721, avg_loss=0.07516]\n",
      "Step 508079  [5.311 sec/step, loss=0.07691, avg_loss=0.07518]\n",
      "Step 508080  [5.323 sec/step, loss=0.07678, avg_loss=0.07527]\n",
      "Step 508081  [5.318 sec/step, loss=0.07236, avg_loss=0.07525]\n",
      "Step 508082  [5.318 sec/step, loss=0.07392, avg_loss=0.07527]\n",
      "Step 508083  [5.315 sec/step, loss=0.07644, avg_loss=0.07527]\n",
      "Step 508084  [5.340 sec/step, loss=0.07477, avg_loss=0.07525]\n",
      "Step 508085  [5.352 sec/step, loss=0.07466, avg_loss=0.07523]\n",
      "Step 508086  [5.343 sec/step, loss=0.07427, avg_loss=0.07521]\n",
      "Step 508087  [5.332 sec/step, loss=0.07639, avg_loss=0.07520]\n",
      "Step 508088  [5.307 sec/step, loss=0.07540, avg_loss=0.07519]\n",
      "Step 508089  [5.304 sec/step, loss=0.07526, avg_loss=0.07517]\n",
      "Step 508090  [5.307 sec/step, loss=0.07556, avg_loss=0.07517]\n",
      "Step 508091  [5.277 sec/step, loss=0.06734, avg_loss=0.07507]\n",
      "Step 508092  [5.252 sec/step, loss=0.07551, avg_loss=0.07505]\n",
      "Step 508093  [5.310 sec/step, loss=0.06845, avg_loss=0.07498]\n",
      "Step 508094  [5.338 sec/step, loss=0.07424, avg_loss=0.07500]\n",
      "Step 508095  [5.334 sec/step, loss=0.07539, avg_loss=0.07500]\n",
      "Generated 32 batches of size 32 in 2.392 sec\n",
      "Step 508096  [5.343 sec/step, loss=0.07622, avg_loss=0.07502]\n",
      "Step 508097  [5.348 sec/step, loss=0.07785, avg_loss=0.07502]\n",
      "Step 508098  [5.304 sec/step, loss=0.07609, avg_loss=0.07512]\n",
      "Step 508099  [5.301 sec/step, loss=0.07610, avg_loss=0.07510]\n",
      "Step 508100  [5.304 sec/step, loss=0.07703, avg_loss=0.07513]\n",
      "Writing summary at step: 508100\n",
      "Step 508101  [5.294 sec/step, loss=0.07644, avg_loss=0.07512]\n",
      "Step 508102  [5.283 sec/step, loss=0.07478, avg_loss=0.07509]\n",
      "Step 508103  [5.285 sec/step, loss=0.07809, avg_loss=0.07512]\n",
      "Step 508104  [5.310 sec/step, loss=0.07742, avg_loss=0.07516]\n",
      "Step 508105  [5.316 sec/step, loss=0.07556, avg_loss=0.07515]\n",
      "Step 508106  [5.327 sec/step, loss=0.07639, avg_loss=0.07516]\n",
      "Step 508107  [5.323 sec/step, loss=0.07435, avg_loss=0.07514]\n",
      "Step 508108  [5.318 sec/step, loss=0.07621, avg_loss=0.07514]\n",
      "Step 508109  [5.334 sec/step, loss=0.07759, avg_loss=0.07517]\n",
      "Step 508110  [5.349 sec/step, loss=0.07696, avg_loss=0.07518]\n",
      "Step 508111  [5.353 sec/step, loss=0.07531, avg_loss=0.07516]\n",
      "Step 508112  [5.330 sec/step, loss=0.06740, avg_loss=0.07506]\n",
      "Step 508113  [5.310 sec/step, loss=0.07620, avg_loss=0.07505]\n",
      "Step 508114  [5.350 sec/step, loss=0.06693, avg_loss=0.07495]\n",
      "Step 508115  [5.350 sec/step, loss=0.07357, avg_loss=0.07492]\n",
      "Step 508116  [5.320 sec/step, loss=0.07487, avg_loss=0.07491]\n",
      "Step 508117  [5.311 sec/step, loss=0.07595, avg_loss=0.07489]\n",
      "Step 508118  [5.332 sec/step, loss=0.07653, avg_loss=0.07491]\n",
      "Step 508119  [5.318 sec/step, loss=0.07327, avg_loss=0.07491]\n",
      "Step 508120  [5.316 sec/step, loss=0.07585, avg_loss=0.07490]\n",
      "Step 508121  [5.322 sec/step, loss=0.07485, avg_loss=0.07489]\n",
      "Step 508122  [5.318 sec/step, loss=0.07602, avg_loss=0.07490]\n",
      "Step 508123  [5.319 sec/step, loss=0.07506, avg_loss=0.07491]\n",
      "Step 508124  [5.328 sec/step, loss=0.07649, avg_loss=0.07492]\n",
      "Step 508125  [5.359 sec/step, loss=0.07379, avg_loss=0.07489]\n",
      "Step 508126  [5.356 sec/step, loss=0.07561, avg_loss=0.07491]\n",
      "Generated 32 batches of size 32 in 2.476 sec\n",
      "Step 508127  [5.371 sec/step, loss=0.07550, avg_loss=0.07499]\n",
      "Step 508128  [5.365 sec/step, loss=0.07593, avg_loss=0.07499]\n",
      "Step 508129  [5.351 sec/step, loss=0.07499, avg_loss=0.07498]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 508130  [5.337 sec/step, loss=0.07347, avg_loss=0.07497]\n",
      "Step 508131  [5.357 sec/step, loss=0.07753, avg_loss=0.07502]\n",
      "Step 508132  [5.352 sec/step, loss=0.07805, avg_loss=0.07503]\n",
      "Step 508133  [5.374 sec/step, loss=0.07445, avg_loss=0.07502]\n",
      "Step 508134  [5.369 sec/step, loss=0.07679, avg_loss=0.07501]\n",
      "Step 508135  [5.375 sec/step, loss=0.07624, avg_loss=0.07501]\n",
      "Step 508136  [5.374 sec/step, loss=0.07656, avg_loss=0.07501]\n",
      "Step 508137  [5.388 sec/step, loss=0.07688, avg_loss=0.07502]\n",
      "Step 508138  [5.398 sec/step, loss=0.07487, avg_loss=0.07503]\n",
      "Step 508139  [5.391 sec/step, loss=0.07310, avg_loss=0.07500]\n",
      "Step 508140  [5.343 sec/step, loss=0.07584, avg_loss=0.07509]\n",
      "Step 508141  [5.325 sec/step, loss=0.07132, avg_loss=0.07502]\n",
      "Step 508142  [5.320 sec/step, loss=0.07528, avg_loss=0.07502]\n",
      "Step 508143  [5.343 sec/step, loss=0.07757, avg_loss=0.07506]\n",
      "Step 508144  [5.353 sec/step, loss=0.07773, avg_loss=0.07511]\n",
      "Step 508145  [5.366 sec/step, loss=0.07751, avg_loss=0.07516]\n",
      "Step 508146  [5.411 sec/step, loss=0.06669, avg_loss=0.07506]\n",
      "Step 508147  [5.417 sec/step, loss=0.07634, avg_loss=0.07507]\n",
      "Step 508148  [5.420 sec/step, loss=0.07663, avg_loss=0.07510]\n",
      "Step 508149  [5.403 sec/step, loss=0.07352, avg_loss=0.07507]\n",
      "Step 508150  [5.407 sec/step, loss=0.07556, avg_loss=0.07508]\n",
      "Step 508151  [5.381 sec/step, loss=0.06719, avg_loss=0.07497]\n",
      "Step 508152  [5.376 sec/step, loss=0.07626, avg_loss=0.07496]\n",
      "Step 508153  [5.410 sec/step, loss=0.07693, avg_loss=0.07507]\n",
      "Step 508154  [5.405 sec/step, loss=0.07615, avg_loss=0.07509]\n",
      "Step 508155  [5.408 sec/step, loss=0.07428, avg_loss=0.07509]\n",
      "Step 508156  [5.417 sec/step, loss=0.07359, avg_loss=0.07508]\n",
      "Step 508157  [5.414 sec/step, loss=0.07680, avg_loss=0.07507]\n",
      "Step 508158  [5.411 sec/step, loss=0.07630, avg_loss=0.07507]\n",
      "Generated 32 batches of size 32 in 2.504 sec\n",
      "Step 508159  [5.404 sec/step, loss=0.07523, avg_loss=0.07507]\n",
      "Step 508160  [5.400 sec/step, loss=0.07732, avg_loss=0.07510]\n",
      "Step 508161  [5.389 sec/step, loss=0.07640, avg_loss=0.07511]\n",
      "Step 508162  [5.346 sec/step, loss=0.07545, avg_loss=0.07519]\n",
      "Step 508163  [5.355 sec/step, loss=0.07389, avg_loss=0.07519]\n",
      "Step 508164  [5.340 sec/step, loss=0.07167, avg_loss=0.07514]\n",
      "Step 508165  [5.340 sec/step, loss=0.07595, avg_loss=0.07512]\n",
      "Step 508166  [5.330 sec/step, loss=0.07417, avg_loss=0.07512]\n",
      "Step 508167  [5.352 sec/step, loss=0.07698, avg_loss=0.07513]\n",
      "Step 508168  [5.364 sec/step, loss=0.07516, avg_loss=0.07510]\n",
      "Step 508169  [5.368 sec/step, loss=0.07613, avg_loss=0.07510]\n",
      "Step 508170  [5.367 sec/step, loss=0.07314, avg_loss=0.07510]\n",
      "Step 508171  [5.371 sec/step, loss=0.07670, avg_loss=0.07512]\n",
      "Step 508172  [5.386 sec/step, loss=0.07602, avg_loss=0.07512]\n",
      "Step 508173  [5.373 sec/step, loss=0.07523, avg_loss=0.07510]\n",
      "Step 508174  [5.353 sec/step, loss=0.07477, avg_loss=0.07508]\n",
      "Step 508175  [5.343 sec/step, loss=0.07508, avg_loss=0.07510]\n",
      "Step 508176  [5.347 sec/step, loss=0.07542, avg_loss=0.07509]\n",
      "Step 508177  [5.362 sec/step, loss=0.07682, avg_loss=0.07513]\n",
      "Step 508178  [5.362 sec/step, loss=0.07762, avg_loss=0.07513]\n",
      "Step 508179  [5.356 sec/step, loss=0.07435, avg_loss=0.07511]\n",
      "Step 508180  [5.370 sec/step, loss=0.07733, avg_loss=0.07511]\n",
      "Step 508181  [5.366 sec/step, loss=0.06773, avg_loss=0.07507]\n",
      "Step 508182  [5.357 sec/step, loss=0.07379, avg_loss=0.07507]\n",
      "Step 508183  [5.355 sec/step, loss=0.07468, avg_loss=0.07505]\n",
      "Step 508184  [5.356 sec/step, loss=0.07486, avg_loss=0.07505]\n",
      "Step 508185  [5.356 sec/step, loss=0.07716, avg_loss=0.07507]\n",
      "Step 508186  [5.358 sec/step, loss=0.07736, avg_loss=0.07511]\n",
      "Step 508187  [5.369 sec/step, loss=0.07705, avg_loss=0.07511]\n",
      "Step 508188  [5.427 sec/step, loss=0.06799, avg_loss=0.07504]\n",
      "Step 508189  [5.438 sec/step, loss=0.07751, avg_loss=0.07506]\n",
      "Step 508190  [5.428 sec/step, loss=0.07646, avg_loss=0.07507]\n",
      "Generated 32 batches of size 32 in 2.393 sec\n",
      "Step 508191  [5.466 sec/step, loss=0.07600, avg_loss=0.07516]\n",
      "Step 508192  [5.475 sec/step, loss=0.07664, avg_loss=0.07517]\n",
      "Step 508193  [5.422 sec/step, loss=0.07341, avg_loss=0.07522]\n",
      "Step 508194  [5.416 sec/step, loss=0.07559, avg_loss=0.07523]\n",
      "Step 508195  [5.425 sec/step, loss=0.07693, avg_loss=0.07525]\n",
      "Step 508196  [5.417 sec/step, loss=0.07608, avg_loss=0.07524]\n",
      "Step 508197  [5.398 sec/step, loss=0.07518, avg_loss=0.07522]\n",
      "Step 508198  [5.394 sec/step, loss=0.07393, avg_loss=0.07520]\n",
      "Step 508199  [5.397 sec/step, loss=0.07589, avg_loss=0.07519]\n",
      "Step 508200  [5.395 sec/step, loss=0.07525, avg_loss=0.07518]\n",
      "Writing summary at step: 508200\n",
      "Step 508201  [5.393 sec/step, loss=0.07636, avg_loss=0.07518]\n",
      "Step 508202  [5.402 sec/step, loss=0.07618, avg_loss=0.07519]\n",
      "Step 508203  [5.404 sec/step, loss=0.07745, avg_loss=0.07518]\n",
      "Step 508204  [5.390 sec/step, loss=0.07721, avg_loss=0.07518]\n",
      "Step 508205  [5.415 sec/step, loss=0.07524, avg_loss=0.07518]\n",
      "Step 508206  [5.414 sec/step, loss=0.07503, avg_loss=0.07516]\n",
      "Step 508207  [5.427 sec/step, loss=0.07743, avg_loss=0.07520]\n",
      "Step 508208  [5.393 sec/step, loss=0.06767, avg_loss=0.07511]\n",
      "Step 508209  [5.393 sec/step, loss=0.07635, avg_loss=0.07510]\n",
      "Step 508210  [5.372 sec/step, loss=0.07572, avg_loss=0.07508]\n",
      "Step 508211  [5.370 sec/step, loss=0.07704, avg_loss=0.07510]\n",
      "Step 508212  [5.396 sec/step, loss=0.07734, avg_loss=0.07520]\n",
      "Step 508213  [5.405 sec/step, loss=0.07661, avg_loss=0.07521]\n",
      "Step 508214  [5.369 sec/step, loss=0.07674, avg_loss=0.07530]\n",
      "Step 508215  [5.359 sec/step, loss=0.07542, avg_loss=0.07532]\n",
      "Step 508216  [5.367 sec/step, loss=0.07735, avg_loss=0.07535]\n",
      "Step 508217  [5.362 sec/step, loss=0.07470, avg_loss=0.07533]\n",
      "Step 508218  [5.350 sec/step, loss=0.07543, avg_loss=0.07532]\n",
      "Step 508219  [5.414 sec/step, loss=0.06646, avg_loss=0.07526]\n",
      "Step 508220  [5.421 sec/step, loss=0.07672, avg_loss=0.07526]\n",
      "Step 508221  [5.426 sec/step, loss=0.07767, avg_loss=0.07529]\n",
      "Generated 32 batches of size 32 in 2.486 sec\n",
      "Step 508222  [5.417 sec/step, loss=0.07571, avg_loss=0.07529]\n",
      "Step 508223  [5.431 sec/step, loss=0.07516, avg_loss=0.07529]\n",
      "Step 508224  [5.428 sec/step, loss=0.07385, avg_loss=0.07526]\n",
      "Step 508225  [5.398 sec/step, loss=0.07505, avg_loss=0.07528]\n",
      "Step 508226  [5.398 sec/step, loss=0.07242, avg_loss=0.07524]\n",
      "Step 508227  [5.403 sec/step, loss=0.07446, avg_loss=0.07523]\n",
      "Step 508228  [5.392 sec/step, loss=0.07322, avg_loss=0.07521]\n",
      "Step 508229  [5.404 sec/step, loss=0.07629, avg_loss=0.07522]\n",
      "Step 508230  [5.426 sec/step, loss=0.07803, avg_loss=0.07527]\n",
      "Step 508231  [5.417 sec/step, loss=0.07519, avg_loss=0.07524]\n",
      "Step 508232  [5.392 sec/step, loss=0.07305, avg_loss=0.07519]\n",
      "Step 508233  [5.388 sec/step, loss=0.07567, avg_loss=0.07520]\n",
      "Step 508234  [5.373 sec/step, loss=0.07575, avg_loss=0.07519]\n",
      "Step 508235  [5.372 sec/step, loss=0.07698, avg_loss=0.07520]\n",
      "Step 508236  [5.365 sec/step, loss=0.07684, avg_loss=0.07520]\n",
      "Step 508237  [5.360 sec/step, loss=0.07528, avg_loss=0.07519]\n",
      "Step 508238  [5.369 sec/step, loss=0.07726, avg_loss=0.07521]\n",
      "Step 508239  [5.376 sec/step, loss=0.07648, avg_loss=0.07525]\n",
      "Step 508240  [5.377 sec/step, loss=0.07716, avg_loss=0.07526]\n",
      "Step 508241  [5.388 sec/step, loss=0.07780, avg_loss=0.07532]\n",
      "Step 508242  [5.400 sec/step, loss=0.07764, avg_loss=0.07535]\n",
      "Step 508243  [5.399 sec/step, loss=0.07810, avg_loss=0.07535]\n",
      "Step 508244  [5.400 sec/step, loss=0.07585, avg_loss=0.07533]\n",
      "Step 508245  [5.387 sec/step, loss=0.07266, avg_loss=0.07529]\n",
      "Step 508246  [5.388 sec/step, loss=0.06913, avg_loss=0.07531]\n",
      "Step 508247  [5.389 sec/step, loss=0.07613, avg_loss=0.07531]\n",
      "Step 508248  [5.370 sec/step, loss=0.06633, avg_loss=0.07521]\n",
      "Step 508249  [5.386 sec/step, loss=0.07799, avg_loss=0.07525]\n",
      "Step 508250  [5.413 sec/step, loss=0.07547, avg_loss=0.07525]\n",
      "Step 508251  [5.443 sec/step, loss=0.07772, avg_loss=0.07535]\n",
      "Step 508252  [5.446 sec/step, loss=0.07823, avg_loss=0.07537]\n",
      "Step 508253  [5.425 sec/step, loss=0.07366, avg_loss=0.07534]\n",
      "Generated 32 batches of size 32 in 2.436 sec\n",
      "Step 508254  [5.431 sec/step, loss=0.07634, avg_loss=0.07534]\n",
      "Step 508255  [5.447 sec/step, loss=0.07501, avg_loss=0.07535]\n",
      "Step 508256  [5.461 sec/step, loss=0.07800, avg_loss=0.07539]\n",
      "Step 508257  [5.446 sec/step, loss=0.07296, avg_loss=0.07536]\n",
      "Step 508258  [5.445 sec/step, loss=0.07537, avg_loss=0.07535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 508259  [5.421 sec/step, loss=0.07568, avg_loss=0.07535]\n",
      "Step 508260  [5.406 sec/step, loss=0.07715, avg_loss=0.07535]\n",
      "Step 508261  [5.397 sec/step, loss=0.07462, avg_loss=0.07533]\n",
      "Step 508262  [5.396 sec/step, loss=0.07626, avg_loss=0.07534]\n",
      "Step 508263  [5.394 sec/step, loss=0.07627, avg_loss=0.07536]\n",
      "Step 508264  [5.404 sec/step, loss=0.07532, avg_loss=0.07540]\n",
      "Step 508265  [5.393 sec/step, loss=0.07703, avg_loss=0.07541]\n",
      "Step 508266  [5.390 sec/step, loss=0.07247, avg_loss=0.07539]\n",
      "Step 508267  [5.379 sec/step, loss=0.07732, avg_loss=0.07540]\n",
      "Step 508268  [5.362 sec/step, loss=0.07701, avg_loss=0.07542]\n",
      "Step 508269  [5.369 sec/step, loss=0.07819, avg_loss=0.07544]\n",
      "Step 508270  [5.378 sec/step, loss=0.07316, avg_loss=0.07544]\n",
      "Step 508271  [5.385 sec/step, loss=0.07756, avg_loss=0.07545]\n",
      "Step 508272  [5.363 sec/step, loss=0.06903, avg_loss=0.07538]\n",
      "Step 508273  [5.369 sec/step, loss=0.07558, avg_loss=0.07538]\n",
      "Step 508274  [5.402 sec/step, loss=0.07401, avg_loss=0.07537]\n",
      "Step 508275  [5.407 sec/step, loss=0.07529, avg_loss=0.07537]\n",
      "Step 508276  [5.404 sec/step, loss=0.07656, avg_loss=0.07538]\n",
      "Step 508277  [5.411 sec/step, loss=0.07783, avg_loss=0.07540]\n",
      "Step 508278  [5.409 sec/step, loss=0.07682, avg_loss=0.07539]\n",
      "Step 508279  [5.426 sec/step, loss=0.07771, avg_loss=0.07542]\n",
      "Step 508280  [5.419 sec/step, loss=0.07575, avg_loss=0.07540]\n",
      "Step 508281  [5.486 sec/step, loss=0.06746, avg_loss=0.07540]\n",
      "Step 508282  [5.500 sec/step, loss=0.07655, avg_loss=0.07543]\n",
      "Step 508283  [5.516 sec/step, loss=0.07734, avg_loss=0.07546]\n",
      "Step 508284  [5.481 sec/step, loss=0.07565, avg_loss=0.07546]\n",
      "Step 508285  [5.476 sec/step, loss=0.07628, avg_loss=0.07546]\n",
      "Generated 32 batches of size 32 in 2.378 sec\n",
      "Step 508286  [5.485 sec/step, loss=0.07504, avg_loss=0.07543]\n",
      "Step 508287  [5.470 sec/step, loss=0.07621, avg_loss=0.07542]\n",
      "Step 508288  [5.407 sec/step, loss=0.07341, avg_loss=0.07548]\n",
      "Step 508289  [5.418 sec/step, loss=0.07701, avg_loss=0.07547]\n",
      "Step 508290  [5.422 sec/step, loss=0.07705, avg_loss=0.07548]\n",
      "Step 508291  [5.395 sec/step, loss=0.07525, avg_loss=0.07547]\n",
      "Step 508292  [5.395 sec/step, loss=0.07659, avg_loss=0.07547]\n",
      "Step 508293  [5.408 sec/step, loss=0.07556, avg_loss=0.07549]\n",
      "Step 508294  [5.399 sec/step, loss=0.07621, avg_loss=0.07550]\n",
      "Step 508295  [5.405 sec/step, loss=0.07756, avg_loss=0.07550]\n",
      "Step 508296  [5.414 sec/step, loss=0.07744, avg_loss=0.07552]\n",
      "Step 508297  [5.408 sec/step, loss=0.07222, avg_loss=0.07549]\n",
      "Step 508298  [5.396 sec/step, loss=0.07529, avg_loss=0.07550]\n",
      "Step 508299  [5.392 sec/step, loss=0.07438, avg_loss=0.07549]\n",
      "Step 508300  [5.405 sec/step, loss=0.07749, avg_loss=0.07551]\n",
      "Writing summary at step: 508300\n",
      "Step 508301  [5.394 sec/step, loss=0.07242, avg_loss=0.07547]\n",
      "Step 508302  [5.401 sec/step, loss=0.07602, avg_loss=0.07547]\n",
      "Step 508303  [5.389 sec/step, loss=0.07601, avg_loss=0.07545]\n",
      "Step 508304  [5.396 sec/step, loss=0.07585, avg_loss=0.07544]\n",
      "Step 508305  [5.378 sec/step, loss=0.07356, avg_loss=0.07542]\n",
      "Step 508306  [5.426 sec/step, loss=0.06820, avg_loss=0.07536]\n",
      "Step 508307  [5.423 sec/step, loss=0.07681, avg_loss=0.07535]\n",
      "Step 508308  [5.449 sec/step, loss=0.07521, avg_loss=0.07543]\n",
      "Step 508309  [5.433 sec/step, loss=0.07500, avg_loss=0.07541]\n",
      "Step 508310  [5.438 sec/step, loss=0.07640, avg_loss=0.07542]\n",
      "Step 508311  [5.435 sec/step, loss=0.07414, avg_loss=0.07539]\n",
      "Step 508312  [5.433 sec/step, loss=0.07715, avg_loss=0.07539]\n",
      "Step 508313  [5.426 sec/step, loss=0.07696, avg_loss=0.07539]\n",
      "Step 508314  [5.413 sec/step, loss=0.07757, avg_loss=0.07540]\n",
      "Step 508315  [5.450 sec/step, loss=0.07452, avg_loss=0.07539]\n",
      "Step 508316  [5.451 sec/step, loss=0.07431, avg_loss=0.07536]\n",
      "Generated 32 batches of size 32 in 2.484 sec\n",
      "Step 508317  [5.469 sec/step, loss=0.07787, avg_loss=0.07539]\n",
      "Step 508318  [5.483 sec/step, loss=0.07734, avg_loss=0.07541]\n",
      "Step 508319  [5.445 sec/step, loss=0.07784, avg_loss=0.07552]\n",
      "Step 508320  [5.419 sec/step, loss=0.07490, avg_loss=0.07551]\n",
      "Step 508321  [5.409 sec/step, loss=0.07428, avg_loss=0.07547]\n",
      "Step 508322  [5.408 sec/step, loss=0.07486, avg_loss=0.07546]\n",
      "Step 508323  [5.379 sec/step, loss=0.06737, avg_loss=0.07539]\n",
      "Step 508324  [5.381 sec/step, loss=0.07565, avg_loss=0.07540]\n",
      "Step 508325  [5.384 sec/step, loss=0.07630, avg_loss=0.07542]\n",
      "Step 508326  [5.402 sec/step, loss=0.07606, avg_loss=0.07545]\n",
      "Step 508327  [5.384 sec/step, loss=0.06757, avg_loss=0.07538]\n",
      "Step 508328  [5.416 sec/step, loss=0.07407, avg_loss=0.07539]\n",
      "Step 508329  [5.401 sec/step, loss=0.07543, avg_loss=0.07538]\n",
      "Step 508330  [5.400 sec/step, loss=0.07478, avg_loss=0.07535]\n",
      "Step 508331  [5.401 sec/step, loss=0.07631, avg_loss=0.07536]\n",
      "Step 508332  [5.428 sec/step, loss=0.07649, avg_loss=0.07540]\n",
      "Step 508333  [5.433 sec/step, loss=0.07682, avg_loss=0.07541]\n",
      "Step 508334  [5.449 sec/step, loss=0.07608, avg_loss=0.07541]\n",
      "Step 508335  [5.442 sec/step, loss=0.07225, avg_loss=0.07536]\n",
      "Step 508336  [5.440 sec/step, loss=0.07364, avg_loss=0.07533]\n",
      "Step 508337  [5.481 sec/step, loss=0.06705, avg_loss=0.07525]\n",
      "Step 508338  [5.468 sec/step, loss=0.07512, avg_loss=0.07523]\n",
      "Step 508339  [5.465 sec/step, loss=0.07704, avg_loss=0.07523]\n",
      "Step 508340  [5.469 sec/step, loss=0.07503, avg_loss=0.07521]\n",
      "Step 508341  [5.469 sec/step, loss=0.07596, avg_loss=0.07519]\n",
      "Step 508342  [5.473 sec/step, loss=0.07645, avg_loss=0.07518]\n",
      "Step 508343  [5.463 sec/step, loss=0.07648, avg_loss=0.07517]\n",
      "Step 508344  [5.462 sec/step, loss=0.07754, avg_loss=0.07518]\n",
      "Step 508345  [5.457 sec/step, loss=0.07234, avg_loss=0.07518]\n",
      "Step 508346  [5.405 sec/step, loss=0.07649, avg_loss=0.07525]\n",
      "Step 508347  [5.402 sec/step, loss=0.07559, avg_loss=0.07525]\n",
      "Step 508348  [5.429 sec/step, loss=0.07789, avg_loss=0.07536]\n",
      "Generated 32 batches of size 32 in 2.463 sec\n",
      "Step 508349  [5.424 sec/step, loss=0.07505, avg_loss=0.07533]\n",
      "Step 508350  [5.394 sec/step, loss=0.07395, avg_loss=0.07532]\n",
      "Step 508351  [5.368 sec/step, loss=0.07468, avg_loss=0.07529]\n",
      "Step 508352  [5.369 sec/step, loss=0.07650, avg_loss=0.07527]\n",
      "Step 508353  [5.384 sec/step, loss=0.07730, avg_loss=0.07531]\n",
      "Step 508354  [5.388 sec/step, loss=0.07752, avg_loss=0.07532]\n",
      "Step 508355  [5.364 sec/step, loss=0.07769, avg_loss=0.07535]\n",
      "Step 508356  [5.348 sec/step, loss=0.07545, avg_loss=0.07532]\n",
      "Step 508357  [5.385 sec/step, loss=0.07493, avg_loss=0.07534]\n",
      "Step 508358  [5.390 sec/step, loss=0.07301, avg_loss=0.07532]\n",
      "Step 508359  [5.394 sec/step, loss=0.07297, avg_loss=0.07529]\n",
      "Step 508360  [5.411 sec/step, loss=0.07617, avg_loss=0.07528]\n",
      "Step 508361  [5.414 sec/step, loss=0.07688, avg_loss=0.07530]\n",
      "Step 508362  [5.422 sec/step, loss=0.07783, avg_loss=0.07532]\n",
      "Step 508363  [5.421 sec/step, loss=0.07627, avg_loss=0.07532]\n",
      "Step 508364  [5.434 sec/step, loss=0.07665, avg_loss=0.07533]\n",
      "Step 508365  [5.442 sec/step, loss=0.07668, avg_loss=0.07533]\n",
      "Step 508366  [5.453 sec/step, loss=0.07678, avg_loss=0.07537]\n",
      "Step 508367  [5.450 sec/step, loss=0.07749, avg_loss=0.07537]\n",
      "Step 508368  [5.473 sec/step, loss=0.07631, avg_loss=0.07537]\n",
      "Step 508369  [5.454 sec/step, loss=0.07395, avg_loss=0.07532]\n",
      "Step 508370  [5.443 sec/step, loss=0.07256, avg_loss=0.07532]\n",
      "Step 508371  [5.432 sec/step, loss=0.07563, avg_loss=0.07530]\n",
      "Step 508372  [5.446 sec/step, loss=0.07694, avg_loss=0.07538]\n",
      "Step 508373  [5.496 sec/step, loss=0.06609, avg_loss=0.07528]\n",
      "Step 508374  [5.476 sec/step, loss=0.07675, avg_loss=0.07531]\n",
      "Step 508375  [5.485 sec/step, loss=0.07633, avg_loss=0.07532]\n",
      "Step 508376  [5.484 sec/step, loss=0.07759, avg_loss=0.07533]\n",
      "Step 508377  [5.466 sec/step, loss=0.07549, avg_loss=0.07531]\n",
      "Step 508378  [5.450 sec/step, loss=0.07123, avg_loss=0.07525]\n",
      "Step 508379  [5.434 sec/step, loss=0.07558, avg_loss=0.07523]\n",
      "Step 508380  [5.438 sec/step, loss=0.07764, avg_loss=0.07525]\n",
      "Generated 32 batches of size 32 in 2.384 sec\n",
      "Step 508381  [5.393 sec/step, loss=0.07690, avg_loss=0.07534]\n",
      "Step 508382  [5.411 sec/step, loss=0.07713, avg_loss=0.07535]\n",
      "Step 508383  [5.407 sec/step, loss=0.07494, avg_loss=0.07533]\n",
      "Step 508384  [5.415 sec/step, loss=0.07302, avg_loss=0.07530]\n",
      "Step 508385  [5.419 sec/step, loss=0.07610, avg_loss=0.07530]\n",
      "Step 508386  [5.428 sec/step, loss=0.07775, avg_loss=0.07532]\n",
      "Step 508387  [5.416 sec/step, loss=0.06741, avg_loss=0.07524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 508388  [5.422 sec/step, loss=0.07542, avg_loss=0.07526]\n",
      "Step 508389  [5.404 sec/step, loss=0.07543, avg_loss=0.07524]\n",
      "Step 508390  [5.416 sec/step, loss=0.07719, avg_loss=0.07524]\n",
      "Step 508391  [5.421 sec/step, loss=0.07471, avg_loss=0.07524]\n",
      "Step 508392  [5.421 sec/step, loss=0.07157, avg_loss=0.07519]\n",
      "Step 508393  [5.400 sec/step, loss=0.07263, avg_loss=0.07516]\n",
      "Step 508394  [5.399 sec/step, loss=0.07644, avg_loss=0.07516]\n",
      "Step 508395  [5.387 sec/step, loss=0.07469, avg_loss=0.07513]\n",
      "Step 508396  [5.384 sec/step, loss=0.07459, avg_loss=0.07510]\n",
      "Step 508397  [5.402 sec/step, loss=0.07711, avg_loss=0.07515]\n",
      "Step 508398  [5.433 sec/step, loss=0.07463, avg_loss=0.07514]\n",
      "Step 508399  [5.433 sec/step, loss=0.07638, avg_loss=0.07516]\n",
      "Step 508400  [5.422 sec/step, loss=0.07706, avg_loss=0.07516]\n",
      "Writing summary at step: 508400\n",
      "Step 508401  [5.429 sec/step, loss=0.07233, avg_loss=0.07516]\n",
      "Step 508402  [5.422 sec/step, loss=0.07556, avg_loss=0.07515]\n",
      "Step 508403  [5.412 sec/step, loss=0.07581, avg_loss=0.07515]\n",
      "Step 508404  [5.420 sec/step, loss=0.07662, avg_loss=0.07516]\n",
      "Step 508405  [5.425 sec/step, loss=0.07738, avg_loss=0.07520]\n",
      "Step 508406  [5.361 sec/step, loss=0.07309, avg_loss=0.07525]\n",
      "Step 508407  [5.355 sec/step, loss=0.07644, avg_loss=0.07524]\n",
      "Step 508408  [5.328 sec/step, loss=0.06812, avg_loss=0.07517]\n",
      "Step 508409  [5.340 sec/step, loss=0.07550, avg_loss=0.07518]\n",
      "Step 508410  [5.394 sec/step, loss=0.06788, avg_loss=0.07509]\n",
      "Step 508411  [5.422 sec/step, loss=0.07530, avg_loss=0.07510]\n",
      "Generated 32 batches of size 32 in 2.614 sec\n",
      "Step 508412  [5.416 sec/step, loss=0.07684, avg_loss=0.07510]\n",
      "Step 508413  [5.420 sec/step, loss=0.07677, avg_loss=0.07510]\n",
      "Step 508414  [5.428 sec/step, loss=0.07595, avg_loss=0.07508]\n",
      "Step 508415  [5.399 sec/step, loss=0.07522, avg_loss=0.07509]\n",
      "Step 508416  [5.398 sec/step, loss=0.07601, avg_loss=0.07511]\n",
      "Step 508417  [5.383 sec/step, loss=0.07653, avg_loss=0.07509]\n",
      "Step 508418  [5.357 sec/step, loss=0.07503, avg_loss=0.07507]\n",
      "Step 508419  [5.348 sec/step, loss=0.07512, avg_loss=0.07504]\n",
      "Step 508420  [5.365 sec/step, loss=0.07748, avg_loss=0.07507]\n",
      "Step 508421  [5.382 sec/step, loss=0.07725, avg_loss=0.07510]\n",
      "Step 508422  [5.382 sec/step, loss=0.07357, avg_loss=0.07509]\n",
      "Step 508423  [5.393 sec/step, loss=0.07391, avg_loss=0.07515]\n",
      "Step 508424  [5.393 sec/step, loss=0.07710, avg_loss=0.07517]\n",
      "Step 508425  [5.388 sec/step, loss=0.07696, avg_loss=0.07517]\n",
      "Step 508426  [5.391 sec/step, loss=0.07725, avg_loss=0.07518]\n",
      "Step 508427  [5.397 sec/step, loss=0.07540, avg_loss=0.07526]\n",
      "Step 508428  [5.387 sec/step, loss=0.07725, avg_loss=0.07529]\n",
      "Step 508429  [5.407 sec/step, loss=0.07600, avg_loss=0.07530]\n",
      "Step 508430  [5.446 sec/step, loss=0.06731, avg_loss=0.07523]\n",
      "Step 508431  [5.458 sec/step, loss=0.07667, avg_loss=0.07523]\n",
      "Step 508432  [5.449 sec/step, loss=0.07639, avg_loss=0.07523]\n",
      "Step 508433  [5.440 sec/step, loss=0.07602, avg_loss=0.07522]\n",
      "Step 508434  [5.442 sec/step, loss=0.07759, avg_loss=0.07524]\n",
      "Step 508435  [5.437 sec/step, loss=0.07503, avg_loss=0.07526]\n",
      "Step 508436  [5.428 sec/step, loss=0.07266, avg_loss=0.07525]\n",
      "Step 508437  [5.374 sec/step, loss=0.07596, avg_loss=0.07534]\n",
      "Step 508438  [5.388 sec/step, loss=0.07700, avg_loss=0.07536]\n",
      "Step 508439  [5.390 sec/step, loss=0.07628, avg_loss=0.07535]\n",
      "Step 508440  [5.381 sec/step, loss=0.07464, avg_loss=0.07535]\n",
      "Step 508441  [5.380 sec/step, loss=0.07480, avg_loss=0.07534]\n",
      "Step 508442  [5.394 sec/step, loss=0.07487, avg_loss=0.07532]\n",
      "Step 508443  [5.410 sec/step, loss=0.07800, avg_loss=0.07534]\n",
      "Generated 32 batches of size 32 in 2.383 sec\n",
      "Step 508444  [5.415 sec/step, loss=0.07492, avg_loss=0.07531]\n",
      "Step 508445  [5.451 sec/step, loss=0.07512, avg_loss=0.07534]\n",
      "Step 508446  [5.449 sec/step, loss=0.07484, avg_loss=0.07532]\n",
      "Step 508447  [5.435 sec/step, loss=0.06604, avg_loss=0.07523]\n",
      "Step 508448  [5.414 sec/step, loss=0.07154, avg_loss=0.07516]\n",
      "Step 508449  [5.411 sec/step, loss=0.07660, avg_loss=0.07518]\n",
      "Step 508450  [5.423 sec/step, loss=0.07736, avg_loss=0.07521]\n",
      "Step 508451  [5.439 sec/step, loss=0.07383, avg_loss=0.07520]\n",
      "Step 508452  [5.430 sec/step, loss=0.07591, avg_loss=0.07520]\n",
      "Step 508453  [5.408 sec/step, loss=0.07385, avg_loss=0.07516]\n",
      "Step 508454  [5.403 sec/step, loss=0.07683, avg_loss=0.07516]\n",
      "Step 508455  [5.394 sec/step, loss=0.07544, avg_loss=0.07513]\n",
      "Step 508456  [5.397 sec/step, loss=0.07722, avg_loss=0.07515]\n",
      "Step 508457  [5.371 sec/step, loss=0.07451, avg_loss=0.07515]\n",
      "Step 508458  [5.367 sec/step, loss=0.07569, avg_loss=0.07517]\n",
      "Step 508459  [5.381 sec/step, loss=0.07496, avg_loss=0.07519]\n",
      "Step 508460  [5.361 sec/step, loss=0.07488, avg_loss=0.07518]\n",
      "Step 508461  [5.372 sec/step, loss=0.07752, avg_loss=0.07519]\n",
      "Step 508462  [5.351 sec/step, loss=0.07228, avg_loss=0.07513]\n",
      "Step 508463  [5.350 sec/step, loss=0.07721, avg_loss=0.07514]\n",
      "Step 508464  [5.351 sec/step, loss=0.07857, avg_loss=0.07516]\n",
      "Step 508465  [5.359 sec/step, loss=0.07855, avg_loss=0.07518]\n",
      "Step 508466  [5.384 sec/step, loss=0.07514, avg_loss=0.07516]\n",
      "Step 508467  [5.378 sec/step, loss=0.07846, avg_loss=0.07517]\n",
      "Step 508468  [5.365 sec/step, loss=0.07601, avg_loss=0.07517]\n",
      "Step 508469  [5.389 sec/step, loss=0.07804, avg_loss=0.07521]\n",
      "Step 508470  [5.413 sec/step, loss=0.07698, avg_loss=0.07526]\n",
      "Step 508471  [5.423 sec/step, loss=0.07805, avg_loss=0.07528]\n",
      "Step 508472  [5.440 sec/step, loss=0.07832, avg_loss=0.07529]\n",
      "Step 508473  [5.394 sec/step, loss=0.07647, avg_loss=0.07540]\n",
      "Step 508474  [5.390 sec/step, loss=0.07674, avg_loss=0.07540]\n",
      "Step 508475  [5.390 sec/step, loss=0.07731, avg_loss=0.07541]\n",
      "Generated 32 batches of size 32 in 2.528 sec\n",
      "Step 508476  [5.387 sec/step, loss=0.07334, avg_loss=0.07536]\n",
      "Step 508477  [5.397 sec/step, loss=0.07688, avg_loss=0.07538]\n",
      "Step 508478  [5.405 sec/step, loss=0.07751, avg_loss=0.07544]\n",
      "Step 508479  [5.403 sec/step, loss=0.07544, avg_loss=0.07544]\n",
      "Step 508480  [5.395 sec/step, loss=0.07629, avg_loss=0.07543]\n",
      "Step 508481  [5.439 sec/step, loss=0.06830, avg_loss=0.07534]\n",
      "Step 508482  [5.417 sec/step, loss=0.07570, avg_loss=0.07533]\n",
      "Step 508483  [5.393 sec/step, loss=0.07458, avg_loss=0.07532]\n",
      "Step 508484  [5.378 sec/step, loss=0.06905, avg_loss=0.07528]\n",
      "Step 508485  [5.360 sec/step, loss=0.07559, avg_loss=0.07528]\n",
      "Step 508486  [5.358 sec/step, loss=0.07841, avg_loss=0.07528]\n",
      "Step 508487  [5.368 sec/step, loss=0.07556, avg_loss=0.07537]\n",
      "Step 508488  [5.384 sec/step, loss=0.07773, avg_loss=0.07539]\n",
      "Step 508489  [5.395 sec/step, loss=0.07523, avg_loss=0.07539]\n",
      "Step 508490  [5.382 sec/step, loss=0.07722, avg_loss=0.07539]\n",
      "Step 508491  [5.400 sec/step, loss=0.07664, avg_loss=0.07541]\n",
      "Step 508492  [5.406 sec/step, loss=0.07652, avg_loss=0.07546]\n",
      "Step 508493  [5.424 sec/step, loss=0.07683, avg_loss=0.07550]\n",
      "Step 508494  [5.407 sec/step, loss=0.07264, avg_loss=0.07546]\n",
      "Step 508495  [5.412 sec/step, loss=0.07571, avg_loss=0.07547]\n",
      "Step 508496  [5.450 sec/step, loss=0.06791, avg_loss=0.07540]\n",
      "Step 508497  [5.456 sec/step, loss=0.07772, avg_loss=0.07541]\n",
      "Step 508498  [5.442 sec/step, loss=0.07788, avg_loss=0.07544]\n",
      "Step 508499  [5.432 sec/step, loss=0.07546, avg_loss=0.07543]\n",
      "Step 508500  [5.419 sec/step, loss=0.07626, avg_loss=0.07542]\n",
      "Writing summary at step: 508500\n",
      "Step 508501  [5.431 sec/step, loss=0.07618, avg_loss=0.07546]\n",
      "Step 508502  [5.429 sec/step, loss=0.07648, avg_loss=0.07547]\n",
      "Step 508503  [5.442 sec/step, loss=0.07697, avg_loss=0.07548]\n",
      "Step 508504  [5.453 sec/step, loss=0.07707, avg_loss=0.07549]\n",
      "Step 508505  [5.443 sec/step, loss=0.07636, avg_loss=0.07548]\n",
      "Step 508506  [5.452 sec/step, loss=0.07402, avg_loss=0.07549]\n",
      "Generated 32 batches of size 32 in 2.508 sec\n",
      "Step 508507  [5.459 sec/step, loss=0.07467, avg_loss=0.07547]\n",
      "Step 508508  [5.465 sec/step, loss=0.07300, avg_loss=0.07552]\n",
      "Step 508509  [5.469 sec/step, loss=0.07807, avg_loss=0.07554]\n",
      "Step 508510  [5.434 sec/step, loss=0.07636, avg_loss=0.07563]\n",
      "Step 508511  [5.392 sec/step, loss=0.06702, avg_loss=0.07555]\n",
      "Step 508512  [5.387 sec/step, loss=0.07571, avg_loss=0.07554]\n",
      "Step 508513  [5.387 sec/step, loss=0.07701, avg_loss=0.07554]\n",
      "Step 508514  [5.375 sec/step, loss=0.07691, avg_loss=0.07555]\n",
      "Step 508515  [5.389 sec/step, loss=0.07805, avg_loss=0.07558]\n",
      "Step 508516  [5.395 sec/step, loss=0.07665, avg_loss=0.07558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 508517  [5.394 sec/step, loss=0.07700, avg_loss=0.07559]\n",
      "Step 508518  [5.411 sec/step, loss=0.07793, avg_loss=0.07562]\n",
      "Step 508519  [5.423 sec/step, loss=0.07711, avg_loss=0.07564]\n",
      "Step 508520  [5.414 sec/step, loss=0.07600, avg_loss=0.07562]\n",
      "Step 508521  [5.391 sec/step, loss=0.07531, avg_loss=0.07560]\n",
      "Step 508522  [5.378 sec/step, loss=0.06770, avg_loss=0.07554]\n",
      "Step 508523  [5.375 sec/step, loss=0.07626, avg_loss=0.07557]\n",
      "Step 508524  [5.378 sec/step, loss=0.07577, avg_loss=0.07555]\n",
      "Step 508525  [5.383 sec/step, loss=0.07663, avg_loss=0.07555]\n",
      "Step 508526  [5.376 sec/step, loss=0.07586, avg_loss=0.07554]\n",
      "Step 508527  [5.437 sec/step, loss=0.06832, avg_loss=0.07546]\n",
      "Step 508528  [5.433 sec/step, loss=0.07658, avg_loss=0.07546]\n",
      "Step 508529  [5.422 sec/step, loss=0.07685, avg_loss=0.07547]\n",
      "Step 508530  [5.364 sec/step, loss=0.07538, avg_loss=0.07555]\n",
      "Step 508531  [5.350 sec/step, loss=0.07275, avg_loss=0.07551]\n",
      "Step 508532  [5.341 sec/step, loss=0.07646, avg_loss=0.07551]\n",
      "Step 508533  [5.349 sec/step, loss=0.07543, avg_loss=0.07550]\n",
      "Step 508534  [5.353 sec/step, loss=0.07780, avg_loss=0.07551]\n",
      "Step 508535  [5.369 sec/step, loss=0.07725, avg_loss=0.07553]\n",
      "Step 508536  [5.379 sec/step, loss=0.07552, avg_loss=0.07556]\n",
      "Step 508537  [5.383 sec/step, loss=0.07666, avg_loss=0.07556]\n",
      "Step 508538  [5.396 sec/step, loss=0.07428, avg_loss=0.07554]\n",
      "Generated 32 batches of size 32 in 2.750 sec\n",
      "Step 508539  [5.384 sec/step, loss=0.07358, avg_loss=0.07551]\n",
      "Step 508540  [5.401 sec/step, loss=0.07740, avg_loss=0.07554]\n",
      "Step 508541  [5.429 sec/step, loss=0.07464, avg_loss=0.07553]\n",
      "Step 508542  [5.421 sec/step, loss=0.07684, avg_loss=0.07555]\n",
      "Step 508543  [5.397 sec/step, loss=0.07191, avg_loss=0.07549]\n",
      "Step 508544  [5.396 sec/step, loss=0.07519, avg_loss=0.07550]\n",
      "Step 508545  [5.365 sec/step, loss=0.07322, avg_loss=0.07548]\n",
      "Step 508546  [5.373 sec/step, loss=0.07615, avg_loss=0.07549]\n",
      "Step 508547  [5.391 sec/step, loss=0.07438, avg_loss=0.07557]\n",
      "Step 508548  [5.408 sec/step, loss=0.07618, avg_loss=0.07562]\n",
      "Step 508549  [5.424 sec/step, loss=0.07707, avg_loss=0.07562]\n",
      "Step 508550  [5.421 sec/step, loss=0.07573, avg_loss=0.07561]\n",
      "Step 508551  [5.423 sec/step, loss=0.07598, avg_loss=0.07563]\n",
      "Step 508552  [5.435 sec/step, loss=0.07731, avg_loss=0.07564]\n",
      "Step 508553  [5.454 sec/step, loss=0.07612, avg_loss=0.07567]\n",
      "Step 508554  [5.456 sec/step, loss=0.07772, avg_loss=0.07568]\n",
      "Step 508555  [5.453 sec/step, loss=0.07284, avg_loss=0.07565]\n",
      "Step 508556  [5.454 sec/step, loss=0.07598, avg_loss=0.07564]\n",
      "Step 508557  [5.446 sec/step, loss=0.07583, avg_loss=0.07565]\n",
      "Step 508558  [5.458 sec/step, loss=0.07762, avg_loss=0.07567]\n",
      "Step 508559  [5.456 sec/step, loss=0.07752, avg_loss=0.07570]\n",
      "Step 508560  [5.462 sec/step, loss=0.07656, avg_loss=0.07571]\n",
      "Step 508561  [5.468 sec/step, loss=0.07738, avg_loss=0.07571]\n",
      "Step 508562  [5.479 sec/step, loss=0.07413, avg_loss=0.07573]\n",
      "Step 508563  [5.492 sec/step, loss=0.07558, avg_loss=0.07571]\n",
      "Step 508564  [5.479 sec/step, loss=0.07416, avg_loss=0.07567]\n",
      "Step 508565  [5.462 sec/step, loss=0.07540, avg_loss=0.07564]\n",
      "Step 508566  [5.424 sec/step, loss=0.07247, avg_loss=0.07561]\n",
      "Step 508567  [5.400 sec/step, loss=0.06669, avg_loss=0.07549]\n",
      "Step 508568  [5.414 sec/step, loss=0.07454, avg_loss=0.07548]\n",
      "Step 508569  [5.405 sec/step, loss=0.07779, avg_loss=0.07548]\n",
      "Step 508570  [5.385 sec/step, loss=0.07196, avg_loss=0.07543]\n",
      "Generated 32 batches of size 32 in 2.424 sec\n",
      "Step 508571  [5.379 sec/step, loss=0.07666, avg_loss=0.07541]\n",
      "Step 508572  [5.364 sec/step, loss=0.07584, avg_loss=0.07539]\n",
      "Step 508573  [5.411 sec/step, loss=0.06707, avg_loss=0.07529]\n",
      "Step 508574  [5.421 sec/step, loss=0.07546, avg_loss=0.07528]\n",
      "Step 508575  [5.419 sec/step, loss=0.07696, avg_loss=0.07528]\n",
      "Step 508576  [5.407 sec/step, loss=0.07477, avg_loss=0.07529]\n",
      "Step 508577  [5.408 sec/step, loss=0.07622, avg_loss=0.07528]\n",
      "Step 508578  [5.405 sec/step, loss=0.07421, avg_loss=0.07525]\n",
      "Step 508579  [5.408 sec/step, loss=0.07653, avg_loss=0.07526]\n",
      "Step 508580  [5.421 sec/step, loss=0.07654, avg_loss=0.07526]\n",
      "Step 508581  [5.357 sec/step, loss=0.07237, avg_loss=0.07531]\n",
      "Step 508582  [5.361 sec/step, loss=0.07692, avg_loss=0.07532]\n",
      "Step 508583  [5.385 sec/step, loss=0.07756, avg_loss=0.07535]\n",
      "Step 508584  [5.411 sec/step, loss=0.07496, avg_loss=0.07541]\n",
      "Step 508585  [5.468 sec/step, loss=0.06804, avg_loss=0.07533]\n",
      "Step 508586  [5.440 sec/step, loss=0.06683, avg_loss=0.07521]\n",
      "Step 508587  [5.449 sec/step, loss=0.07659, avg_loss=0.07523]\n",
      "Step 508588  [5.450 sec/step, loss=0.07570, avg_loss=0.07520]\n",
      "Step 508589  [5.432 sec/step, loss=0.07455, avg_loss=0.07520]\n",
      "Step 508590  [5.437 sec/step, loss=0.07752, avg_loss=0.07520]\n",
      "Step 508591  [5.421 sec/step, loss=0.07267, avg_loss=0.07516]\n",
      "Step 508592  [5.426 sec/step, loss=0.07620, avg_loss=0.07516]\n",
      "Step 508593  [5.414 sec/step, loss=0.07671, avg_loss=0.07516]\n",
      "Step 508594  [5.436 sec/step, loss=0.07768, avg_loss=0.07521]\n",
      "Step 508595  [5.449 sec/step, loss=0.07725, avg_loss=0.07522]\n",
      "Step 508596  [5.403 sec/step, loss=0.07563, avg_loss=0.07530]\n",
      "Step 508597  [5.390 sec/step, loss=0.07625, avg_loss=0.07529]\n",
      "Step 508598  [5.388 sec/step, loss=0.07655, avg_loss=0.07527]\n",
      "Step 508599  [5.391 sec/step, loss=0.07582, avg_loss=0.07528]\n",
      "Step 508600  [5.421 sec/step, loss=0.07428, avg_loss=0.07526]\n",
      "Writing summary at step: 508600\n",
      "Step 508601  [5.408 sec/step, loss=0.07508, avg_loss=0.07524]\n",
      "Generated 32 batches of size 32 in 2.608 sec\n",
      "Step 508602  [5.409 sec/step, loss=0.07623, avg_loss=0.07524]\n",
      "Step 508603  [5.402 sec/step, loss=0.07377, avg_loss=0.07521]\n",
      "Step 508604  [5.376 sec/step, loss=0.07543, avg_loss=0.07519]\n",
      "Step 508605  [5.366 sec/step, loss=0.07541, avg_loss=0.07518]\n",
      "Step 508606  [5.379 sec/step, loss=0.07732, avg_loss=0.07522]\n",
      "Step 508607  [5.369 sec/step, loss=0.07322, avg_loss=0.07520]\n",
      "Step 508608  [5.384 sec/step, loss=0.07649, avg_loss=0.07524]\n",
      "Step 508609  [5.376 sec/step, loss=0.07507, avg_loss=0.07521]\n",
      "Step 508610  [5.350 sec/step, loss=0.07263, avg_loss=0.07517]\n",
      "Step 508611  [5.353 sec/step, loss=0.07438, avg_loss=0.07524]\n",
      "Step 508612  [5.360 sec/step, loss=0.07411, avg_loss=0.07523]\n",
      "Step 508613  [5.351 sec/step, loss=0.07541, avg_loss=0.07521]\n",
      "Step 508614  [5.351 sec/step, loss=0.07662, avg_loss=0.07521]\n",
      "Step 508615  [5.333 sec/step, loss=0.07452, avg_loss=0.07517]\n",
      "Step 508616  [5.343 sec/step, loss=0.07675, avg_loss=0.07517]\n",
      "Step 508617  [5.344 sec/step, loss=0.07640, avg_loss=0.07517]\n",
      "Step 508618  [5.348 sec/step, loss=0.07726, avg_loss=0.07516]\n",
      "Step 508619  [5.339 sec/step, loss=0.07773, avg_loss=0.07517]\n",
      "Step 508620  [5.381 sec/step, loss=0.06956, avg_loss=0.07510]\n",
      "Step 508621  [5.386 sec/step, loss=0.07335, avg_loss=0.07508]\n",
      "Step 508622  [5.408 sec/step, loss=0.07429, avg_loss=0.07515]\n",
      "Step 508623  [5.412 sec/step, loss=0.07466, avg_loss=0.07513]\n",
      "Step 508624  [5.416 sec/step, loss=0.07694, avg_loss=0.07515]\n",
      "Step 508625  [5.400 sec/step, loss=0.06832, avg_loss=0.07506]\n",
      "Step 508626  [5.404 sec/step, loss=0.07699, avg_loss=0.07507]\n",
      "Step 508627  [5.354 sec/step, loss=0.07354, avg_loss=0.07513]\n",
      "Step 508628  [5.358 sec/step, loss=0.07447, avg_loss=0.07511]\n",
      "Step 508629  [5.367 sec/step, loss=0.07766, avg_loss=0.07511]\n",
      "Step 508630  [5.388 sec/step, loss=0.07772, avg_loss=0.07514]\n",
      "Step 508631  [5.403 sec/step, loss=0.07526, avg_loss=0.07516]\n",
      "Step 508632  [5.403 sec/step, loss=0.07610, avg_loss=0.07516]\n",
      "Step 508633  [5.389 sec/step, loss=0.07610, avg_loss=0.07517]\n",
      "Generated 32 batches of size 32 in 2.308 sec\n",
      "Step 508634  [5.383 sec/step, loss=0.07697, avg_loss=0.07516]\n",
      "Step 508635  [5.402 sec/step, loss=0.07450, avg_loss=0.07513]\n",
      "Step 508636  [5.411 sec/step, loss=0.07545, avg_loss=0.07513]\n",
      "Step 508637  [5.424 sec/step, loss=0.07745, avg_loss=0.07514]\n",
      "Step 508638  [5.396 sec/step, loss=0.07201, avg_loss=0.07511]\n",
      "Step 508639  [5.397 sec/step, loss=0.07579, avg_loss=0.07514]\n",
      "Step 508640  [5.386 sec/step, loss=0.07612, avg_loss=0.07512]\n",
      "Step 508641  [5.363 sec/step, loss=0.07620, avg_loss=0.07514]\n",
      "Step 508642  [5.342 sec/step, loss=0.07318, avg_loss=0.07510]\n",
      "Step 508643  [5.353 sec/step, loss=0.07676, avg_loss=0.07515]\n",
      "Step 508644  [5.339 sec/step, loss=0.07568, avg_loss=0.07516]\n",
      "Step 508645  [5.337 sec/step, loss=0.07341, avg_loss=0.07516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 508646  [5.345 sec/step, loss=0.07736, avg_loss=0.07517]\n",
      "Step 508647  [5.362 sec/step, loss=0.07638, avg_loss=0.07519]\n",
      "Step 508648  [5.360 sec/step, loss=0.07672, avg_loss=0.07519]\n",
      "Step 508649  [5.339 sec/step, loss=0.07717, avg_loss=0.07520]\n",
      "Step 508650  [5.347 sec/step, loss=0.07708, avg_loss=0.07521]\n",
      "Step 508651  [5.342 sec/step, loss=0.07578, avg_loss=0.07521]\n",
      "Step 508652  [5.333 sec/step, loss=0.07603, avg_loss=0.07519]\n",
      "Step 508653  [5.333 sec/step, loss=0.07643, avg_loss=0.07520]\n",
      "Step 508654  [5.336 sec/step, loss=0.07749, avg_loss=0.07520]\n",
      "Step 508655  [5.329 sec/step, loss=0.06651, avg_loss=0.07513]\n",
      "Step 508656  [5.329 sec/step, loss=0.07485, avg_loss=0.07512]\n",
      "Step 508657  [5.335 sec/step, loss=0.07571, avg_loss=0.07512]\n",
      "Step 508658  [5.375 sec/step, loss=0.06842, avg_loss=0.07503]\n",
      "Step 508659  [5.357 sec/step, loss=0.07532, avg_loss=0.07501]\n",
      "Step 508660  [5.370 sec/step, loss=0.07688, avg_loss=0.07501]\n",
      "Step 508661  [5.344 sec/step, loss=0.07130, avg_loss=0.07495]\n",
      "Step 508662  [5.369 sec/step, loss=0.07431, avg_loss=0.07495]\n",
      "Step 508663  [5.348 sec/step, loss=0.07528, avg_loss=0.07495]\n",
      "Step 508664  [5.347 sec/step, loss=0.07527, avg_loss=0.07496]\n",
      "Step 508665  [5.348 sec/step, loss=0.07248, avg_loss=0.07493]\n",
      "Generated 32 batches of size 32 in 2.527 sec\n",
      "Step 508666  [5.362 sec/step, loss=0.07632, avg_loss=0.07497]\n",
      "Step 508667  [5.389 sec/step, loss=0.07644, avg_loss=0.07506]\n",
      "Step 508668  [5.356 sec/step, loss=0.07379, avg_loss=0.07506]\n",
      "Step 508669  [5.333 sec/step, loss=0.07140, avg_loss=0.07499]\n",
      "Step 508670  [5.354 sec/step, loss=0.07788, avg_loss=0.07505]\n",
      "Step 508671  [5.363 sec/step, loss=0.07512, avg_loss=0.07504]\n",
      "Step 508672  [5.363 sec/step, loss=0.07484, avg_loss=0.07503]\n",
      "Step 508673  [5.314 sec/step, loss=0.07648, avg_loss=0.07512]\n",
      "Step 508674  [5.308 sec/step, loss=0.07765, avg_loss=0.07514]\n",
      "Step 508675  [5.302 sec/step, loss=0.07487, avg_loss=0.07512]\n",
      "Step 508676  [5.293 sec/step, loss=0.06781, avg_loss=0.07505]\n",
      "Step 508677  [5.291 sec/step, loss=0.07530, avg_loss=0.07504]\n",
      "Step 508678  [5.312 sec/step, loss=0.07630, avg_loss=0.07506]\n",
      "Step 508679  [5.308 sec/step, loss=0.07439, avg_loss=0.07504]\n",
      "Step 508680  [5.302 sec/step, loss=0.07494, avg_loss=0.07503]\n",
      "Step 508681  [5.319 sec/step, loss=0.07594, avg_loss=0.07506]\n",
      "Step 508682  [5.326 sec/step, loss=0.07731, avg_loss=0.07507]\n",
      "Step 508683  [5.310 sec/step, loss=0.07337, avg_loss=0.07502]\n",
      "Step 508684  [5.294 sec/step, loss=0.07493, avg_loss=0.07502]\n",
      "Step 508685  [5.242 sec/step, loss=0.07394, avg_loss=0.07508]\n",
      "Step 508686  [5.307 sec/step, loss=0.06546, avg_loss=0.07507]\n",
      "Step 508687  [5.331 sec/step, loss=0.07559, avg_loss=0.07506]\n",
      "Step 508688  [5.322 sec/step, loss=0.07723, avg_loss=0.07507]\n",
      "Step 508689  [5.337 sec/step, loss=0.07684, avg_loss=0.07510]\n",
      "Step 508690  [5.340 sec/step, loss=0.07468, avg_loss=0.07507]\n",
      "Step 508691  [5.331 sec/step, loss=0.07519, avg_loss=0.07509]\n",
      "Step 508692  [5.335 sec/step, loss=0.07690, avg_loss=0.07510]\n",
      "Step 508693  [5.350 sec/step, loss=0.07720, avg_loss=0.07511]\n",
      "Step 508694  [5.345 sec/step, loss=0.07561, avg_loss=0.07509]\n",
      "Step 508695  [5.330 sec/step, loss=0.07618, avg_loss=0.07508]\n",
      "Step 508696  [5.316 sec/step, loss=0.07531, avg_loss=0.07507]\n",
      "Step 508697  [5.316 sec/step, loss=0.07536, avg_loss=0.07506]\n",
      "Generated 32 batches of size 32 in 2.385 sec\n",
      "Step 508698  [5.321 sec/step, loss=0.07666, avg_loss=0.07506]\n",
      "Step 508699  [5.338 sec/step, loss=0.07638, avg_loss=0.07507]\n",
      "Step 508700  [5.319 sec/step, loss=0.07635, avg_loss=0.07509]\n",
      "Writing summary at step: 508700\n",
      "Step 508701  [5.338 sec/step, loss=0.07760, avg_loss=0.07512]\n",
      "Step 508702  [5.323 sec/step, loss=0.07262, avg_loss=0.07508]\n",
      "Step 508703  [5.325 sec/step, loss=0.07671, avg_loss=0.07511]\n",
      "Step 508704  [5.325 sec/step, loss=0.07661, avg_loss=0.07512]\n",
      "Step 508705  [5.342 sec/step, loss=0.07717, avg_loss=0.07514]\n",
      "Step 508706  [5.325 sec/step, loss=0.07184, avg_loss=0.07508]\n",
      "Step 508707  [5.333 sec/step, loss=0.07609, avg_loss=0.07511]\n",
      "Step 508708  [5.329 sec/step, loss=0.07409, avg_loss=0.07509]\n",
      "Step 508709  [5.329 sec/step, loss=0.07627, avg_loss=0.07510]\n",
      "Step 508710  [5.343 sec/step, loss=0.07342, avg_loss=0.07511]\n",
      "Step 508711  [5.382 sec/step, loss=0.07485, avg_loss=0.07511]\n",
      "Step 508712  [5.376 sec/step, loss=0.07546, avg_loss=0.07513]\n",
      "Step 508713  [5.389 sec/step, loss=0.07284, avg_loss=0.07510]\n",
      "Step 508714  [5.403 sec/step, loss=0.07723, avg_loss=0.07511]\n",
      "Step 508715  [5.404 sec/step, loss=0.07627, avg_loss=0.07512]\n",
      "Step 508716  [5.400 sec/step, loss=0.07426, avg_loss=0.07510]\n",
      "Step 508717  [5.413 sec/step, loss=0.07736, avg_loss=0.07511]\n",
      "Step 508718  [5.410 sec/step, loss=0.07740, avg_loss=0.07511]\n",
      "Step 508719  [5.414 sec/step, loss=0.07407, avg_loss=0.07507]\n",
      "Step 508720  [5.377 sec/step, loss=0.07593, avg_loss=0.07514]\n",
      "Step 508721  [5.367 sec/step, loss=0.07418, avg_loss=0.07515]\n",
      "Step 508722  [5.412 sec/step, loss=0.06738, avg_loss=0.07508]\n",
      "Step 508723  [5.417 sec/step, loss=0.07519, avg_loss=0.07508]\n",
      "Step 508724  [5.430 sec/step, loss=0.07479, avg_loss=0.07506]\n",
      "Step 508725  [5.452 sec/step, loss=0.07756, avg_loss=0.07515]\n",
      "Step 508726  [5.441 sec/step, loss=0.07543, avg_loss=0.07514]\n",
      "Step 508727  [5.436 sec/step, loss=0.07480, avg_loss=0.07515]\n",
      "Step 508728  [5.411 sec/step, loss=0.06692, avg_loss=0.07507]\n",
      "Generated 32 batches of size 32 in 2.497 sec\n",
      "Step 508729  [5.398 sec/step, loss=0.07537, avg_loss=0.07505]\n",
      "Step 508730  [5.384 sec/step, loss=0.07313, avg_loss=0.07501]\n",
      "Step 508731  [5.380 sec/step, loss=0.07728, avg_loss=0.07503]\n",
      "Step 508732  [5.374 sec/step, loss=0.07454, avg_loss=0.07501]\n",
      "Step 508733  [5.377 sec/step, loss=0.07614, avg_loss=0.07501]\n",
      "Step 508734  [5.382 sec/step, loss=0.07749, avg_loss=0.07502]\n",
      "Step 508735  [5.351 sec/step, loss=0.07145, avg_loss=0.07499]\n",
      "Step 508736  [5.354 sec/step, loss=0.07741, avg_loss=0.07500]\n",
      "Step 508737  [5.336 sec/step, loss=0.07501, avg_loss=0.07498]\n",
      "Step 508738  [5.341 sec/step, loss=0.07656, avg_loss=0.07503]\n",
      "Step 508739  [5.349 sec/step, loss=0.07621, avg_loss=0.07503]\n",
      "Step 508740  [5.354 sec/step, loss=0.07643, avg_loss=0.07503]\n",
      "Step 508741  [5.354 sec/step, loss=0.07577, avg_loss=0.07503]\n",
      "Step 508742  [5.372 sec/step, loss=0.07730, avg_loss=0.07507]\n",
      "Step 508743  [5.364 sec/step, loss=0.07440, avg_loss=0.07505]\n",
      "Step 508744  [5.361 sec/step, loss=0.07558, avg_loss=0.07505]\n",
      "Step 508745  [5.366 sec/step, loss=0.07445, avg_loss=0.07506]\n",
      "Step 508746  [5.337 sec/step, loss=0.06647, avg_loss=0.07495]\n",
      "Step 508747  [5.315 sec/step, loss=0.07619, avg_loss=0.07495]\n",
      "Step 508748  [5.311 sec/step, loss=0.07640, avg_loss=0.07494]\n",
      "Step 508749  [5.324 sec/step, loss=0.07563, avg_loss=0.07493]\n",
      "Step 508750  [5.322 sec/step, loss=0.07786, avg_loss=0.07493]\n",
      "Step 508751  [5.309 sec/step, loss=0.07260, avg_loss=0.07490]\n",
      "Step 508752  [5.325 sec/step, loss=0.07441, avg_loss=0.07489]\n",
      "Step 508753  [5.329 sec/step, loss=0.07534, avg_loss=0.07488]\n",
      "Step 508754  [5.346 sec/step, loss=0.07443, avg_loss=0.07485]\n",
      "Step 508755  [5.354 sec/step, loss=0.07227, avg_loss=0.07490]\n",
      "Step 508756  [5.347 sec/step, loss=0.07497, avg_loss=0.07490]\n",
      "Step 508757  [5.352 sec/step, loss=0.07649, avg_loss=0.07491]\n",
      "Step 508758  [5.307 sec/step, loss=0.07662, avg_loss=0.07499]\n",
      "Step 508759  [5.307 sec/step, loss=0.07507, avg_loss=0.07499]\n",
      "Step 508760  [5.306 sec/step, loss=0.07717, avg_loss=0.07499]\n",
      "Generated 32 batches of size 32 in 2.547 sec\n",
      "Step 508761  [5.314 sec/step, loss=0.07525, avg_loss=0.07503]\n",
      "Step 508762  [5.298 sec/step, loss=0.07760, avg_loss=0.07507]\n",
      "Step 508763  [5.307 sec/step, loss=0.07329, avg_loss=0.07505]\n",
      "Step 508764  [5.304 sec/step, loss=0.07214, avg_loss=0.07502]\n",
      "Step 508765  [5.308 sec/step, loss=0.07642, avg_loss=0.07505]\n",
      "Step 508766  [5.316 sec/step, loss=0.07499, avg_loss=0.07504]\n",
      "Step 508767  [5.357 sec/step, loss=0.06834, avg_loss=0.07496]\n",
      "Step 508768  [5.377 sec/step, loss=0.07743, avg_loss=0.07500]\n",
      "Step 508769  [5.386 sec/step, loss=0.07466, avg_loss=0.07503]\n",
      "Step 508770  [5.382 sec/step, loss=0.07782, avg_loss=0.07503]\n",
      "Step 508771  [5.357 sec/step, loss=0.07177, avg_loss=0.07500]\n",
      "Step 508772  [5.360 sec/step, loss=0.07689, avg_loss=0.07502]\n",
      "Step 508773  [5.373 sec/step, loss=0.07645, avg_loss=0.07502]\n",
      "Step 508774  [5.393 sec/step, loss=0.07628, avg_loss=0.07500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 508775  [5.397 sec/step, loss=0.07573, avg_loss=0.07501]\n",
      "Step 508776  [5.422 sec/step, loss=0.07769, avg_loss=0.07511]\n",
      "Step 508777  [5.420 sec/step, loss=0.07345, avg_loss=0.07509]\n",
      "Step 508778  [5.402 sec/step, loss=0.07664, avg_loss=0.07509]\n",
      "Step 508779  [5.400 sec/step, loss=0.07530, avg_loss=0.07510]\n",
      "Step 508780  [5.398 sec/step, loss=0.07662, avg_loss=0.07512]\n",
      "Step 508781  [5.408 sec/step, loss=0.07732, avg_loss=0.07513]\n",
      "Step 508782  [5.394 sec/step, loss=0.07517, avg_loss=0.07511]\n",
      "Step 508783  [5.402 sec/step, loss=0.07718, avg_loss=0.07515]\n",
      "Step 508784  [5.420 sec/step, loss=0.07540, avg_loss=0.07516]\n",
      "Step 508785  [5.419 sec/step, loss=0.07669, avg_loss=0.07518]\n",
      "Step 508786  [5.367 sec/step, loss=0.07469, avg_loss=0.07528]\n",
      "Step 508787  [5.335 sec/step, loss=0.07495, avg_loss=0.07527]\n",
      "Step 508788  [5.330 sec/step, loss=0.07531, avg_loss=0.07525]\n",
      "Step 508789  [5.333 sec/step, loss=0.07716, avg_loss=0.07525]\n",
      "Step 508790  [5.327 sec/step, loss=0.07507, avg_loss=0.07526]\n",
      "Step 508791  [5.339 sec/step, loss=0.07584, avg_loss=0.07526]\n",
      "Step 508792  [5.325 sec/step, loss=0.07580, avg_loss=0.07525]\n",
      "Generated 32 batches of size 32 in 2.827 sec\n",
      "Step 508793  [5.304 sec/step, loss=0.06786, avg_loss=0.07516]\n",
      "Step 508794  [5.317 sec/step, loss=0.07696, avg_loss=0.07517]\n",
      "Step 508795  [5.309 sec/step, loss=0.07177, avg_loss=0.07513]\n",
      "Step 508796  [5.369 sec/step, loss=0.06658, avg_loss=0.07504]\n",
      "Step 508797  [5.373 sec/step, loss=0.07648, avg_loss=0.07505]\n",
      "Step 508798  [5.372 sec/step, loss=0.07674, avg_loss=0.07505]\n",
      "Step 508799  [5.358 sec/step, loss=0.07620, avg_loss=0.07505]\n",
      "Step 508800  [5.368 sec/step, loss=0.07482, avg_loss=0.07504]\n",
      "Writing summary at step: 508800\n",
      "Step 508801  [5.352 sec/step, loss=0.07494, avg_loss=0.07501]\n",
      "Step 508802  [5.358 sec/step, loss=0.07565, avg_loss=0.07504]\n",
      "Step 508803  [5.357 sec/step, loss=0.07319, avg_loss=0.07500]\n",
      "Step 508804  [5.371 sec/step, loss=0.07726, avg_loss=0.07501]\n",
      "Step 508805  [5.377 sec/step, loss=0.07542, avg_loss=0.07499]\n",
      "Step 508806  [5.389 sec/step, loss=0.07587, avg_loss=0.07503]\n",
      "Step 508807  [5.377 sec/step, loss=0.07494, avg_loss=0.07502]\n",
      "Step 508808  [5.402 sec/step, loss=0.07483, avg_loss=0.07503]\n",
      "Step 508809  [5.391 sec/step, loss=0.07468, avg_loss=0.07501]\n",
      "Step 508810  [5.387 sec/step, loss=0.07622, avg_loss=0.07504]\n",
      "Step 508811  [5.366 sec/step, loss=0.07652, avg_loss=0.07506]\n",
      "Step 508812  [5.376 sec/step, loss=0.07619, avg_loss=0.07507]\n",
      "Step 508813  [5.383 sec/step, loss=0.07703, avg_loss=0.07511]\n",
      "Step 508814  [5.382 sec/step, loss=0.07542, avg_loss=0.07509]\n",
      "Step 508815  [5.383 sec/step, loss=0.07642, avg_loss=0.07509]\n",
      "Step 508816  [5.389 sec/step, loss=0.07688, avg_loss=0.07512]\n",
      "Step 508817  [5.376 sec/step, loss=0.07471, avg_loss=0.07509]\n",
      "Step 508818  [5.381 sec/step, loss=0.07786, avg_loss=0.07509]\n",
      "Step 508819  [5.372 sec/step, loss=0.07600, avg_loss=0.07511]\n",
      "Step 508820  [5.364 sec/step, loss=0.07363, avg_loss=0.07509]\n",
      "Step 508821  [5.386 sec/step, loss=0.07721, avg_loss=0.07512]\n",
      "Step 508822  [5.322 sec/step, loss=0.07464, avg_loss=0.07519]\n",
      "Step 508823  [5.317 sec/step, loss=0.07459, avg_loss=0.07519]\n",
      "Generated 32 batches of size 32 in 2.381 sec\n",
      "Step 508824  [5.307 sec/step, loss=0.07757, avg_loss=0.07522]\n",
      "Step 508825  [5.297 sec/step, loss=0.07629, avg_loss=0.07520]\n",
      "Step 508826  [5.285 sec/step, loss=0.06747, avg_loss=0.07512]\n",
      "Step 508827  [5.294 sec/step, loss=0.07624, avg_loss=0.07514]\n",
      "Step 508828  [5.311 sec/step, loss=0.07701, avg_loss=0.07524]\n",
      "Step 508829  [5.324 sec/step, loss=0.07647, avg_loss=0.07525]\n",
      "Step 508830  [5.375 sec/step, loss=0.06814, avg_loss=0.07520]\n",
      "Step 508831  [5.370 sec/step, loss=0.07627, avg_loss=0.07519]\n",
      "Step 508832  [5.374 sec/step, loss=0.07224, avg_loss=0.07517]\n",
      "Step 508833  [5.373 sec/step, loss=0.07648, avg_loss=0.07517]\n",
      "Step 508834  [5.375 sec/step, loss=0.07718, avg_loss=0.07517]\n",
      "Step 508835  [5.384 sec/step, loss=0.07496, avg_loss=0.07520]\n",
      "Step 508836  [5.372 sec/step, loss=0.07510, avg_loss=0.07518]\n",
      "Step 508837  [5.373 sec/step, loss=0.07697, avg_loss=0.07520]\n",
      "Step 508838  [5.369 sec/step, loss=0.07501, avg_loss=0.07518]\n",
      "Step 508839  [5.377 sec/step, loss=0.07735, avg_loss=0.07519]\n",
      "Step 508840  [5.376 sec/step, loss=0.07551, avg_loss=0.07519]\n",
      "Step 508841  [5.378 sec/step, loss=0.07502, avg_loss=0.07518]\n",
      "Step 508842  [5.381 sec/step, loss=0.07739, avg_loss=0.07518]\n",
      "Step 508843  [5.388 sec/step, loss=0.07511, avg_loss=0.07519]\n",
      "Step 508844  [5.371 sec/step, loss=0.06697, avg_loss=0.07510]\n",
      "Step 508845  [5.358 sec/step, loss=0.07150, avg_loss=0.07507]\n",
      "Step 508846  [5.398 sec/step, loss=0.07662, avg_loss=0.07517]\n",
      "Step 508847  [5.397 sec/step, loss=0.07418, avg_loss=0.07515]\n",
      "Step 508848  [5.403 sec/step, loss=0.07578, avg_loss=0.07515]\n",
      "Step 508849  [5.389 sec/step, loss=0.07643, avg_loss=0.07515]\n",
      "Step 508850  [5.373 sec/step, loss=0.07029, avg_loss=0.07508]\n",
      "Step 508851  [5.385 sec/step, loss=0.07552, avg_loss=0.07511]\n",
      "Step 508852  [5.415 sec/step, loss=0.06643, avg_loss=0.07503]\n",
      "Step 508853  [5.410 sec/step, loss=0.07750, avg_loss=0.07505]\n",
      "Step 508854  [5.383 sec/step, loss=0.07632, avg_loss=0.07507]\n",
      "Step 508855  [5.410 sec/step, loss=0.07639, avg_loss=0.07511]\n",
      "Generated 32 batches of size 32 in 2.558 sec\n",
      "Step 508856  [5.415 sec/step, loss=0.07194, avg_loss=0.07508]\n",
      "Step 508857  [5.421 sec/step, loss=0.07738, avg_loss=0.07509]\n",
      "Step 508858  [5.431 sec/step, loss=0.07646, avg_loss=0.07509]\n",
      "Step 508859  [5.431 sec/step, loss=0.07558, avg_loss=0.07509]\n",
      "Step 508860  [5.412 sec/step, loss=0.07382, avg_loss=0.07506]\n",
      "Step 508861  [5.416 sec/step, loss=0.07661, avg_loss=0.07507]\n",
      "Step 508862  [5.410 sec/step, loss=0.07665, avg_loss=0.07506]\n",
      "Step 508863  [5.418 sec/step, loss=0.07703, avg_loss=0.07510]\n",
      "Step 508864  [5.426 sec/step, loss=0.07678, avg_loss=0.07515]\n",
      "Step 508865  [5.452 sec/step, loss=0.07460, avg_loss=0.07513]\n",
      "Step 508866  [5.462 sec/step, loss=0.07467, avg_loss=0.07512]\n",
      "Step 508867  [5.420 sec/step, loss=0.07731, avg_loss=0.07521]\n",
      "Step 508868  [5.404 sec/step, loss=0.07641, avg_loss=0.07520]\n",
      "Step 508869  [5.405 sec/step, loss=0.07507, avg_loss=0.07521]\n",
      "Step 508870  [5.383 sec/step, loss=0.07352, avg_loss=0.07516]\n",
      "Step 508871  [5.401 sec/step, loss=0.07458, avg_loss=0.07519]\n",
      "Step 508872  [5.396 sec/step, loss=0.07596, avg_loss=0.07518]\n",
      "Step 508873  [5.372 sec/step, loss=0.07220, avg_loss=0.07514]\n",
      "Step 508874  [5.341 sec/step, loss=0.07109, avg_loss=0.07509]\n",
      "Step 508875  [5.357 sec/step, loss=0.07801, avg_loss=0.07511]\n",
      "Step 508876  [5.359 sec/step, loss=0.07547, avg_loss=0.07509]\n",
      "Step 508877  [5.371 sec/step, loss=0.07739, avg_loss=0.07513]\n",
      "Step 508878  [5.386 sec/step, loss=0.07751, avg_loss=0.07514]\n",
      "Step 508879  [5.405 sec/step, loss=0.07686, avg_loss=0.07515]\n",
      "Step 508880  [5.397 sec/step, loss=0.07348, avg_loss=0.07512]\n",
      "Step 508881  [5.388 sec/step, loss=0.07614, avg_loss=0.07511]\n",
      "Step 508882  [5.383 sec/step, loss=0.07318, avg_loss=0.07509]\n",
      "Step 508883  [5.382 sec/step, loss=0.07504, avg_loss=0.07507]\n",
      "Step 508884  [5.378 sec/step, loss=0.07525, avg_loss=0.07507]\n",
      "Step 508885  [5.391 sec/step, loss=0.07574, avg_loss=0.07506]\n",
      "Step 508886  [5.395 sec/step, loss=0.07668, avg_loss=0.07508]\n",
      "Step 508887  [5.399 sec/step, loss=0.07514, avg_loss=0.07508]\n",
      "Generated 32 batches of size 32 in 2.311 sec\n",
      "Step 508888  [5.414 sec/step, loss=0.07723, avg_loss=0.07510]\n",
      "Step 508889  [5.396 sec/step, loss=0.07539, avg_loss=0.07508]\n",
      "Step 508890  [5.396 sec/step, loss=0.07632, avg_loss=0.07509]\n",
      "Step 508891  [5.392 sec/step, loss=0.07598, avg_loss=0.07509]\n",
      "Step 508892  [5.443 sec/step, loss=0.06722, avg_loss=0.07501]\n",
      "Step 508893  [5.454 sec/step, loss=0.07626, avg_loss=0.07509]\n",
      "Step 508894  [5.438 sec/step, loss=0.07676, avg_loss=0.07509]\n",
      "Step 508895  [5.461 sec/step, loss=0.07733, avg_loss=0.07515]\n",
      "Step 508896  [5.395 sec/step, loss=0.06732, avg_loss=0.07515]\n",
      "Step 508897  [5.388 sec/step, loss=0.07641, avg_loss=0.07515]\n",
      "Step 508898  [5.376 sec/step, loss=0.07460, avg_loss=0.07513]\n",
      "Step 508899  [5.367 sec/step, loss=0.07582, avg_loss=0.07513]\n",
      "Step 508900  [5.361 sec/step, loss=0.07523, avg_loss=0.07513]\n",
      "Writing summary at step: 508900\n",
      "Step 508901  [5.378 sec/step, loss=0.07649, avg_loss=0.07515]\n",
      "Step 508902  [5.385 sec/step, loss=0.07450, avg_loss=0.07514]\n",
      "Step 508903  [5.380 sec/step, loss=0.07642, avg_loss=0.07517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 508904  [5.365 sec/step, loss=0.07473, avg_loss=0.07514]\n",
      "Step 508905  [5.350 sec/step, loss=0.07373, avg_loss=0.07513]\n",
      "Step 508906  [5.346 sec/step, loss=0.07265, avg_loss=0.07509]\n",
      "Step 508907  [5.356 sec/step, loss=0.07700, avg_loss=0.07511]\n",
      "Step 508908  [5.319 sec/step, loss=0.07136, avg_loss=0.07508]\n",
      "Step 508909  [5.338 sec/step, loss=0.07720, avg_loss=0.07511]\n",
      "Step 508910  [5.328 sec/step, loss=0.07569, avg_loss=0.07510]\n",
      "Step 508911  [5.334 sec/step, loss=0.07755, avg_loss=0.07511]\n",
      "Step 508912  [5.318 sec/step, loss=0.07172, avg_loss=0.07507]\n",
      "Step 508913  [5.304 sec/step, loss=0.07622, avg_loss=0.07506]\n",
      "Step 508914  [5.300 sec/step, loss=0.07703, avg_loss=0.07507]\n",
      "Step 508915  [5.304 sec/step, loss=0.07701, avg_loss=0.07508]\n",
      "Step 508916  [5.300 sec/step, loss=0.07575, avg_loss=0.07507]\n",
      "Step 508917  [5.326 sec/step, loss=0.07505, avg_loss=0.07507]\n",
      "Step 508918  [5.317 sec/step, loss=0.07715, avg_loss=0.07506]\n",
      "Generated 32 batches of size 32 in 2.377 sec\n",
      "Step 508919  [5.323 sec/step, loss=0.07609, avg_loss=0.07507]\n",
      "Step 508920  [5.331 sec/step, loss=0.07654, avg_loss=0.07509]\n",
      "Step 508921  [5.374 sec/step, loss=0.06767, avg_loss=0.07500]\n",
      "Step 508922  [5.405 sec/step, loss=0.07670, avg_loss=0.07502]\n",
      "Step 508923  [5.419 sec/step, loss=0.07721, avg_loss=0.07505]\n",
      "Step 508924  [5.403 sec/step, loss=0.07327, avg_loss=0.07500]\n",
      "Step 508925  [5.389 sec/step, loss=0.06642, avg_loss=0.07490]\n",
      "Step 508926  [5.407 sec/step, loss=0.07633, avg_loss=0.07499]\n",
      "Step 508927  [5.389 sec/step, loss=0.07186, avg_loss=0.07495]\n",
      "Step 508928  [5.390 sec/step, loss=0.07418, avg_loss=0.07492]\n",
      "Step 508929  [5.375 sec/step, loss=0.07303, avg_loss=0.07489]\n",
      "Step 508930  [5.332 sec/step, loss=0.07456, avg_loss=0.07495]\n",
      "Step 508931  [5.334 sec/step, loss=0.07612, avg_loss=0.07495]\n",
      "Step 508932  [5.349 sec/step, loss=0.07597, avg_loss=0.07499]\n",
      "Step 508933  [5.340 sec/step, loss=0.07455, avg_loss=0.07497]\n",
      "Step 508934  [5.320 sec/step, loss=0.07578, avg_loss=0.07495]\n",
      "Step 508935  [5.314 sec/step, loss=0.07545, avg_loss=0.07496]\n",
      "Step 508936  [5.331 sec/step, loss=0.07725, avg_loss=0.07498]\n",
      "Step 508937  [5.316 sec/step, loss=0.06731, avg_loss=0.07488]\n",
      "Step 508938  [5.312 sec/step, loss=0.07230, avg_loss=0.07486]\n",
      "Step 508939  [5.324 sec/step, loss=0.07527, avg_loss=0.07483]\n",
      "Step 508940  [5.315 sec/step, loss=0.07610, avg_loss=0.07484]\n",
      "Step 508941  [5.315 sec/step, loss=0.07864, avg_loss=0.07488]\n",
      "Step 508942  [5.317 sec/step, loss=0.07514, avg_loss=0.07485]\n",
      "Step 508943  [5.319 sec/step, loss=0.07656, avg_loss=0.07487]\n",
      "Step 508944  [5.335 sec/step, loss=0.07566, avg_loss=0.07496]\n",
      "Step 508945  [5.343 sec/step, loss=0.07533, avg_loss=0.07499]\n",
      "Step 508946  [5.329 sec/step, loss=0.07672, avg_loss=0.07500]\n",
      "Step 508947  [5.321 sec/step, loss=0.07269, avg_loss=0.07498]\n",
      "Step 508948  [5.341 sec/step, loss=0.07457, avg_loss=0.07497]\n",
      "Step 508949  [5.349 sec/step, loss=0.07737, avg_loss=0.07498]\n",
      "Step 508950  [5.373 sec/step, loss=0.07772, avg_loss=0.07505]\n",
      "Generated 32 batches of size 32 in 2.306 sec\n",
      "Step 508951  [5.380 sec/step, loss=0.07684, avg_loss=0.07507]\n",
      "Step 508952  [5.342 sec/step, loss=0.07739, avg_loss=0.07517]\n",
      "Step 508953  [5.344 sec/step, loss=0.07534, avg_loss=0.07515]\n",
      "Step 508954  [5.349 sec/step, loss=0.07484, avg_loss=0.07514]\n",
      "Step 508955  [5.328 sec/step, loss=0.07441, avg_loss=0.07512]\n",
      "Step 508956  [5.377 sec/step, loss=0.06699, avg_loss=0.07507]\n",
      "Step 508957  [5.357 sec/step, loss=0.07537, avg_loss=0.07505]\n",
      "Step 508958  [5.341 sec/step, loss=0.07287, avg_loss=0.07501]\n",
      "Step 508959  [5.356 sec/step, loss=0.07612, avg_loss=0.07502]\n",
      "Step 508960  [5.387 sec/step, loss=0.07452, avg_loss=0.07503]\n",
      "Step 508961  [5.395 sec/step, loss=0.07731, avg_loss=0.07503]\n",
      "Step 508962  [5.392 sec/step, loss=0.07573, avg_loss=0.07502]\n",
      "Step 508963  [5.397 sec/step, loss=0.07743, avg_loss=0.07503]\n",
      "Step 508964  [5.392 sec/step, loss=0.07664, avg_loss=0.07503]\n",
      "Step 508965  [5.358 sec/step, loss=0.07546, avg_loss=0.07503]\n",
      "Step 508966  [5.349 sec/step, loss=0.07817, avg_loss=0.07507]\n",
      "Step 508967  [5.333 sec/step, loss=0.07181, avg_loss=0.07501]\n",
      "Step 508968  [5.344 sec/step, loss=0.07717, avg_loss=0.07502]\n",
      "Step 508969  [5.344 sec/step, loss=0.07460, avg_loss=0.07502]\n",
      "Step 508970  [5.367 sec/step, loss=0.07600, avg_loss=0.07504]\n",
      "Step 508971  [5.383 sec/step, loss=0.07450, avg_loss=0.07504]\n",
      "Step 508972  [5.400 sec/step, loss=0.07693, avg_loss=0.07505]\n",
      "Step 508973  [5.412 sec/step, loss=0.07566, avg_loss=0.07509]\n",
      "Step 508974  [5.422 sec/step, loss=0.07578, avg_loss=0.07513]\n",
      "Step 508975  [5.402 sec/step, loss=0.07518, avg_loss=0.07510]\n",
      "Step 508976  [5.398 sec/step, loss=0.07745, avg_loss=0.07512]\n",
      "Step 508977  [5.390 sec/step, loss=0.07567, avg_loss=0.07511]\n",
      "Step 508978  [5.378 sec/step, loss=0.07672, avg_loss=0.07510]\n",
      "Step 508979  [5.370 sec/step, loss=0.07479, avg_loss=0.07508]\n",
      "Step 508980  [5.371 sec/step, loss=0.07659, avg_loss=0.07511]\n",
      "Step 508981  [5.352 sec/step, loss=0.06765, avg_loss=0.07502]\n",
      "Step 508982  [5.374 sec/step, loss=0.07737, avg_loss=0.07507]\n",
      "Generated 32 batches of size 32 in 2.554 sec\n",
      "Step 508983  [5.376 sec/step, loss=0.07639, avg_loss=0.07508]\n",
      "Step 508984  [5.375 sec/step, loss=0.07506, avg_loss=0.07508]\n",
      "Step 508985  [5.361 sec/step, loss=0.07166, avg_loss=0.07504]\n",
      "Step 508986  [5.361 sec/step, loss=0.07631, avg_loss=0.07503]\n",
      "Step 508987  [5.354 sec/step, loss=0.07360, avg_loss=0.07502]\n",
      "Step 508988  [5.341 sec/step, loss=0.07388, avg_loss=0.07498]\n",
      "Step 508989  [5.334 sec/step, loss=0.07341, avg_loss=0.07497]\n",
      "Step 508990  [5.380 sec/step, loss=0.06816, avg_loss=0.07488]\n",
      "Step 508991  [5.385 sec/step, loss=0.07608, avg_loss=0.07488]\n",
      "Step 508992  [5.335 sec/step, loss=0.07699, avg_loss=0.07498]\n",
      "Step 508993  [5.328 sec/step, loss=0.07507, avg_loss=0.07497]\n",
      "Step 508994  [5.322 sec/step, loss=0.07635, avg_loss=0.07497]\n",
      "Step 508995  [5.317 sec/step, loss=0.07757, avg_loss=0.07497]\n",
      "Step 508996  [5.340 sec/step, loss=0.07720, avg_loss=0.07507]\n",
      "Step 508997  [5.332 sec/step, loss=0.07256, avg_loss=0.07503]\n",
      "Step 508998  [5.338 sec/step, loss=0.07687, avg_loss=0.07505]\n",
      "Step 508999  [5.356 sec/step, loss=0.07584, avg_loss=0.07505]\n",
      "Step 509000  [5.349 sec/step, loss=0.07381, avg_loss=0.07504]\n",
      "Writing summary at step: 509000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-509000\n",
      "Saving audio and alignment...\n",
      "Input: tshaylnay or piisnay kay tdhoorrii ddayr baadd bayrrijaan tshirraa kur nanii tsirrjaa kay pankhay samiitd bayloon kii tdarf dzistd lagaaii~_\n",
      "Step 509001  [5.340 sec/step, loss=0.07646, avg_loss=0.07504]\n",
      "Step 509002  [5.338 sec/step, loss=0.07634, avg_loss=0.07506]\n",
      "Step 509003  [5.334 sec/step, loss=0.07510, avg_loss=0.07504]\n",
      "Step 509004  [5.325 sec/step, loss=0.07282, avg_loss=0.07502]\n",
      "Step 509005  [5.326 sec/step, loss=0.07574, avg_loss=0.07504]\n",
      "Step 509006  [5.321 sec/step, loss=0.07377, avg_loss=0.07505]\n",
      "Step 509007  [5.322 sec/step, loss=0.07634, avg_loss=0.07505]\n",
      "Step 509008  [5.345 sec/step, loss=0.07437, avg_loss=0.07508]\n",
      "Step 509009  [5.327 sec/step, loss=0.07551, avg_loss=0.07506]\n",
      "Step 509010  [5.335 sec/step, loss=0.07569, avg_loss=0.07506]\n",
      "Step 509011  [5.308 sec/step, loss=0.06730, avg_loss=0.07496]\n",
      "Step 509012  [5.340 sec/step, loss=0.07545, avg_loss=0.07500]\n",
      "Generated 32 batches of size 32 in 2.342 sec\n",
      "Step 509013  [5.349 sec/step, loss=0.07583, avg_loss=0.07499]\n",
      "Step 509014  [5.363 sec/step, loss=0.07564, avg_loss=0.07498]\n",
      "Step 509015  [5.370 sec/step, loss=0.07735, avg_loss=0.07498]\n",
      "Step 509016  [5.364 sec/step, loss=0.07734, avg_loss=0.07500]\n",
      "Step 509017  [5.334 sec/step, loss=0.07477, avg_loss=0.07499]\n",
      "Step 509018  [5.328 sec/step, loss=0.07318, avg_loss=0.07496]\n",
      "Step 509019  [5.323 sec/step, loss=0.07278, avg_loss=0.07492]\n",
      "Step 509020  [5.333 sec/step, loss=0.07635, avg_loss=0.07492]\n",
      "Step 509021  [5.333 sec/step, loss=0.06707, avg_loss=0.07491]\n",
      "Step 509022  [5.317 sec/step, loss=0.07621, avg_loss=0.07491]\n",
      "Step 509023  [5.320 sec/step, loss=0.07708, avg_loss=0.07491]\n",
      "Step 509024  [5.327 sec/step, loss=0.07403, avg_loss=0.07492]\n",
      "Step 509025  [5.342 sec/step, loss=0.07651, avg_loss=0.07502]\n",
      "Step 509026  [5.324 sec/step, loss=0.06624, avg_loss=0.07492]\n",
      "Step 509027  [5.387 sec/step, loss=0.06717, avg_loss=0.07487]\n",
      "Step 509028  [5.412 sec/step, loss=0.07277, avg_loss=0.07485]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 509029  [5.413 sec/step, loss=0.07695, avg_loss=0.07489]\n",
      "Step 509030  [5.410 sec/step, loss=0.07528, avg_loss=0.07490]\n",
      "Step 509031  [5.413 sec/step, loss=0.07457, avg_loss=0.07489]\n",
      "Step 509032  [5.402 sec/step, loss=0.07395, avg_loss=0.07487]\n",
      "Step 509033  [5.420 sec/step, loss=0.07723, avg_loss=0.07489]\n",
      "Step 509034  [5.428 sec/step, loss=0.07603, avg_loss=0.07489]\n",
      "Step 509035  [5.434 sec/step, loss=0.07669, avg_loss=0.07491]\n",
      "Step 509036  [5.413 sec/step, loss=0.07559, avg_loss=0.07489]\n",
      "Step 509037  [5.439 sec/step, loss=0.07789, avg_loss=0.07500]\n",
      "Step 509038  [5.464 sec/step, loss=0.07611, avg_loss=0.07503]\n",
      "Step 509039  [5.454 sec/step, loss=0.07512, avg_loss=0.07503]\n",
      "Step 509040  [5.453 sec/step, loss=0.07489, avg_loss=0.07502]\n",
      "Step 509041  [5.444 sec/step, loss=0.07174, avg_loss=0.07495]\n",
      "Step 509042  [5.434 sec/step, loss=0.07566, avg_loss=0.07496]\n",
      "Step 509043  [5.437 sec/step, loss=0.07660, avg_loss=0.07496]\n",
      "Step 509044  [5.455 sec/step, loss=0.07506, avg_loss=0.07495]\n",
      "Generated 32 batches of size 32 in 2.559 sec\n",
      "Step 509045  [5.453 sec/step, loss=0.07158, avg_loss=0.07491]\n",
      "Step 509046  [5.451 sec/step, loss=0.07736, avg_loss=0.07492]\n",
      "Step 509047  [5.474 sec/step, loss=0.07705, avg_loss=0.07496]\n",
      "Step 509048  [5.450 sec/step, loss=0.07545, avg_loss=0.07497]\n",
      "Step 509049  [5.432 sec/step, loss=0.07312, avg_loss=0.07493]\n",
      "Step 509050  [5.438 sec/step, loss=0.07578, avg_loss=0.07491]\n",
      "Step 509051  [5.425 sec/step, loss=0.07410, avg_loss=0.07488]\n",
      "Step 509052  [5.418 sec/step, loss=0.07666, avg_loss=0.07488]\n",
      "Step 509053  [5.398 sec/step, loss=0.07598, avg_loss=0.07488]\n",
      "Step 509054  [5.405 sec/step, loss=0.07544, avg_loss=0.07489]\n",
      "Step 509055  [5.407 sec/step, loss=0.07644, avg_loss=0.07491]\n",
      "Step 509056  [5.340 sec/step, loss=0.06741, avg_loss=0.07491]\n",
      "Step 509057  [5.343 sec/step, loss=0.07511, avg_loss=0.07491]\n",
      "Step 509058  [5.364 sec/step, loss=0.07643, avg_loss=0.07495]\n",
      "Step 509059  [5.351 sec/step, loss=0.07450, avg_loss=0.07493]\n",
      "Step 509060  [5.318 sec/step, loss=0.07246, avg_loss=0.07491]\n",
      "Step 509061  [5.323 sec/step, loss=0.07756, avg_loss=0.07491]\n",
      "Step 509062  [5.313 sec/step, loss=0.07520, avg_loss=0.07491]\n",
      "Step 509063  [5.303 sec/step, loss=0.07548, avg_loss=0.07489]\n",
      "Step 509064  [5.304 sec/step, loss=0.07655, avg_loss=0.07489]\n",
      "Step 509065  [5.316 sec/step, loss=0.07621, avg_loss=0.07489]\n",
      "Step 509066  [5.314 sec/step, loss=0.07646, avg_loss=0.07488]\n",
      "Step 509067  [5.319 sec/step, loss=0.07458, avg_loss=0.07490]\n",
      "Step 509068  [5.297 sec/step, loss=0.07201, avg_loss=0.07485]\n",
      "Step 509069  [5.314 sec/step, loss=0.07740, avg_loss=0.07488]\n",
      "Step 509070  [5.310 sec/step, loss=0.07642, avg_loss=0.07488]\n",
      "Step 509071  [5.341 sec/step, loss=0.06693, avg_loss=0.07481]\n",
      "Step 509072  [5.341 sec/step, loss=0.07596, avg_loss=0.07480]\n",
      "Step 509073  [5.341 sec/step, loss=0.07648, avg_loss=0.07481]\n",
      "Step 509074  [5.333 sec/step, loss=0.07504, avg_loss=0.07480]\n",
      "Step 509075  [5.339 sec/step, loss=0.07578, avg_loss=0.07481]\n",
      "Step 509076  [5.332 sec/step, loss=0.07702, avg_loss=0.07480]\n",
      "Generated 32 batches of size 32 in 2.501 sec\n",
      "Step 509077  [5.339 sec/step, loss=0.07674, avg_loss=0.07481]\n",
      "Step 509078  [5.363 sec/step, loss=0.07715, avg_loss=0.07482]\n",
      "Step 509079  [5.364 sec/step, loss=0.07557, avg_loss=0.07482]\n",
      "Step 509080  [5.374 sec/step, loss=0.07752, avg_loss=0.07483]\n",
      "Step 509081  [5.393 sec/step, loss=0.07377, avg_loss=0.07489]\n",
      "Step 509082  [5.380 sec/step, loss=0.07305, avg_loss=0.07485]\n",
      "Step 509083  [5.387 sec/step, loss=0.07734, avg_loss=0.07486]\n",
      "Step 509084  [5.392 sec/step, loss=0.07723, avg_loss=0.07488]\n",
      "Step 509085  [5.406 sec/step, loss=0.07690, avg_loss=0.07493]\n",
      "Step 509086  [5.407 sec/step, loss=0.07709, avg_loss=0.07494]\n",
      "Step 509087  [5.422 sec/step, loss=0.07628, avg_loss=0.07497]\n",
      "Step 509088  [5.423 sec/step, loss=0.07506, avg_loss=0.07498]\n",
      "Step 509089  [5.430 sec/step, loss=0.07532, avg_loss=0.07500]\n",
      "Step 509090  [5.378 sec/step, loss=0.07596, avg_loss=0.07508]\n",
      "Step 509091  [5.378 sec/step, loss=0.07466, avg_loss=0.07506]\n",
      "Step 509092  [5.389 sec/step, loss=0.07510, avg_loss=0.07505]\n",
      "Step 509093  [5.408 sec/step, loss=0.07759, avg_loss=0.07507]\n",
      "Step 509094  [5.399 sec/step, loss=0.07250, avg_loss=0.07503]\n",
      "Step 509095  [5.389 sec/step, loss=0.07651, avg_loss=0.07502]\n",
      "Step 509096  [5.390 sec/step, loss=0.07534, avg_loss=0.07500]\n",
      "Step 509097  [5.451 sec/step, loss=0.06793, avg_loss=0.07496]\n",
      "Step 509098  [5.477 sec/step, loss=0.07470, avg_loss=0.07493]\n",
      "Step 509099  [5.471 sec/step, loss=0.07652, avg_loss=0.07494]\n",
      "Step 509100  [5.488 sec/step, loss=0.07739, avg_loss=0.07498]\n",
      "Writing summary at step: 509100\n",
      "Step 509101  [5.477 sec/step, loss=0.07473, avg_loss=0.07496]\n",
      "Step 509102  [5.478 sec/step, loss=0.07306, avg_loss=0.07493]\n",
      "Step 509103  [5.489 sec/step, loss=0.07658, avg_loss=0.07494]\n",
      "Step 509104  [5.512 sec/step, loss=0.07756, avg_loss=0.07499]\n",
      "Step 509105  [5.510 sec/step, loss=0.07413, avg_loss=0.07497]\n",
      "Step 509106  [5.504 sec/step, loss=0.07168, avg_loss=0.07495]\n",
      "Step 509107  [5.511 sec/step, loss=0.07454, avg_loss=0.07493]\n",
      "Generated 32 batches of size 32 in 2.406 sec\n",
      "Step 509108  [5.512 sec/step, loss=0.07581, avg_loss=0.07495]\n",
      "Step 509109  [5.543 sec/step, loss=0.07406, avg_loss=0.07493]\n",
      "Step 509110  [5.528 sec/step, loss=0.06675, avg_loss=0.07484]\n",
      "Step 509111  [5.547 sec/step, loss=0.07614, avg_loss=0.07493]\n",
      "Step 509112  [5.512 sec/step, loss=0.07572, avg_loss=0.07494]\n",
      "Step 509113  [5.505 sec/step, loss=0.07445, avg_loss=0.07492]\n",
      "Step 509114  [5.484 sec/step, loss=0.07640, avg_loss=0.07493]\n",
      "Step 509115  [5.477 sec/step, loss=0.07593, avg_loss=0.07492]\n",
      "Step 509116  [5.483 sec/step, loss=0.07741, avg_loss=0.07492]\n",
      "Step 509117  [5.539 sec/step, loss=0.06690, avg_loss=0.07484]\n",
      "Step 509118  [5.547 sec/step, loss=0.07580, avg_loss=0.07486]\n",
      "Step 509119  [5.570 sec/step, loss=0.07647, avg_loss=0.07490]\n",
      "Step 509120  [5.575 sec/step, loss=0.07692, avg_loss=0.07491]\n",
      "Step 509121  [5.510 sec/step, loss=0.07364, avg_loss=0.07497]\n",
      "Step 509122  [5.507 sec/step, loss=0.07663, avg_loss=0.07498]\n",
      "Step 509123  [5.500 sec/step, loss=0.07578, avg_loss=0.07496]\n",
      "Step 509124  [5.511 sec/step, loss=0.07765, avg_loss=0.07500]\n",
      "Step 509125  [5.514 sec/step, loss=0.07568, avg_loss=0.07499]\n",
      "Step 509126  [5.534 sec/step, loss=0.07742, avg_loss=0.07510]\n",
      "Step 509127  [5.488 sec/step, loss=0.07692, avg_loss=0.07520]\n",
      "Step 509128  [5.452 sec/step, loss=0.07327, avg_loss=0.07521]\n",
      "Step 509129  [5.452 sec/step, loss=0.07296, avg_loss=0.07517]\n",
      "Step 509130  [5.431 sec/step, loss=0.06823, avg_loss=0.07509]\n",
      "Step 509131  [5.414 sec/step, loss=0.07520, avg_loss=0.07510]\n",
      "Step 509132  [5.410 sec/step, loss=0.07516, avg_loss=0.07511]\n",
      "Step 509133  [5.411 sec/step, loss=0.07583, avg_loss=0.07510]\n",
      "Step 509134  [5.415 sec/step, loss=0.07603, avg_loss=0.07510]\n",
      "Step 509135  [5.408 sec/step, loss=0.07670, avg_loss=0.07510]\n",
      "Step 509136  [5.417 sec/step, loss=0.07270, avg_loss=0.07507]\n",
      "Step 509137  [5.422 sec/step, loss=0.07730, avg_loss=0.07506]\n",
      "Step 509138  [5.412 sec/step, loss=0.07270, avg_loss=0.07503]\n",
      "Step 509139  [5.415 sec/step, loss=0.07745, avg_loss=0.07505]\n",
      "Generated 32 batches of size 32 in 2.453 sec\n",
      "Step 509140  [5.440 sec/step, loss=0.07683, avg_loss=0.07507]\n",
      "Step 509141  [5.444 sec/step, loss=0.07518, avg_loss=0.07511]\n",
      "Step 509142  [5.447 sec/step, loss=0.07641, avg_loss=0.07512]\n",
      "Step 509143  [5.440 sec/step, loss=0.07490, avg_loss=0.07510]\n",
      "Step 509144  [5.434 sec/step, loss=0.07773, avg_loss=0.07512]\n",
      "Step 509145  [5.438 sec/step, loss=0.07383, avg_loss=0.07515]\n",
      "Step 509146  [5.440 sec/step, loss=0.07711, avg_loss=0.07514]\n",
      "Step 509147  [5.438 sec/step, loss=0.07690, avg_loss=0.07514]\n",
      "Step 509148  [5.430 sec/step, loss=0.07437, avg_loss=0.07513]\n",
      "Step 509149  [5.440 sec/step, loss=0.07479, avg_loss=0.07515]\n",
      "Step 509150  [5.426 sec/step, loss=0.07566, avg_loss=0.07515]\n",
      "Step 509151  [5.437 sec/step, loss=0.07637, avg_loss=0.07517]\n",
      "Step 509152  [5.445 sec/step, loss=0.07475, avg_loss=0.07515]\n",
      "Step 509153  [5.452 sec/step, loss=0.07122, avg_loss=0.07510]\n",
      "Step 509154  [5.440 sec/step, loss=0.07583, avg_loss=0.07511]\n",
      "Step 509155  [5.443 sec/step, loss=0.07703, avg_loss=0.07511]\n",
      "Step 509156  [5.474 sec/step, loss=0.07760, avg_loss=0.07522]\n",
      "Step 509157  [5.487 sec/step, loss=0.07574, avg_loss=0.07522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 509158  [5.458 sec/step, loss=0.07257, avg_loss=0.07518]\n",
      "Step 509159  [5.517 sec/step, loss=0.06786, avg_loss=0.07512]\n",
      "Step 509160  [5.554 sec/step, loss=0.07465, avg_loss=0.07514]\n",
      "Step 509161  [5.548 sec/step, loss=0.07800, avg_loss=0.07514]\n",
      "Step 509162  [5.563 sec/step, loss=0.07583, avg_loss=0.07515]\n",
      "Step 509163  [5.557 sec/step, loss=0.07666, avg_loss=0.07516]\n",
      "Step 509164  [5.561 sec/step, loss=0.07431, avg_loss=0.07514]\n",
      "Step 509165  [5.549 sec/step, loss=0.07486, avg_loss=0.07513]\n",
      "Step 509166  [5.555 sec/step, loss=0.07503, avg_loss=0.07511]\n",
      "Step 509167  [5.568 sec/step, loss=0.07476, avg_loss=0.07511]\n",
      "Step 509168  [5.602 sec/step, loss=0.07643, avg_loss=0.07516]\n",
      "Step 509169  [5.570 sec/step, loss=0.06713, avg_loss=0.07505]\n",
      "Step 509170  [5.558 sec/step, loss=0.07554, avg_loss=0.07505]\n",
      "Step 509171  [5.517 sec/step, loss=0.07695, avg_loss=0.07515]\n",
      "Generated 32 batches of size 32 in 2.548 sec\n",
      "Step 509172  [5.504 sec/step, loss=0.07271, avg_loss=0.07511]\n",
      "Step 509173  [5.503 sec/step, loss=0.07679, avg_loss=0.07512]\n",
      "Step 509174  [5.496 sec/step, loss=0.07540, avg_loss=0.07512]\n",
      "Step 509175  [5.509 sec/step, loss=0.07730, avg_loss=0.07514]\n",
      "Step 509176  [5.494 sec/step, loss=0.07296, avg_loss=0.07509]\n",
      "Step 509177  [5.490 sec/step, loss=0.07616, avg_loss=0.07509]\n",
      "Step 509178  [5.462 sec/step, loss=0.07653, avg_loss=0.07508]\n",
      "Step 509179  [5.461 sec/step, loss=0.07576, avg_loss=0.07508]\n",
      "Step 509180  [5.464 sec/step, loss=0.07761, avg_loss=0.07509]\n",
      "Step 509181  [5.487 sec/step, loss=0.07585, avg_loss=0.07511]\n",
      "Step 509182  [5.503 sec/step, loss=0.07695, avg_loss=0.07515]\n",
      "Step 509183  [5.494 sec/step, loss=0.07499, avg_loss=0.07512]\n",
      "Step 509184  [5.472 sec/step, loss=0.07528, avg_loss=0.07510]\n",
      "Step 509185  [5.469 sec/step, loss=0.07664, avg_loss=0.07510]\n",
      "Step 509186  [5.450 sec/step, loss=0.06653, avg_loss=0.07499]\n",
      "Step 509187  [5.449 sec/step, loss=0.07526, avg_loss=0.07498]\n",
      "Step 509188  [5.457 sec/step, loss=0.07607, avg_loss=0.07499]\n",
      "Step 509189  [5.466 sec/step, loss=0.07590, avg_loss=0.07500]\n",
      "Step 509190  [5.475 sec/step, loss=0.07707, avg_loss=0.07501]\n",
      "Step 509191  [5.481 sec/step, loss=0.07576, avg_loss=0.07502]\n",
      "Step 509192  [5.466 sec/step, loss=0.07430, avg_loss=0.07501]\n",
      "Step 509193  [5.466 sec/step, loss=0.07471, avg_loss=0.07499]\n",
      "Step 509194  [5.481 sec/step, loss=0.07701, avg_loss=0.07503]\n",
      "Step 509195  [5.480 sec/step, loss=0.07071, avg_loss=0.07497]\n",
      "Step 509196  [5.473 sec/step, loss=0.07308, avg_loss=0.07495]\n",
      "Step 509197  [5.425 sec/step, loss=0.07626, avg_loss=0.07503]\n",
      "Step 509198  [5.440 sec/step, loss=0.06966, avg_loss=0.07498]\n",
      "Step 509199  [5.443 sec/step, loss=0.07739, avg_loss=0.07499]\n",
      "Step 509200  [5.444 sec/step, loss=0.07537, avg_loss=0.07497]\n",
      "Writing summary at step: 509200\n",
      "Step 509201  [5.448 sec/step, loss=0.07462, avg_loss=0.07497]\n",
      "Step 509202  [5.470 sec/step, loss=0.07705, avg_loss=0.07501]\n",
      "Generated 32 batches of size 32 in 2.435 sec\n",
      "Step 509203  [5.483 sec/step, loss=0.07781, avg_loss=0.07502]\n",
      "Step 509204  [5.466 sec/step, loss=0.07491, avg_loss=0.07500]\n",
      "Step 509205  [5.469 sec/step, loss=0.07623, avg_loss=0.07502]\n",
      "Step 509206  [5.470 sec/step, loss=0.07162, avg_loss=0.07502]\n",
      "Step 509207  [5.456 sec/step, loss=0.07534, avg_loss=0.07502]\n",
      "Step 509208  [5.452 sec/step, loss=0.07728, avg_loss=0.07504]\n",
      "Step 509209  [5.432 sec/step, loss=0.07562, avg_loss=0.07505]\n",
      "Step 509210  [5.448 sec/step, loss=0.07710, avg_loss=0.07516]\n",
      "Step 509211  [5.432 sec/step, loss=0.07296, avg_loss=0.07513]\n",
      "Step 509212  [5.429 sec/step, loss=0.07364, avg_loss=0.07511]\n",
      "Step 509213  [5.445 sec/step, loss=0.07521, avg_loss=0.07511]\n",
      "Step 509214  [5.456 sec/step, loss=0.07671, avg_loss=0.07512]\n",
      "Step 509215  [5.455 sec/step, loss=0.07318, avg_loss=0.07509]\n",
      "Step 509216  [5.431 sec/step, loss=0.07385, avg_loss=0.07505]\n",
      "Step 509217  [5.431 sec/step, loss=0.06850, avg_loss=0.07507]\n",
      "Step 509218  [5.426 sec/step, loss=0.07668, avg_loss=0.07508]\n",
      "Step 509219  [5.393 sec/step, loss=0.07606, avg_loss=0.07507]\n",
      "Step 509220  [5.384 sec/step, loss=0.07712, avg_loss=0.07508]\n",
      "Step 509221  [5.391 sec/step, loss=0.07530, avg_loss=0.07509]\n",
      "Step 509222  [5.389 sec/step, loss=0.07501, avg_loss=0.07508]\n",
      "Step 509223  [5.388 sec/step, loss=0.07310, avg_loss=0.07505]\n",
      "Step 509224  [5.367 sec/step, loss=0.07568, avg_loss=0.07503]\n",
      "Step 509225  [5.370 sec/step, loss=0.07620, avg_loss=0.07504]\n",
      "Step 509226  [5.374 sec/step, loss=0.07745, avg_loss=0.07504]\n",
      "Step 509227  [5.375 sec/step, loss=0.07598, avg_loss=0.07503]\n",
      "Step 509228  [5.391 sec/step, loss=0.07639, avg_loss=0.07506]\n",
      "Step 509229  [5.390 sec/step, loss=0.07512, avg_loss=0.07508]\n",
      "Step 509230  [5.408 sec/step, loss=0.07693, avg_loss=0.07517]\n",
      "Step 509231  [5.427 sec/step, loss=0.07759, avg_loss=0.07519]\n",
      "Step 509232  [5.449 sec/step, loss=0.07866, avg_loss=0.07522]\n",
      "Step 509233  [5.462 sec/step, loss=0.07519, avg_loss=0.07522]\n",
      "Step 509234  [5.464 sec/step, loss=0.07689, avg_loss=0.07523]\n",
      "Generated 32 batches of size 32 in 2.445 sec\n",
      "Step 509235  [5.473 sec/step, loss=0.07550, avg_loss=0.07521]\n",
      "Step 509236  [5.469 sec/step, loss=0.07640, avg_loss=0.07525]\n",
      "Step 509237  [5.438 sec/step, loss=0.06851, avg_loss=0.07516]\n",
      "Step 509238  [5.432 sec/step, loss=0.07366, avg_loss=0.07517]\n",
      "Step 509239  [5.443 sec/step, loss=0.07547, avg_loss=0.07515]\n",
      "Step 509240  [5.425 sec/step, loss=0.07472, avg_loss=0.07513]\n",
      "Step 509241  [5.434 sec/step, loss=0.07751, avg_loss=0.07516]\n",
      "Step 509242  [5.442 sec/step, loss=0.07573, avg_loss=0.07515]\n",
      "Step 509243  [5.447 sec/step, loss=0.07626, avg_loss=0.07516]\n",
      "Step 509244  [5.446 sec/step, loss=0.07702, avg_loss=0.07516]\n",
      "Step 509245  [5.432 sec/step, loss=0.06668, avg_loss=0.07508]\n",
      "Step 509246  [5.423 sec/step, loss=0.07398, avg_loss=0.07505]\n",
      "Step 509247  [5.412 sec/step, loss=0.07449, avg_loss=0.07503]\n",
      "Step 509248  [5.431 sec/step, loss=0.07515, avg_loss=0.07504]\n",
      "Step 509249  [5.433 sec/step, loss=0.07171, avg_loss=0.07501]\n",
      "Step 509250  [5.434 sec/step, loss=0.07463, avg_loss=0.07500]\n",
      "Step 509251  [5.422 sec/step, loss=0.07596, avg_loss=0.07499]\n",
      "Step 509252  [5.415 sec/step, loss=0.07687, avg_loss=0.07501]\n",
      "Step 509253  [5.407 sec/step, loss=0.07073, avg_loss=0.07501]\n",
      "Step 509254  [5.394 sec/step, loss=0.07260, avg_loss=0.07498]\n",
      "Step 509255  [5.407 sec/step, loss=0.07552, avg_loss=0.07496]\n",
      "Step 509256  [5.400 sec/step, loss=0.07485, avg_loss=0.07493]\n",
      "Step 509257  [5.394 sec/step, loss=0.07564, avg_loss=0.07493]\n",
      "Step 509258  [5.412 sec/step, loss=0.07704, avg_loss=0.07498]\n",
      "Step 509259  [5.365 sec/step, loss=0.07585, avg_loss=0.07506]\n",
      "Step 509260  [5.348 sec/step, loss=0.07840, avg_loss=0.07509]\n",
      "Step 509261  [5.340 sec/step, loss=0.07659, avg_loss=0.07508]\n",
      "Step 509262  [5.327 sec/step, loss=0.07319, avg_loss=0.07505]\n",
      "Step 509263  [5.333 sec/step, loss=0.07768, avg_loss=0.07506]\n",
      "Step 509264  [5.382 sec/step, loss=0.06654, avg_loss=0.07499]\n",
      "Step 509265  [5.408 sec/step, loss=0.07658, avg_loss=0.07500]\n",
      "Step 509266  [5.407 sec/step, loss=0.07615, avg_loss=0.07501]\n",
      "Generated 32 batches of size 32 in 2.568 sec\n",
      "Step 509267  [5.404 sec/step, loss=0.07617, avg_loss=0.07503]\n",
      "Step 509268  [5.383 sec/step, loss=0.07714, avg_loss=0.07504]\n",
      "Step 509269  [5.397 sec/step, loss=0.07253, avg_loss=0.07509]\n",
      "Step 509270  [5.394 sec/step, loss=0.07571, avg_loss=0.07509]\n",
      "Step 509271  [5.390 sec/step, loss=0.07705, avg_loss=0.07509]\n",
      "Step 509272  [5.416 sec/step, loss=0.07613, avg_loss=0.07513]\n",
      "Step 509273  [5.413 sec/step, loss=0.07660, avg_loss=0.07512]\n",
      "Step 509274  [5.426 sec/step, loss=0.07609, avg_loss=0.07513]\n",
      "Step 509275  [5.424 sec/step, loss=0.07685, avg_loss=0.07513]\n",
      "Step 509276  [5.463 sec/step, loss=0.07778, avg_loss=0.07518]\n",
      "Step 509277  [5.456 sec/step, loss=0.07546, avg_loss=0.07517]\n",
      "Step 509278  [5.457 sec/step, loss=0.07270, avg_loss=0.07513]\n",
      "Step 509279  [5.464 sec/step, loss=0.07748, avg_loss=0.07515]\n",
      "Step 509280  [5.457 sec/step, loss=0.07681, avg_loss=0.07514]\n",
      "Step 509281  [5.432 sec/step, loss=0.07450, avg_loss=0.07513]\n",
      "Step 509282  [5.430 sec/step, loss=0.07731, avg_loss=0.07513]\n",
      "Step 509283  [5.433 sec/step, loss=0.07591, avg_loss=0.07514]\n",
      "Step 509284  [5.454 sec/step, loss=0.07508, avg_loss=0.07514]\n",
      "Step 509285  [5.443 sec/step, loss=0.07465, avg_loss=0.07512]\n",
      "Step 509286  [5.467 sec/step, loss=0.07701, avg_loss=0.07522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 509287  [5.478 sec/step, loss=0.07680, avg_loss=0.07524]\n",
      "Step 509288  [5.476 sec/step, loss=0.07360, avg_loss=0.07521]\n",
      "Step 509289  [5.473 sec/step, loss=0.07661, avg_loss=0.07522]\n",
      "Step 509290  [5.468 sec/step, loss=0.07735, avg_loss=0.07522]\n",
      "Step 509291  [5.459 sec/step, loss=0.07377, avg_loss=0.07520]\n",
      "Step 509292  [5.471 sec/step, loss=0.07744, avg_loss=0.07523]\n",
      "Step 509293  [5.450 sec/step, loss=0.07605, avg_loss=0.07525]\n",
      "Step 509294  [5.456 sec/step, loss=0.07743, avg_loss=0.07525]\n",
      "Step 509295  [5.462 sec/step, loss=0.07674, avg_loss=0.07531]\n",
      "Step 509296  [5.512 sec/step, loss=0.06727, avg_loss=0.07525]\n",
      "Step 509297  [5.502 sec/step, loss=0.07585, avg_loss=0.07525]\n",
      "Step 509298  [5.457 sec/step, loss=0.07655, avg_loss=0.07532]\n",
      "Generated 32 batches of size 32 in 2.726 sec\n",
      "Step 509299  [5.440 sec/step, loss=0.07404, avg_loss=0.07528]\n",
      "Step 509300  [5.414 sec/step, loss=0.07279, avg_loss=0.07526]\n",
      "Writing summary at step: 509300\n",
      "Step 509301  [5.422 sec/step, loss=0.07621, avg_loss=0.07527]\n",
      "Step 509302  [5.414 sec/step, loss=0.07762, avg_loss=0.07528]\n",
      "Step 509303  [5.382 sec/step, loss=0.06673, avg_loss=0.07517]\n",
      "Step 509304  [5.403 sec/step, loss=0.07739, avg_loss=0.07519]\n",
      "Step 509305  [5.399 sec/step, loss=0.07521, avg_loss=0.07518]\n",
      "Step 509306  [5.408 sec/step, loss=0.07591, avg_loss=0.07523]\n",
      "Step 509307  [5.412 sec/step, loss=0.07367, avg_loss=0.07521]\n",
      "Step 509308  [5.414 sec/step, loss=0.07557, avg_loss=0.07519]\n",
      "Step 509309  [5.419 sec/step, loss=0.07742, avg_loss=0.07521]\n",
      "Step 509310  [5.421 sec/step, loss=0.07606, avg_loss=0.07520]\n",
      "Step 509311  [5.423 sec/step, loss=0.07258, avg_loss=0.07520]\n",
      "Step 509312  [5.450 sec/step, loss=0.07791, avg_loss=0.07524]\n",
      "Step 509313  [5.483 sec/step, loss=0.06807, avg_loss=0.07517]\n",
      "Step 509314  [5.499 sec/step, loss=0.07493, avg_loss=0.07515]\n",
      "Step 509315  [5.514 sec/step, loss=0.07841, avg_loss=0.07520]\n",
      "Step 509316  [5.529 sec/step, loss=0.07634, avg_loss=0.07523]\n",
      "Step 509317  [5.475 sec/step, loss=0.07513, avg_loss=0.07529]\n",
      "Step 509318  [5.477 sec/step, loss=0.07362, avg_loss=0.07526]\n",
      "Step 509319  [5.482 sec/step, loss=0.07702, avg_loss=0.07527]\n",
      "Step 509320  [5.476 sec/step, loss=0.07636, avg_loss=0.07527]\n",
      "Step 509321  [5.492 sec/step, loss=0.07715, avg_loss=0.07528]\n",
      "Step 509322  [5.501 sec/step, loss=0.07665, avg_loss=0.07530]\n",
      "Step 509323  [5.498 sec/step, loss=0.07668, avg_loss=0.07534]\n",
      "Step 509324  [5.520 sec/step, loss=0.07849, avg_loss=0.07536]\n",
      "Step 509325  [5.506 sec/step, loss=0.07186, avg_loss=0.07532]\n",
      "Step 509326  [5.495 sec/step, loss=0.07536, avg_loss=0.07530]\n",
      "Step 509327  [5.477 sec/step, loss=0.07218, avg_loss=0.07526]\n",
      "Step 509328  [5.469 sec/step, loss=0.07280, avg_loss=0.07523]\n",
      "Step 509329  [5.474 sec/step, loss=0.07625, avg_loss=0.07524]\n",
      "Generated 32 batches of size 32 in 2.488 sec\n",
      "Step 509330  [5.487 sec/step, loss=0.07585, avg_loss=0.07523]\n",
      "Step 509331  [5.479 sec/step, loss=0.07604, avg_loss=0.07521]\n",
      "Step 509332  [5.472 sec/step, loss=0.07724, avg_loss=0.07520]\n",
      "Step 509333  [5.441 sec/step, loss=0.07540, avg_loss=0.07520]\n",
      "Step 509334  [5.427 sec/step, loss=0.07561, avg_loss=0.07519]\n",
      "Step 509335  [5.429 sec/step, loss=0.07712, avg_loss=0.07520]\n",
      "Step 509336  [5.452 sec/step, loss=0.07446, avg_loss=0.07518]\n",
      "Step 509337  [5.468 sec/step, loss=0.07507, avg_loss=0.07525]\n",
      "Step 509338  [5.452 sec/step, loss=0.06648, avg_loss=0.07518]\n",
      "Step 509339  [5.436 sec/step, loss=0.07766, avg_loss=0.07520]\n",
      "Step 509340  [5.430 sec/step, loss=0.07623, avg_loss=0.07521]\n",
      "Step 509341  [5.424 sec/step, loss=0.07639, avg_loss=0.07520]\n",
      "Step 509342  [5.402 sec/step, loss=0.07339, avg_loss=0.07518]\n",
      "Step 509343  [5.395 sec/step, loss=0.07247, avg_loss=0.07514]\n",
      "Step 509344  [5.385 sec/step, loss=0.07294, avg_loss=0.07510]\n",
      "Step 509345  [5.409 sec/step, loss=0.07738, avg_loss=0.07521]\n",
      "Step 509346  [5.423 sec/step, loss=0.07747, avg_loss=0.07524]\n",
      "Step 509347  [5.428 sec/step, loss=0.07581, avg_loss=0.07525]\n",
      "Step 509348  [5.428 sec/step, loss=0.07542, avg_loss=0.07526]\n",
      "Step 509349  [5.449 sec/step, loss=0.07383, avg_loss=0.07528]\n",
      "Step 509350  [5.494 sec/step, loss=0.06750, avg_loss=0.07521]\n",
      "Step 509351  [5.483 sec/step, loss=0.06691, avg_loss=0.07512]\n",
      "Step 509352  [5.474 sec/step, loss=0.07573, avg_loss=0.07511]\n",
      "Step 509353  [5.489 sec/step, loss=0.07471, avg_loss=0.07515]\n",
      "Step 509354  [5.503 sec/step, loss=0.07606, avg_loss=0.07518]\n",
      "Step 509355  [5.484 sec/step, loss=0.07482, avg_loss=0.07517]\n",
      "Step 509356  [5.473 sec/step, loss=0.07482, avg_loss=0.07517]\n",
      "Step 509357  [5.474 sec/step, loss=0.07571, avg_loss=0.07517]\n",
      "Step 509358  [5.475 sec/step, loss=0.07722, avg_loss=0.07518]\n",
      "Step 509359  [5.471 sec/step, loss=0.07546, avg_loss=0.07517]\n",
      "Step 509360  [5.488 sec/step, loss=0.07465, avg_loss=0.07513]\n",
      "Step 509361  [5.496 sec/step, loss=0.07595, avg_loss=0.07513]\n",
      "Generated 32 batches of size 32 in 2.407 sec\n",
      "Step 509362  [5.520 sec/step, loss=0.07770, avg_loss=0.07517]\n",
      "Step 509363  [5.504 sec/step, loss=0.07092, avg_loss=0.07510]\n",
      "Step 509364  [5.457 sec/step, loss=0.07746, avg_loss=0.07521]\n",
      "Step 509365  [5.437 sec/step, loss=0.07672, avg_loss=0.07522]\n",
      "Step 509366  [5.436 sec/step, loss=0.07705, avg_loss=0.07522]\n",
      "Step 509367  [5.420 sec/step, loss=0.07541, avg_loss=0.07522]\n",
      "Step 509368  [5.409 sec/step, loss=0.07254, avg_loss=0.07517]\n",
      "Step 509369  [5.427 sec/step, loss=0.07636, avg_loss=0.07521]\n",
      "Step 509370  [5.441 sec/step, loss=0.07602, avg_loss=0.07521]\n",
      "Step 509371  [5.444 sec/step, loss=0.07674, avg_loss=0.07521]\n",
      "Step 509372  [5.427 sec/step, loss=0.07581, avg_loss=0.07521]\n",
      "Step 509373  [5.428 sec/step, loss=0.07526, avg_loss=0.07519]\n",
      "Step 509374  [5.426 sec/step, loss=0.07656, avg_loss=0.07520]\n",
      "Step 509375  [5.409 sec/step, loss=0.07516, avg_loss=0.07518]\n",
      "Step 509376  [5.402 sec/step, loss=0.07648, avg_loss=0.07517]\n",
      "Step 509377  [5.455 sec/step, loss=0.06751, avg_loss=0.07509]\n",
      "Step 509378  [5.471 sec/step, loss=0.07669, avg_loss=0.07513]\n",
      "Step 509379  [5.465 sec/step, loss=0.07618, avg_loss=0.07511]\n",
      "Step 509380  [5.458 sec/step, loss=0.07650, avg_loss=0.07511]\n",
      "Step 509381  [5.469 sec/step, loss=0.07716, avg_loss=0.07514]\n",
      "Step 509382  [5.462 sec/step, loss=0.07650, avg_loss=0.07513]\n",
      "Step 509383  [5.463 sec/step, loss=0.07586, avg_loss=0.07513]\n",
      "Step 509384  [5.445 sec/step, loss=0.07566, avg_loss=0.07514]\n",
      "Step 509385  [5.465 sec/step, loss=0.07702, avg_loss=0.07516]\n",
      "Step 509386  [5.455 sec/step, loss=0.07296, avg_loss=0.07512]\n",
      "Step 509387  [5.454 sec/step, loss=0.07478, avg_loss=0.07510]\n",
      "Step 509388  [5.462 sec/step, loss=0.07677, avg_loss=0.07513]\n",
      "Step 509389  [5.468 sec/step, loss=0.07536, avg_loss=0.07512]\n",
      "Step 509390  [5.467 sec/step, loss=0.07297, avg_loss=0.07507]\n",
      "Step 509391  [5.462 sec/step, loss=0.07416, avg_loss=0.07508]\n",
      "Step 509392  [5.455 sec/step, loss=0.07691, avg_loss=0.07507]\n",
      "Step 509393  [5.473 sec/step, loss=0.07604, avg_loss=0.07507]\n",
      "Generated 32 batches of size 32 in 2.592 sec\n",
      "Step 509394  [5.466 sec/step, loss=0.07631, avg_loss=0.07506]\n",
      "Step 509395  [5.471 sec/step, loss=0.07795, avg_loss=0.07507]\n",
      "Step 509396  [5.420 sec/step, loss=0.07559, avg_loss=0.07516]\n",
      "Step 509397  [5.457 sec/step, loss=0.07296, avg_loss=0.07513]\n",
      "Step 509398  [5.448 sec/step, loss=0.07560, avg_loss=0.07512]\n",
      "Step 509399  [5.444 sec/step, loss=0.07356, avg_loss=0.07511]\n",
      "Step 509400  [5.438 sec/step, loss=0.06784, avg_loss=0.07506]\n",
      "Writing summary at step: 509400\n",
      "Step 509401  [5.426 sec/step, loss=0.07484, avg_loss=0.07505]\n",
      "Step 509402  [5.401 sec/step, loss=0.07288, avg_loss=0.07500]\n",
      "Step 509403  [5.419 sec/step, loss=0.07662, avg_loss=0.07510]\n",
      "Step 509404  [5.402 sec/step, loss=0.07627, avg_loss=0.07509]\n",
      "Step 509405  [5.421 sec/step, loss=0.07729, avg_loss=0.07511]\n",
      "Step 509406  [5.420 sec/step, loss=0.07499, avg_loss=0.07510]\n",
      "Step 509407  [5.418 sec/step, loss=0.07660, avg_loss=0.07513]\n",
      "Step 509408  [5.410 sec/step, loss=0.07647, avg_loss=0.07514]\n",
      "Step 509409  [5.398 sec/step, loss=0.07534, avg_loss=0.07512]\n",
      "Step 509410  [5.391 sec/step, loss=0.07455, avg_loss=0.07510]\n",
      "Step 509411  [5.403 sec/step, loss=0.07682, avg_loss=0.07515]\n",
      "Step 509412  [5.417 sec/step, loss=0.07467, avg_loss=0.07511]\n",
      "Step 509413  [5.374 sec/step, loss=0.07692, avg_loss=0.07520]\n",
      "Step 509414  [5.355 sec/step, loss=0.07767, avg_loss=0.07523]\n",
      "Step 509415  [5.344 sec/step, loss=0.07638, avg_loss=0.07521]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 509416  [5.345 sec/step, loss=0.07629, avg_loss=0.07521]\n",
      "Step 509417  [5.352 sec/step, loss=0.07556, avg_loss=0.07521]\n",
      "Step 509418  [5.331 sec/step, loss=0.06650, avg_loss=0.07514]\n",
      "Step 509419  [5.324 sec/step, loss=0.07236, avg_loss=0.07510]\n",
      "Step 509420  [5.320 sec/step, loss=0.07523, avg_loss=0.07508]\n",
      "Step 509421  [5.310 sec/step, loss=0.07607, avg_loss=0.07507]\n",
      "Step 509422  [5.301 sec/step, loss=0.07337, avg_loss=0.07504]\n",
      "Step 509423  [5.309 sec/step, loss=0.07468, avg_loss=0.07502]\n",
      "Step 509424  [5.308 sec/step, loss=0.07767, avg_loss=0.07501]\n",
      "Generated 32 batches of size 32 in 2.358 sec\n",
      "Step 509425  [5.331 sec/step, loss=0.07626, avg_loss=0.07506]\n",
      "Step 509426  [5.336 sec/step, loss=0.07308, avg_loss=0.07503]\n",
      "Step 509427  [5.356 sec/step, loss=0.07689, avg_loss=0.07508]\n",
      "Step 509428  [5.347 sec/step, loss=0.07537, avg_loss=0.07511]\n",
      "Step 509429  [5.366 sec/step, loss=0.07539, avg_loss=0.07510]\n",
      "Step 509430  [5.364 sec/step, loss=0.07772, avg_loss=0.07512]\n",
      "Step 509431  [5.415 sec/step, loss=0.06685, avg_loss=0.07503]\n",
      "Step 509432  [5.419 sec/step, loss=0.07562, avg_loss=0.07501]\n",
      "Step 509433  [5.422 sec/step, loss=0.07426, avg_loss=0.07500]\n",
      "Step 509434  [5.431 sec/step, loss=0.07646, avg_loss=0.07501]\n",
      "Step 509435  [5.427 sec/step, loss=0.07565, avg_loss=0.07499]\n",
      "Step 509436  [5.395 sec/step, loss=0.07503, avg_loss=0.07500]\n",
      "Step 509437  [5.388 sec/step, loss=0.07218, avg_loss=0.07497]\n",
      "Step 509438  [5.413 sec/step, loss=0.07729, avg_loss=0.07508]\n",
      "Step 509439  [5.402 sec/step, loss=0.07632, avg_loss=0.07506]\n",
      "Step 509440  [5.416 sec/step, loss=0.07420, avg_loss=0.07504]\n",
      "Step 509441  [5.413 sec/step, loss=0.07637, avg_loss=0.07504]\n",
      "Step 509442  [5.426 sec/step, loss=0.07643, avg_loss=0.07507]\n",
      "Step 509443  [5.432 sec/step, loss=0.07457, avg_loss=0.07509]\n",
      "Step 509444  [5.434 sec/step, loss=0.07567, avg_loss=0.07512]\n",
      "Step 509445  [5.410 sec/step, loss=0.06634, avg_loss=0.07501]\n",
      "Step 509446  [5.407 sec/step, loss=0.07562, avg_loss=0.07499]\n",
      "Step 509447  [5.392 sec/step, loss=0.07232, avg_loss=0.07496]\n",
      "Step 509448  [5.382 sec/step, loss=0.07661, avg_loss=0.07497]\n",
      "Step 509449  [5.372 sec/step, loss=0.07583, avg_loss=0.07499]\n",
      "Step 509450  [5.332 sec/step, loss=0.07728, avg_loss=0.07509]\n",
      "Step 509451  [5.377 sec/step, loss=0.07435, avg_loss=0.07516]\n",
      "Step 509452  [5.398 sec/step, loss=0.07673, avg_loss=0.07517]\n",
      "Step 509453  [5.390 sec/step, loss=0.07518, avg_loss=0.07518]\n",
      "Step 509454  [5.408 sec/step, loss=0.07635, avg_loss=0.07518]\n",
      "Step 509455  [5.410 sec/step, loss=0.07655, avg_loss=0.07520]\n",
      "Step 509456  [5.429 sec/step, loss=0.07672, avg_loss=0.07522]\n",
      "Generated 32 batches of size 32 in 2.631 sec\n",
      "Step 509457  [5.430 sec/step, loss=0.07177, avg_loss=0.07518]\n",
      "Step 509458  [5.420 sec/step, loss=0.07552, avg_loss=0.07516]\n",
      "Step 509459  [5.428 sec/step, loss=0.07725, avg_loss=0.07518]\n",
      "Step 509460  [5.398 sec/step, loss=0.07326, avg_loss=0.07516]\n",
      "Step 509461  [5.382 sec/step, loss=0.07458, avg_loss=0.07515]\n",
      "Step 509462  [5.358 sec/step, loss=0.07474, avg_loss=0.07512]\n",
      "Step 509463  [5.380 sec/step, loss=0.07788, avg_loss=0.07519]\n",
      "Step 509464  [5.428 sec/step, loss=0.06674, avg_loss=0.07508]\n",
      "Step 509465  [5.435 sec/step, loss=0.07585, avg_loss=0.07507]\n",
      "Step 509466  [5.413 sec/step, loss=0.07522, avg_loss=0.07506]\n",
      "Step 509467  [5.425 sec/step, loss=0.07633, avg_loss=0.07506]\n",
      "Step 509468  [5.438 sec/step, loss=0.07525, avg_loss=0.07509]\n",
      "Step 509469  [5.432 sec/step, loss=0.07723, avg_loss=0.07510]\n",
      "Step 509470  [5.417 sec/step, loss=0.07250, avg_loss=0.07507]\n",
      "Step 509471  [5.418 sec/step, loss=0.07751, avg_loss=0.07507]\n",
      "Step 509472  [5.413 sec/step, loss=0.07587, avg_loss=0.07507]\n",
      "Step 509473  [5.414 sec/step, loss=0.07622, avg_loss=0.07508]\n",
      "Step 509474  [5.407 sec/step, loss=0.07544, avg_loss=0.07507]\n",
      "Step 509475  [5.454 sec/step, loss=0.07057, avg_loss=0.07503]\n",
      "Step 509476  [5.423 sec/step, loss=0.07384, avg_loss=0.07500]\n",
      "Step 509477  [5.357 sec/step, loss=0.06702, avg_loss=0.07499]\n",
      "Step 509478  [5.337 sec/step, loss=0.07484, avg_loss=0.07498]\n",
      "Step 509479  [5.331 sec/step, loss=0.07695, avg_loss=0.07498]\n",
      "Step 509480  [5.350 sec/step, loss=0.07707, avg_loss=0.07499]\n",
      "Step 509481  [5.350 sec/step, loss=0.07572, avg_loss=0.07498]\n",
      "Step 509482  [5.359 sec/step, loss=0.07799, avg_loss=0.07499]\n",
      "Step 509483  [5.353 sec/step, loss=0.07634, avg_loss=0.07499]\n",
      "Step 509484  [5.357 sec/step, loss=0.07496, avg_loss=0.07499]\n",
      "Step 509485  [5.354 sec/step, loss=0.07597, avg_loss=0.07498]\n",
      "Step 509486  [5.370 sec/step, loss=0.07682, avg_loss=0.07502]\n",
      "Step 509487  [5.361 sec/step, loss=0.07650, avg_loss=0.07503]\n",
      "Step 509488  [5.362 sec/step, loss=0.07736, avg_loss=0.07504]\n",
      "Generated 32 batches of size 32 in 2.435 sec\n",
      "Step 509489  [5.390 sec/step, loss=0.07368, avg_loss=0.07502]\n",
      "Step 509490  [5.395 sec/step, loss=0.07299, avg_loss=0.07502]\n",
      "Step 509491  [5.396 sec/step, loss=0.07486, avg_loss=0.07503]\n",
      "Step 509492  [5.394 sec/step, loss=0.07639, avg_loss=0.07502]\n",
      "Step 509493  [5.388 sec/step, loss=0.07390, avg_loss=0.07500]\n",
      "Step 509494  [5.384 sec/step, loss=0.07487, avg_loss=0.07499]\n",
      "Step 509495  [5.376 sec/step, loss=0.07609, avg_loss=0.07497]\n",
      "Step 509496  [5.384 sec/step, loss=0.07703, avg_loss=0.07498]\n",
      "Step 509497  [5.364 sec/step, loss=0.07733, avg_loss=0.07503]\n",
      "Step 509498  [5.378 sec/step, loss=0.07600, avg_loss=0.07503]\n",
      "Step 509499  [5.400 sec/step, loss=0.07782, avg_loss=0.07507]\n",
      "Step 509500  [5.443 sec/step, loss=0.07383, avg_loss=0.07513]\n",
      "Writing summary at step: 509500\n",
      "Step 509501  [5.458 sec/step, loss=0.07541, avg_loss=0.07514]\n",
      "Step 509502  [5.474 sec/step, loss=0.07607, avg_loss=0.07517]\n",
      "Step 509503  [5.473 sec/step, loss=0.07382, avg_loss=0.07514]\n",
      "Step 509504  [5.484 sec/step, loss=0.07666, avg_loss=0.07515]\n",
      "Step 509505  [5.458 sec/step, loss=0.07184, avg_loss=0.07509]\n",
      "Step 509506  [5.463 sec/step, loss=0.07625, avg_loss=0.07511]\n",
      "Step 509507  [5.447 sec/step, loss=0.06682, avg_loss=0.07501]\n",
      "Step 509508  [5.455 sec/step, loss=0.07708, avg_loss=0.07501]\n",
      "Step 509509  [5.462 sec/step, loss=0.07582, avg_loss=0.07502]\n",
      "Step 509510  [5.473 sec/step, loss=0.07681, avg_loss=0.07504]\n",
      "Step 509511  [5.487 sec/step, loss=0.07690, avg_loss=0.07504]\n",
      "Step 509512  [5.457 sec/step, loss=0.07628, avg_loss=0.07506]\n",
      "Step 509513  [5.459 sec/step, loss=0.07761, avg_loss=0.07507]\n",
      "Step 509514  [5.446 sec/step, loss=0.07471, avg_loss=0.07504]\n",
      "Step 509515  [5.443 sec/step, loss=0.07533, avg_loss=0.07503]\n",
      "Step 509516  [5.457 sec/step, loss=0.07692, avg_loss=0.07503]\n",
      "Step 509517  [5.438 sec/step, loss=0.07210, avg_loss=0.07500]\n",
      "Step 509518  [5.451 sec/step, loss=0.07453, avg_loss=0.07508]\n",
      "Step 509519  [5.457 sec/step, loss=0.07330, avg_loss=0.07509]\n",
      "Generated 32 batches of size 32 in 2.454 sec\n",
      "Step 509520  [5.468 sec/step, loss=0.07501, avg_loss=0.07508]\n",
      "Step 509521  [5.467 sec/step, loss=0.07627, avg_loss=0.07509]\n",
      "Step 509522  [5.483 sec/step, loss=0.07756, avg_loss=0.07513]\n",
      "Step 509523  [5.486 sec/step, loss=0.07689, avg_loss=0.07515]\n",
      "Step 509524  [5.479 sec/step, loss=0.07566, avg_loss=0.07513]\n",
      "Step 509525  [5.459 sec/step, loss=0.07482, avg_loss=0.07512]\n",
      "Step 509526  [5.448 sec/step, loss=0.07552, avg_loss=0.07514]\n",
      "Step 509527  [5.445 sec/step, loss=0.07689, avg_loss=0.07514]\n",
      "Step 509528  [5.448 sec/step, loss=0.07534, avg_loss=0.07514]\n",
      "Step 509529  [5.436 sec/step, loss=0.07722, avg_loss=0.07516]\n",
      "Step 509530  [5.426 sec/step, loss=0.07489, avg_loss=0.07513]\n",
      "Step 509531  [5.371 sec/step, loss=0.07501, avg_loss=0.07521]\n",
      "Step 509532  [5.365 sec/step, loss=0.07514, avg_loss=0.07521]\n",
      "Step 509533  [5.372 sec/step, loss=0.07584, avg_loss=0.07522]\n",
      "Step 509534  [5.363 sec/step, loss=0.07524, avg_loss=0.07521]\n",
      "Step 509535  [5.353 sec/step, loss=0.07166, avg_loss=0.07517]\n",
      "Step 509536  [5.361 sec/step, loss=0.07629, avg_loss=0.07518]\n",
      "Step 509537  [5.379 sec/step, loss=0.07718, avg_loss=0.07523]\n",
      "Step 509538  [5.389 sec/step, loss=0.07606, avg_loss=0.07522]\n",
      "Step 509539  [5.441 sec/step, loss=0.06809, avg_loss=0.07514]\n",
      "Step 509540  [5.431 sec/step, loss=0.07694, avg_loss=0.07517]\n",
      "Step 509541  [5.437 sec/step, loss=0.07633, avg_loss=0.07517]\n",
      "Step 509542  [5.440 sec/step, loss=0.07729, avg_loss=0.07517]\n",
      "Step 509543  [5.453 sec/step, loss=0.07523, avg_loss=0.07518]\n",
      "Step 509544  [5.463 sec/step, loss=0.07596, avg_loss=0.07518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 509545  [5.490 sec/step, loss=0.07763, avg_loss=0.07530]\n",
      "Step 509546  [5.493 sec/step, loss=0.07697, avg_loss=0.07531]\n",
      "Step 509547  [5.508 sec/step, loss=0.07558, avg_loss=0.07534]\n",
      "Step 509548  [5.523 sec/step, loss=0.07652, avg_loss=0.07534]\n",
      "Step 509549  [5.541 sec/step, loss=0.07388, avg_loss=0.07532]\n",
      "Step 509550  [5.528 sec/step, loss=0.07597, avg_loss=0.07531]\n",
      "Step 509551  [5.485 sec/step, loss=0.06706, avg_loss=0.07524]\n",
      "Generated 32 batches of size 32 in 2.641 sec\n",
      "Step 509552  [5.464 sec/step, loss=0.07384, avg_loss=0.07521]\n",
      "Step 509553  [5.469 sec/step, loss=0.07393, avg_loss=0.07520]\n",
      "Step 509554  [5.450 sec/step, loss=0.07513, avg_loss=0.07518]\n",
      "Step 509555  [5.454 sec/step, loss=0.07645, avg_loss=0.07518]\n",
      "Step 509556  [5.436 sec/step, loss=0.07630, avg_loss=0.07518]\n",
      "Step 509557  [5.440 sec/step, loss=0.07554, avg_loss=0.07522]\n",
      "Step 509558  [5.426 sec/step, loss=0.07297, avg_loss=0.07519]\n",
      "Step 509559  [5.409 sec/step, loss=0.07201, avg_loss=0.07514]\n",
      "Step 509560  [5.408 sec/step, loss=0.07499, avg_loss=0.07515]\n",
      "Step 509561  [5.434 sec/step, loss=0.07468, avg_loss=0.07516]\n",
      "Step 509562  [5.443 sec/step, loss=0.07691, avg_loss=0.07518]\n",
      "Step 509563  [5.482 sec/step, loss=0.06726, avg_loss=0.07507]\n",
      "Step 509564  [5.420 sec/step, loss=0.07477, avg_loss=0.07515]\n",
      "Step 509565  [5.441 sec/step, loss=0.07437, avg_loss=0.07514]\n",
      "Step 509566  [5.458 sec/step, loss=0.07435, avg_loss=0.07513]\n",
      "Step 509567  [5.460 sec/step, loss=0.07615, avg_loss=0.07513]\n",
      "Step 509568  [5.459 sec/step, loss=0.07562, avg_loss=0.07513]\n",
      "Step 509569  [5.447 sec/step, loss=0.07495, avg_loss=0.07511]\n",
      "Step 509570  [5.469 sec/step, loss=0.07722, avg_loss=0.07515]\n",
      "Step 509571  [5.468 sec/step, loss=0.07483, avg_loss=0.07513]\n",
      "Step 509572  [5.461 sec/step, loss=0.07483, avg_loss=0.07512]\n",
      "Step 509573  [5.461 sec/step, loss=0.07624, avg_loss=0.07512]\n",
      "Step 509574  [5.482 sec/step, loss=0.07723, avg_loss=0.07514]\n",
      "Step 509575  [5.454 sec/step, loss=0.07780, avg_loss=0.07521]\n",
      "Step 509576  [5.474 sec/step, loss=0.07476, avg_loss=0.07522]\n",
      "Step 509577  [5.492 sec/step, loss=0.07650, avg_loss=0.07531]\n",
      "Step 509578  [5.499 sec/step, loss=0.07407, avg_loss=0.07530]\n",
      "Step 509579  [5.496 sec/step, loss=0.07582, avg_loss=0.07529]\n",
      "Step 509580  [5.471 sec/step, loss=0.07382, avg_loss=0.07526]\n",
      "Step 509581  [5.449 sec/step, loss=0.07181, avg_loss=0.07522]\n",
      "Step 509582  [5.421 sec/step, loss=0.07278, avg_loss=0.07517]\n",
      "Step 509583  [5.421 sec/step, loss=0.07629, avg_loss=0.07517]\n",
      "Generated 32 batches of size 32 in 2.439 sec\n",
      "Step 509584  [5.437 sec/step, loss=0.07699, avg_loss=0.07519]\n",
      "Step 509585  [5.436 sec/step, loss=0.07765, avg_loss=0.07521]\n",
      "Step 509586  [5.419 sec/step, loss=0.07277, avg_loss=0.07517]\n",
      "Step 509587  [5.411 sec/step, loss=0.07693, avg_loss=0.07517]\n",
      "Step 509588  [5.402 sec/step, loss=0.07486, avg_loss=0.07514]\n",
      "Step 509589  [5.379 sec/step, loss=0.07751, avg_loss=0.07518]\n",
      "Step 509590  [5.358 sec/step, loss=0.06769, avg_loss=0.07513]\n",
      "Step 509591  [5.369 sec/step, loss=0.07782, avg_loss=0.07516]\n",
      "Step 509592  [5.373 sec/step, loss=0.07600, avg_loss=0.07516]\n",
      "Step 509593  [5.370 sec/step, loss=0.07194, avg_loss=0.07514]\n",
      "Step 509594  [5.385 sec/step, loss=0.07513, avg_loss=0.07514]\n",
      "Step 509595  [5.395 sec/step, loss=0.07729, avg_loss=0.07515]\n",
      "Step 509596  [5.382 sec/step, loss=0.07458, avg_loss=0.07513]\n",
      "Step 509597  [5.360 sec/step, loss=0.07217, avg_loss=0.07507]\n",
      "Step 509598  [5.356 sec/step, loss=0.07650, avg_loss=0.07508]\n",
      "Step 509599  [5.359 sec/step, loss=0.07764, avg_loss=0.07508]\n",
      "Step 509600  [5.346 sec/step, loss=0.07782, avg_loss=0.07512]\n",
      "Writing summary at step: 509600\n",
      "Step 509601  [5.341 sec/step, loss=0.07355, avg_loss=0.07510]\n",
      "Step 509602  [5.345 sec/step, loss=0.07374, avg_loss=0.07508]\n",
      "Step 509603  [5.352 sec/step, loss=0.07674, avg_loss=0.07510]\n",
      "Step 509604  [5.357 sec/step, loss=0.07531, avg_loss=0.07509]\n",
      "Step 509605  [5.394 sec/step, loss=0.07682, avg_loss=0.07514]\n",
      "Step 509606  [5.411 sec/step, loss=0.07431, avg_loss=0.07512]\n",
      "Step 509607  [5.426 sec/step, loss=0.07653, avg_loss=0.07522]\n",
      "Step 509608  [5.409 sec/step, loss=0.07649, avg_loss=0.07521]\n",
      "Step 509609  [5.411 sec/step, loss=0.07641, avg_loss=0.07522]\n",
      "Step 509610  [5.408 sec/step, loss=0.07556, avg_loss=0.07521]\n",
      "Step 509611  [5.385 sec/step, loss=0.07566, avg_loss=0.07519]\n",
      "Step 509612  [5.381 sec/step, loss=0.07398, avg_loss=0.07517]\n",
      "Step 509613  [5.423 sec/step, loss=0.06671, avg_loss=0.07506]\n",
      "Step 509614  [5.438 sec/step, loss=0.07750, avg_loss=0.07509]\n",
      "Generated 32 batches of size 32 in 2.471 sec\n",
      "Step 509615  [5.444 sec/step, loss=0.07326, avg_loss=0.07507]\n",
      "Step 509616  [5.435 sec/step, loss=0.07789, avg_loss=0.07508]\n",
      "Step 509617  [5.455 sec/step, loss=0.07615, avg_loss=0.07512]\n",
      "Step 509618  [5.447 sec/step, loss=0.07241, avg_loss=0.07510]\n",
      "Step 509619  [5.453 sec/step, loss=0.07517, avg_loss=0.07512]\n",
      "Step 509620  [5.448 sec/step, loss=0.07619, avg_loss=0.07513]\n",
      "Step 509621  [5.434 sec/step, loss=0.06641, avg_loss=0.07503]\n",
      "Step 509622  [5.418 sec/step, loss=0.07433, avg_loss=0.07500]\n",
      "Step 509623  [5.396 sec/step, loss=0.07559, avg_loss=0.07498]\n",
      "Step 509624  [5.411 sec/step, loss=0.07489, avg_loss=0.07498]\n",
      "Step 509625  [5.414 sec/step, loss=0.07653, avg_loss=0.07499]\n",
      "Step 509626  [5.425 sec/step, loss=0.07545, avg_loss=0.07499]\n",
      "Step 509627  [5.422 sec/step, loss=0.07567, avg_loss=0.07498]\n",
      "Step 509628  [5.456 sec/step, loss=0.07523, avg_loss=0.07498]\n",
      "Step 509629  [5.438 sec/step, loss=0.07299, avg_loss=0.07494]\n",
      "Step 509630  [5.442 sec/step, loss=0.07820, avg_loss=0.07497]\n",
      "Step 509631  [5.451 sec/step, loss=0.07544, avg_loss=0.07497]\n",
      "Step 509632  [5.427 sec/step, loss=0.06802, avg_loss=0.07490]\n",
      "Step 509633  [5.427 sec/step, loss=0.07657, avg_loss=0.07491]\n",
      "Step 509634  [5.436 sec/step, loss=0.07646, avg_loss=0.07492]\n",
      "Step 509635  [5.434 sec/step, loss=0.07507, avg_loss=0.07496]\n",
      "Step 509636  [5.445 sec/step, loss=0.07741, avg_loss=0.07497]\n",
      "Step 509637  [5.442 sec/step, loss=0.07624, avg_loss=0.07496]\n",
      "Step 509638  [5.433 sec/step, loss=0.07678, avg_loss=0.07497]\n",
      "Step 509639  [5.396 sec/step, loss=0.07502, avg_loss=0.07504]\n",
      "Step 509640  [5.403 sec/step, loss=0.07492, avg_loss=0.07502]\n",
      "Step 509641  [5.396 sec/step, loss=0.07625, avg_loss=0.07501]\n",
      "Step 509642  [5.379 sec/step, loss=0.07206, avg_loss=0.07496]\n",
      "Step 509643  [5.362 sec/step, loss=0.07430, avg_loss=0.07495]\n",
      "Step 509644  [5.348 sec/step, loss=0.07641, avg_loss=0.07496]\n",
      "Step 509645  [5.350 sec/step, loss=0.07756, avg_loss=0.07496]\n",
      "Step 509646  [5.387 sec/step, loss=0.06794, avg_loss=0.07487]\n",
      "Generated 32 batches of size 32 in 2.498 sec\n",
      "Step 509647  [5.402 sec/step, loss=0.07762, avg_loss=0.07489]\n",
      "Step 509648  [5.382 sec/step, loss=0.07512, avg_loss=0.07487]\n",
      "Step 509649  [5.360 sec/step, loss=0.07487, avg_loss=0.07488]\n",
      "Step 509650  [5.376 sec/step, loss=0.07678, avg_loss=0.07489]\n",
      "Step 509651  [5.390 sec/step, loss=0.07267, avg_loss=0.07495]\n",
      "Step 509652  [5.384 sec/step, loss=0.07392, avg_loss=0.07495]\n",
      "Step 509653  [5.387 sec/step, loss=0.07709, avg_loss=0.07498]\n",
      "Step 509654  [5.386 sec/step, loss=0.07437, avg_loss=0.07497]\n",
      "Step 509655  [5.378 sec/step, loss=0.07543, avg_loss=0.07496]\n",
      "Step 509656  [5.378 sec/step, loss=0.07641, avg_loss=0.07496]\n",
      "Step 509657  [5.384 sec/step, loss=0.07765, avg_loss=0.07498]\n",
      "Step 509658  [5.391 sec/step, loss=0.07533, avg_loss=0.07501]\n",
      "Step 509659  [5.397 sec/step, loss=0.07296, avg_loss=0.07502]\n",
      "Step 509660  [5.406 sec/step, loss=0.07526, avg_loss=0.07502]\n",
      "Step 509661  [5.395 sec/step, loss=0.07665, avg_loss=0.07504]\n",
      "Step 509662  [5.391 sec/step, loss=0.07632, avg_loss=0.07503]\n",
      "Step 509663  [5.391 sec/step, loss=0.06699, avg_loss=0.07503]\n",
      "Step 509664  [5.410 sec/step, loss=0.07775, avg_loss=0.07506]\n",
      "Step 509665  [5.384 sec/step, loss=0.07529, avg_loss=0.07507]\n",
      "Step 509666  [5.361 sec/step, loss=0.06715, avg_loss=0.07500]\n",
      "Step 509667  [5.360 sec/step, loss=0.07678, avg_loss=0.07500]\n",
      "Step 509668  [5.370 sec/step, loss=0.07734, avg_loss=0.07502]\n",
      "Step 509669  [5.389 sec/step, loss=0.07744, avg_loss=0.07505]\n",
      "Step 509670  [5.378 sec/step, loss=0.07591, avg_loss=0.07503]\n",
      "Step 509671  [5.382 sec/step, loss=0.07496, avg_loss=0.07503]\n",
      "Step 509672  [5.388 sec/step, loss=0.07645, avg_loss=0.07505]\n",
      "Step 509673  [5.398 sec/step, loss=0.07512, avg_loss=0.07504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 509674  [5.377 sec/step, loss=0.07456, avg_loss=0.07501]\n",
      "Step 509675  [5.362 sec/step, loss=0.07420, avg_loss=0.07498]\n",
      "Step 509676  [5.373 sec/step, loss=0.07697, avg_loss=0.07500]\n",
      "Step 509677  [5.377 sec/step, loss=0.07625, avg_loss=0.07500]\n",
      "Step 509678  [5.367 sec/step, loss=0.07557, avg_loss=0.07501]\n",
      "Generated 32 batches of size 32 in 2.661 sec\n",
      "Step 509679  [5.374 sec/step, loss=0.07512, avg_loss=0.07500]\n",
      "Step 509680  [5.368 sec/step, loss=0.07327, avg_loss=0.07500]\n",
      "Step 509681  [5.383 sec/step, loss=0.07619, avg_loss=0.07504]\n",
      "Step 509682  [5.424 sec/step, loss=0.07418, avg_loss=0.07506]\n",
      "Step 509683  [5.431 sec/step, loss=0.07561, avg_loss=0.07505]\n",
      "Step 509684  [5.420 sec/step, loss=0.07627, avg_loss=0.07504]\n",
      "Step 509685  [5.421 sec/step, loss=0.07683, avg_loss=0.07503]\n",
      "Step 509686  [5.415 sec/step, loss=0.07185, avg_loss=0.07502]\n",
      "Step 509687  [5.415 sec/step, loss=0.07359, avg_loss=0.07499]\n",
      "Step 509688  [5.398 sec/step, loss=0.07231, avg_loss=0.07497]\n",
      "Step 509689  [5.386 sec/step, loss=0.07234, avg_loss=0.07491]\n",
      "Step 509690  [5.394 sec/step, loss=0.07474, avg_loss=0.07498]\n",
      "Step 509691  [5.386 sec/step, loss=0.07527, avg_loss=0.07496]\n",
      "Step 509692  [5.395 sec/step, loss=0.07694, avg_loss=0.07497]\n",
      "Step 509693  [5.406 sec/step, loss=0.07739, avg_loss=0.07502]\n",
      "Step 509694  [5.379 sec/step, loss=0.06599, avg_loss=0.07493]\n",
      "Step 509695  [5.363 sec/step, loss=0.07488, avg_loss=0.07491]\n",
      "Step 509696  [5.372 sec/step, loss=0.07588, avg_loss=0.07492]\n",
      "Step 509697  [5.382 sec/step, loss=0.07423, avg_loss=0.07494]\n",
      "Step 509698  [5.384 sec/step, loss=0.07331, avg_loss=0.07491]\n",
      "Step 509699  [5.369 sec/step, loss=0.07503, avg_loss=0.07488]\n",
      "Step 509700  [5.364 sec/step, loss=0.07724, avg_loss=0.07488]\n",
      "Writing summary at step: 509700\n",
      "Step 509701  [5.367 sec/step, loss=0.07737, avg_loss=0.07492]\n",
      "Step 509702  [5.373 sec/step, loss=0.07651, avg_loss=0.07494]\n",
      "Step 509703  [5.390 sec/step, loss=0.07369, avg_loss=0.07491]\n",
      "Step 509704  [5.382 sec/step, loss=0.07623, avg_loss=0.07492]\n",
      "Step 509705  [5.367 sec/step, loss=0.07596, avg_loss=0.07491]\n",
      "Step 509706  [5.359 sec/step, loss=0.07737, avg_loss=0.07494]\n",
      "Step 509707  [5.362 sec/step, loss=0.07221, avg_loss=0.07490]\n",
      "Step 509708  [5.384 sec/step, loss=0.07643, avg_loss=0.07490]\n",
      "Step 509709  [5.372 sec/step, loss=0.07506, avg_loss=0.07489]\n",
      "Generated 32 batches of size 32 in 2.616 sec\n",
      "Step 509710  [5.426 sec/step, loss=0.06789, avg_loss=0.07481]\n",
      "Step 509711  [5.443 sec/step, loss=0.07486, avg_loss=0.07480]\n",
      "Step 509712  [5.447 sec/step, loss=0.07600, avg_loss=0.07482]\n",
      "Step 509713  [5.393 sec/step, loss=0.07685, avg_loss=0.07492]\n",
      "Step 509714  [5.400 sec/step, loss=0.07649, avg_loss=0.07491]\n",
      "Step 509715  [5.394 sec/step, loss=0.07655, avg_loss=0.07495]\n",
      "Step 509716  [5.390 sec/step, loss=0.07654, avg_loss=0.07493]\n",
      "Step 509717  [5.388 sec/step, loss=0.07644, avg_loss=0.07494]\n",
      "Step 509718  [5.387 sec/step, loss=0.07232, avg_loss=0.07494]\n",
      "Step 509719  [5.375 sec/step, loss=0.07113, avg_loss=0.07489]\n",
      "Step 509720  [5.369 sec/step, loss=0.07514, avg_loss=0.07488]\n",
      "Step 509721  [5.397 sec/step, loss=0.07687, avg_loss=0.07499]\n",
      "Step 509722  [5.397 sec/step, loss=0.07626, avg_loss=0.07501]\n",
      "Step 509723  [5.414 sec/step, loss=0.07724, avg_loss=0.07502]\n",
      "Step 509724  [5.409 sec/step, loss=0.07630, avg_loss=0.07504]\n",
      "Step 509725  [5.420 sec/step, loss=0.07705, avg_loss=0.07504]\n",
      "Step 509726  [5.416 sec/step, loss=0.07472, avg_loss=0.07504]\n",
      "Step 509727  [5.421 sec/step, loss=0.07613, avg_loss=0.07504]\n",
      "Step 509728  [5.405 sec/step, loss=0.07751, avg_loss=0.07506]\n",
      "Step 509729  [5.408 sec/step, loss=0.07505, avg_loss=0.07508]\n",
      "Step 509730  [5.409 sec/step, loss=0.07664, avg_loss=0.07507]\n",
      "Step 509731  [5.391 sec/step, loss=0.07238, avg_loss=0.07504]\n",
      "Step 509732  [5.407 sec/step, loss=0.07314, avg_loss=0.07509]\n",
      "Step 509733  [5.387 sec/step, loss=0.06606, avg_loss=0.07498]\n",
      "Step 509734  [5.390 sec/step, loss=0.07538, avg_loss=0.07497]\n",
      "Step 509735  [5.399 sec/step, loss=0.07652, avg_loss=0.07499]\n",
      "Step 509736  [5.411 sec/step, loss=0.07426, avg_loss=0.07496]\n",
      "Step 509737  [5.419 sec/step, loss=0.07717, avg_loss=0.07497]\n",
      "Step 509738  [5.409 sec/step, loss=0.07618, avg_loss=0.07496]\n",
      "Step 509739  [5.405 sec/step, loss=0.07692, avg_loss=0.07498]\n",
      "Step 509740  [5.424 sec/step, loss=0.07413, avg_loss=0.07497]\n",
      "Step 509741  [5.420 sec/step, loss=0.07382, avg_loss=0.07495]\n",
      "Generated 32 batches of size 32 in 2.405 sec\n",
      "Step 509742  [5.434 sec/step, loss=0.07241, avg_loss=0.07495]\n",
      "Step 509743  [5.440 sec/step, loss=0.07679, avg_loss=0.07498]\n",
      "Step 509744  [5.436 sec/step, loss=0.07473, avg_loss=0.07496]\n",
      "Step 509745  [5.433 sec/step, loss=0.07523, avg_loss=0.07494]\n",
      "Step 509746  [5.387 sec/step, loss=0.07582, avg_loss=0.07501]\n",
      "Step 509747  [5.360 sec/step, loss=0.07556, avg_loss=0.07499]\n",
      "Step 509748  [5.414 sec/step, loss=0.06698, avg_loss=0.07491]\n",
      "Step 509749  [5.411 sec/step, loss=0.07699, avg_loss=0.07493]\n",
      "Step 509750  [5.396 sec/step, loss=0.07562, avg_loss=0.07492]\n",
      "Step 509751  [5.382 sec/step, loss=0.06821, avg_loss=0.07488]\n",
      "Step 509752  [5.397 sec/step, loss=0.07582, avg_loss=0.07490]\n",
      "Step 509753  [5.386 sec/step, loss=0.07486, avg_loss=0.07487]\n",
      "Step 509754  [5.437 sec/step, loss=0.06641, avg_loss=0.07479]\n",
      "Step 509755  [5.469 sec/step, loss=0.07579, avg_loss=0.07480]\n",
      "Step 509756  [5.490 sec/step, loss=0.07662, avg_loss=0.07480]\n",
      "Step 509757  [5.474 sec/step, loss=0.07283, avg_loss=0.07475]\n",
      "Step 509758  [5.488 sec/step, loss=0.07759, avg_loss=0.07477]\n",
      "Step 509759  [5.490 sec/step, loss=0.07574, avg_loss=0.07480]\n",
      "Step 509760  [5.484 sec/step, loss=0.07627, avg_loss=0.07481]\n",
      "Step 509761  [5.465 sec/step, loss=0.07250, avg_loss=0.07477]\n",
      "Step 509762  [5.483 sec/step, loss=0.07710, avg_loss=0.07478]\n",
      "Step 509763  [5.442 sec/step, loss=0.07426, avg_loss=0.07485]\n",
      "Step 509764  [5.441 sec/step, loss=0.07632, avg_loss=0.07484]\n",
      "Step 509765  [5.445 sec/step, loss=0.07585, avg_loss=0.07484]\n",
      "Step 509766  [5.467 sec/step, loss=0.07628, avg_loss=0.07493]\n",
      "Step 509767  [5.478 sec/step, loss=0.07539, avg_loss=0.07492]\n",
      "Step 509768  [5.468 sec/step, loss=0.07275, avg_loss=0.07487]\n",
      "Step 509769  [5.455 sec/step, loss=0.07665, avg_loss=0.07487]\n",
      "Step 509770  [5.459 sec/step, loss=0.07532, avg_loss=0.07486]\n",
      "Step 509771  [5.455 sec/step, loss=0.07693, avg_loss=0.07488]\n",
      "Step 509772  [5.458 sec/step, loss=0.07658, avg_loss=0.07488]\n",
      "Step 509773  [5.440 sec/step, loss=0.07456, avg_loss=0.07488]\n",
      "Generated 32 batches of size 32 in 2.411 sec\n",
      "Step 509774  [5.467 sec/step, loss=0.07745, avg_loss=0.07490]\n",
      "Step 509775  [5.457 sec/step, loss=0.07546, avg_loss=0.07492]\n",
      "Step 509776  [5.440 sec/step, loss=0.07539, avg_loss=0.07490]\n",
      "Step 509777  [5.419 sec/step, loss=0.07325, avg_loss=0.07487]\n",
      "Step 509778  [5.426 sec/step, loss=0.07464, avg_loss=0.07486]\n",
      "Step 509779  [5.429 sec/step, loss=0.07664, avg_loss=0.07488]\n",
      "Step 509780  [5.443 sec/step, loss=0.07464, avg_loss=0.07489]\n",
      "Step 509781  [5.450 sec/step, loss=0.07752, avg_loss=0.07490]\n",
      "Step 509782  [5.421 sec/step, loss=0.07654, avg_loss=0.07493]\n",
      "Step 509783  [5.411 sec/step, loss=0.07482, avg_loss=0.07492]\n",
      "Step 509784  [5.398 sec/step, loss=0.07518, avg_loss=0.07491]\n",
      "Step 509785  [5.368 sec/step, loss=0.06649, avg_loss=0.07481]\n",
      "Step 509786  [5.364 sec/step, loss=0.07188, avg_loss=0.07481]\n",
      "Step 509787  [5.385 sec/step, loss=0.07656, avg_loss=0.07484]\n",
      "Step 509788  [5.395 sec/step, loss=0.07431, avg_loss=0.07486]\n",
      "Step 509789  [5.425 sec/step, loss=0.07375, avg_loss=0.07487]\n",
      "Step 509790  [5.438 sec/step, loss=0.07773, avg_loss=0.07490]\n",
      "Step 509791  [5.439 sec/step, loss=0.07635, avg_loss=0.07491]\n",
      "Step 509792  [5.435 sec/step, loss=0.07758, avg_loss=0.07492]\n",
      "Step 509793  [5.430 sec/step, loss=0.07605, avg_loss=0.07490]\n",
      "Step 509794  [5.443 sec/step, loss=0.07644, avg_loss=0.07501]\n",
      "Step 509795  [5.448 sec/step, loss=0.07515, avg_loss=0.07501]\n",
      "Step 509796  [5.445 sec/step, loss=0.07277, avg_loss=0.07498]\n",
      "Step 509797  [5.459 sec/step, loss=0.07521, avg_loss=0.07499]\n",
      "Step 509798  [5.507 sec/step, loss=0.06650, avg_loss=0.07492]\n",
      "Step 509799  [5.504 sec/step, loss=0.07505, avg_loss=0.07492]\n",
      "Step 509800  [5.504 sec/step, loss=0.07734, avg_loss=0.07492]\n",
      "Writing summary at step: 509800\n",
      "Step 509801  [5.498 sec/step, loss=0.07540, avg_loss=0.07490]\n",
      "Step 509802  [5.491 sec/step, loss=0.07479, avg_loss=0.07489]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 509803  [5.455 sec/step, loss=0.07179, avg_loss=0.07487]\n",
      "Step 509804  [5.454 sec/step, loss=0.07632, avg_loss=0.07487]\n",
      "Generated 32 batches of size 32 in 2.430 sec\n",
      "Step 509805  [5.458 sec/step, loss=0.07615, avg_loss=0.07487]\n",
      "Step 509806  [5.449 sec/step, loss=0.07656, avg_loss=0.07486]\n",
      "Step 509807  [5.446 sec/step, loss=0.07309, avg_loss=0.07487]\n",
      "Step 509808  [5.445 sec/step, loss=0.07644, avg_loss=0.07487]\n",
      "Step 509809  [5.470 sec/step, loss=0.07718, avg_loss=0.07489]\n",
      "Step 509810  [5.426 sec/step, loss=0.07759, avg_loss=0.07499]\n",
      "Step 509811  [5.417 sec/step, loss=0.07688, avg_loss=0.07501]\n",
      "Step 509812  [5.413 sec/step, loss=0.07468, avg_loss=0.07500]\n",
      "Step 509813  [5.414 sec/step, loss=0.07661, avg_loss=0.07499]\n",
      "Step 509814  [5.408 sec/step, loss=0.07656, avg_loss=0.07499]\n",
      "Step 509815  [5.409 sec/step, loss=0.07684, avg_loss=0.07500]\n",
      "Step 509816  [5.430 sec/step, loss=0.07444, avg_loss=0.07498]\n",
      "Step 509817  [5.445 sec/step, loss=0.07700, avg_loss=0.07498]\n",
      "Step 509818  [5.449 sec/step, loss=0.07476, avg_loss=0.07501]\n",
      "Step 509819  [5.456 sec/step, loss=0.07457, avg_loss=0.07504]\n",
      "Step 509820  [5.467 sec/step, loss=0.07729, avg_loss=0.07506]\n",
      "Step 509821  [5.461 sec/step, loss=0.07633, avg_loss=0.07506]\n",
      "Step 509822  [5.461 sec/step, loss=0.07602, avg_loss=0.07505]\n",
      "Step 509823  [5.463 sec/step, loss=0.07780, avg_loss=0.07506]\n",
      "Step 509824  [5.443 sec/step, loss=0.07474, avg_loss=0.07504]\n",
      "Step 509825  [5.441 sec/step, loss=0.07660, avg_loss=0.07504]\n",
      "Step 509826  [5.449 sec/step, loss=0.07617, avg_loss=0.07505]\n",
      "Step 509827  [5.450 sec/step, loss=0.07744, avg_loss=0.07507]\n",
      "Step 509828  [5.423 sec/step, loss=0.06642, avg_loss=0.07496]\n",
      "Step 509829  [5.432 sec/step, loss=0.07346, avg_loss=0.07494]\n",
      "Step 509830  [5.435 sec/step, loss=0.07647, avg_loss=0.07494]\n",
      "Step 509831  [5.499 sec/step, loss=0.06877, avg_loss=0.07490]\n",
      "Step 509832  [5.500 sec/step, loss=0.07521, avg_loss=0.07492]\n",
      "Step 509833  [5.531 sec/step, loss=0.07787, avg_loss=0.07504]\n",
      "Step 509834  [5.529 sec/step, loss=0.07620, avg_loss=0.07505]\n",
      "Step 509835  [5.518 sec/step, loss=0.07530, avg_loss=0.07504]\n",
      "Step 509836  [5.497 sec/step, loss=0.07566, avg_loss=0.07505]\n",
      "Generated 32 batches of size 32 in 2.407 sec\n",
      "Step 509837  [5.498 sec/step, loss=0.07449, avg_loss=0.07502]\n",
      "Step 509838  [5.493 sec/step, loss=0.07483, avg_loss=0.07501]\n",
      "Step 509839  [5.484 sec/step, loss=0.07258, avg_loss=0.07497]\n",
      "Step 509840  [5.459 sec/step, loss=0.07692, avg_loss=0.07499]\n",
      "Step 509841  [5.462 sec/step, loss=0.07387, avg_loss=0.07500]\n",
      "Step 509842  [5.470 sec/step, loss=0.07495, avg_loss=0.07502]\n",
      "Step 509843  [5.458 sec/step, loss=0.07382, avg_loss=0.07499]\n",
      "Step 509844  [5.470 sec/step, loss=0.07635, avg_loss=0.07501]\n",
      "Step 509845  [5.447 sec/step, loss=0.07371, avg_loss=0.07499]\n",
      "Step 509846  [5.437 sec/step, loss=0.07303, avg_loss=0.07496]\n",
      "Step 509847  [5.437 sec/step, loss=0.07180, avg_loss=0.07493]\n",
      "Step 509848  [5.370 sec/step, loss=0.06713, avg_loss=0.07493]\n",
      "Step 509849  [5.417 sec/step, loss=0.06699, avg_loss=0.07483]\n",
      "Step 509850  [5.413 sec/step, loss=0.07422, avg_loss=0.07481]\n",
      "Step 509851  [5.437 sec/step, loss=0.07577, avg_loss=0.07489]\n",
      "Step 509852  [5.446 sec/step, loss=0.07734, avg_loss=0.07490]\n",
      "Step 509853  [5.466 sec/step, loss=0.07737, avg_loss=0.07493]\n",
      "Step 509854  [5.425 sec/step, loss=0.07722, avg_loss=0.07504]\n",
      "Step 509855  [5.396 sec/step, loss=0.07526, avg_loss=0.07503]\n",
      "Step 509856  [5.382 sec/step, loss=0.07661, avg_loss=0.07503]\n",
      "Step 509857  [5.410 sec/step, loss=0.07476, avg_loss=0.07505]\n",
      "Step 509858  [5.408 sec/step, loss=0.07544, avg_loss=0.07503]\n",
      "Step 509859  [5.402 sec/step, loss=0.07444, avg_loss=0.07502]\n",
      "Step 509860  [5.407 sec/step, loss=0.07249, avg_loss=0.07498]\n",
      "Step 509861  [5.420 sec/step, loss=0.07699, avg_loss=0.07502]\n",
      "Step 509862  [5.420 sec/step, loss=0.07700, avg_loss=0.07502]\n",
      "Step 509863  [5.412 sec/step, loss=0.07629, avg_loss=0.07504]\n",
      "Step 509864  [5.395 sec/step, loss=0.07525, avg_loss=0.07503]\n",
      "Step 509865  [5.396 sec/step, loss=0.07687, avg_loss=0.07504]\n",
      "Step 509866  [5.382 sec/step, loss=0.07484, avg_loss=0.07503]\n",
      "Step 509867  [5.369 sec/step, loss=0.07575, avg_loss=0.07503]\n",
      "Step 509868  [5.358 sec/step, loss=0.07202, avg_loss=0.07502]\n",
      "Generated 32 batches of size 32 in 2.439 sec\n",
      "Step 509869  [5.363 sec/step, loss=0.07630, avg_loss=0.07502]\n",
      "Step 509870  [5.383 sec/step, loss=0.07474, avg_loss=0.07502]\n",
      "Step 509871  [5.384 sec/step, loss=0.07453, avg_loss=0.07499]\n",
      "Step 509872  [5.385 sec/step, loss=0.07558, avg_loss=0.07498]\n",
      "Step 509873  [5.389 sec/step, loss=0.07615, avg_loss=0.07500]\n",
      "Step 509874  [5.371 sec/step, loss=0.07496, avg_loss=0.07497]\n",
      "Step 509875  [5.378 sec/step, loss=0.07640, avg_loss=0.07498]\n",
      "Step 509876  [5.387 sec/step, loss=0.07693, avg_loss=0.07500]\n",
      "Step 509877  [5.414 sec/step, loss=0.07747, avg_loss=0.07504]\n",
      "Step 509878  [5.426 sec/step, loss=0.07718, avg_loss=0.07507]\n",
      "Step 509879  [5.474 sec/step, loss=0.06623, avg_loss=0.07496]\n",
      "Step 509880  [5.467 sec/step, loss=0.07269, avg_loss=0.07494]\n",
      "Step 509881  [5.452 sec/step, loss=0.07390, avg_loss=0.07491]\n",
      "Step 509882  [5.470 sec/step, loss=0.07739, avg_loss=0.07491]\n",
      "Step 509883  [5.474 sec/step, loss=0.07505, avg_loss=0.07492]\n",
      "Step 509884  [5.486 sec/step, loss=0.07631, avg_loss=0.07493]\n",
      "Step 509885  [5.500 sec/step, loss=0.07622, avg_loss=0.07502]\n",
      "Step 509886  [5.526 sec/step, loss=0.07538, avg_loss=0.07506]\n",
      "Step 509887  [5.514 sec/step, loss=0.07624, avg_loss=0.07506]\n",
      "Step 509888  [5.529 sec/step, loss=0.07736, avg_loss=0.07509]\n",
      "Step 509889  [5.515 sec/step, loss=0.07523, avg_loss=0.07510]\n",
      "Step 509890  [5.505 sec/step, loss=0.07504, avg_loss=0.07507]\n",
      "Step 509891  [5.505 sec/step, loss=0.07286, avg_loss=0.07504]\n",
      "Step 509892  [5.490 sec/step, loss=0.07107, avg_loss=0.07497]\n",
      "Step 509893  [5.491 sec/step, loss=0.07592, avg_loss=0.07497]\n",
      "Step 509894  [5.504 sec/step, loss=0.07710, avg_loss=0.07498]\n",
      "Step 509895  [5.498 sec/step, loss=0.07534, avg_loss=0.07498]\n",
      "Step 509896  [5.522 sec/step, loss=0.07588, avg_loss=0.07501]\n",
      "Step 509897  [5.513 sec/step, loss=0.07665, avg_loss=0.07503]\n",
      "Step 509898  [5.465 sec/step, loss=0.07640, avg_loss=0.07513]\n",
      "Step 509899  [5.481 sec/step, loss=0.07635, avg_loss=0.07514]\n",
      "Step 509900  [5.473 sec/step, loss=0.07636, avg_loss=0.07513]\n",
      "Writing summary at step: 509900\n",
      "Generated 32 batches of size 32 in 2.344 sec\n",
      "Step 509901  [5.459 sec/step, loss=0.07338, avg_loss=0.07511]\n",
      "Step 509902  [5.453 sec/step, loss=0.07707, avg_loss=0.07513]\n",
      "Step 509903  [5.482 sec/step, loss=0.07720, avg_loss=0.07519]\n",
      "Step 509904  [5.483 sec/step, loss=0.07643, avg_loss=0.07519]\n",
      "Step 509905  [5.452 sec/step, loss=0.06693, avg_loss=0.07509]\n",
      "Step 509906  [5.447 sec/step, loss=0.07526, avg_loss=0.07508]\n",
      "Step 509907  [5.454 sec/step, loss=0.07590, avg_loss=0.07511]\n",
      "Step 509908  [5.427 sec/step, loss=0.07300, avg_loss=0.07508]\n",
      "Step 509909  [5.415 sec/step, loss=0.07659, avg_loss=0.07507]\n",
      "Step 509910  [5.404 sec/step, loss=0.07568, avg_loss=0.07505]\n",
      "Step 509911  [5.406 sec/step, loss=0.07636, avg_loss=0.07505]\n",
      "Step 509912  [5.396 sec/step, loss=0.06777, avg_loss=0.07498]\n",
      "Step 509913  [5.406 sec/step, loss=0.07710, avg_loss=0.07498]\n",
      "Step 509914  [5.406 sec/step, loss=0.07775, avg_loss=0.07499]\n",
      "Step 509915  [5.402 sec/step, loss=0.07600, avg_loss=0.07498]\n",
      "Step 509916  [5.388 sec/step, loss=0.07653, avg_loss=0.07501]\n",
      "Step 509917  [5.375 sec/step, loss=0.07644, avg_loss=0.07500]\n",
      "Step 509918  [5.382 sec/step, loss=0.07393, avg_loss=0.07499]\n",
      "Step 509919  [5.387 sec/step, loss=0.07547, avg_loss=0.07500]\n",
      "Step 509920  [5.365 sec/step, loss=0.07207, avg_loss=0.07495]\n",
      "Step 509921  [5.369 sec/step, loss=0.07488, avg_loss=0.07493]\n",
      "Step 509922  [5.381 sec/step, loss=0.07716, avg_loss=0.07495]\n",
      "Step 509923  [5.365 sec/step, loss=0.07302, avg_loss=0.07490]\n",
      "Step 509924  [5.379 sec/step, loss=0.07738, avg_loss=0.07492]\n",
      "Step 509925  [5.366 sec/step, loss=0.07539, avg_loss=0.07491]\n",
      "Step 509926  [5.359 sec/step, loss=0.07642, avg_loss=0.07491]\n",
      "Step 509927  [5.371 sec/step, loss=0.07647, avg_loss=0.07491]\n",
      "Step 509928  [5.388 sec/step, loss=0.07569, avg_loss=0.07500]\n",
      "Step 509929  [5.377 sec/step, loss=0.07535, avg_loss=0.07502]\n",
      "Step 509930  [5.373 sec/step, loss=0.07470, avg_loss=0.07500]\n",
      "Step 509931  [5.338 sec/step, loss=0.07563, avg_loss=0.07507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 32 batches of size 32 in 2.580 sec\n",
      "Step 509932  [5.343 sec/step, loss=0.07169, avg_loss=0.07503]\n",
      "Step 509933  [5.343 sec/step, loss=0.07677, avg_loss=0.07502]\n",
      "Step 509934  [5.336 sec/step, loss=0.07485, avg_loss=0.07501]\n",
      "Step 509935  [5.350 sec/step, loss=0.07450, avg_loss=0.07500]\n",
      "Step 509936  [5.352 sec/step, loss=0.07627, avg_loss=0.07501]\n",
      "Step 509937  [5.335 sec/step, loss=0.07314, avg_loss=0.07499]\n",
      "Step 509938  [5.329 sec/step, loss=0.07310, avg_loss=0.07498]\n",
      "Step 509939  [5.381 sec/step, loss=0.06755, avg_loss=0.07492]\n",
      "Step 509940  [5.407 sec/step, loss=0.07376, avg_loss=0.07489]\n",
      "Step 509941  [5.412 sec/step, loss=0.07369, avg_loss=0.07489]\n",
      "Step 509942  [5.399 sec/step, loss=0.07480, avg_loss=0.07489]\n",
      "Step 509943  [5.402 sec/step, loss=0.07483, avg_loss=0.07490]\n",
      "Step 509944  [5.394 sec/step, loss=0.07603, avg_loss=0.07490]\n",
      "Step 509945  [5.393 sec/step, loss=0.07337, avg_loss=0.07489]\n",
      "Step 509946  [5.400 sec/step, loss=0.07693, avg_loss=0.07493]\n",
      "Step 509947  [5.415 sec/step, loss=0.07550, avg_loss=0.07497]\n",
      "Step 509948  [5.429 sec/step, loss=0.07626, avg_loss=0.07506]\n",
      "Step 509949  [5.397 sec/step, loss=0.07693, avg_loss=0.07516]\n",
      "Step 509950  [5.393 sec/step, loss=0.07432, avg_loss=0.07516]\n",
      "Step 509951  [5.381 sec/step, loss=0.07512, avg_loss=0.07515]\n",
      "Step 509952  [5.365 sec/step, loss=0.07469, avg_loss=0.07513]\n",
      "Step 509953  [5.351 sec/step, loss=0.07549, avg_loss=0.07511]\n",
      "Step 509954  [5.340 sec/step, loss=0.07306, avg_loss=0.07507]\n",
      "Step 509955  [5.355 sec/step, loss=0.07736, avg_loss=0.07509]\n",
      "Step 509956  [5.361 sec/step, loss=0.07750, avg_loss=0.07510]\n",
      "Step 509957  [5.351 sec/step, loss=0.07718, avg_loss=0.07512]\n",
      "Step 509958  [5.351 sec/step, loss=0.07666, avg_loss=0.07513]\n",
      "Step 509959  [5.365 sec/step, loss=0.07587, avg_loss=0.07515]\n",
      "Step 509960  [5.370 sec/step, loss=0.07743, avg_loss=0.07520]\n",
      "Step 509961  [5.359 sec/step, loss=0.07188, avg_loss=0.07515]\n",
      "Step 509962  [5.337 sec/step, loss=0.07534, avg_loss=0.07513]\n",
      "Step 509963  [5.385 sec/step, loss=0.06687, avg_loss=0.07504]\n",
      "Generated 32 batches of size 32 in 2.419 sec\n",
      "Step 509964  [5.429 sec/step, loss=0.07394, avg_loss=0.07502]\n",
      "Step 509965  [5.425 sec/step, loss=0.07637, avg_loss=0.07502]\n",
      "Step 509966  [5.447 sec/step, loss=0.07536, avg_loss=0.07502]\n",
      "Step 509967  [5.430 sec/step, loss=0.06729, avg_loss=0.07494]\n",
      "Step 509968  [5.449 sec/step, loss=0.07610, avg_loss=0.07498]\n",
      "Step 509969  [5.454 sec/step, loss=0.07583, avg_loss=0.07497]\n",
      "Step 509970  [5.439 sec/step, loss=0.07770, avg_loss=0.07500]\n",
      "Step 509971  [5.430 sec/step, loss=0.07336, avg_loss=0.07499]\n",
      "Step 509972  [5.430 sec/step, loss=0.07684, avg_loss=0.07501]\n",
      "Step 509973  [5.459 sec/step, loss=0.07467, avg_loss=0.07499]\n",
      "Step 509974  [5.448 sec/step, loss=0.07576, avg_loss=0.07500]\n",
      "Step 509975  [5.451 sec/step, loss=0.07689, avg_loss=0.07500]\n",
      "Step 509976  [5.437 sec/step, loss=0.07462, avg_loss=0.07498]\n",
      "Step 509977  [5.414 sec/step, loss=0.07097, avg_loss=0.07492]\n",
      "Step 509978  [5.406 sec/step, loss=0.07609, avg_loss=0.07490]\n",
      "Step 509979  [5.371 sec/step, loss=0.07694, avg_loss=0.07501]\n",
      "Step 509980  [5.378 sec/step, loss=0.07266, avg_loss=0.07501]\n",
      "Step 509981  [5.431 sec/step, loss=0.06728, avg_loss=0.07495]\n",
      "Step 509982  [5.436 sec/step, loss=0.07333, avg_loss=0.07490]\n",
      "Step 509983  [5.447 sec/step, loss=0.07610, avg_loss=0.07492]\n",
      "Step 509984  [5.441 sec/step, loss=0.07296, avg_loss=0.07488]\n",
      "Step 509985  [5.451 sec/step, loss=0.07736, avg_loss=0.07489]\n",
      "Step 509986  [5.425 sec/step, loss=0.07178, avg_loss=0.07486]\n",
      "Step 509987  [5.423 sec/step, loss=0.07462, avg_loss=0.07484]\n",
      "Step 509988  [5.412 sec/step, loss=0.07542, avg_loss=0.07482]\n",
      "Step 509989  [5.397 sec/step, loss=0.07277, avg_loss=0.07480]\n",
      "Step 509990  [5.398 sec/step, loss=0.07671, avg_loss=0.07481]\n",
      "Step 509991  [5.401 sec/step, loss=0.07656, avg_loss=0.07485]\n",
      "Step 509992  [5.416 sec/step, loss=0.07769, avg_loss=0.07492]\n",
      "Step 509993  [5.404 sec/step, loss=0.07502, avg_loss=0.07491]\n",
      "Step 509994  [5.392 sec/step, loss=0.07657, avg_loss=0.07490]\n",
      "Step 509995  [5.401 sec/step, loss=0.07644, avg_loss=0.07491]\n",
      "Generated 32 batches of size 32 in 2.395 sec\n",
      "Step 509996  [5.391 sec/step, loss=0.07729, avg_loss=0.07493]\n",
      "Step 509997  [5.402 sec/step, loss=0.07737, avg_loss=0.07493]\n",
      "Step 509998  [5.410 sec/step, loss=0.07663, avg_loss=0.07494]\n",
      "Step 509999  [5.403 sec/step, loss=0.07613, avg_loss=0.07493]\n",
      "Step 510000  [5.406 sec/step, loss=0.07595, avg_loss=0.07493]\n",
      "Writing summary at step: 510000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-510000\n",
      "Saving audio and alignment...\n",
      "Input: ggajjuur jaang nay dzabrii masruufijaatd or budzhaaii huuii aag kay ddhooayn sae biphar kur balggam or xuun uglaa~____________________\n",
      "Step 510001  [5.402 sec/step, loss=0.06565, avg_loss=0.07485]\n",
      "Step 510002  [5.411 sec/step, loss=0.07684, avg_loss=0.07485]\n",
      "Step 510003  [5.384 sec/step, loss=0.07496, avg_loss=0.07483]\n",
      "Step 510004  [5.380 sec/step, loss=0.07321, avg_loss=0.07480]\n",
      "Step 510005  [5.397 sec/step, loss=0.07613, avg_loss=0.07489]\n",
      "Step 510006  [5.404 sec/step, loss=0.07539, avg_loss=0.07489]\n",
      "Step 510007  [5.414 sec/step, loss=0.07572, avg_loss=0.07489]\n",
      "Step 510008  [5.443 sec/step, loss=0.07454, avg_loss=0.07490]\n",
      "Step 510009  [5.433 sec/step, loss=0.07473, avg_loss=0.07488]\n",
      "Step 510010  [5.441 sec/step, loss=0.07676, avg_loss=0.07490]\n",
      "Step 510011  [5.441 sec/step, loss=0.07327, avg_loss=0.07486]\n",
      "Step 510012  [5.445 sec/step, loss=0.07255, avg_loss=0.07491]\n",
      "Step 510013  [5.426 sec/step, loss=0.07531, avg_loss=0.07489]\n",
      "Step 510014  [5.421 sec/step, loss=0.07694, avg_loss=0.07489]\n",
      "Step 510015  [5.414 sec/step, loss=0.07216, avg_loss=0.07485]\n",
      "Step 510016  [5.400 sec/step, loss=0.07567, avg_loss=0.07484]\n",
      "Step 510017  [5.405 sec/step, loss=0.07617, avg_loss=0.07484]\n",
      "Step 510018  [5.389 sec/step, loss=0.06671, avg_loss=0.07476]\n",
      "Step 510019  [5.399 sec/step, loss=0.07558, avg_loss=0.07477]\n",
      "Step 510020  [5.407 sec/step, loss=0.07350, avg_loss=0.07478]\n",
      "Step 510021  [5.398 sec/step, loss=0.07740, avg_loss=0.07481]\n",
      "Step 510022  [5.402 sec/step, loss=0.07721, avg_loss=0.07481]\n",
      "Step 510023  [5.408 sec/step, loss=0.07255, avg_loss=0.07480]\n",
      "Step 510024  [5.394 sec/step, loss=0.07603, avg_loss=0.07479]\n",
      "Step 510025  [5.406 sec/step, loss=0.07409, avg_loss=0.07477]\n",
      "Generated 32 batches of size 32 in 2.570 sec\n",
      "Step 510026  [5.462 sec/step, loss=0.06805, avg_loss=0.07469]\n",
      "Step 510027  [5.448 sec/step, loss=0.07654, avg_loss=0.07469]\n",
      "Step 510028  [5.453 sec/step, loss=0.07747, avg_loss=0.07471]\n",
      "Step 510029  [5.458 sec/step, loss=0.07467, avg_loss=0.07470]\n",
      "Step 510030  [5.447 sec/step, loss=0.07457, avg_loss=0.07470]\n",
      "Step 510031  [5.444 sec/step, loss=0.07691, avg_loss=0.07471]\n",
      "Step 510032  [5.447 sec/step, loss=0.07689, avg_loss=0.07477]\n",
      "Step 510033  [5.460 sec/step, loss=0.07489, avg_loss=0.07475]\n",
      "Step 510034  [5.465 sec/step, loss=0.07652, avg_loss=0.07476]\n",
      "Step 510035  [5.457 sec/step, loss=0.07351, avg_loss=0.07475]\n",
      "Step 510036  [5.441 sec/step, loss=0.07285, avg_loss=0.07472]\n",
      "Step 510037  [5.442 sec/step, loss=0.07291, avg_loss=0.07472]\n",
      "Step 510038  [5.448 sec/step, loss=0.07502, avg_loss=0.07474]\n",
      "Step 510039  [5.386 sec/step, loss=0.07212, avg_loss=0.07478]\n",
      "Step 510040  [5.358 sec/step, loss=0.07683, avg_loss=0.07481]\n",
      "Step 510041  [5.370 sec/step, loss=0.07686, avg_loss=0.07484]\n",
      "Step 510042  [5.366 sec/step, loss=0.07601, avg_loss=0.07486]\n",
      "Step 510043  [5.382 sec/step, loss=0.07716, avg_loss=0.07488]\n",
      "Step 510044  [5.389 sec/step, loss=0.07460, avg_loss=0.07487]\n",
      "Step 510045  [5.403 sec/step, loss=0.07566, avg_loss=0.07489]\n",
      "Step 510046  [5.415 sec/step, loss=0.07571, avg_loss=0.07488]\n",
      "Step 510047  [5.410 sec/step, loss=0.07554, avg_loss=0.07488]\n",
      "Step 510048  [5.423 sec/step, loss=0.07669, avg_loss=0.07488]\n",
      "Step 510049  [5.409 sec/step, loss=0.07445, avg_loss=0.07486]\n",
      "Step 510050  [5.420 sec/step, loss=0.07540, avg_loss=0.07487]\n",
      "Step 510051  [5.408 sec/step, loss=0.06751, avg_loss=0.07479]\n",
      "Step 510052  [5.421 sec/step, loss=0.07768, avg_loss=0.07482]\n",
      "Step 510053  [5.416 sec/step, loss=0.07431, avg_loss=0.07481]\n",
      "Step 510054  [5.444 sec/step, loss=0.07505, avg_loss=0.07483]\n",
      "Step 510055  [5.443 sec/step, loss=0.07746, avg_loss=0.07483]\n",
      "Step 510056  [5.425 sec/step, loss=0.07455, avg_loss=0.07480]\n",
      "Step 510057  [5.412 sec/step, loss=0.07617, avg_loss=0.07479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 32 batches of size 32 in 2.351 sec\n",
      "Step 510058  [5.415 sec/step, loss=0.07588, avg_loss=0.07478]\n",
      "Step 510059  [5.411 sec/step, loss=0.07634, avg_loss=0.07479]\n",
      "Step 510060  [5.400 sec/step, loss=0.07538, avg_loss=0.07477]\n",
      "Step 510061  [5.458 sec/step, loss=0.06601, avg_loss=0.07471]\n",
      "Step 510062  [5.474 sec/step, loss=0.07794, avg_loss=0.07473]\n",
      "Step 510063  [5.414 sec/step, loss=0.07535, avg_loss=0.07482]\n",
      "Step 510064  [5.378 sec/step, loss=0.07615, avg_loss=0.07484]\n",
      "Step 510065  [5.387 sec/step, loss=0.07646, avg_loss=0.07484]\n",
      "Step 510066  [5.383 sec/step, loss=0.07538, avg_loss=0.07484]\n",
      "Step 510067  [5.395 sec/step, loss=0.07447, avg_loss=0.07491]\n",
      "Step 510068  [5.379 sec/step, loss=0.07255, avg_loss=0.07488]\n",
      "Step 510069  [5.365 sec/step, loss=0.07474, avg_loss=0.07487]\n",
      "Step 510070  [5.362 sec/step, loss=0.07561, avg_loss=0.07485]\n",
      "Step 510071  [5.379 sec/step, loss=0.07447, avg_loss=0.07486]\n",
      "Step 510072  [5.423 sec/step, loss=0.06870, avg_loss=0.07478]\n",
      "Step 510073  [5.393 sec/step, loss=0.07677, avg_loss=0.07480]\n",
      "Step 510074  [5.395 sec/step, loss=0.07577, avg_loss=0.07480]\n",
      "Step 510075  [5.398 sec/step, loss=0.07665, avg_loss=0.07480]\n",
      "Step 510076  [5.408 sec/step, loss=0.07779, avg_loss=0.07483]\n",
      "Step 510077  [5.434 sec/step, loss=0.07746, avg_loss=0.07489]\n",
      "Step 510078  [5.436 sec/step, loss=0.07729, avg_loss=0.07490]\n",
      "Step 510079  [5.426 sec/step, loss=0.07599, avg_loss=0.07489]\n",
      "Step 510080  [5.420 sec/step, loss=0.07408, avg_loss=0.07491]\n",
      "Step 510081  [5.374 sec/step, loss=0.07640, avg_loss=0.07500]\n",
      "Step 510082  [5.344 sec/step, loss=0.07552, avg_loss=0.07502]\n",
      "Step 510083  [5.329 sec/step, loss=0.07597, avg_loss=0.07502]\n",
      "Step 510084  [5.341 sec/step, loss=0.07748, avg_loss=0.07507]\n",
      "Step 510085  [5.333 sec/step, loss=0.07634, avg_loss=0.07506]\n",
      "Step 510086  [5.345 sec/step, loss=0.07579, avg_loss=0.07510]\n",
      "Step 510087  [5.326 sec/step, loss=0.07242, avg_loss=0.07507]\n",
      "Step 510088  [5.333 sec/step, loss=0.07691, avg_loss=0.07509]\n",
      "Step 510089  [5.336 sec/step, loss=0.07637, avg_loss=0.07512]\n",
      "Generated 32 batches of size 32 in 2.381 sec\n",
      "Step 510090  [5.354 sec/step, loss=0.07563, avg_loss=0.07511]\n",
      "Step 510091  [5.361 sec/step, loss=0.07784, avg_loss=0.07513]\n",
      "Step 510092  [5.364 sec/step, loss=0.07700, avg_loss=0.07512]\n",
      "Step 510093  [5.372 sec/step, loss=0.07671, avg_loss=0.07514]\n",
      "Step 510094  [5.401 sec/step, loss=0.07504, avg_loss=0.07512]\n",
      "Step 510095  [5.397 sec/step, loss=0.07296, avg_loss=0.07509]\n",
      "Step 510096  [5.376 sec/step, loss=0.07378, avg_loss=0.07505]\n",
      "Step 510097  [5.375 sec/step, loss=0.07491, avg_loss=0.07503]\n",
      "Step 510098  [5.349 sec/step, loss=0.06749, avg_loss=0.07493]\n",
      "Step 510099  [5.348 sec/step, loss=0.07632, avg_loss=0.07494]\n",
      "Step 510100  [5.356 sec/step, loss=0.07725, avg_loss=0.07495]\n",
      "Writing summary at step: 510100\n",
      "Step 510101  [5.400 sec/step, loss=0.07428, avg_loss=0.07504]\n",
      "Step 510102  [5.377 sec/step, loss=0.06720, avg_loss=0.07494]\n",
      "Step 510103  [5.388 sec/step, loss=0.07661, avg_loss=0.07496]\n",
      "Step 510104  [5.377 sec/step, loss=0.07248, avg_loss=0.07495]\n",
      "Step 510105  [5.388 sec/step, loss=0.07776, avg_loss=0.07497]\n",
      "Step 510106  [5.380 sec/step, loss=0.07620, avg_loss=0.07497]\n",
      "Step 510107  [5.365 sec/step, loss=0.07717, avg_loss=0.07499]\n",
      "Step 510108  [5.350 sec/step, loss=0.07546, avg_loss=0.07500]\n",
      "Step 510109  [5.356 sec/step, loss=0.07570, avg_loss=0.07501]\n",
      "Step 510110  [5.358 sec/step, loss=0.07542, avg_loss=0.07499]\n",
      "Step 510111  [5.349 sec/step, loss=0.07480, avg_loss=0.07501]\n",
      "Step 510112  [5.364 sec/step, loss=0.07605, avg_loss=0.07504]\n",
      "Step 510113  [5.424 sec/step, loss=0.06678, avg_loss=0.07496]\n",
      "Step 510114  [5.422 sec/step, loss=0.07541, avg_loss=0.07494]\n",
      "Step 510115  [5.429 sec/step, loss=0.07282, avg_loss=0.07495]\n",
      "Step 510116  [5.431 sec/step, loss=0.07314, avg_loss=0.07492]\n",
      "Step 510117  [5.416 sec/step, loss=0.07424, avg_loss=0.07490]\n",
      "Step 510118  [5.432 sec/step, loss=0.07593, avg_loss=0.07500]\n",
      "Step 510119  [5.411 sec/step, loss=0.07515, avg_loss=0.07499]\n",
      "Step 510120  [5.422 sec/step, loss=0.07583, avg_loss=0.07502]\n",
      "Generated 32 batches of size 32 in 2.393 sec\n",
      "Step 510121  [5.436 sec/step, loss=0.07743, avg_loss=0.07502]\n",
      "Step 510122  [5.417 sec/step, loss=0.07540, avg_loss=0.07500]\n",
      "Step 510123  [5.416 sec/step, loss=0.07692, avg_loss=0.07504]\n",
      "Step 510124  [5.434 sec/step, loss=0.07729, avg_loss=0.07505]\n",
      "Step 510125  [5.422 sec/step, loss=0.07526, avg_loss=0.07507]\n",
      "Step 510126  [5.377 sec/step, loss=0.07755, avg_loss=0.07516]\n",
      "Step 510127  [5.358 sec/step, loss=0.07303, avg_loss=0.07513]\n",
      "Step 510128  [5.370 sec/step, loss=0.07485, avg_loss=0.07510]\n",
      "Step 510129  [5.381 sec/step, loss=0.07617, avg_loss=0.07512]\n",
      "Step 510130  [5.385 sec/step, loss=0.07665, avg_loss=0.07514]\n",
      "Step 510131  [5.386 sec/step, loss=0.07620, avg_loss=0.07513]\n",
      "Step 510132  [5.385 sec/step, loss=0.07752, avg_loss=0.07514]\n",
      "Step 510133  [5.407 sec/step, loss=0.06775, avg_loss=0.07506]\n",
      "Step 510134  [5.393 sec/step, loss=0.07116, avg_loss=0.07501]\n",
      "Step 510135  [5.404 sec/step, loss=0.07760, avg_loss=0.07505]\n",
      "Step 510136  [5.435 sec/step, loss=0.07669, avg_loss=0.07509]\n",
      "Step 510137  [5.463 sec/step, loss=0.07355, avg_loss=0.07510]\n",
      "Step 510138  [5.461 sec/step, loss=0.07526, avg_loss=0.07510]\n",
      "Step 510139  [5.476 sec/step, loss=0.07606, avg_loss=0.07514]\n",
      "Step 510140  [5.471 sec/step, loss=0.07355, avg_loss=0.07510]\n",
      "Step 510141  [5.447 sec/step, loss=0.07347, avg_loss=0.07507]\n",
      "Step 510142  [5.459 sec/step, loss=0.07577, avg_loss=0.07507]\n",
      "Step 510143  [5.452 sec/step, loss=0.07595, avg_loss=0.07506]\n",
      "Step 510144  [5.429 sec/step, loss=0.06585, avg_loss=0.07497]\n",
      "Step 510145  [5.430 sec/step, loss=0.07724, avg_loss=0.07498]\n",
      "Step 510146  [5.424 sec/step, loss=0.07729, avg_loss=0.07500]\n",
      "Step 510147  [5.424 sec/step, loss=0.07499, avg_loss=0.07500]\n",
      "Step 510148  [5.412 sec/step, loss=0.07267, avg_loss=0.07495]\n",
      "Step 510149  [5.403 sec/step, loss=0.07649, avg_loss=0.07498]\n",
      "Step 510150  [5.394 sec/step, loss=0.07508, avg_loss=0.07497]\n",
      "Step 510151  [5.407 sec/step, loss=0.07438, avg_loss=0.07504]\n",
      "Step 510152  [5.409 sec/step, loss=0.07716, avg_loss=0.07504]\n",
      "Generated 32 batches of size 32 in 2.386 sec\n",
      "Step 510153  [5.426 sec/step, loss=0.07545, avg_loss=0.07505]\n",
      "Step 510154  [5.414 sec/step, loss=0.07775, avg_loss=0.07507]\n",
      "Step 510155  [5.388 sec/step, loss=0.07272, avg_loss=0.07503]\n",
      "Step 510156  [5.394 sec/step, loss=0.07641, avg_loss=0.07505]\n",
      "Step 510157  [5.397 sec/step, loss=0.07547, avg_loss=0.07504]\n",
      "Step 510158  [5.387 sec/step, loss=0.07516, avg_loss=0.07503]\n",
      "Step 510159  [5.382 sec/step, loss=0.07616, avg_loss=0.07503]\n",
      "Step 510160  [5.390 sec/step, loss=0.07640, avg_loss=0.07504]\n",
      "Step 510161  [5.353 sec/step, loss=0.07681, avg_loss=0.07515]\n",
      "Step 510162  [5.329 sec/step, loss=0.06861, avg_loss=0.07505]\n",
      "Step 510163  [5.346 sec/step, loss=0.07772, avg_loss=0.07508]\n",
      "Step 510164  [5.344 sec/step, loss=0.07488, avg_loss=0.07507]\n",
      "Step 510165  [5.343 sec/step, loss=0.07720, avg_loss=0.07507]\n",
      "Step 510166  [5.332 sec/step, loss=0.07551, avg_loss=0.07507]\n",
      "Step 510167  [5.386 sec/step, loss=0.06657, avg_loss=0.07499]\n",
      "Step 510168  [5.401 sec/step, loss=0.07411, avg_loss=0.07501]\n",
      "Step 510169  [5.400 sec/step, loss=0.07528, avg_loss=0.07502]\n",
      "Step 510170  [5.381 sec/step, loss=0.06945, avg_loss=0.07495]\n",
      "Step 510171  [5.363 sec/step, loss=0.07398, avg_loss=0.07495]\n",
      "Step 510172  [5.315 sec/step, loss=0.07674, avg_loss=0.07503]\n",
      "Step 510173  [5.311 sec/step, loss=0.07506, avg_loss=0.07501]\n",
      "Step 510174  [5.312 sec/step, loss=0.07337, avg_loss=0.07499]\n",
      "Step 510175  [5.312 sec/step, loss=0.07598, avg_loss=0.07498]\n",
      "Step 510176  [5.319 sec/step, loss=0.07665, avg_loss=0.07497]\n",
      "Step 510177  [5.305 sec/step, loss=0.07615, avg_loss=0.07496]\n",
      "Step 510178  [5.299 sec/step, loss=0.07643, avg_loss=0.07495]\n",
      "Step 510179  [5.307 sec/step, loss=0.07541, avg_loss=0.07494]\n",
      "Step 510180  [5.319 sec/step, loss=0.07602, avg_loss=0.07496]\n",
      "Step 510181  [5.313 sec/step, loss=0.07531, avg_loss=0.07495]\n",
      "Step 510182  [5.333 sec/step, loss=0.07522, avg_loss=0.07495]\n",
      "Step 510183  [5.333 sec/step, loss=0.07640, avg_loss=0.07495]\n",
      "Step 510184  [5.338 sec/step, loss=0.07811, avg_loss=0.07496]\n",
      "Generated 32 batches of size 32 in 2.323 sec\n",
      "Step 510185  [5.349 sec/step, loss=0.07626, avg_loss=0.07496]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 510186  [5.346 sec/step, loss=0.07194, avg_loss=0.07492]\n",
      "Step 510187  [5.380 sec/step, loss=0.07385, avg_loss=0.07493]\n",
      "Step 510188  [5.382 sec/step, loss=0.07734, avg_loss=0.07494]\n",
      "Step 510189  [5.387 sec/step, loss=0.07742, avg_loss=0.07495]\n",
      "Step 510190  [5.380 sec/step, loss=0.07340, avg_loss=0.07493]\n",
      "Step 510191  [5.394 sec/step, loss=0.07652, avg_loss=0.07491]\n",
      "Step 510192  [5.377 sec/step, loss=0.07431, avg_loss=0.07489]\n",
      "Step 510193  [5.366 sec/step, loss=0.07514, avg_loss=0.07487]\n",
      "Step 510194  [5.348 sec/step, loss=0.07703, avg_loss=0.07489]\n",
      "Step 510195  [5.343 sec/step, loss=0.07414, avg_loss=0.07490]\n",
      "Step 510196  [5.342 sec/step, loss=0.07490, avg_loss=0.07491]\n",
      "Step 510197  [5.313 sec/step, loss=0.06647, avg_loss=0.07483]\n",
      "Step 510198  [5.328 sec/step, loss=0.07518, avg_loss=0.07491]\n",
      "Step 510199  [5.350 sec/step, loss=0.07373, avg_loss=0.07488]\n",
      "Step 510200  [5.335 sec/step, loss=0.07219, avg_loss=0.07483]\n",
      "Writing summary at step: 510200\n",
      "Step 510201  [5.309 sec/step, loss=0.07608, avg_loss=0.07485]\n",
      "Step 510202  [5.329 sec/step, loss=0.07590, avg_loss=0.07494]\n",
      "Step 510203  [5.335 sec/step, loss=0.07737, avg_loss=0.07494]\n",
      "Step 510204  [5.353 sec/step, loss=0.07628, avg_loss=0.07498]\n",
      "Step 510205  [5.332 sec/step, loss=0.07138, avg_loss=0.07492]\n",
      "Step 510206  [5.346 sec/step, loss=0.07746, avg_loss=0.07493]\n",
      "Step 510207  [5.340 sec/step, loss=0.07319, avg_loss=0.07489]\n",
      "Step 510208  [5.348 sec/step, loss=0.07527, avg_loss=0.07489]\n",
      "Step 510209  [5.399 sec/step, loss=0.06819, avg_loss=0.07481]\n",
      "Step 510210  [5.388 sec/step, loss=0.07520, avg_loss=0.07481]\n",
      "Step 510211  [5.420 sec/step, loss=0.07469, avg_loss=0.07481]\n",
      "Step 510212  [5.409 sec/step, loss=0.07550, avg_loss=0.07480]\n",
      "Step 510213  [5.363 sec/step, loss=0.07467, avg_loss=0.07488]\n",
      "Step 510214  [5.358 sec/step, loss=0.07544, avg_loss=0.07488]\n",
      "Step 510215  [5.377 sec/step, loss=0.07745, avg_loss=0.07493]\n",
      "Generated 32 batches of size 32 in 2.536 sec\n",
      "Step 510216  [5.376 sec/step, loss=0.07473, avg_loss=0.07495]\n",
      "Step 510217  [5.366 sec/step, loss=0.07157, avg_loss=0.07492]\n",
      "Step 510218  [5.368 sec/step, loss=0.07661, avg_loss=0.07493]\n",
      "Step 510219  [5.386 sec/step, loss=0.07741, avg_loss=0.07495]\n",
      "Step 510220  [5.386 sec/step, loss=0.07641, avg_loss=0.07495]\n",
      "Step 510221  [5.380 sec/step, loss=0.07493, avg_loss=0.07493]\n",
      "Step 510222  [5.385 sec/step, loss=0.07617, avg_loss=0.07494]\n",
      "Step 510223  [5.389 sec/step, loss=0.07662, avg_loss=0.07493]\n",
      "Step 510224  [5.373 sec/step, loss=0.07665, avg_loss=0.07493]\n",
      "Step 510225  [5.390 sec/step, loss=0.07724, avg_loss=0.07495]\n",
      "Step 510226  [5.382 sec/step, loss=0.07645, avg_loss=0.07494]\n",
      "Step 510227  [5.415 sec/step, loss=0.07434, avg_loss=0.07495]\n",
      "Step 510228  [5.406 sec/step, loss=0.07486, avg_loss=0.07495]\n",
      "Step 510229  [5.397 sec/step, loss=0.07549, avg_loss=0.07494]\n",
      "Step 510230  [5.386 sec/step, loss=0.07501, avg_loss=0.07493]\n",
      "Step 510231  [5.370 sec/step, loss=0.07628, avg_loss=0.07493]\n",
      "Step 510232  [5.370 sec/step, loss=0.07725, avg_loss=0.07492]\n",
      "Step 510233  [5.320 sec/step, loss=0.07607, avg_loss=0.07501]\n",
      "Step 510234  [5.357 sec/step, loss=0.07401, avg_loss=0.07504]\n",
      "Step 510235  [5.401 sec/step, loss=0.06735, avg_loss=0.07493]\n",
      "Step 510236  [5.388 sec/step, loss=0.07584, avg_loss=0.07492]\n",
      "Step 510237  [5.370 sec/step, loss=0.07661, avg_loss=0.07496]\n",
      "Step 510238  [5.379 sec/step, loss=0.07410, avg_loss=0.07494]\n",
      "Step 510239  [5.372 sec/step, loss=0.07178, avg_loss=0.07490]\n",
      "Step 510240  [5.369 sec/step, loss=0.07358, avg_loss=0.07490]\n",
      "Step 510241  [5.376 sec/step, loss=0.07631, avg_loss=0.07493]\n",
      "Step 510242  [5.364 sec/step, loss=0.07631, avg_loss=0.07494]\n",
      "Step 510243  [5.376 sec/step, loss=0.07672, avg_loss=0.07494]\n",
      "Step 510244  [5.399 sec/step, loss=0.07646, avg_loss=0.07505]\n",
      "Step 510245  [5.407 sec/step, loss=0.07700, avg_loss=0.07505]\n",
      "Step 510246  [5.396 sec/step, loss=0.07473, avg_loss=0.07502]\n",
      "Step 510247  [5.383 sec/step, loss=0.07241, avg_loss=0.07500]\n",
      "Generated 32 batches of size 32 in 2.517 sec\n",
      "Step 510248  [5.381 sec/step, loss=0.07594, avg_loss=0.07503]\n",
      "Step 510249  [5.400 sec/step, loss=0.07714, avg_loss=0.07503]\n",
      "Step 510250  [5.403 sec/step, loss=0.07441, avg_loss=0.07503]\n",
      "Step 510251  [5.415 sec/step, loss=0.07648, avg_loss=0.07505]\n",
      "Step 510252  [5.417 sec/step, loss=0.07485, avg_loss=0.07503]\n",
      "Step 510253  [5.389 sec/step, loss=0.06559, avg_loss=0.07493]\n",
      "Step 510254  [5.367 sec/step, loss=0.07108, avg_loss=0.07486]\n",
      "Step 510255  [5.385 sec/step, loss=0.07566, avg_loss=0.07489]\n",
      "Step 510256  [5.391 sec/step, loss=0.07374, avg_loss=0.07486]\n",
      "Step 510257  [5.390 sec/step, loss=0.07608, avg_loss=0.07487]\n",
      "Step 510258  [5.387 sec/step, loss=0.07637, avg_loss=0.07488]\n",
      "Step 510259  [5.380 sec/step, loss=0.07481, avg_loss=0.07487]\n",
      "Step 510260  [5.366 sec/step, loss=0.07493, avg_loss=0.07485]\n",
      "Step 510261  [5.370 sec/step, loss=0.07670, avg_loss=0.07485]\n",
      "Step 510262  [5.390 sec/step, loss=0.07595, avg_loss=0.07493]\n",
      "Step 510263  [5.396 sec/step, loss=0.07622, avg_loss=0.07491]\n",
      "Step 510264  [5.400 sec/step, loss=0.07543, avg_loss=0.07492]\n",
      "Step 510265  [5.402 sec/step, loss=0.07664, avg_loss=0.07491]\n",
      "Step 510266  [5.405 sec/step, loss=0.07690, avg_loss=0.07492]\n",
      "Step 510267  [5.364 sec/step, loss=0.07576, avg_loss=0.07502]\n",
      "Step 510268  [5.358 sec/step, loss=0.07579, avg_loss=0.07503]\n",
      "Step 510269  [5.362 sec/step, loss=0.07624, avg_loss=0.07504]\n",
      "Step 510270  [5.379 sec/step, loss=0.07514, avg_loss=0.07510]\n",
      "Step 510271  [5.373 sec/step, loss=0.07365, avg_loss=0.07510]\n",
      "Step 510272  [5.361 sec/step, loss=0.07342, avg_loss=0.07506]\n",
      "Step 510273  [5.370 sec/step, loss=0.07647, avg_loss=0.07508]\n",
      "Step 510274  [5.382 sec/step, loss=0.07583, avg_loss=0.07510]\n",
      "Step 510275  [5.373 sec/step, loss=0.07457, avg_loss=0.07509]\n",
      "Step 510276  [5.370 sec/step, loss=0.07721, avg_loss=0.07509]\n",
      "Step 510277  [5.418 sec/step, loss=0.06646, avg_loss=0.07500]\n",
      "Step 510278  [5.429 sec/step, loss=0.07713, avg_loss=0.07500]\n",
      "Step 510279  [5.414 sec/step, loss=0.07628, avg_loss=0.07501]\n",
      "Generated 32 batches of size 32 in 2.515 sec\n",
      "Step 510280  [5.404 sec/step, loss=0.07529, avg_loss=0.07500]\n",
      "Step 510281  [5.415 sec/step, loss=0.07758, avg_loss=0.07503]\n",
      "Step 510282  [5.391 sec/step, loss=0.07318, avg_loss=0.07501]\n",
      "Step 510283  [5.395 sec/step, loss=0.07200, avg_loss=0.07496]\n",
      "Step 510284  [5.392 sec/step, loss=0.07434, avg_loss=0.07493]\n",
      "Step 510285  [5.380 sec/step, loss=0.07382, avg_loss=0.07490]\n",
      "Step 510286  [5.409 sec/step, loss=0.07541, avg_loss=0.07494]\n",
      "Step 510287  [5.372 sec/step, loss=0.06814, avg_loss=0.07488]\n",
      "Step 510288  [5.371 sec/step, loss=0.07716, avg_loss=0.07488]\n",
      "Step 510289  [5.368 sec/step, loss=0.07626, avg_loss=0.07486]\n",
      "Step 510290  [5.369 sec/step, loss=0.07653, avg_loss=0.07490]\n",
      "Step 510291  [5.341 sec/step, loss=0.07435, avg_loss=0.07487]\n",
      "Step 510292  [5.328 sec/step, loss=0.06631, avg_loss=0.07479]\n",
      "Step 510293  [5.350 sec/step, loss=0.07674, avg_loss=0.07481]\n",
      "Step 510294  [5.342 sec/step, loss=0.07262, avg_loss=0.07477]\n",
      "Step 510295  [5.376 sec/step, loss=0.07341, avg_loss=0.07476]\n",
      "Step 510296  [5.385 sec/step, loss=0.07664, avg_loss=0.07478]\n",
      "Step 510297  [5.394 sec/step, loss=0.07446, avg_loss=0.07486]\n",
      "Step 510298  [5.396 sec/step, loss=0.07597, avg_loss=0.07486]\n",
      "Step 510299  [5.369 sec/step, loss=0.07631, avg_loss=0.07489]\n",
      "Step 510300  [5.362 sec/step, loss=0.07533, avg_loss=0.07492]\n",
      "Writing summary at step: 510300\n",
      "Step 510301  [5.365 sec/step, loss=0.07600, avg_loss=0.07492]\n",
      "Step 510302  [5.372 sec/step, loss=0.07347, avg_loss=0.07490]\n",
      "Step 510303  [5.360 sec/step, loss=0.07250, avg_loss=0.07485]\n",
      "Step 510304  [5.352 sec/step, loss=0.07577, avg_loss=0.07484]\n",
      "Step 510305  [5.368 sec/step, loss=0.07650, avg_loss=0.07489]\n",
      "Step 510306  [5.357 sec/step, loss=0.07452, avg_loss=0.07486]\n",
      "Step 510307  [5.350 sec/step, loss=0.07349, avg_loss=0.07487]\n",
      "Step 510308  [5.342 sec/step, loss=0.07507, avg_loss=0.07487]\n",
      "Step 510309  [5.306 sec/step, loss=0.07620, avg_loss=0.07495]\n",
      "Step 510310  [5.315 sec/step, loss=0.07680, avg_loss=0.07496]\n",
      "Generated 32 batches of size 32 in 2.494 sec\n",
      "Step 510311  [5.285 sec/step, loss=0.07530, avg_loss=0.07497]\n",
      "Step 510312  [5.285 sec/step, loss=0.07317, avg_loss=0.07494]\n",
      "Step 510313  [5.282 sec/step, loss=0.07620, avg_loss=0.07496]\n",
      "Step 510314  [5.300 sec/step, loss=0.07591, avg_loss=0.07496]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 510315  [5.279 sec/step, loss=0.07429, avg_loss=0.07493]\n",
      "Step 510316  [5.291 sec/step, loss=0.07737, avg_loss=0.07496]\n",
      "Step 510317  [5.316 sec/step, loss=0.07758, avg_loss=0.07502]\n",
      "Step 510318  [5.365 sec/step, loss=0.06715, avg_loss=0.07492]\n",
      "Step 510319  [5.364 sec/step, loss=0.07594, avg_loss=0.07491]\n",
      "Step 510320  [5.366 sec/step, loss=0.07602, avg_loss=0.07491]\n",
      "Step 510321  [5.353 sec/step, loss=0.07641, avg_loss=0.07492]\n",
      "Step 510322  [5.351 sec/step, loss=0.07283, avg_loss=0.07489]\n",
      "Step 510323  [5.357 sec/step, loss=0.07732, avg_loss=0.07489]\n",
      "Step 510324  [5.367 sec/step, loss=0.07705, avg_loss=0.07490]\n",
      "Step 510325  [5.358 sec/step, loss=0.07660, avg_loss=0.07489]\n",
      "Step 510326  [5.361 sec/step, loss=0.07602, avg_loss=0.07489]\n",
      "Step 510327  [5.355 sec/step, loss=0.07783, avg_loss=0.07492]\n",
      "Step 510328  [5.331 sec/step, loss=0.06783, avg_loss=0.07485]\n",
      "Step 510329  [5.329 sec/step, loss=0.07230, avg_loss=0.07482]\n",
      "Step 510330  [5.353 sec/step, loss=0.07751, avg_loss=0.07484]\n",
      "Step 510331  [5.360 sec/step, loss=0.07558, avg_loss=0.07484]\n",
      "Step 510332  [5.356 sec/step, loss=0.07607, avg_loss=0.07483]\n",
      "Step 510333  [5.349 sec/step, loss=0.07521, avg_loss=0.07482]\n",
      "Step 510334  [5.324 sec/step, loss=0.07665, avg_loss=0.07484]\n",
      "Step 510335  [5.285 sec/step, loss=0.07518, avg_loss=0.07492]\n",
      "Step 510336  [5.330 sec/step, loss=0.06688, avg_loss=0.07483]\n",
      "Step 510337  [5.341 sec/step, loss=0.07446, avg_loss=0.07481]\n",
      "Step 510338  [5.337 sec/step, loss=0.07499, avg_loss=0.07482]\n",
      "Step 510339  [5.340 sec/step, loss=0.07674, avg_loss=0.07487]\n",
      "Step 510340  [5.338 sec/step, loss=0.07069, avg_loss=0.07484]\n",
      "Step 510341  [5.334 sec/step, loss=0.07606, avg_loss=0.07484]\n",
      "Step 510342  [5.337 sec/step, loss=0.07546, avg_loss=0.07483]\n",
      "Generated 32 batches of size 32 in 2.448 sec\n",
      "Step 510343  [5.327 sec/step, loss=0.07569, avg_loss=0.07482]\n",
      "Step 510344  [5.333 sec/step, loss=0.07619, avg_loss=0.07482]\n",
      "Step 510345  [5.350 sec/step, loss=0.07437, avg_loss=0.07479]\n",
      "Step 510346  [5.346 sec/step, loss=0.07417, avg_loss=0.07478]\n",
      "Step 510347  [5.359 sec/step, loss=0.07452, avg_loss=0.07481]\n",
      "Step 510348  [5.348 sec/step, loss=0.07208, avg_loss=0.07477]\n",
      "Step 510349  [5.347 sec/step, loss=0.07487, avg_loss=0.07474]\n",
      "Step 510350  [5.341 sec/step, loss=0.07533, avg_loss=0.07475]\n",
      "Step 510351  [5.343 sec/step, loss=0.07740, avg_loss=0.07476]\n",
      "Step 510352  [5.329 sec/step, loss=0.07566, avg_loss=0.07477]\n",
      "Step 510353  [5.354 sec/step, loss=0.07725, avg_loss=0.07489]\n",
      "Step 510354  [5.366 sec/step, loss=0.07641, avg_loss=0.07494]\n",
      "Step 510355  [5.372 sec/step, loss=0.07739, avg_loss=0.07496]\n",
      "Step 510356  [5.373 sec/step, loss=0.07582, avg_loss=0.07498]\n",
      "Step 510357  [5.367 sec/step, loss=0.07617, avg_loss=0.07498]\n",
      "Step 510358  [5.383 sec/step, loss=0.07727, avg_loss=0.07499]\n",
      "Step 510359  [5.398 sec/step, loss=0.07563, avg_loss=0.07500]\n",
      "Step 510360  [5.420 sec/step, loss=0.07707, avg_loss=0.07502]\n",
      "Step 510361  [5.396 sec/step, loss=0.07500, avg_loss=0.07500]\n",
      "Step 510362  [5.376 sec/step, loss=0.06811, avg_loss=0.07492]\n",
      "Step 510363  [5.364 sec/step, loss=0.07625, avg_loss=0.07492]\n",
      "Step 510364  [5.373 sec/step, loss=0.07705, avg_loss=0.07494]\n",
      "Step 510365  [5.364 sec/step, loss=0.07611, avg_loss=0.07493]\n",
      "Step 510366  [5.363 sec/step, loss=0.07666, avg_loss=0.07493]\n",
      "Step 510367  [5.351 sec/step, loss=0.07642, avg_loss=0.07494]\n",
      "Step 510368  [5.347 sec/step, loss=0.07474, avg_loss=0.07493]\n",
      "Step 510369  [5.343 sec/step, loss=0.07387, avg_loss=0.07490]\n",
      "Step 510370  [5.337 sec/step, loss=0.07279, avg_loss=0.07488]\n",
      "Step 510371  [5.392 sec/step, loss=0.06757, avg_loss=0.07482]\n",
      "Step 510372  [5.394 sec/step, loss=0.07182, avg_loss=0.07480]\n",
      "Step 510373  [5.407 sec/step, loss=0.07656, avg_loss=0.07481]\n",
      "Step 510374  [5.429 sec/step, loss=0.07401, avg_loss=0.07479]\n",
      "Generated 32 batches of size 32 in 2.671 sec\n",
      "Step 510375  [5.424 sec/step, loss=0.07332, avg_loss=0.07477]\n",
      "Step 510376  [5.425 sec/step, loss=0.07419, avg_loss=0.07474]\n",
      "Step 510377  [5.379 sec/step, loss=0.07568, avg_loss=0.07484]\n",
      "Step 510378  [5.371 sec/step, loss=0.07521, avg_loss=0.07482]\n",
      "Step 510379  [5.366 sec/step, loss=0.07513, avg_loss=0.07481]\n",
      "Step 510380  [5.375 sec/step, loss=0.07563, avg_loss=0.07481]\n",
      "Step 510381  [5.360 sec/step, loss=0.07441, avg_loss=0.07478]\n",
      "Step 510382  [5.363 sec/step, loss=0.07248, avg_loss=0.07477]\n",
      "Step 510383  [5.381 sec/step, loss=0.07376, avg_loss=0.07479]\n",
      "Step 510384  [5.368 sec/step, loss=0.07419, avg_loss=0.07479]\n",
      "Step 510385  [5.369 sec/step, loss=0.07183, avg_loss=0.07477]\n",
      "Step 510386  [5.394 sec/step, loss=0.06675, avg_loss=0.07468]\n",
      "Step 510387  [5.406 sec/step, loss=0.07604, avg_loss=0.07476]\n",
      "Step 510388  [5.410 sec/step, loss=0.07534, avg_loss=0.07474]\n",
      "Step 510389  [5.412 sec/step, loss=0.07733, avg_loss=0.07475]\n",
      "Step 510390  [5.403 sec/step, loss=0.07609, avg_loss=0.07475]\n",
      "Step 510391  [5.421 sec/step, loss=0.07765, avg_loss=0.07478]\n",
      "Step 510392  [5.446 sec/step, loss=0.07689, avg_loss=0.07489]\n",
      "Step 510393  [5.428 sec/step, loss=0.07379, avg_loss=0.07486]\n",
      "Step 510394  [5.451 sec/step, loss=0.07634, avg_loss=0.07489]\n",
      "Step 510395  [5.433 sec/step, loss=0.07716, avg_loss=0.07493]\n",
      "Step 510396  [5.449 sec/step, loss=0.07621, avg_loss=0.07493]\n",
      "Step 510397  [5.458 sec/step, loss=0.07299, avg_loss=0.07491]\n",
      "Step 510398  [5.451 sec/step, loss=0.07380, avg_loss=0.07489]\n",
      "Step 510399  [5.438 sec/step, loss=0.06564, avg_loss=0.07478]\n",
      "Step 510400  [5.435 sec/step, loss=0.07257, avg_loss=0.07476]\n",
      "Writing summary at step: 510400\n",
      "Step 510401  [5.435 sec/step, loss=0.07630, avg_loss=0.07476]\n",
      "Step 510402  [5.420 sec/step, loss=0.07525, avg_loss=0.07478]\n",
      "Step 510403  [5.436 sec/step, loss=0.07725, avg_loss=0.07482]\n",
      "Step 510404  [5.447 sec/step, loss=0.07737, avg_loss=0.07484]\n",
      "Step 510405  [5.445 sec/step, loss=0.07573, avg_loss=0.07483]\n",
      "Generated 32 batches of size 32 in 2.450 sec\n",
      "Step 510406  [5.452 sec/step, loss=0.07655, avg_loss=0.07485]\n",
      "Step 510407  [5.475 sec/step, loss=0.07550, avg_loss=0.07487]\n",
      "Step 510408  [5.467 sec/step, loss=0.07653, avg_loss=0.07489]\n",
      "Step 510409  [5.444 sec/step, loss=0.07202, avg_loss=0.07485]\n",
      "Step 510410  [5.429 sec/step, loss=0.07475, avg_loss=0.07483]\n",
      "Step 510411  [5.437 sec/step, loss=0.07285, avg_loss=0.07480]\n",
      "Step 510412  [5.448 sec/step, loss=0.07678, avg_loss=0.07484]\n",
      "Step 510413  [5.437 sec/step, loss=0.07566, avg_loss=0.07483]\n",
      "Step 510414  [5.418 sec/step, loss=0.07624, avg_loss=0.07484]\n",
      "Step 510415  [5.407 sec/step, loss=0.06814, avg_loss=0.07477]\n",
      "Step 510416  [5.407 sec/step, loss=0.07558, avg_loss=0.07476]\n",
      "Step 510417  [5.400 sec/step, loss=0.07657, avg_loss=0.07475]\n",
      "Step 510418  [5.362 sec/step, loss=0.07762, avg_loss=0.07485]\n",
      "Step 510419  [5.403 sec/step, loss=0.06850, avg_loss=0.07478]\n",
      "Step 510420  [5.394 sec/step, loss=0.07344, avg_loss=0.07475]\n",
      "Step 510421  [5.396 sec/step, loss=0.07650, avg_loss=0.07475]\n",
      "Step 510422  [5.392 sec/step, loss=0.07341, avg_loss=0.07476]\n",
      "Step 510423  [5.392 sec/step, loss=0.07795, avg_loss=0.07476]\n",
      "Step 510424  [5.380 sec/step, loss=0.07467, avg_loss=0.07474]\n",
      "Step 510425  [5.374 sec/step, loss=0.07625, avg_loss=0.07474]\n",
      "Step 510426  [5.384 sec/step, loss=0.07958, avg_loss=0.07477]\n",
      "Step 510427  [5.368 sec/step, loss=0.07420, avg_loss=0.07474]\n",
      "Step 510428  [5.382 sec/step, loss=0.07660, avg_loss=0.07482]\n",
      "Step 510429  [5.386 sec/step, loss=0.07803, avg_loss=0.07488]\n",
      "Step 510430  [5.370 sec/step, loss=0.07769, avg_loss=0.07488]\n",
      "Step 510431  [5.370 sec/step, loss=0.07708, avg_loss=0.07490]\n",
      "Step 510432  [5.389 sec/step, loss=0.07698, avg_loss=0.07491]\n",
      "Step 510433  [5.398 sec/step, loss=0.07733, avg_loss=0.07493]\n",
      "Step 510434  [5.400 sec/step, loss=0.07428, avg_loss=0.07490]\n",
      "Step 510435  [5.383 sec/step, loss=0.07573, avg_loss=0.07491]\n",
      "Step 510436  [5.326 sec/step, loss=0.07620, avg_loss=0.07500]\n",
      "Step 510437  [5.312 sec/step, loss=0.07416, avg_loss=0.07500]\n",
      "Generated 32 batches of size 32 in 2.351 sec\n",
      "Step 510438  [5.332 sec/step, loss=0.07612, avg_loss=0.07501]\n",
      "Step 510439  [5.340 sec/step, loss=0.07833, avg_loss=0.07503]\n",
      "Step 510440  [5.341 sec/step, loss=0.07657, avg_loss=0.07509]\n",
      "Step 510441  [5.341 sec/step, loss=0.07773, avg_loss=0.07510]\n",
      "Step 510442  [5.365 sec/step, loss=0.07645, avg_loss=0.07511]\n",
      "Step 510443  [5.347 sec/step, loss=0.07426, avg_loss=0.07510]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 510444  [5.350 sec/step, loss=0.07879, avg_loss=0.07512]\n",
      "Step 510445  [5.335 sec/step, loss=0.07751, avg_loss=0.07515]\n",
      "Step 510446  [5.350 sec/step, loss=0.07807, avg_loss=0.07519]\n",
      "Step 510447  [5.341 sec/step, loss=0.07304, avg_loss=0.07518]\n",
      "Step 510448  [5.365 sec/step, loss=0.07865, avg_loss=0.07524]\n",
      "Step 510449  [5.351 sec/step, loss=0.07665, avg_loss=0.07526]\n",
      "Step 510450  [5.343 sec/step, loss=0.06798, avg_loss=0.07519]\n",
      "Step 510451  [5.336 sec/step, loss=0.07741, avg_loss=0.07519]\n",
      "Step 510452  [5.346 sec/step, loss=0.07792, avg_loss=0.07521]\n",
      "Step 510453  [5.335 sec/step, loss=0.07187, avg_loss=0.07516]\n",
      "Step 510454  [5.326 sec/step, loss=0.07582, avg_loss=0.07515]\n",
      "Step 510455  [5.323 sec/step, loss=0.07674, avg_loss=0.07515]\n",
      "Step 510456  [5.342 sec/step, loss=0.07654, avg_loss=0.07515]\n",
      "Step 510457  [5.349 sec/step, loss=0.07718, avg_loss=0.07516]\n",
      "Step 510458  [5.331 sec/step, loss=0.07315, avg_loss=0.07512]\n",
      "Step 510459  [5.322 sec/step, loss=0.07434, avg_loss=0.07511]\n",
      "Step 510460  [5.311 sec/step, loss=0.07608, avg_loss=0.07510]\n",
      "Step 510461  [5.304 sec/step, loss=0.07327, avg_loss=0.07508]\n",
      "Step 510462  [5.325 sec/step, loss=0.07599, avg_loss=0.07516]\n",
      "Step 510463  [5.373 sec/step, loss=0.06661, avg_loss=0.07506]\n",
      "Step 510464  [5.367 sec/step, loss=0.07751, avg_loss=0.07507]\n",
      "Step 510465  [5.363 sec/step, loss=0.07667, avg_loss=0.07507]\n",
      "Step 510466  [5.359 sec/step, loss=0.07675, avg_loss=0.07507]\n",
      "Step 510467  [5.373 sec/step, loss=0.07799, avg_loss=0.07509]\n",
      "Step 510468  [5.368 sec/step, loss=0.07594, avg_loss=0.07510]\n",
      "Step 510469  [5.379 sec/step, loss=0.07806, avg_loss=0.07514]\n",
      "Generated 32 batches of size 32 in 2.385 sec\n",
      "Step 510470  [5.381 sec/step, loss=0.07630, avg_loss=0.07518]\n",
      "Step 510471  [5.344 sec/step, loss=0.07696, avg_loss=0.07527]\n",
      "Step 510472  [5.357 sec/step, loss=0.07515, avg_loss=0.07531]\n",
      "Step 510473  [5.340 sec/step, loss=0.07588, avg_loss=0.07530]\n",
      "Step 510474  [5.323 sec/step, loss=0.07726, avg_loss=0.07533]\n",
      "Step 510475  [5.347 sec/step, loss=0.07640, avg_loss=0.07536]\n",
      "Step 510476  [5.334 sec/step, loss=0.07672, avg_loss=0.07539]\n",
      "Step 510477  [5.348 sec/step, loss=0.07643, avg_loss=0.07540]\n",
      "Step 510478  [5.342 sec/step, loss=0.07557, avg_loss=0.07540]\n",
      "Step 510479  [5.350 sec/step, loss=0.07666, avg_loss=0.07541]\n",
      "Step 510480  [5.333 sec/step, loss=0.07228, avg_loss=0.07538]\n",
      "Step 510481  [5.346 sec/step, loss=0.07780, avg_loss=0.07542]\n",
      "Step 510482  [5.359 sec/step, loss=0.07626, avg_loss=0.07545]\n",
      "Step 510483  [5.349 sec/step, loss=0.07618, avg_loss=0.07548]\n",
      "Step 510484  [5.353 sec/step, loss=0.07658, avg_loss=0.07550]\n",
      "Step 510485  [5.357 sec/step, loss=0.07663, avg_loss=0.07555]\n",
      "Step 510486  [5.306 sec/step, loss=0.07675, avg_loss=0.07565]\n",
      "Step 510487  [5.312 sec/step, loss=0.07292, avg_loss=0.07562]\n",
      "Step 510488  [5.319 sec/step, loss=0.07422, avg_loss=0.07561]\n",
      "Step 510489  [5.311 sec/step, loss=0.07632, avg_loss=0.07560]\n",
      "Step 510490  [5.324 sec/step, loss=0.07752, avg_loss=0.07561]\n",
      "Step 510491  [5.302 sec/step, loss=0.07568, avg_loss=0.07559]\n",
      "Step 510492  [5.286 sec/step, loss=0.07431, avg_loss=0.07557]\n",
      "Step 510493  [5.278 sec/step, loss=0.07314, avg_loss=0.07556]\n",
      "Step 510494  [5.264 sec/step, loss=0.07520, avg_loss=0.07555]\n",
      "Step 510495  [5.238 sec/step, loss=0.06718, avg_loss=0.07545]\n",
      "Step 510496  [5.224 sec/step, loss=0.07580, avg_loss=0.07544]\n",
      "Step 510497  [5.217 sec/step, loss=0.07380, avg_loss=0.07545]\n",
      "Step 510498  [5.219 sec/step, loss=0.07580, avg_loss=0.07547]\n",
      "Step 510499  [5.232 sec/step, loss=0.07499, avg_loss=0.07556]\n",
      "Step 510500  [5.259 sec/step, loss=0.07618, avg_loss=0.07560]\n",
      "Writing summary at step: 510500\n",
      "Generated 32 batches of size 32 in 2.585 sec\n",
      "Step 510501  [5.310 sec/step, loss=0.06753, avg_loss=0.07551]\n",
      "Step 510502  [5.325 sec/step, loss=0.07773, avg_loss=0.07554]\n",
      "Step 510503  [5.301 sec/step, loss=0.07526, avg_loss=0.07552]\n",
      "Step 510504  [5.298 sec/step, loss=0.07698, avg_loss=0.07551]\n",
      "Step 510505  [5.319 sec/step, loss=0.07494, avg_loss=0.07551]\n",
      "Step 510506  [5.309 sec/step, loss=0.07664, avg_loss=0.07551]\n",
      "Step 510507  [5.310 sec/step, loss=0.07772, avg_loss=0.07553]\n",
      "Step 510508  [5.320 sec/step, loss=0.07766, avg_loss=0.07554]\n",
      "Step 510509  [5.325 sec/step, loss=0.07472, avg_loss=0.07557]\n",
      "Step 510510  [5.339 sec/step, loss=0.07774, avg_loss=0.07560]\n",
      "Step 510511  [5.344 sec/step, loss=0.07513, avg_loss=0.07562]\n",
      "Step 510512  [5.393 sec/step, loss=0.06723, avg_loss=0.07553]\n",
      "Step 510513  [5.413 sec/step, loss=0.07819, avg_loss=0.07555]\n",
      "Step 510514  [5.416 sec/step, loss=0.07675, avg_loss=0.07556]\n",
      "Step 510515  [5.432 sec/step, loss=0.07688, avg_loss=0.07564]\n",
      "Step 510516  [5.424 sec/step, loss=0.07581, avg_loss=0.07565]\n",
      "Step 510517  [5.412 sec/step, loss=0.07504, avg_loss=0.07563]\n",
      "Step 510518  [5.384 sec/step, loss=0.06806, avg_loss=0.07553]\n",
      "Step 510519  [5.341 sec/step, loss=0.07643, avg_loss=0.07561]\n",
      "Step 510520  [5.354 sec/step, loss=0.07685, avg_loss=0.07565]\n",
      "Step 510521  [5.351 sec/step, loss=0.07597, avg_loss=0.07564]\n",
      "Step 510522  [5.373 sec/step, loss=0.07784, avg_loss=0.07569]\n",
      "Step 510523  [5.355 sec/step, loss=0.07537, avg_loss=0.07566]\n",
      "Step 510524  [5.364 sec/step, loss=0.07620, avg_loss=0.07568]\n",
      "Step 510525  [5.373 sec/step, loss=0.07647, avg_loss=0.07568]\n",
      "Step 510526  [5.354 sec/step, loss=0.07490, avg_loss=0.07563]\n",
      "Step 510527  [5.374 sec/step, loss=0.07696, avg_loss=0.07566]\n",
      "Step 510528  [5.383 sec/step, loss=0.07761, avg_loss=0.07567]\n",
      "Step 510529  [5.374 sec/step, loss=0.07370, avg_loss=0.07563]\n",
      "Step 510530  [5.367 sec/step, loss=0.07213, avg_loss=0.07557]\n",
      "Step 510531  [5.358 sec/step, loss=0.07471, avg_loss=0.07555]\n",
      "Step 510532  [5.331 sec/step, loss=0.07646, avg_loss=0.07554]\n",
      "Generated 32 batches of size 32 in 2.390 sec\n",
      "Step 510533  [5.333 sec/step, loss=0.07573, avg_loss=0.07553]\n",
      "Step 510534  [5.329 sec/step, loss=0.07448, avg_loss=0.07553]\n",
      "Step 510535  [5.361 sec/step, loss=0.07660, avg_loss=0.07554]\n",
      "Step 510536  [5.379 sec/step, loss=0.07782, avg_loss=0.07555]\n",
      "Step 510537  [5.381 sec/step, loss=0.07507, avg_loss=0.07556]\n",
      "Step 510538  [5.360 sec/step, loss=0.07275, avg_loss=0.07553]\n",
      "Step 510539  [5.349 sec/step, loss=0.07591, avg_loss=0.07550]\n",
      "Step 510540  [5.361 sec/step, loss=0.07646, avg_loss=0.07550]\n",
      "Step 510541  [5.351 sec/step, loss=0.07309, avg_loss=0.07546]\n",
      "Step 510542  [5.332 sec/step, loss=0.07561, avg_loss=0.07545]\n",
      "Step 510543  [5.359 sec/step, loss=0.07714, avg_loss=0.07548]\n",
      "Step 510544  [5.338 sec/step, loss=0.07122, avg_loss=0.07540]\n",
      "Step 510545  [5.323 sec/step, loss=0.07646, avg_loss=0.07539]\n",
      "Step 510546  [5.318 sec/step, loss=0.07568, avg_loss=0.07537]\n",
      "Step 510547  [5.337 sec/step, loss=0.07481, avg_loss=0.07538]\n",
      "Step 510548  [5.327 sec/step, loss=0.07662, avg_loss=0.07536]\n",
      "Step 510549  [5.314 sec/step, loss=0.07214, avg_loss=0.07532]\n",
      "Step 510550  [5.340 sec/step, loss=0.07798, avg_loss=0.07542]\n",
      "Step 510551  [5.325 sec/step, loss=0.07228, avg_loss=0.07537]\n",
      "Step 510552  [5.310 sec/step, loss=0.07404, avg_loss=0.07533]\n",
      "Step 510553  [5.297 sec/step, loss=0.06581, avg_loss=0.07527]\n",
      "Step 510554  [5.314 sec/step, loss=0.07714, avg_loss=0.07528]\n",
      "Step 510555  [5.307 sec/step, loss=0.07352, avg_loss=0.07525]\n",
      "Step 510556  [5.299 sec/step, loss=0.07688, avg_loss=0.07525]\n",
      "Step 510557  [5.306 sec/step, loss=0.07684, avg_loss=0.07525]\n",
      "Step 510558  [5.307 sec/step, loss=0.07502, avg_loss=0.07527]\n",
      "Step 510559  [5.358 sec/step, loss=0.06783, avg_loss=0.07520]\n",
      "Step 510560  [5.361 sec/step, loss=0.07667, avg_loss=0.07521]\n",
      "Step 510561  [5.385 sec/step, loss=0.07460, avg_loss=0.07522]\n",
      "Step 510562  [5.377 sec/step, loss=0.07622, avg_loss=0.07522]\n",
      "Step 510563  [5.353 sec/step, loss=0.07451, avg_loss=0.07530]\n",
      "Step 510564  [5.353 sec/step, loss=0.07691, avg_loss=0.07530]\n",
      "Generated 32 batches of size 32 in 2.374 sec\n",
      "Step 510565  [5.358 sec/step, loss=0.07640, avg_loss=0.07529]\n",
      "Step 510566  [5.362 sec/step, loss=0.07602, avg_loss=0.07529]\n",
      "Step 510567  [5.345 sec/step, loss=0.07524, avg_loss=0.07526]\n",
      "Step 510568  [5.359 sec/step, loss=0.07614, avg_loss=0.07526]\n",
      "Step 510569  [5.351 sec/step, loss=0.07575, avg_loss=0.07524]\n",
      "Step 510570  [5.357 sec/step, loss=0.07762, avg_loss=0.07525]\n",
      "Step 510571  [5.336 sec/step, loss=0.07527, avg_loss=0.07524]\n",
      "Step 510572  [5.330 sec/step, loss=0.07298, avg_loss=0.07521]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 510573  [5.346 sec/step, loss=0.07555, avg_loss=0.07521]\n",
      "Step 510574  [5.332 sec/step, loss=0.07354, avg_loss=0.07517]\n",
      "Step 510575  [5.316 sec/step, loss=0.07490, avg_loss=0.07516]\n",
      "Step 510576  [5.302 sec/step, loss=0.06670, avg_loss=0.07506]\n",
      "Step 510577  [5.274 sec/step, loss=0.07115, avg_loss=0.07500]\n",
      "Step 510578  [5.286 sec/step, loss=0.07433, avg_loss=0.07499]\n",
      "Step 510579  [5.292 sec/step, loss=0.07749, avg_loss=0.07500]\n",
      "Step 510580  [5.296 sec/step, loss=0.07398, avg_loss=0.07502]\n",
      "Step 510581  [5.296 sec/step, loss=0.07754, avg_loss=0.07502]\n",
      "Step 510582  [5.303 sec/step, loss=0.07744, avg_loss=0.07503]\n",
      "Step 510583  [5.296 sec/step, loss=0.07309, avg_loss=0.07500]\n",
      "Step 510584  [5.344 sec/step, loss=0.06766, avg_loss=0.07491]\n",
      "Step 510585  [5.368 sec/step, loss=0.07443, avg_loss=0.07488]\n",
      "Step 510586  [5.387 sec/step, loss=0.07423, avg_loss=0.07486]\n",
      "Step 510587  [5.383 sec/step, loss=0.07622, avg_loss=0.07489]\n",
      "Step 510588  [5.374 sec/step, loss=0.07708, avg_loss=0.07492]\n",
      "Step 510589  [5.382 sec/step, loss=0.07729, avg_loss=0.07493]\n",
      "Step 510590  [5.368 sec/step, loss=0.07627, avg_loss=0.07492]\n",
      "Step 510591  [5.388 sec/step, loss=0.07707, avg_loss=0.07493]\n",
      "Step 510592  [5.397 sec/step, loss=0.07734, avg_loss=0.07496]\n",
      "Step 510593  [5.403 sec/step, loss=0.07503, avg_loss=0.07498]\n",
      "Step 510594  [5.403 sec/step, loss=0.07702, avg_loss=0.07500]\n",
      "Step 510595  [5.428 sec/step, loss=0.07616, avg_loss=0.07509]\n",
      "Step 510596  [5.414 sec/step, loss=0.07265, avg_loss=0.07506]\n",
      "Generated 32 batches of size 32 in 2.462 sec\n",
      "Step 510597  [5.438 sec/step, loss=0.07646, avg_loss=0.07508]\n",
      "Step 510598  [5.440 sec/step, loss=0.07528, avg_loss=0.07508]\n",
      "Step 510599  [5.444 sec/step, loss=0.07312, avg_loss=0.07506]\n",
      "Step 510600  [5.417 sec/step, loss=0.07246, avg_loss=0.07502]\n",
      "Writing summary at step: 510600\n",
      "Step 510601  [5.359 sec/step, loss=0.07640, avg_loss=0.07511]\n",
      "Step 510602  [5.349 sec/step, loss=0.07628, avg_loss=0.07510]\n",
      "Step 510603  [5.352 sec/step, loss=0.07563, avg_loss=0.07510]\n",
      "Step 510604  [5.353 sec/step, loss=0.07656, avg_loss=0.07510]\n",
      "Step 510605  [5.338 sec/step, loss=0.07504, avg_loss=0.07510]\n",
      "Step 510606  [5.340 sec/step, loss=0.07495, avg_loss=0.07508]\n",
      "Step 510607  [5.342 sec/step, loss=0.07774, avg_loss=0.07508]\n",
      "Step 510608  [5.320 sec/step, loss=0.06710, avg_loss=0.07498]\n",
      "Step 510609  [5.374 sec/step, loss=0.06670, avg_loss=0.07490]\n",
      "Step 510610  [5.364 sec/step, loss=0.07639, avg_loss=0.07488]\n",
      "Step 510611  [5.357 sec/step, loss=0.07633, avg_loss=0.07489]\n",
      "Step 510612  [5.292 sec/step, loss=0.07304, avg_loss=0.07495]\n",
      "Step 510613  [5.285 sec/step, loss=0.07645, avg_loss=0.07493]\n",
      "Step 510614  [5.302 sec/step, loss=0.07676, avg_loss=0.07493]\n",
      "Step 510615  [5.298 sec/step, loss=0.07439, avg_loss=0.07491]\n",
      "Step 510616  [5.298 sec/step, loss=0.07552, avg_loss=0.07491]\n",
      "Step 510617  [5.310 sec/step, loss=0.07623, avg_loss=0.07492]\n",
      "Step 510618  [5.326 sec/step, loss=0.07568, avg_loss=0.07500]\n",
      "Step 510619  [5.311 sec/step, loss=0.07424, avg_loss=0.07497]\n",
      "Step 510620  [5.287 sec/step, loss=0.07558, avg_loss=0.07496]\n",
      "Step 510621  [5.290 sec/step, loss=0.07589, avg_loss=0.07496]\n",
      "Step 510622  [5.278 sec/step, loss=0.07679, avg_loss=0.07495]\n",
      "Step 510623  [5.284 sec/step, loss=0.07486, avg_loss=0.07494]\n",
      "Step 510624  [5.273 sec/step, loss=0.07494, avg_loss=0.07493]\n",
      "Step 510625  [5.283 sec/step, loss=0.07657, avg_loss=0.07493]\n",
      "Step 510626  [5.297 sec/step, loss=0.07549, avg_loss=0.07494]\n",
      "Step 510627  [5.288 sec/step, loss=0.07769, avg_loss=0.07495]\n",
      "Generated 32 batches of size 32 in 2.424 sec\n",
      "Step 510628  [5.311 sec/step, loss=0.07405, avg_loss=0.07491]\n",
      "Step 510629  [5.326 sec/step, loss=0.07748, avg_loss=0.07495]\n",
      "Step 510630  [5.350 sec/step, loss=0.07502, avg_loss=0.07498]\n",
      "Step 510631  [5.344 sec/step, loss=0.07273, avg_loss=0.07496]\n",
      "Step 510632  [5.350 sec/step, loss=0.07644, avg_loss=0.07496]\n",
      "Step 510633  [5.344 sec/step, loss=0.07258, avg_loss=0.07493]\n",
      "Step 510634  [5.342 sec/step, loss=0.07668, avg_loss=0.07495]\n",
      "Step 510635  [5.327 sec/step, loss=0.07460, avg_loss=0.07493]\n",
      "Step 510636  [5.328 sec/step, loss=0.07731, avg_loss=0.07492]\n",
      "Step 510637  [5.320 sec/step, loss=0.07568, avg_loss=0.07493]\n",
      "Step 510638  [5.328 sec/step, loss=0.07518, avg_loss=0.07495]\n",
      "Step 510639  [5.326 sec/step, loss=0.07439, avg_loss=0.07494]\n",
      "Step 510640  [5.324 sec/step, loss=0.07611, avg_loss=0.07493]\n",
      "Step 510641  [5.347 sec/step, loss=0.07731, avg_loss=0.07498]\n",
      "Step 510642  [5.326 sec/step, loss=0.06619, avg_loss=0.07488]\n",
      "Step 510643  [5.329 sec/step, loss=0.07590, avg_loss=0.07487]\n",
      "Step 510644  [5.339 sec/step, loss=0.07665, avg_loss=0.07492]\n",
      "Step 510645  [5.332 sec/step, loss=0.07498, avg_loss=0.07491]\n",
      "Step 510646  [5.327 sec/step, loss=0.07447, avg_loss=0.07490]\n",
      "Step 510647  [5.327 sec/step, loss=0.07546, avg_loss=0.07490]\n",
      "Step 510648  [5.322 sec/step, loss=0.07397, avg_loss=0.07488]\n",
      "Step 510649  [5.321 sec/step, loss=0.07221, avg_loss=0.07488]\n",
      "Step 510650  [5.310 sec/step, loss=0.07235, avg_loss=0.07482]\n",
      "Step 510651  [5.321 sec/step, loss=0.07746, avg_loss=0.07487]\n",
      "Step 510652  [5.330 sec/step, loss=0.07737, avg_loss=0.07491]\n",
      "Step 510653  [5.347 sec/step, loss=0.07751, avg_loss=0.07502]\n",
      "Step 510654  [5.348 sec/step, loss=0.07889, avg_loss=0.07504]\n",
      "Step 510655  [5.352 sec/step, loss=0.07810, avg_loss=0.07509]\n",
      "Step 510656  [5.359 sec/step, loss=0.07731, avg_loss=0.07509]\n",
      "Step 510657  [5.355 sec/step, loss=0.07935, avg_loss=0.07512]\n",
      "Step 510658  [5.410 sec/step, loss=0.06924, avg_loss=0.07506]\n",
      "Step 510659  [5.372 sec/step, loss=0.07970, avg_loss=0.07518]\n",
      "Generated 32 batches of size 32 in 2.423 sec\n",
      "Step 510660  [5.378 sec/step, loss=0.07857, avg_loss=0.07520]\n",
      "Step 510661  [5.362 sec/step, loss=0.07649, avg_loss=0.07521]\n",
      "Step 510662  [5.373 sec/step, loss=0.07886, avg_loss=0.07524]\n",
      "Step 510663  [5.336 sec/step, loss=0.07343, avg_loss=0.07523]\n",
      "Step 510664  [5.331 sec/step, loss=0.07813, avg_loss=0.07524]\n",
      "Step 510665  [5.341 sec/step, loss=0.07730, avg_loss=0.07525]\n",
      "Step 510666  [5.340 sec/step, loss=0.07763, avg_loss=0.07527]\n",
      "Step 510667  [5.339 sec/step, loss=0.07635, avg_loss=0.07528]\n",
      "Step 510668  [5.346 sec/step, loss=0.07877, avg_loss=0.07531]\n",
      "Step 510669  [5.351 sec/step, loss=0.07709, avg_loss=0.07532]\n",
      "Step 510670  [5.343 sec/step, loss=0.07390, avg_loss=0.07528]\n",
      "Step 510671  [5.347 sec/step, loss=0.07623, avg_loss=0.07529]\n",
      "Step 510672  [5.353 sec/step, loss=0.07694, avg_loss=0.07533]\n",
      "Step 510673  [5.333 sec/step, loss=0.07599, avg_loss=0.07533]\n",
      "Step 510674  [5.331 sec/step, loss=0.07588, avg_loss=0.07536]\n",
      "Step 510675  [5.340 sec/step, loss=0.07852, avg_loss=0.07539]\n",
      "Step 510676  [5.366 sec/step, loss=0.07877, avg_loss=0.07552]\n",
      "Step 510677  [5.378 sec/step, loss=0.07847, avg_loss=0.07559]\n",
      "Step 510678  [5.370 sec/step, loss=0.07743, avg_loss=0.07562]\n",
      "Step 510679  [5.349 sec/step, loss=0.07432, avg_loss=0.07559]\n",
      "Step 510680  [5.368 sec/step, loss=0.07642, avg_loss=0.07561]\n",
      "Step 510681  [5.345 sec/step, loss=0.06893, avg_loss=0.07553]\n",
      "Step 510682  [5.342 sec/step, loss=0.07821, avg_loss=0.07553]\n",
      "Step 510683  [5.348 sec/step, loss=0.07700, avg_loss=0.07557]\n",
      "Step 510684  [5.323 sec/step, loss=0.07606, avg_loss=0.07566]\n",
      "Step 510685  [5.324 sec/step, loss=0.07589, avg_loss=0.07567]\n",
      "Step 510686  [5.322 sec/step, loss=0.07715, avg_loss=0.07570]\n",
      "Step 510687  [5.337 sec/step, loss=0.07502, avg_loss=0.07569]\n",
      "Step 510688  [5.328 sec/step, loss=0.07756, avg_loss=0.07569]\n",
      "Step 510689  [5.312 sec/step, loss=0.07628, avg_loss=0.07568]\n",
      "Step 510690  [5.314 sec/step, loss=0.07697, avg_loss=0.07569]\n",
      "Step 510691  [5.317 sec/step, loss=0.07562, avg_loss=0.07568]\n",
      "Generated 32 batches of size 32 in 2.519 sec\n",
      "Step 510692  [5.309 sec/step, loss=0.07419, avg_loss=0.07564]\n",
      "Step 510693  [5.313 sec/step, loss=0.07702, avg_loss=0.07566]\n",
      "Step 510694  [5.303 sec/step, loss=0.07641, avg_loss=0.07566]\n",
      "Step 510695  [5.294 sec/step, loss=0.07597, avg_loss=0.07566]\n",
      "Step 510696  [5.346 sec/step, loss=0.07133, avg_loss=0.07564]\n",
      "Step 510697  [5.337 sec/step, loss=0.07565, avg_loss=0.07563]\n",
      "Step 510698  [5.337 sec/step, loss=0.07316, avg_loss=0.07561]\n",
      "Step 510699  [5.330 sec/step, loss=0.07456, avg_loss=0.07563]\n",
      "Step 510700  [5.348 sec/step, loss=0.07690, avg_loss=0.07567]\n",
      "Writing summary at step: 510700\n",
      "Step 510701  [5.335 sec/step, loss=0.06733, avg_loss=0.07558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 510702  [5.331 sec/step, loss=0.07663, avg_loss=0.07559]\n",
      "Step 510703  [5.338 sec/step, loss=0.07573, avg_loss=0.07559]\n",
      "Step 510704  [5.355 sec/step, loss=0.07507, avg_loss=0.07557]\n",
      "Step 510705  [5.350 sec/step, loss=0.07647, avg_loss=0.07559]\n",
      "Step 510706  [5.347 sec/step, loss=0.07370, avg_loss=0.07557]\n",
      "Step 510707  [5.346 sec/step, loss=0.07788, avg_loss=0.07557]\n",
      "Step 510708  [5.358 sec/step, loss=0.07533, avg_loss=0.07566]\n",
      "Step 510709  [5.301 sec/step, loss=0.07420, avg_loss=0.07573]\n",
      "Step 510710  [5.321 sec/step, loss=0.07454, avg_loss=0.07571]\n",
      "Step 510711  [5.323 sec/step, loss=0.07623, avg_loss=0.07571]\n",
      "Step 510712  [5.338 sec/step, loss=0.07687, avg_loss=0.07575]\n",
      "Step 510713  [5.333 sec/step, loss=0.07535, avg_loss=0.07574]\n",
      "Step 510714  [5.306 sec/step, loss=0.07563, avg_loss=0.07573]\n",
      "Step 510715  [5.301 sec/step, loss=0.07539, avg_loss=0.07574]\n",
      "Step 510716  [5.306 sec/step, loss=0.07764, avg_loss=0.07576]\n",
      "Step 510717  [5.311 sec/step, loss=0.07535, avg_loss=0.07575]\n",
      "Step 510718  [5.362 sec/step, loss=0.06797, avg_loss=0.07567]\n",
      "Step 510719  [5.371 sec/step, loss=0.07713, avg_loss=0.07570]\n",
      "Step 510720  [5.380 sec/step, loss=0.07570, avg_loss=0.07570]\n",
      "Step 510721  [5.372 sec/step, loss=0.07100, avg_loss=0.07566]\n",
      "Step 510722  [5.367 sec/step, loss=0.07698, avg_loss=0.07566]\n",
      "Generated 32 batches of size 32 in 2.401 sec\n",
      "Step 510723  [5.387 sec/step, loss=0.07737, avg_loss=0.07568]\n",
      "Step 510724  [5.402 sec/step, loss=0.07707, avg_loss=0.07570]\n",
      "Step 510725  [5.402 sec/step, loss=0.07723, avg_loss=0.07571]\n",
      "Step 510726  [5.401 sec/step, loss=0.07777, avg_loss=0.07573]\n",
      "Step 510727  [5.379 sec/step, loss=0.07224, avg_loss=0.07568]\n",
      "Step 510728  [5.353 sec/step, loss=0.07588, avg_loss=0.07570]\n",
      "Step 510729  [5.350 sec/step, loss=0.07663, avg_loss=0.07569]\n",
      "Step 510730  [5.350 sec/step, loss=0.07736, avg_loss=0.07571]\n",
      "Step 510731  [5.357 sec/step, loss=0.07208, avg_loss=0.07571]\n",
      "Step 510732  [5.357 sec/step, loss=0.07714, avg_loss=0.07571]\n",
      "Step 510733  [5.359 sec/step, loss=0.07486, avg_loss=0.07573]\n",
      "Step 510734  [5.361 sec/step, loss=0.07542, avg_loss=0.07572]\n",
      "Step 510735  [5.347 sec/step, loss=0.07665, avg_loss=0.07574]\n",
      "Step 510736  [5.342 sec/step, loss=0.07733, avg_loss=0.07574]\n",
      "Step 510737  [5.392 sec/step, loss=0.06593, avg_loss=0.07565]\n",
      "Step 510738  [5.400 sec/step, loss=0.07792, avg_loss=0.07567]\n",
      "Step 510739  [5.391 sec/step, loss=0.07329, avg_loss=0.07566]\n",
      "Step 510740  [5.404 sec/step, loss=0.07739, avg_loss=0.07567]\n",
      "Step 510741  [5.393 sec/step, loss=0.07282, avg_loss=0.07563]\n",
      "Step 510742  [5.412 sec/step, loss=0.07638, avg_loss=0.07573]\n",
      "Step 510743  [5.404 sec/step, loss=0.07695, avg_loss=0.07574]\n",
      "Step 510744  [5.418 sec/step, loss=0.07622, avg_loss=0.07574]\n",
      "Step 510745  [5.432 sec/step, loss=0.07656, avg_loss=0.07575]\n",
      "Step 510746  [5.426 sec/step, loss=0.07520, avg_loss=0.07576]\n",
      "Step 510747  [5.408 sec/step, loss=0.07312, avg_loss=0.07574]\n",
      "Step 510748  [5.396 sec/step, loss=0.06693, avg_loss=0.07567]\n",
      "Step 510749  [5.407 sec/step, loss=0.07462, avg_loss=0.07569]\n",
      "Step 510750  [5.415 sec/step, loss=0.07609, avg_loss=0.07573]\n",
      "Step 510751  [5.416 sec/step, loss=0.07733, avg_loss=0.07573]\n",
      "Step 510752  [5.409 sec/step, loss=0.07622, avg_loss=0.07572]\n",
      "Step 510753  [5.433 sec/step, loss=0.07511, avg_loss=0.07569]\n",
      "Step 510754  [5.417 sec/step, loss=0.07471, avg_loss=0.07565]\n",
      "Generated 32 batches of size 32 in 2.300 sec\n",
      "Step 510755  [5.420 sec/step, loss=0.07347, avg_loss=0.07560]\n",
      "Step 510756  [5.396 sec/step, loss=0.07490, avg_loss=0.07558]\n",
      "Step 510757  [5.380 sec/step, loss=0.07186, avg_loss=0.07551]\n",
      "Step 510758  [5.341 sec/step, loss=0.07761, avg_loss=0.07559]\n",
      "Step 510759  [5.339 sec/step, loss=0.07738, avg_loss=0.07557]\n",
      "Step 510760  [5.346 sec/step, loss=0.07669, avg_loss=0.07555]\n",
      "Step 510761  [5.346 sec/step, loss=0.07552, avg_loss=0.07554]\n",
      "Step 510762  [5.345 sec/step, loss=0.07712, avg_loss=0.07552]\n",
      "Step 510763  [5.367 sec/step, loss=0.07479, avg_loss=0.07553]\n",
      "Step 510764  [5.355 sec/step, loss=0.06764, avg_loss=0.07543]\n",
      "Step 510765  [5.336 sec/step, loss=0.07545, avg_loss=0.07541]\n",
      "Step 510766  [5.343 sec/step, loss=0.07652, avg_loss=0.07540]\n",
      "Step 510767  [5.362 sec/step, loss=0.07663, avg_loss=0.07540]\n",
      "Step 510768  [5.348 sec/step, loss=0.07647, avg_loss=0.07538]\n",
      "Step 510769  [5.355 sec/step, loss=0.07697, avg_loss=0.07538]\n",
      "Step 510770  [5.352 sec/step, loss=0.07589, avg_loss=0.07540]\n",
      "Step 510771  [5.406 sec/step, loss=0.06801, avg_loss=0.07532]\n",
      "Step 510772  [5.407 sec/step, loss=0.07617, avg_loss=0.07531]\n",
      "Step 510773  [5.434 sec/step, loss=0.07394, avg_loss=0.07529]\n",
      "Step 510774  [5.436 sec/step, loss=0.07504, avg_loss=0.07528]\n",
      "Step 510775  [5.439 sec/step, loss=0.07571, avg_loss=0.07525]\n",
      "Step 510776  [5.421 sec/step, loss=0.07494, avg_loss=0.07521]\n",
      "Step 510777  [5.410 sec/step, loss=0.07536, avg_loss=0.07518]\n",
      "Step 510778  [5.409 sec/step, loss=0.07399, avg_loss=0.07515]\n",
      "Step 510779  [5.429 sec/step, loss=0.07746, avg_loss=0.07518]\n",
      "Step 510780  [5.427 sec/step, loss=0.07790, avg_loss=0.07519]\n",
      "Step 510781  [5.471 sec/step, loss=0.07434, avg_loss=0.07525]\n",
      "Step 510782  [5.451 sec/step, loss=0.07251, avg_loss=0.07519]\n",
      "Step 510783  [5.451 sec/step, loss=0.07739, avg_loss=0.07519]\n",
      "Step 510784  [5.428 sec/step, loss=0.07272, avg_loss=0.07516]\n",
      "Step 510785  [5.404 sec/step, loss=0.07737, avg_loss=0.07518]\n",
      "Step 510786  [5.383 sec/step, loss=0.07444, avg_loss=0.07515]\n",
      "Generated 32 batches of size 32 in 2.549 sec\n",
      "Step 510787  [5.369 sec/step, loss=0.07556, avg_loss=0.07515]\n",
      "Step 510788  [5.375 sec/step, loss=0.07766, avg_loss=0.07515]\n",
      "Step 510789  [5.375 sec/step, loss=0.07274, avg_loss=0.07512]\n",
      "Step 510790  [5.373 sec/step, loss=0.07393, avg_loss=0.07509]\n",
      "Step 510791  [5.361 sec/step, loss=0.07360, avg_loss=0.07507]\n",
      "Step 510792  [5.372 sec/step, loss=0.07602, avg_loss=0.07509]\n",
      "Step 510793  [5.379 sec/step, loss=0.07601, avg_loss=0.07508]\n",
      "Step 510794  [5.380 sec/step, loss=0.07665, avg_loss=0.07508]\n",
      "Step 510795  [5.396 sec/step, loss=0.07723, avg_loss=0.07509]\n",
      "Step 510796  [5.381 sec/step, loss=0.07400, avg_loss=0.07512]\n",
      "Step 510797  [5.375 sec/step, loss=0.07672, avg_loss=0.07513]\n",
      "Step 510798  [5.392 sec/step, loss=0.07743, avg_loss=0.07517]\n",
      "Step 510799  [5.390 sec/step, loss=0.07425, avg_loss=0.07517]\n",
      "Step 510800  [5.372 sec/step, loss=0.07260, avg_loss=0.07513]\n",
      "Writing summary at step: 510800\n",
      "Step 510801  [5.398 sec/step, loss=0.07489, avg_loss=0.07520]\n",
      "Step 510802  [5.402 sec/step, loss=0.07596, avg_loss=0.07519]\n",
      "Step 510803  [5.399 sec/step, loss=0.07269, avg_loss=0.07516]\n",
      "Step 510804  [5.358 sec/step, loss=0.06681, avg_loss=0.07508]\n",
      "Step 510805  [5.363 sec/step, loss=0.07531, avg_loss=0.07507]\n",
      "Step 510806  [5.363 sec/step, loss=0.07478, avg_loss=0.07508]\n",
      "Step 510807  [5.351 sec/step, loss=0.07521, avg_loss=0.07505]\n",
      "Step 510808  [5.355 sec/step, loss=0.07693, avg_loss=0.07507]\n",
      "Step 510809  [5.362 sec/step, loss=0.07436, avg_loss=0.07507]\n",
      "Step 510810  [5.394 sec/step, loss=0.06680, avg_loss=0.07499]\n",
      "Step 510811  [5.391 sec/step, loss=0.07674, avg_loss=0.07500]\n",
      "Step 510812  [5.398 sec/step, loss=0.07728, avg_loss=0.07500]\n",
      "Step 510813  [5.408 sec/step, loss=0.07640, avg_loss=0.07501]\n",
      "Step 510814  [5.430 sec/step, loss=0.07708, avg_loss=0.07503]\n",
      "Step 510815  [5.443 sec/step, loss=0.07409, avg_loss=0.07502]\n",
      "Step 510816  [5.424 sec/step, loss=0.07152, avg_loss=0.07495]\n",
      "Step 510817  [5.424 sec/step, loss=0.07742, avg_loss=0.07498]\n",
      "Generated 32 batches of size 32 in 2.537 sec\n",
      "Step 510818  [5.369 sec/step, loss=0.07514, avg_loss=0.07505]\n",
      "Step 510819  [5.365 sec/step, loss=0.07640, avg_loss=0.07504]\n",
      "Step 510820  [5.369 sec/step, loss=0.07566, avg_loss=0.07504]\n",
      "Step 510821  [5.380 sec/step, loss=0.07334, avg_loss=0.07506]\n",
      "Step 510822  [5.376 sec/step, loss=0.07514, avg_loss=0.07504]\n",
      "Step 510823  [5.359 sec/step, loss=0.07624, avg_loss=0.07503]\n",
      "Step 510824  [5.344 sec/step, loss=0.07336, avg_loss=0.07500]\n",
      "Step 510825  [5.342 sec/step, loss=0.07692, avg_loss=0.07499]\n",
      "Step 510826  [5.352 sec/step, loss=0.07427, avg_loss=0.07496]\n",
      "Step 510827  [5.374 sec/step, loss=0.07669, avg_loss=0.07500]\n",
      "Step 510828  [5.370 sec/step, loss=0.07635, avg_loss=0.07501]\n",
      "Step 510829  [5.361 sec/step, loss=0.07482, avg_loss=0.07499]\n",
      "Step 510830  [5.352 sec/step, loss=0.07606, avg_loss=0.07498]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 510831  [5.366 sec/step, loss=0.07496, avg_loss=0.07500]\n",
      "Step 510832  [5.371 sec/step, loss=0.07502, avg_loss=0.07498]\n",
      "Step 510833  [5.376 sec/step, loss=0.07529, avg_loss=0.07499]\n",
      "Step 510834  [5.374 sec/step, loss=0.07641, avg_loss=0.07500]\n",
      "Step 510835  [5.381 sec/step, loss=0.07679, avg_loss=0.07500]\n",
      "Step 510836  [5.370 sec/step, loss=0.07500, avg_loss=0.07498]\n",
      "Step 510837  [5.319 sec/step, loss=0.07662, avg_loss=0.07508]\n",
      "Step 510838  [5.325 sec/step, loss=0.07664, avg_loss=0.07507]\n",
      "Step 510839  [5.325 sec/step, loss=0.07297, avg_loss=0.07507]\n",
      "Step 510840  [5.316 sec/step, loss=0.07628, avg_loss=0.07506]\n",
      "Step 510841  [5.317 sec/step, loss=0.07551, avg_loss=0.07508]\n",
      "Step 510842  [5.321 sec/step, loss=0.07536, avg_loss=0.07507]\n",
      "Step 510843  [5.309 sec/step, loss=0.07331, avg_loss=0.07504]\n",
      "Step 510844  [5.283 sec/step, loss=0.07541, avg_loss=0.07503]\n",
      "Step 510845  [5.293 sec/step, loss=0.07767, avg_loss=0.07504]\n",
      "Step 510846  [5.299 sec/step, loss=0.07529, avg_loss=0.07504]\n",
      "Step 510847  [5.321 sec/step, loss=0.07757, avg_loss=0.07508]\n",
      "Step 510848  [5.338 sec/step, loss=0.07704, avg_loss=0.07518]\n",
      "Step 510849  [5.332 sec/step, loss=0.07284, avg_loss=0.07517]\n",
      "Generated 32 batches of size 32 in 2.338 sec\n",
      "Step 510850  [5.340 sec/step, loss=0.07763, avg_loss=0.07518]\n",
      "Step 510851  [5.331 sec/step, loss=0.07556, avg_loss=0.07516]\n",
      "Step 510852  [5.347 sec/step, loss=0.07678, avg_loss=0.07517]\n",
      "Step 510853  [5.308 sec/step, loss=0.06752, avg_loss=0.07509]\n",
      "Step 510854  [5.312 sec/step, loss=0.07603, avg_loss=0.07511]\n",
      "Step 510855  [5.297 sec/step, loss=0.07516, avg_loss=0.07512]\n",
      "Step 510856  [5.347 sec/step, loss=0.06776, avg_loss=0.07505]\n",
      "Step 510857  [5.382 sec/step, loss=0.07491, avg_loss=0.07508]\n",
      "Step 510858  [5.383 sec/step, loss=0.07752, avg_loss=0.07508]\n",
      "Step 510859  [5.362 sec/step, loss=0.07498, avg_loss=0.07506]\n",
      "Step 510860  [5.395 sec/step, loss=0.06613, avg_loss=0.07495]\n",
      "Step 510861  [5.407 sec/step, loss=0.07483, avg_loss=0.07495]\n",
      "Step 510862  [5.401 sec/step, loss=0.07701, avg_loss=0.07494]\n",
      "Step 510863  [5.395 sec/step, loss=0.07538, avg_loss=0.07495]\n",
      "Step 510864  [5.407 sec/step, loss=0.07616, avg_loss=0.07504]\n",
      "Step 510865  [5.393 sec/step, loss=0.06622, avg_loss=0.07494]\n",
      "Step 510866  [5.387 sec/step, loss=0.07232, avg_loss=0.07490]\n",
      "Step 510867  [5.372 sec/step, loss=0.07530, avg_loss=0.07489]\n",
      "Step 510868  [5.379 sec/step, loss=0.07649, avg_loss=0.07489]\n",
      "Step 510869  [5.368 sec/step, loss=0.07531, avg_loss=0.07487]\n",
      "Step 510870  [5.359 sec/step, loss=0.07178, avg_loss=0.07483]\n",
      "Step 510871  [5.309 sec/step, loss=0.07518, avg_loss=0.07490]\n",
      "Step 510872  [5.319 sec/step, loss=0.07714, avg_loss=0.07491]\n",
      "Step 510873  [5.295 sec/step, loss=0.07463, avg_loss=0.07492]\n",
      "Step 510874  [5.286 sec/step, loss=0.07304, avg_loss=0.07490]\n",
      "Step 510875  [5.289 sec/step, loss=0.07685, avg_loss=0.07491]\n",
      "Step 510876  [5.306 sec/step, loss=0.07621, avg_loss=0.07492]\n",
      "Step 510877  [5.340 sec/step, loss=0.07665, avg_loss=0.07494]\n",
      "Step 510878  [5.345 sec/step, loss=0.07691, avg_loss=0.07497]\n",
      "Step 510879  [5.345 sec/step, loss=0.07622, avg_loss=0.07495]\n",
      "Step 510880  [5.348 sec/step, loss=0.07698, avg_loss=0.07494]\n",
      "Step 510881  [5.329 sec/step, loss=0.07711, avg_loss=0.07497]\n",
      "Generated 32 batches of size 32 in 2.380 sec\n",
      "Step 510882  [5.355 sec/step, loss=0.07751, avg_loss=0.07502]\n",
      "Step 510883  [5.359 sec/step, loss=0.07507, avg_loss=0.07500]\n",
      "Step 510884  [5.352 sec/step, loss=0.07517, avg_loss=0.07502]\n",
      "Step 510885  [5.346 sec/step, loss=0.07254, avg_loss=0.07497]\n",
      "Step 510886  [5.350 sec/step, loss=0.07661, avg_loss=0.07500]\n",
      "Step 510887  [5.363 sec/step, loss=0.07537, avg_loss=0.07499]\n",
      "Step 510888  [5.367 sec/step, loss=0.07733, avg_loss=0.07499]\n",
      "Step 510889  [5.369 sec/step, loss=0.07325, avg_loss=0.07500]\n",
      "Step 510890  [5.366 sec/step, loss=0.07602, avg_loss=0.07502]\n",
      "Step 510891  [5.373 sec/step, loss=0.07413, avg_loss=0.07502]\n",
      "Step 510892  [5.396 sec/step, loss=0.07423, avg_loss=0.07500]\n",
      "Step 510893  [5.388 sec/step, loss=0.07464, avg_loss=0.07499]\n",
      "Step 510894  [5.394 sec/step, loss=0.07552, avg_loss=0.07498]\n",
      "Step 510895  [5.373 sec/step, loss=0.07445, avg_loss=0.07495]\n",
      "Step 510896  [5.336 sec/step, loss=0.07274, avg_loss=0.07494]\n",
      "Step 510897  [5.324 sec/step, loss=0.07124, avg_loss=0.07488]\n",
      "Step 510898  [5.304 sec/step, loss=0.07437, avg_loss=0.07485]\n",
      "Step 510899  [5.316 sec/step, loss=0.07613, avg_loss=0.07487]\n",
      "Step 510900  [5.325 sec/step, loss=0.07595, avg_loss=0.07491]\n",
      "Writing summary at step: 510900\n",
      "Step 510901  [5.322 sec/step, loss=0.07722, avg_loss=0.07493]\n",
      "Step 510902  [5.371 sec/step, loss=0.06884, avg_loss=0.07486]\n",
      "Step 510903  [5.390 sec/step, loss=0.07648, avg_loss=0.07490]\n",
      "Step 510904  [5.417 sec/step, loss=0.07754, avg_loss=0.07500]\n",
      "Step 510905  [5.409 sec/step, loss=0.07661, avg_loss=0.07502]\n",
      "Step 510906  [5.422 sec/step, loss=0.07697, avg_loss=0.07504]\n",
      "Step 510907  [5.427 sec/step, loss=0.07563, avg_loss=0.07504]\n",
      "Step 510908  [5.427 sec/step, loss=0.07253, avg_loss=0.07500]\n",
      "Step 510909  [5.429 sec/step, loss=0.07654, avg_loss=0.07502]\n",
      "Step 510910  [5.386 sec/step, loss=0.07634, avg_loss=0.07512]\n",
      "Step 510911  [5.369 sec/step, loss=0.06715, avg_loss=0.07502]\n",
      "Step 510912  [5.347 sec/step, loss=0.07220, avg_loss=0.07497]\n",
      "Generated 32 batches of size 32 in 2.421 sec\n",
      "Step 510913  [5.341 sec/step, loss=0.07515, avg_loss=0.07496]\n",
      "Step 510914  [5.341 sec/step, loss=0.07728, avg_loss=0.07496]\n",
      "Step 510915  [5.335 sec/step, loss=0.07584, avg_loss=0.07498]\n",
      "Step 510916  [5.362 sec/step, loss=0.07598, avg_loss=0.07502]\n",
      "Step 510917  [5.355 sec/step, loss=0.07645, avg_loss=0.07501]\n",
      "Step 510918  [5.358 sec/step, loss=0.07660, avg_loss=0.07502]\n",
      "Step 510919  [5.359 sec/step, loss=0.07296, avg_loss=0.07499]\n",
      "Step 510920  [5.369 sec/step, loss=0.07752, avg_loss=0.07501]\n",
      "Step 510921  [5.373 sec/step, loss=0.07602, avg_loss=0.07504]\n",
      "Step 510922  [5.390 sec/step, loss=0.07504, avg_loss=0.07503]\n",
      "Step 510923  [5.379 sec/step, loss=0.07298, avg_loss=0.07500]\n",
      "Step 510924  [5.399 sec/step, loss=0.07693, avg_loss=0.07504]\n",
      "Step 510925  [5.392 sec/step, loss=0.07673, avg_loss=0.07504]\n",
      "Step 510926  [5.378 sec/step, loss=0.07613, avg_loss=0.07505]\n",
      "Step 510927  [5.372 sec/step, loss=0.07619, avg_loss=0.07505]\n",
      "Step 510928  [5.390 sec/step, loss=0.07423, avg_loss=0.07503]\n",
      "Step 510929  [5.400 sec/step, loss=0.07372, avg_loss=0.07502]\n",
      "Step 510930  [5.392 sec/step, loss=0.07391, avg_loss=0.07500]\n",
      "Step 510931  [5.389 sec/step, loss=0.07678, avg_loss=0.07501]\n",
      "Step 510932  [5.381 sec/step, loss=0.07622, avg_loss=0.07503]\n",
      "Step 510933  [5.364 sec/step, loss=0.07217, avg_loss=0.07500]\n",
      "Step 510934  [5.371 sec/step, loss=0.07592, avg_loss=0.07499]\n",
      "Step 510935  [5.366 sec/step, loss=0.07418, avg_loss=0.07496]\n",
      "Step 510936  [5.368 sec/step, loss=0.07599, avg_loss=0.07497]\n",
      "Step 510937  [5.382 sec/step, loss=0.07465, avg_loss=0.07495]\n",
      "Step 510938  [5.368 sec/step, loss=0.07581, avg_loss=0.07495]\n",
      "Step 510939  [5.409 sec/step, loss=0.07423, avg_loss=0.07496]\n",
      "Step 510940  [5.396 sec/step, loss=0.07527, avg_loss=0.07495]\n",
      "Step 510941  [5.405 sec/step, loss=0.07699, avg_loss=0.07496]\n",
      "Step 510942  [5.405 sec/step, loss=0.07679, avg_loss=0.07498]\n",
      "Step 510943  [5.405 sec/step, loss=0.07544, avg_loss=0.07500]\n",
      "Step 510944  [5.416 sec/step, loss=0.07721, avg_loss=0.07502]\n",
      "Generated 32 batches of size 32 in 2.411 sec\n",
      "Step 510945  [5.408 sec/step, loss=0.07466, avg_loss=0.07499]\n",
      "Step 510946  [5.401 sec/step, loss=0.07482, avg_loss=0.07498]\n",
      "Step 510947  [5.374 sec/step, loss=0.06759, avg_loss=0.07488]\n",
      "Step 510948  [5.423 sec/step, loss=0.06710, avg_loss=0.07478]\n",
      "Step 510949  [5.434 sec/step, loss=0.07309, avg_loss=0.07479]\n",
      "Step 510950  [5.420 sec/step, loss=0.07652, avg_loss=0.07477]\n",
      "Step 510951  [5.439 sec/step, loss=0.07717, avg_loss=0.07479]\n",
      "Step 510952  [5.415 sec/step, loss=0.07544, avg_loss=0.07478]\n",
      "Step 510953  [5.445 sec/step, loss=0.07707, avg_loss=0.07487]\n",
      "Step 510954  [5.439 sec/step, loss=0.07423, avg_loss=0.07485]\n",
      "Step 510955  [5.454 sec/step, loss=0.07655, avg_loss=0.07487]\n",
      "Step 510956  [5.400 sec/step, loss=0.07449, avg_loss=0.07494]\n",
      "Step 510957  [5.379 sec/step, loss=0.07463, avg_loss=0.07493]\n",
      "Step 510958  [5.366 sec/step, loss=0.07370, avg_loss=0.07489]\n",
      "Step 510959  [5.378 sec/step, loss=0.07484, avg_loss=0.07489]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 510960  [5.338 sec/step, loss=0.07455, avg_loss=0.07498]\n",
      "Step 510961  [5.334 sec/step, loss=0.07554, avg_loss=0.07498]\n",
      "Step 510962  [5.322 sec/step, loss=0.07508, avg_loss=0.07497]\n",
      "Step 510963  [5.343 sec/step, loss=0.07309, avg_loss=0.07494]\n",
      "Step 510964  [5.357 sec/step, loss=0.07698, avg_loss=0.07495]\n",
      "Step 510965  [5.364 sec/step, loss=0.07225, avg_loss=0.07501]\n",
      "Step 510966  [5.350 sec/step, loss=0.06614, avg_loss=0.07495]\n",
      "Step 510967  [5.355 sec/step, loss=0.07549, avg_loss=0.07495]\n",
      "Step 510968  [5.360 sec/step, loss=0.07747, avg_loss=0.07496]\n",
      "Step 510969  [5.358 sec/step, loss=0.07597, avg_loss=0.07497]\n",
      "Step 510970  [5.419 sec/step, loss=0.06682, avg_loss=0.07492]\n",
      "Step 510971  [5.432 sec/step, loss=0.07618, avg_loss=0.07493]\n",
      "Step 510972  [5.418 sec/step, loss=0.07654, avg_loss=0.07492]\n",
      "Step 510973  [5.419 sec/step, loss=0.07297, avg_loss=0.07491]\n",
      "Step 510974  [5.430 sec/step, loss=0.07579, avg_loss=0.07493]\n",
      "Step 510975  [5.415 sec/step, loss=0.07596, avg_loss=0.07492]\n",
      "Step 510976  [5.398 sec/step, loss=0.07473, avg_loss=0.07491]\n",
      "Generated 32 batches of size 32 in 2.460 sec\n",
      "Step 510977  [5.395 sec/step, loss=0.07661, avg_loss=0.07491]\n",
      "Step 510978  [5.380 sec/step, loss=0.07208, avg_loss=0.07486]\n",
      "Step 510979  [5.383 sec/step, loss=0.07717, avg_loss=0.07487]\n",
      "Step 510980  [5.366 sec/step, loss=0.07451, avg_loss=0.07485]\n",
      "Step 510981  [5.357 sec/step, loss=0.07558, avg_loss=0.07483]\n",
      "Step 510982  [5.358 sec/step, loss=0.07562, avg_loss=0.07481]\n",
      "Step 510983  [5.351 sec/step, loss=0.07487, avg_loss=0.07481]\n",
      "Step 510984  [5.363 sec/step, loss=0.07663, avg_loss=0.07482]\n",
      "Step 510985  [5.376 sec/step, loss=0.07807, avg_loss=0.07488]\n",
      "Step 510986  [5.381 sec/step, loss=0.07671, avg_loss=0.07488]\n",
      "Step 510987  [5.373 sec/step, loss=0.07380, avg_loss=0.07486]\n",
      "Step 510988  [5.374 sec/step, loss=0.07789, avg_loss=0.07487]\n",
      "Step 510989  [5.378 sec/step, loss=0.07466, avg_loss=0.07488]\n",
      "Step 510990  [5.389 sec/step, loss=0.07734, avg_loss=0.07490]\n",
      "Step 510991  [5.391 sec/step, loss=0.07736, avg_loss=0.07493]\n",
      "Step 510992  [5.359 sec/step, loss=0.07171, avg_loss=0.07490]\n",
      "Step 510993  [5.368 sec/step, loss=0.07633, avg_loss=0.07492]\n",
      "Step 510994  [5.362 sec/step, loss=0.07650, avg_loss=0.07493]\n",
      "Step 510995  [5.378 sec/step, loss=0.07688, avg_loss=0.07496]\n",
      "Step 510996  [5.384 sec/step, loss=0.07474, avg_loss=0.07498]\n",
      "Step 510997  [5.406 sec/step, loss=0.07579, avg_loss=0.07502]\n",
      "Step 510998  [5.414 sec/step, loss=0.07296, avg_loss=0.07501]\n",
      "Step 510999  [5.408 sec/step, loss=0.07630, avg_loss=0.07501]\n",
      "Step 511000  [5.416 sec/step, loss=0.07588, avg_loss=0.07501]\n",
      "Writing summary at step: 511000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-511000\n",
      "Saving audio and alignment...\n",
      "Input: muhaqqiqiin nay mutdtdafiqaa baaoondrii kii tdaariix nazroon miin rakkhii~______________\n",
      "Step 511001  [5.407 sec/step, loss=0.07562, avg_loss=0.07499]\n",
      "Step 511002  [5.357 sec/step, loss=0.07630, avg_loss=0.07507]\n",
      "Step 511003  [5.353 sec/step, loss=0.07701, avg_loss=0.07507]\n",
      "Step 511004  [5.337 sec/step, loss=0.07613, avg_loss=0.07506]\n",
      "Step 511005  [5.344 sec/step, loss=0.07709, avg_loss=0.07506]\n",
      "Step 511006  [5.353 sec/step, loss=0.07428, avg_loss=0.07504]\n",
      "Generated 32 batches of size 32 in 2.304 sec\n",
      "Step 511007  [5.363 sec/step, loss=0.07557, avg_loss=0.07503]\n",
      "Step 511008  [5.390 sec/step, loss=0.07436, avg_loss=0.07505]\n",
      "Step 511009  [5.439 sec/step, loss=0.06735, avg_loss=0.07496]\n",
      "Step 511010  [5.422 sec/step, loss=0.07523, avg_loss=0.07495]\n",
      "Step 511011  [5.438 sec/step, loss=0.07537, avg_loss=0.07503]\n",
      "Step 511012  [5.444 sec/step, loss=0.07532, avg_loss=0.07506]\n",
      "Step 511013  [5.427 sec/step, loss=0.07250, avg_loss=0.07504]\n",
      "Step 511014  [5.406 sec/step, loss=0.07289, avg_loss=0.07499]\n",
      "Step 511015  [5.410 sec/step, loss=0.07603, avg_loss=0.07499]\n",
      "Step 511016  [5.397 sec/step, loss=0.07559, avg_loss=0.07499]\n",
      "Step 511017  [5.386 sec/step, loss=0.07449, avg_loss=0.07497]\n",
      "Step 511018  [5.389 sec/step, loss=0.07661, avg_loss=0.07497]\n",
      "Step 511019  [5.385 sec/step, loss=0.07529, avg_loss=0.07499]\n",
      "Step 511020  [5.376 sec/step, loss=0.07596, avg_loss=0.07498]\n",
      "Step 511021  [5.366 sec/step, loss=0.07616, avg_loss=0.07498]\n",
      "Step 511022  [5.368 sec/step, loss=0.07711, avg_loss=0.07500]\n",
      "Step 511023  [5.377 sec/step, loss=0.07222, avg_loss=0.07499]\n",
      "Step 511024  [5.362 sec/step, loss=0.07556, avg_loss=0.07498]\n",
      "Step 511025  [5.358 sec/step, loss=0.07284, avg_loss=0.07494]\n",
      "Step 511026  [5.349 sec/step, loss=0.07600, avg_loss=0.07494]\n",
      "Step 511027  [5.350 sec/step, loss=0.07564, avg_loss=0.07493]\n",
      "Step 511028  [5.344 sec/step, loss=0.07646, avg_loss=0.07496]\n",
      "Step 511029  [5.344 sec/step, loss=0.07651, avg_loss=0.07498]\n",
      "Step 511030  [5.340 sec/step, loss=0.07511, avg_loss=0.07500]\n",
      "Step 511031  [5.342 sec/step, loss=0.07741, avg_loss=0.07500]\n",
      "Step 511032  [5.330 sec/step, loss=0.06652, avg_loss=0.07491]\n",
      "Step 511033  [5.337 sec/step, loss=0.07151, avg_loss=0.07490]\n",
      "Step 511034  [5.356 sec/step, loss=0.07618, avg_loss=0.07490]\n",
      "Step 511035  [5.360 sec/step, loss=0.07678, avg_loss=0.07493]\n",
      "Step 511036  [5.362 sec/step, loss=0.07676, avg_loss=0.07494]\n",
      "Step 511037  [5.357 sec/step, loss=0.07584, avg_loss=0.07495]\n",
      "Step 511038  [5.342 sec/step, loss=0.07109, avg_loss=0.07490]\n",
      "Generated 32 batches of size 32 in 2.346 sec\n",
      "Step 511039  [5.335 sec/step, loss=0.07673, avg_loss=0.07493]\n",
      "Step 511040  [5.332 sec/step, loss=0.07522, avg_loss=0.07492]\n",
      "Step 511041  [5.309 sec/step, loss=0.07298, avg_loss=0.07488]\n",
      "Step 511042  [5.311 sec/step, loss=0.07704, avg_loss=0.07489]\n",
      "Step 511043  [5.321 sec/step, loss=0.07753, avg_loss=0.07491]\n",
      "Step 511044  [5.319 sec/step, loss=0.07411, avg_loss=0.07488]\n",
      "Step 511045  [5.364 sec/step, loss=0.06626, avg_loss=0.07479]\n",
      "Step 511046  [5.382 sec/step, loss=0.07450, avg_loss=0.07479]\n",
      "Step 511047  [5.405 sec/step, loss=0.07772, avg_loss=0.07489]\n",
      "Step 511048  [5.405 sec/step, loss=0.06687, avg_loss=0.07489]\n",
      "Step 511049  [5.420 sec/step, loss=0.07632, avg_loss=0.07492]\n",
      "Step 511050  [5.429 sec/step, loss=0.07728, avg_loss=0.07493]\n",
      "Step 511051  [5.420 sec/step, loss=0.07627, avg_loss=0.07492]\n",
      "Step 511052  [5.421 sec/step, loss=0.07405, avg_loss=0.07491]\n",
      "Step 511053  [5.402 sec/step, loss=0.07621, avg_loss=0.07490]\n",
      "Step 511054  [5.393 sec/step, loss=0.06697, avg_loss=0.07482]\n",
      "Step 511055  [5.392 sec/step, loss=0.07477, avg_loss=0.07481]\n",
      "Step 511056  [5.403 sec/step, loss=0.07507, avg_loss=0.07481]\n",
      "Step 511057  [5.392 sec/step, loss=0.07445, avg_loss=0.07481]\n",
      "Step 511058  [5.402 sec/step, loss=0.07371, avg_loss=0.07481]\n",
      "Step 511059  [5.399 sec/step, loss=0.07540, avg_loss=0.07482]\n",
      "Step 511060  [5.397 sec/step, loss=0.07740, avg_loss=0.07484]\n",
      "Step 511061  [5.407 sec/step, loss=0.07749, avg_loss=0.07486]\n",
      "Step 511062  [5.417 sec/step, loss=0.07664, avg_loss=0.07488]\n",
      "Step 511063  [5.391 sec/step, loss=0.07649, avg_loss=0.07491]\n",
      "Step 511064  [5.374 sec/step, loss=0.07479, avg_loss=0.07489]\n",
      "Step 511065  [5.395 sec/step, loss=0.07763, avg_loss=0.07495]\n",
      "Step 511066  [5.415 sec/step, loss=0.07549, avg_loss=0.07504]\n",
      "Step 511067  [5.437 sec/step, loss=0.07398, avg_loss=0.07502]\n",
      "Step 511068  [5.436 sec/step, loss=0.07763, avg_loss=0.07503]\n",
      "Step 511069  [5.425 sec/step, loss=0.07287, avg_loss=0.07499]\n",
      "Step 511070  [5.372 sec/step, loss=0.07383, avg_loss=0.07507]\n",
      "Generated 32 batches of size 32 in 2.333 sec\n",
      "Step 511071  [5.378 sec/step, loss=0.07652, avg_loss=0.07507]\n",
      "Step 511072  [5.381 sec/step, loss=0.07663, avg_loss=0.07507]\n",
      "Step 511073  [5.374 sec/step, loss=0.07254, avg_loss=0.07507]\n",
      "Step 511074  [5.366 sec/step, loss=0.07637, avg_loss=0.07507]\n",
      "Step 511075  [5.368 sec/step, loss=0.07352, avg_loss=0.07505]\n",
      "Step 511076  [5.369 sec/step, loss=0.07598, avg_loss=0.07506]\n",
      "Step 511077  [5.348 sec/step, loss=0.07660, avg_loss=0.07506]\n",
      "Step 511078  [5.359 sec/step, loss=0.07702, avg_loss=0.07511]\n",
      "Step 511079  [5.354 sec/step, loss=0.07701, avg_loss=0.07511]\n",
      "Step 511080  [5.372 sec/step, loss=0.07886, avg_loss=0.07515]\n",
      "Step 511081  [5.384 sec/step, loss=0.07634, avg_loss=0.07516]\n",
      "Step 511082  [5.366 sec/step, loss=0.07706, avg_loss=0.07517]\n",
      "Step 511083  [5.354 sec/step, loss=0.07579, avg_loss=0.07518]\n",
      "Step 511084  [5.336 sec/step, loss=0.07375, avg_loss=0.07515]\n",
      "Step 511085  [5.310 sec/step, loss=0.06726, avg_loss=0.07504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 511086  [5.293 sec/step, loss=0.07587, avg_loss=0.07504]\n",
      "Step 511087  [5.290 sec/step, loss=0.07610, avg_loss=0.07506]\n",
      "Step 511088  [5.276 sec/step, loss=0.07740, avg_loss=0.07505]\n",
      "Step 511089  [5.276 sec/step, loss=0.07295, avg_loss=0.07504]\n",
      "Step 511090  [5.261 sec/step, loss=0.07490, avg_loss=0.07501]\n",
      "Step 511091  [5.256 sec/step, loss=0.07543, avg_loss=0.07499]\n",
      "Step 511092  [5.259 sec/step, loss=0.07555, avg_loss=0.07503]\n",
      "Step 511093  [5.277 sec/step, loss=0.07602, avg_loss=0.07503]\n",
      "Step 511094  [5.286 sec/step, loss=0.07663, avg_loss=0.07503]\n",
      "Step 511095  [5.273 sec/step, loss=0.07686, avg_loss=0.07503]\n",
      "Step 511096  [5.289 sec/step, loss=0.07837, avg_loss=0.07507]\n",
      "Step 511097  [5.265 sec/step, loss=0.07297, avg_loss=0.07504]\n",
      "Step 511098  [5.269 sec/step, loss=0.07816, avg_loss=0.07509]\n",
      "Step 511099  [5.272 sec/step, loss=0.07352, avg_loss=0.07506]\n",
      "Step 511100  [5.261 sec/step, loss=0.07595, avg_loss=0.07506]\n",
      "Writing summary at step: 511100\n",
      "Step 511101  [5.263 sec/step, loss=0.07443, avg_loss=0.07505]\n",
      "Generated 32 batches of size 32 in 2.247 sec\n",
      "Step 511102  [5.276 sec/step, loss=0.07802, avg_loss=0.07507]\n",
      "Step 511103  [5.268 sec/step, loss=0.07715, avg_loss=0.07507]\n",
      "Step 511104  [5.264 sec/step, loss=0.07571, avg_loss=0.07507]\n",
      "Step 511105  [5.262 sec/step, loss=0.07813, avg_loss=0.07508]\n",
      "Step 511106  [5.244 sec/step, loss=0.07728, avg_loss=0.07511]\n",
      "Step 511107  [5.279 sec/step, loss=0.06979, avg_loss=0.07505]\n",
      "Step 511108  [5.278 sec/step, loss=0.07564, avg_loss=0.07506]\n",
      "Step 511109  [5.235 sec/step, loss=0.07849, avg_loss=0.07517]\n",
      "Step 511110  [5.250 sec/step, loss=0.07716, avg_loss=0.07519]\n",
      "Step 511111  [5.254 sec/step, loss=0.07455, avg_loss=0.07518]\n",
      "Step 511112  [5.248 sec/step, loss=0.07336, avg_loss=0.07516]\n",
      "Step 511113  [5.264 sec/step, loss=0.07706, avg_loss=0.07521]\n",
      "Step 511114  [5.265 sec/step, loss=0.07534, avg_loss=0.07523]\n",
      "Step 511115  [5.283 sec/step, loss=0.07637, avg_loss=0.07524]\n",
      "Step 511116  [5.288 sec/step, loss=0.07803, avg_loss=0.07526]\n",
      "Step 511117  [5.286 sec/step, loss=0.07597, avg_loss=0.07528]\n",
      "Step 511118  [5.292 sec/step, loss=0.07714, avg_loss=0.07528]\n",
      "Step 511119  [5.296 sec/step, loss=0.07498, avg_loss=0.07528]\n",
      "Step 511120  [5.299 sec/step, loss=0.07777, avg_loss=0.07530]\n",
      "Step 511121  [5.309 sec/step, loss=0.07625, avg_loss=0.07530]\n",
      "Step 511122  [5.313 sec/step, loss=0.07722, avg_loss=0.07530]\n",
      "Step 511123  [5.318 sec/step, loss=0.07696, avg_loss=0.07535]\n",
      "Step 511124  [5.314 sec/step, loss=0.07542, avg_loss=0.07534]\n",
      "Step 511125  [5.308 sec/step, loss=0.07443, avg_loss=0.07536]\n",
      "Step 511126  [5.307 sec/step, loss=0.07637, avg_loss=0.07536]\n",
      "Step 511127  [5.303 sec/step, loss=0.07597, avg_loss=0.07537]\n",
      "Step 511128  [5.286 sec/step, loss=0.07701, avg_loss=0.07537]\n",
      "Step 511129  [5.270 sec/step, loss=0.07290, avg_loss=0.07534]\n",
      "Step 511130  [5.277 sec/step, loss=0.07598, avg_loss=0.07535]\n",
      "Step 511131  [5.315 sec/step, loss=0.06649, avg_loss=0.07524]\n",
      "Step 511132  [5.341 sec/step, loss=0.07669, avg_loss=0.07534]\n",
      "Step 511133  [5.353 sec/step, loss=0.07486, avg_loss=0.07537]\n",
      "Generated 32 batches of size 32 in 2.418 sec\n",
      "Step 511134  [5.347 sec/step, loss=0.07614, avg_loss=0.07537]\n",
      "Step 511135  [5.356 sec/step, loss=0.07753, avg_loss=0.07538]\n",
      "Step 511136  [5.362 sec/step, loss=0.07767, avg_loss=0.07539]\n",
      "Step 511137  [5.357 sec/step, loss=0.07708, avg_loss=0.07540]\n",
      "Step 511138  [5.353 sec/step, loss=0.06855, avg_loss=0.07538]\n",
      "Step 511139  [5.325 sec/step, loss=0.07534, avg_loss=0.07536]\n",
      "Step 511140  [5.331 sec/step, loss=0.07283, avg_loss=0.07534]\n",
      "Step 511141  [5.356 sec/step, loss=0.07651, avg_loss=0.07537]\n",
      "Step 511142  [5.345 sec/step, loss=0.07576, avg_loss=0.07536]\n",
      "Step 511143  [5.327 sec/step, loss=0.07511, avg_loss=0.07534]\n",
      "Step 511144  [5.331 sec/step, loss=0.07648, avg_loss=0.07536]\n",
      "Step 511145  [5.281 sec/step, loss=0.07667, avg_loss=0.07546]\n",
      "Step 511146  [5.261 sec/step, loss=0.07428, avg_loss=0.07546]\n",
      "Step 511147  [5.277 sec/step, loss=0.07472, avg_loss=0.07543]\n",
      "Step 511148  [5.209 sec/step, loss=0.06724, avg_loss=0.07543]\n",
      "Step 511149  [5.203 sec/step, loss=0.07748, avg_loss=0.07545]\n",
      "Step 511150  [5.246 sec/step, loss=0.06816, avg_loss=0.07536]\n",
      "Step 511151  [5.248 sec/step, loss=0.07548, avg_loss=0.07535]\n",
      "Step 511152  [5.270 sec/step, loss=0.07605, avg_loss=0.07537]\n",
      "Step 511153  [5.270 sec/step, loss=0.07532, avg_loss=0.07536]\n",
      "Step 511154  [5.298 sec/step, loss=0.07575, avg_loss=0.07545]\n",
      "Step 511155  [5.311 sec/step, loss=0.07518, avg_loss=0.07545]\n",
      "Step 511156  [5.315 sec/step, loss=0.07724, avg_loss=0.07547]\n",
      "Step 511157  [5.320 sec/step, loss=0.07507, avg_loss=0.07548]\n",
      "Step 511158  [5.317 sec/step, loss=0.07728, avg_loss=0.07551]\n",
      "Step 511159  [5.315 sec/step, loss=0.07266, avg_loss=0.07549]\n",
      "Step 511160  [5.307 sec/step, loss=0.07589, avg_loss=0.07547]\n",
      "Step 511161  [5.297 sec/step, loss=0.07388, avg_loss=0.07544]\n",
      "Step 511162  [5.309 sec/step, loss=0.07728, avg_loss=0.07544]\n",
      "Step 511163  [5.299 sec/step, loss=0.07214, avg_loss=0.07540]\n",
      "Step 511164  [5.308 sec/step, loss=0.07732, avg_loss=0.07542]\n",
      "Step 511165  [5.307 sec/step, loss=0.07633, avg_loss=0.07541]\n",
      "Generated 32 batches of size 32 in 2.392 sec\n",
      "Step 511166  [5.311 sec/step, loss=0.07677, avg_loss=0.07542]\n",
      "Step 511167  [5.300 sec/step, loss=0.07778, avg_loss=0.07546]\n",
      "Step 511168  [5.291 sec/step, loss=0.07617, avg_loss=0.07545]\n",
      "Step 511169  [5.298 sec/step, loss=0.07550, avg_loss=0.07547]\n",
      "Step 511170  [5.311 sec/step, loss=0.07749, avg_loss=0.07551]\n",
      "Step 511171  [5.290 sec/step, loss=0.07661, avg_loss=0.07551]\n",
      "Step 511172  [5.281 sec/step, loss=0.07367, avg_loss=0.07548]\n",
      "Step 511173  [5.278 sec/step, loss=0.07263, avg_loss=0.07548]\n",
      "Step 511174  [5.291 sec/step, loss=0.07399, avg_loss=0.07546]\n",
      "Step 511175  [5.295 sec/step, loss=0.07615, avg_loss=0.07548]\n",
      "Step 511176  [5.309 sec/step, loss=0.07700, avg_loss=0.07549]\n",
      "Step 511177  [5.320 sec/step, loss=0.07527, avg_loss=0.07548]\n",
      "Step 511178  [5.333 sec/step, loss=0.07724, avg_loss=0.07548]\n",
      "Step 511179  [5.328 sec/step, loss=0.07559, avg_loss=0.07547]\n",
      "Step 511180  [5.306 sec/step, loss=0.07487, avg_loss=0.07543]\n",
      "Step 511181  [5.301 sec/step, loss=0.07605, avg_loss=0.07543]\n",
      "Step 511182  [5.305 sec/step, loss=0.07288, avg_loss=0.07538]\n",
      "Step 511183  [5.312 sec/step, loss=0.07711, avg_loss=0.07540]\n",
      "Step 511184  [5.342 sec/step, loss=0.07754, avg_loss=0.07544]\n",
      "Step 511185  [5.370 sec/step, loss=0.07727, avg_loss=0.07554]\n",
      "Step 511186  [5.380 sec/step, loss=0.07589, avg_loss=0.07554]\n",
      "Step 511187  [5.372 sec/step, loss=0.07244, avg_loss=0.07550]\n",
      "Step 511188  [5.372 sec/step, loss=0.07678, avg_loss=0.07549]\n",
      "Step 511189  [5.379 sec/step, loss=0.07648, avg_loss=0.07553]\n",
      "Step 511190  [5.381 sec/step, loss=0.07635, avg_loss=0.07554]\n",
      "Step 511191  [5.379 sec/step, loss=0.07644, avg_loss=0.07555]\n",
      "Step 511192  [5.389 sec/step, loss=0.07619, avg_loss=0.07556]\n",
      "Step 511193  [5.391 sec/step, loss=0.07638, avg_loss=0.07556]\n",
      "Step 511194  [5.367 sec/step, loss=0.07344, avg_loss=0.07553]\n",
      "Step 511195  [5.369 sec/step, loss=0.07680, avg_loss=0.07553]\n",
      "Step 511196  [5.356 sec/step, loss=0.07311, avg_loss=0.07548]\n",
      "Step 511197  [5.377 sec/step, loss=0.07754, avg_loss=0.07552]\n",
      "Generated 32 batches of size 32 in 2.431 sec\n",
      "Step 511198  [5.391 sec/step, loss=0.07682, avg_loss=0.07551]\n",
      "Step 511199  [5.383 sec/step, loss=0.07521, avg_loss=0.07553]\n",
      "Step 511200  [5.382 sec/step, loss=0.07180, avg_loss=0.07549]\n",
      "Writing summary at step: 511200\n",
      "Step 511201  [5.378 sec/step, loss=0.07512, avg_loss=0.07549]\n",
      "Step 511202  [5.362 sec/step, loss=0.07507, avg_loss=0.07546]\n",
      "Step 511203  [5.363 sec/step, loss=0.07521, avg_loss=0.07544]\n",
      "Step 511204  [5.354 sec/step, loss=0.06626, avg_loss=0.07535]\n",
      "Step 511205  [5.397 sec/step, loss=0.06679, avg_loss=0.07524]\n",
      "Step 511206  [5.396 sec/step, loss=0.07542, avg_loss=0.07522]\n",
      "Step 511207  [5.329 sec/step, loss=0.06621, avg_loss=0.07518]\n",
      "Step 511208  [5.304 sec/step, loss=0.07716, avg_loss=0.07520]\n",
      "Step 511209  [5.346 sec/step, loss=0.06841, avg_loss=0.07510]\n",
      "Step 511210  [5.348 sec/step, loss=0.07599, avg_loss=0.07508]\n",
      "Step 511211  [5.351 sec/step, loss=0.07770, avg_loss=0.07512]\n",
      "Step 511212  [5.373 sec/step, loss=0.07723, avg_loss=0.07515]\n",
      "Step 511213  [5.365 sec/step, loss=0.07613, avg_loss=0.07514]\n",
      "Step 511214  [5.374 sec/step, loss=0.07450, avg_loss=0.07514]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 511215  [5.350 sec/step, loss=0.07544, avg_loss=0.07513]\n",
      "Step 511216  [5.331 sec/step, loss=0.07525, avg_loss=0.07510]\n",
      "Step 511217  [5.355 sec/step, loss=0.07690, avg_loss=0.07511]\n",
      "Step 511218  [5.350 sec/step, loss=0.07621, avg_loss=0.07510]\n",
      "Step 511219  [5.378 sec/step, loss=0.07460, avg_loss=0.07510]\n",
      "Step 511220  [5.367 sec/step, loss=0.07512, avg_loss=0.07507]\n",
      "Step 511221  [5.377 sec/step, loss=0.07725, avg_loss=0.07508]\n",
      "Step 511222  [5.357 sec/step, loss=0.07385, avg_loss=0.07505]\n",
      "Step 511223  [5.362 sec/step, loss=0.07593, avg_loss=0.07503]\n",
      "Step 511224  [5.365 sec/step, loss=0.07258, avg_loss=0.07501]\n",
      "Step 511225  [5.365 sec/step, loss=0.07528, avg_loss=0.07501]\n",
      "Step 511226  [5.380 sec/step, loss=0.07462, avg_loss=0.07500]\n",
      "Step 511227  [5.393 sec/step, loss=0.07701, avg_loss=0.07501]\n",
      "Step 511228  [5.390 sec/step, loss=0.07493, avg_loss=0.07499]\n",
      "Generated 32 batches of size 32 in 2.411 sec\n",
      "Step 511229  [5.414 sec/step, loss=0.07604, avg_loss=0.07502]\n",
      "Step 511230  [5.421 sec/step, loss=0.07550, avg_loss=0.07501]\n",
      "Step 511231  [5.376 sec/step, loss=0.07739, avg_loss=0.07512]\n",
      "Step 511232  [5.368 sec/step, loss=0.07611, avg_loss=0.07512]\n",
      "Step 511233  [5.347 sec/step, loss=0.07324, avg_loss=0.07510]\n",
      "Step 511234  [5.325 sec/step, loss=0.07662, avg_loss=0.07511]\n",
      "Step 511235  [5.305 sec/step, loss=0.07193, avg_loss=0.07505]\n",
      "Step 511236  [5.309 sec/step, loss=0.07753, avg_loss=0.07505]\n",
      "Step 511237  [5.327 sec/step, loss=0.07419, avg_loss=0.07502]\n",
      "Step 511238  [5.339 sec/step, loss=0.07345, avg_loss=0.07507]\n",
      "Step 511239  [5.350 sec/step, loss=0.07722, avg_loss=0.07509]\n",
      "Step 511240  [5.359 sec/step, loss=0.07624, avg_loss=0.07512]\n",
      "Step 511241  [5.352 sec/step, loss=0.07644, avg_loss=0.07512]\n",
      "Step 511242  [5.369 sec/step, loss=0.07709, avg_loss=0.07513]\n",
      "Step 511243  [5.387 sec/step, loss=0.07727, avg_loss=0.07516]\n",
      "Step 511244  [5.385 sec/step, loss=0.07314, avg_loss=0.07512]\n",
      "Step 511245  [5.371 sec/step, loss=0.07256, avg_loss=0.07508]\n",
      "Step 511246  [5.389 sec/step, loss=0.07711, avg_loss=0.07511]\n",
      "Step 511247  [5.377 sec/step, loss=0.07499, avg_loss=0.07511]\n",
      "Step 511248  [5.385 sec/step, loss=0.07526, avg_loss=0.07519]\n",
      "Step 511249  [5.379 sec/step, loss=0.07540, avg_loss=0.07517]\n",
      "Step 511250  [5.379 sec/step, loss=0.06670, avg_loss=0.07516]\n",
      "Step 511251  [5.359 sec/step, loss=0.06794, avg_loss=0.07508]\n",
      "Step 511252  [5.356 sec/step, loss=0.07506, avg_loss=0.07507]\n",
      "Step 511253  [5.362 sec/step, loss=0.07659, avg_loss=0.07508]\n",
      "Step 511254  [5.368 sec/step, loss=0.07625, avg_loss=0.07509]\n",
      "Step 511255  [5.350 sec/step, loss=0.07666, avg_loss=0.07510]\n",
      "Step 511256  [5.341 sec/step, loss=0.07538, avg_loss=0.07509]\n",
      "Step 511257  [5.336 sec/step, loss=0.07462, avg_loss=0.07508]\n",
      "Step 511258  [5.330 sec/step, loss=0.07632, avg_loss=0.07507]\n",
      "Step 511259  [5.333 sec/step, loss=0.07647, avg_loss=0.07511]\n",
      "Step 511260  [5.325 sec/step, loss=0.07459, avg_loss=0.07510]\n",
      "Generated 32 batches of size 32 in 2.577 sec\n",
      "Step 511261  [5.327 sec/step, loss=0.07375, avg_loss=0.07509]\n",
      "Step 511262  [5.308 sec/step, loss=0.07527, avg_loss=0.07507]\n",
      "Step 511263  [5.326 sec/step, loss=0.07635, avg_loss=0.07512]\n",
      "Step 511264  [5.337 sec/step, loss=0.07690, avg_loss=0.07511]\n",
      "Step 511265  [5.338 sec/step, loss=0.07581, avg_loss=0.07511]\n",
      "Step 511266  [5.355 sec/step, loss=0.07672, avg_loss=0.07511]\n",
      "Step 511267  [5.330 sec/step, loss=0.07283, avg_loss=0.07506]\n",
      "Step 511268  [5.344 sec/step, loss=0.07746, avg_loss=0.07507]\n",
      "Step 511269  [5.351 sec/step, loss=0.07596, avg_loss=0.07507]\n",
      "Step 511270  [5.334 sec/step, loss=0.07481, avg_loss=0.07505]\n",
      "Step 511271  [5.337 sec/step, loss=0.07646, avg_loss=0.07505]\n",
      "Step 511272  [5.356 sec/step, loss=0.07692, avg_loss=0.07508]\n",
      "Step 511273  [5.367 sec/step, loss=0.07470, avg_loss=0.07510]\n",
      "Step 511274  [5.364 sec/step, loss=0.07268, avg_loss=0.07509]\n",
      "Step 511275  [5.366 sec/step, loss=0.07677, avg_loss=0.07509]\n",
      "Step 511276  [5.361 sec/step, loss=0.07587, avg_loss=0.07508]\n",
      "Step 511277  [5.362 sec/step, loss=0.07523, avg_loss=0.07508]\n",
      "Step 511278  [5.342 sec/step, loss=0.07303, avg_loss=0.07504]\n",
      "Step 511279  [5.351 sec/step, loss=0.07597, avg_loss=0.07504]\n",
      "Step 511280  [5.356 sec/step, loss=0.07377, avg_loss=0.07503]\n",
      "Step 511281  [5.345 sec/step, loss=0.07421, avg_loss=0.07501]\n",
      "Step 511282  [5.349 sec/step, loss=0.07533, avg_loss=0.07504]\n",
      "Step 511283  [5.360 sec/step, loss=0.07695, avg_loss=0.07504]\n",
      "Step 511284  [5.345 sec/step, loss=0.07631, avg_loss=0.07502]\n",
      "Step 511285  [5.360 sec/step, loss=0.07609, avg_loss=0.07501]\n",
      "Step 511286  [5.347 sec/step, loss=0.07294, avg_loss=0.07498]\n",
      "Step 511287  [5.369 sec/step, loss=0.07520, avg_loss=0.07501]\n",
      "Step 511288  [5.378 sec/step, loss=0.07647, avg_loss=0.07501]\n",
      "Step 511289  [5.369 sec/step, loss=0.07537, avg_loss=0.07500]\n",
      "Step 511290  [5.425 sec/step, loss=0.06660, avg_loss=0.07490]\n",
      "Step 511291  [5.404 sec/step, loss=0.06662, avg_loss=0.07480]\n",
      "Step 511292  [5.394 sec/step, loss=0.07532, avg_loss=0.07479]\n",
      "Generated 32 batches of size 32 in 2.573 sec\n",
      "Step 511293  [5.372 sec/step, loss=0.07636, avg_loss=0.07479]\n",
      "Step 511294  [5.386 sec/step, loss=0.07596, avg_loss=0.07482]\n",
      "Step 511295  [5.396 sec/step, loss=0.07724, avg_loss=0.07482]\n",
      "Step 511296  [5.399 sec/step, loss=0.07715, avg_loss=0.07486]\n",
      "Step 511297  [5.386 sec/step, loss=0.07657, avg_loss=0.07485]\n",
      "Step 511298  [5.382 sec/step, loss=0.07687, avg_loss=0.07485]\n",
      "Step 511299  [5.379 sec/step, loss=0.07506, avg_loss=0.07485]\n",
      "Step 511300  [5.394 sec/step, loss=0.07648, avg_loss=0.07490]\n",
      "Writing summary at step: 511300\n",
      "Step 511301  [5.395 sec/step, loss=0.07298, avg_loss=0.07488]\n",
      "Step 511302  [5.409 sec/step, loss=0.07720, avg_loss=0.07490]\n",
      "Step 511303  [5.409 sec/step, loss=0.07514, avg_loss=0.07490]\n",
      "Step 511304  [5.428 sec/step, loss=0.07601, avg_loss=0.07499]\n",
      "Step 511305  [5.386 sec/step, loss=0.07714, avg_loss=0.07510]\n",
      "Step 511306  [5.437 sec/step, loss=0.06736, avg_loss=0.07502]\n",
      "Step 511307  [5.464 sec/step, loss=0.07670, avg_loss=0.07512]\n",
      "Step 511308  [5.456 sec/step, loss=0.07493, avg_loss=0.07510]\n",
      "Step 511309  [5.420 sec/step, loss=0.07527, avg_loss=0.07517]\n",
      "Step 511310  [5.425 sec/step, loss=0.07675, avg_loss=0.07518]\n",
      "Step 511311  [5.421 sec/step, loss=0.07671, avg_loss=0.07517]\n",
      "Step 511312  [5.410 sec/step, loss=0.07626, avg_loss=0.07516]\n",
      "Step 511313  [5.421 sec/step, loss=0.07561, avg_loss=0.07515]\n",
      "Step 511314  [5.408 sec/step, loss=0.07327, avg_loss=0.07514]\n",
      "Step 511315  [5.406 sec/step, loss=0.07482, avg_loss=0.07513]\n",
      "Step 511316  [5.428 sec/step, loss=0.07554, avg_loss=0.07514]\n",
      "Step 511317  [5.420 sec/step, loss=0.07604, avg_loss=0.07513]\n",
      "Step 511318  [5.416 sec/step, loss=0.07619, avg_loss=0.07513]\n",
      "Step 511319  [5.385 sec/step, loss=0.07420, avg_loss=0.07512]\n",
      "Step 511320  [5.403 sec/step, loss=0.07736, avg_loss=0.07515]\n",
      "Step 511321  [5.414 sec/step, loss=0.07440, avg_loss=0.07512]\n",
      "Step 511322  [5.401 sec/step, loss=0.06796, avg_loss=0.07506]\n",
      "Step 511323  [5.399 sec/step, loss=0.07623, avg_loss=0.07506]\n",
      "Generated 32 batches of size 32 in 2.478 sec\n",
      "Step 511324  [5.405 sec/step, loss=0.07465, avg_loss=0.07508]\n",
      "Step 511325  [5.413 sec/step, loss=0.07601, avg_loss=0.07509]\n",
      "Step 511326  [5.402 sec/step, loss=0.07572, avg_loss=0.07510]\n",
      "Step 511327  [5.405 sec/step, loss=0.07719, avg_loss=0.07510]\n",
      "Step 511328  [5.405 sec/step, loss=0.07488, avg_loss=0.07510]\n",
      "Step 511329  [5.398 sec/step, loss=0.07761, avg_loss=0.07512]\n",
      "Step 511330  [5.384 sec/step, loss=0.07175, avg_loss=0.07508]\n",
      "Step 511331  [5.370 sec/step, loss=0.07227, avg_loss=0.07503]\n",
      "Step 511332  [5.388 sec/step, loss=0.07433, avg_loss=0.07501]\n",
      "Step 511333  [5.405 sec/step, loss=0.07558, avg_loss=0.07503]\n",
      "Step 511334  [5.395 sec/step, loss=0.07309, avg_loss=0.07500]\n",
      "Step 511335  [5.410 sec/step, loss=0.07617, avg_loss=0.07504]\n",
      "Step 511336  [5.388 sec/step, loss=0.07504, avg_loss=0.07502]\n",
      "Step 511337  [5.373 sec/step, loss=0.07674, avg_loss=0.07504]\n",
      "Step 511338  [5.377 sec/step, loss=0.07628, avg_loss=0.07507]\n",
      "Step 511339  [5.379 sec/step, loss=0.07628, avg_loss=0.07506]\n",
      "Step 511340  [5.374 sec/step, loss=0.07505, avg_loss=0.07505]\n",
      "Step 511341  [5.371 sec/step, loss=0.07648, avg_loss=0.07505]\n",
      "Step 511342  [5.405 sec/step, loss=0.06692, avg_loss=0.07495]\n",
      "Step 511343  [5.399 sec/step, loss=0.07355, avg_loss=0.07491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 511344  [5.395 sec/step, loss=0.07618, avg_loss=0.07494]\n",
      "Step 511345  [5.420 sec/step, loss=0.07750, avg_loss=0.07499]\n",
      "Step 511346  [5.438 sec/step, loss=0.07651, avg_loss=0.07498]\n",
      "Step 511347  [5.423 sec/step, loss=0.07621, avg_loss=0.07500]\n",
      "Step 511348  [5.451 sec/step, loss=0.07627, avg_loss=0.07501]\n",
      "Step 511349  [5.440 sec/step, loss=0.07485, avg_loss=0.07500]\n",
      "Step 511350  [5.404 sec/step, loss=0.07712, avg_loss=0.07511]\n",
      "Step 511351  [5.430 sec/step, loss=0.07459, avg_loss=0.07517]\n",
      "Step 511352  [5.417 sec/step, loss=0.07366, avg_loss=0.07516]\n",
      "Step 511353  [5.399 sec/step, loss=0.06685, avg_loss=0.07506]\n",
      "Step 511354  [5.396 sec/step, loss=0.07693, avg_loss=0.07507]\n",
      "Step 511355  [5.394 sec/step, loss=0.07593, avg_loss=0.07506]\n",
      "Generated 32 batches of size 32 in 2.412 sec\n",
      "Step 511356  [5.407 sec/step, loss=0.07499, avg_loss=0.07506]\n",
      "Step 511357  [5.423 sec/step, loss=0.07724, avg_loss=0.07508]\n",
      "Step 511358  [5.426 sec/step, loss=0.07659, avg_loss=0.07508]\n",
      "Step 511359  [5.422 sec/step, loss=0.07445, avg_loss=0.07506]\n",
      "Step 511360  [5.439 sec/step, loss=0.07690, avg_loss=0.07509]\n",
      "Step 511361  [5.432 sec/step, loss=0.07476, avg_loss=0.07510]\n",
      "Step 511362  [5.448 sec/step, loss=0.07698, avg_loss=0.07512]\n",
      "Step 511363  [5.429 sec/step, loss=0.07249, avg_loss=0.07508]\n",
      "Step 511364  [5.409 sec/step, loss=0.07456, avg_loss=0.07505]\n",
      "Step 511365  [5.383 sec/step, loss=0.07125, avg_loss=0.07501]\n",
      "Step 511366  [5.367 sec/step, loss=0.07738, avg_loss=0.07501]\n",
      "Step 511367  [5.375 sec/step, loss=0.07598, avg_loss=0.07505]\n",
      "Step 511368  [5.413 sec/step, loss=0.06659, avg_loss=0.07494]\n",
      "Step 511369  [5.436 sec/step, loss=0.07537, avg_loss=0.07493]\n",
      "Step 511370  [5.445 sec/step, loss=0.07640, avg_loss=0.07495]\n",
      "Step 511371  [5.452 sec/step, loss=0.07542, avg_loss=0.07494]\n",
      "Step 511372  [5.450 sec/step, loss=0.07663, avg_loss=0.07493]\n",
      "Step 511373  [5.455 sec/step, loss=0.07636, avg_loss=0.07495]\n",
      "Step 511374  [5.471 sec/step, loss=0.07644, avg_loss=0.07499]\n",
      "Step 511375  [5.455 sec/step, loss=0.07566, avg_loss=0.07498]\n",
      "Step 511376  [5.460 sec/step, loss=0.07515, avg_loss=0.07497]\n",
      "Step 511377  [5.445 sec/step, loss=0.07444, avg_loss=0.07496]\n",
      "Step 511378  [5.460 sec/step, loss=0.07515, avg_loss=0.07498]\n",
      "Step 511379  [5.451 sec/step, loss=0.07541, avg_loss=0.07498]\n",
      "Step 511380  [5.453 sec/step, loss=0.07350, avg_loss=0.07497]\n",
      "Step 511381  [5.467 sec/step, loss=0.07740, avg_loss=0.07501]\n",
      "Step 511382  [5.474 sec/step, loss=0.07663, avg_loss=0.07502]\n",
      "Step 511383  [5.478 sec/step, loss=0.07538, avg_loss=0.07500]\n",
      "Step 511384  [5.473 sec/step, loss=0.07624, avg_loss=0.07500]\n",
      "Step 511385  [5.453 sec/step, loss=0.07512, avg_loss=0.07499]\n",
      "Step 511386  [5.451 sec/step, loss=0.06446, avg_loss=0.07491]\n",
      "Step 511387  [5.436 sec/step, loss=0.07440, avg_loss=0.07490]\n",
      "Generated 32 batches of size 32 in 2.391 sec\n",
      "Step 511388  [5.436 sec/step, loss=0.07619, avg_loss=0.07490]\n",
      "Step 511389  [5.440 sec/step, loss=0.07474, avg_loss=0.07489]\n",
      "Step 511390  [5.384 sec/step, loss=0.07300, avg_loss=0.07496]\n",
      "Step 511391  [5.403 sec/step, loss=0.07631, avg_loss=0.07505]\n",
      "Step 511392  [5.416 sec/step, loss=0.07744, avg_loss=0.07507]\n",
      "Step 511393  [5.408 sec/step, loss=0.07375, avg_loss=0.07505]\n",
      "Step 511394  [5.400 sec/step, loss=0.07486, avg_loss=0.07504]\n",
      "Step 511395  [5.401 sec/step, loss=0.07744, avg_loss=0.07504]\n",
      "Step 511396  [5.387 sec/step, loss=0.07072, avg_loss=0.07497]\n",
      "Step 511397  [5.390 sec/step, loss=0.07507, avg_loss=0.07496]\n",
      "Step 511398  [5.357 sec/step, loss=0.06776, avg_loss=0.07487]\n",
      "Step 511399  [5.362 sec/step, loss=0.07595, avg_loss=0.07488]\n",
      "Step 511400  [5.361 sec/step, loss=0.07676, avg_loss=0.07488]\n",
      "Writing summary at step: 511400\n",
      "Step 511401  [5.377 sec/step, loss=0.07621, avg_loss=0.07491]\n",
      "Step 511402  [5.390 sec/step, loss=0.07557, avg_loss=0.07490]\n",
      "Step 511403  [5.384 sec/step, loss=0.07384, avg_loss=0.07488]\n",
      "Step 511404  [5.379 sec/step, loss=0.07476, avg_loss=0.07487]\n",
      "Step 511405  [5.378 sec/step, loss=0.07712, avg_loss=0.07487]\n",
      "Step 511406  [5.320 sec/step, loss=0.07536, avg_loss=0.07495]\n",
      "Step 511407  [5.312 sec/step, loss=0.07647, avg_loss=0.07495]\n",
      "Step 511408  [5.329 sec/step, loss=0.07718, avg_loss=0.07497]\n",
      "Step 511409  [5.319 sec/step, loss=0.07576, avg_loss=0.07498]\n",
      "Step 511410  [5.296 sec/step, loss=0.07259, avg_loss=0.07493]\n",
      "Step 511411  [5.343 sec/step, loss=0.06776, avg_loss=0.07484]\n",
      "Step 511412  [5.345 sec/step, loss=0.07564, avg_loss=0.07484]\n",
      "Step 511413  [5.340 sec/step, loss=0.07689, avg_loss=0.07485]\n",
      "Step 511414  [5.368 sec/step, loss=0.07477, avg_loss=0.07487]\n",
      "Step 511415  [5.361 sec/step, loss=0.07506, avg_loss=0.07487]\n",
      "Step 511416  [5.365 sec/step, loss=0.07534, avg_loss=0.07487]\n",
      "Step 511417  [5.371 sec/step, loss=0.07536, avg_loss=0.07486]\n",
      "Step 511418  [5.372 sec/step, loss=0.07275, avg_loss=0.07483]\n",
      "Generated 32 batches of size 32 in 2.359 sec\n",
      "Step 511419  [5.385 sec/step, loss=0.07543, avg_loss=0.07484]\n",
      "Step 511420  [5.380 sec/step, loss=0.07621, avg_loss=0.07483]\n",
      "Step 511421  [5.363 sec/step, loss=0.07758, avg_loss=0.07486]\n",
      "Step 511422  [5.372 sec/step, loss=0.07473, avg_loss=0.07493]\n",
      "Step 511423  [5.371 sec/step, loss=0.07327, avg_loss=0.07490]\n",
      "Step 511424  [5.371 sec/step, loss=0.07644, avg_loss=0.07491]\n",
      "Step 511425  [5.365 sec/step, loss=0.07504, avg_loss=0.07490]\n",
      "Step 511426  [5.367 sec/step, loss=0.07676, avg_loss=0.07491]\n",
      "Step 511427  [5.375 sec/step, loss=0.07539, avg_loss=0.07490]\n",
      "Step 511428  [5.371 sec/step, loss=0.07224, avg_loss=0.07487]\n",
      "Step 511429  [5.364 sec/step, loss=0.07515, avg_loss=0.07485]\n",
      "Step 511430  [5.372 sec/step, loss=0.07328, avg_loss=0.07486]\n",
      "Step 511431  [5.364 sec/step, loss=0.06652, avg_loss=0.07480]\n",
      "Step 511432  [5.353 sec/step, loss=0.07657, avg_loss=0.07483]\n",
      "Step 511433  [5.398 sec/step, loss=0.06750, avg_loss=0.07474]\n",
      "Step 511434  [5.411 sec/step, loss=0.07525, avg_loss=0.07477]\n",
      "Step 511435  [5.402 sec/step, loss=0.07632, avg_loss=0.07477]\n",
      "Step 511436  [5.425 sec/step, loss=0.07665, avg_loss=0.07478]\n",
      "Step 511437  [5.431 sec/step, loss=0.07718, avg_loss=0.07479]\n",
      "Step 511438  [5.429 sec/step, loss=0.07618, avg_loss=0.07479]\n",
      "Step 511439  [5.433 sec/step, loss=0.07722, avg_loss=0.07480]\n",
      "Step 511440  [5.450 sec/step, loss=0.07668, avg_loss=0.07481]\n",
      "Step 511441  [5.454 sec/step, loss=0.07335, avg_loss=0.07478]\n",
      "Step 511442  [5.432 sec/step, loss=0.07462, avg_loss=0.07486]\n",
      "Step 511443  [5.432 sec/step, loss=0.07685, avg_loss=0.07489]\n",
      "Step 511444  [5.432 sec/step, loss=0.07211, avg_loss=0.07485]\n",
      "Step 511445  [5.426 sec/step, loss=0.07705, avg_loss=0.07485]\n",
      "Step 511446  [5.394 sec/step, loss=0.07415, avg_loss=0.07482]\n",
      "Step 511447  [5.389 sec/step, loss=0.07484, avg_loss=0.07481]\n",
      "Step 511448  [5.367 sec/step, loss=0.07475, avg_loss=0.07479]\n",
      "Step 511449  [5.379 sec/step, loss=0.07594, avg_loss=0.07481]\n",
      "Step 511450  [5.368 sec/step, loss=0.07372, avg_loss=0.07477]\n",
      "Generated 32 batches of size 32 in 2.447 sec\n",
      "Step 511451  [5.383 sec/step, loss=0.07452, avg_loss=0.07477]\n",
      "Step 511452  [5.395 sec/step, loss=0.07772, avg_loss=0.07481]\n",
      "Step 511453  [5.415 sec/step, loss=0.07593, avg_loss=0.07490]\n",
      "Step 511454  [5.387 sec/step, loss=0.07138, avg_loss=0.07485]\n",
      "Step 511455  [5.378 sec/step, loss=0.07511, avg_loss=0.07484]\n",
      "Step 511456  [5.371 sec/step, loss=0.07600, avg_loss=0.07485]\n",
      "Step 511457  [5.371 sec/step, loss=0.07683, avg_loss=0.07484]\n",
      "Step 511458  [5.365 sec/step, loss=0.07398, avg_loss=0.07482]\n",
      "Step 511459  [5.379 sec/step, loss=0.07528, avg_loss=0.07483]\n",
      "Step 511460  [5.367 sec/step, loss=0.07602, avg_loss=0.07482]\n",
      "Step 511461  [5.355 sec/step, loss=0.06703, avg_loss=0.07474]\n",
      "Step 511462  [5.355 sec/step, loss=0.07618, avg_loss=0.07473]\n",
      "Step 511463  [5.355 sec/step, loss=0.07503, avg_loss=0.07476]\n",
      "Step 511464  [5.380 sec/step, loss=0.07604, avg_loss=0.07477]\n",
      "Step 511465  [5.396 sec/step, loss=0.07565, avg_loss=0.07482]\n",
      "Step 511466  [5.394 sec/step, loss=0.07462, avg_loss=0.07479]\n",
      "Step 511467  [5.389 sec/step, loss=0.07502, avg_loss=0.07478]\n",
      "Step 511468  [5.364 sec/step, loss=0.07569, avg_loss=0.07487]\n",
      "Step 511469  [5.326 sec/step, loss=0.07394, avg_loss=0.07486]\n",
      "Step 511470  [5.318 sec/step, loss=0.07503, avg_loss=0.07484]\n",
      "Step 511471  [5.311 sec/step, loss=0.07670, avg_loss=0.07485]\n",
      "Step 511472  [5.303 sec/step, loss=0.07617, avg_loss=0.07485]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 511473  [5.297 sec/step, loss=0.07481, avg_loss=0.07483]\n",
      "Step 511474  [5.295 sec/step, loss=0.07598, avg_loss=0.07483]\n",
      "Step 511475  [5.306 sec/step, loss=0.07426, avg_loss=0.07482]\n",
      "Step 511476  [5.302 sec/step, loss=0.07520, avg_loss=0.07482]\n",
      "Step 511477  [5.316 sec/step, loss=0.07472, avg_loss=0.07482]\n",
      "Step 511478  [5.312 sec/step, loss=0.07607, avg_loss=0.07483]\n",
      "Step 511479  [5.302 sec/step, loss=0.07207, avg_loss=0.07480]\n",
      "Step 511480  [5.313 sec/step, loss=0.07738, avg_loss=0.07483]\n",
      "Step 511481  [5.301 sec/step, loss=0.07605, avg_loss=0.07482]\n",
      "Step 511482  [5.299 sec/step, loss=0.07694, avg_loss=0.07482]\n",
      "Generated 32 batches of size 32 in 2.440 sec\n",
      "Step 511483  [5.289 sec/step, loss=0.07582, avg_loss=0.07483]\n",
      "Step 511484  [5.304 sec/step, loss=0.07715, avg_loss=0.07484]\n",
      "Step 511485  [5.308 sec/step, loss=0.07728, avg_loss=0.07486]\n",
      "Step 511486  [5.321 sec/step, loss=0.07270, avg_loss=0.07494]\n",
      "Step 511487  [5.331 sec/step, loss=0.07738, avg_loss=0.07497]\n",
      "Step 511488  [5.318 sec/step, loss=0.07439, avg_loss=0.07495]\n",
      "Step 511489  [5.326 sec/step, loss=0.07502, avg_loss=0.07496]\n",
      "Step 511490  [5.382 sec/step, loss=0.06631, avg_loss=0.07489]\n",
      "Step 511491  [5.379 sec/step, loss=0.07533, avg_loss=0.07488]\n",
      "Step 511492  [5.368 sec/step, loss=0.07366, avg_loss=0.07484]\n",
      "Step 511493  [5.368 sec/step, loss=0.07523, avg_loss=0.07486]\n",
      "Step 511494  [5.426 sec/step, loss=0.06656, avg_loss=0.07477]\n",
      "Step 511495  [5.402 sec/step, loss=0.06602, avg_loss=0.07466]\n",
      "Step 511496  [5.416 sec/step, loss=0.07671, avg_loss=0.07472]\n",
      "Step 511497  [5.412 sec/step, loss=0.07600, avg_loss=0.07473]\n",
      "Step 511498  [5.428 sec/step, loss=0.07518, avg_loss=0.07480]\n",
      "Step 511499  [5.441 sec/step, loss=0.07701, avg_loss=0.07481]\n",
      "Step 511500  [5.451 sec/step, loss=0.07644, avg_loss=0.07481]\n",
      "Writing summary at step: 511500\n",
      "Step 511501  [5.445 sec/step, loss=0.07449, avg_loss=0.07479]\n",
      "Step 511502  [5.413 sec/step, loss=0.07338, avg_loss=0.07477]\n",
      "Step 511503  [5.420 sec/step, loss=0.07556, avg_loss=0.07479]\n",
      "Step 511504  [5.435 sec/step, loss=0.07701, avg_loss=0.07481]\n",
      "Step 511505  [5.424 sec/step, loss=0.07299, avg_loss=0.07477]\n",
      "Step 511506  [5.457 sec/step, loss=0.07382, avg_loss=0.07475]\n",
      "Step 511507  [5.470 sec/step, loss=0.07666, avg_loss=0.07475]\n",
      "Step 511508  [5.459 sec/step, loss=0.07551, avg_loss=0.07474]\n",
      "Step 511509  [5.453 sec/step, loss=0.07276, avg_loss=0.07471]\n",
      "Step 511510  [5.464 sec/step, loss=0.07447, avg_loss=0.07473]\n",
      "Step 511511  [5.420 sec/step, loss=0.07579, avg_loss=0.07481]\n",
      "Step 511512  [5.422 sec/step, loss=0.07649, avg_loss=0.07482]\n",
      "Step 511513  [5.432 sec/step, loss=0.07703, avg_loss=0.07482]\n",
      "Generated 32 batches of size 32 in 2.427 sec\n",
      "Step 511514  [5.430 sec/step, loss=0.07736, avg_loss=0.07484]\n",
      "Step 511515  [5.456 sec/step, loss=0.07657, avg_loss=0.07486]\n",
      "Step 511516  [5.454 sec/step, loss=0.07740, avg_loss=0.07488]\n",
      "Step 511517  [5.455 sec/step, loss=0.07493, avg_loss=0.07487]\n",
      "Step 511518  [5.452 sec/step, loss=0.07483, avg_loss=0.07490]\n",
      "Step 511519  [5.449 sec/step, loss=0.07468, avg_loss=0.07489]\n",
      "Step 511520  [5.427 sec/step, loss=0.07266, avg_loss=0.07485]\n",
      "Step 511521  [5.407 sec/step, loss=0.07264, avg_loss=0.07480]\n",
      "Step 511522  [5.404 sec/step, loss=0.07482, avg_loss=0.07480]\n",
      "Step 511523  [5.421 sec/step, loss=0.07670, avg_loss=0.07484]\n",
      "Step 511524  [5.409 sec/step, loss=0.07462, avg_loss=0.07482]\n",
      "Step 511525  [5.407 sec/step, loss=0.07528, avg_loss=0.07482]\n",
      "Step 511526  [5.405 sec/step, loss=0.07584, avg_loss=0.07481]\n",
      "Step 511527  [5.389 sec/step, loss=0.07730, avg_loss=0.07483]\n",
      "Step 511528  [5.396 sec/step, loss=0.07460, avg_loss=0.07486]\n",
      "Step 511529  [5.410 sec/step, loss=0.07537, avg_loss=0.07486]\n",
      "Step 511530  [5.414 sec/step, loss=0.07575, avg_loss=0.07488]\n",
      "Step 511531  [5.441 sec/step, loss=0.07685, avg_loss=0.07499]\n",
      "Step 511532  [5.441 sec/step, loss=0.07705, avg_loss=0.07499]\n",
      "Step 511533  [5.397 sec/step, loss=0.07614, avg_loss=0.07508]\n",
      "Step 511534  [5.399 sec/step, loss=0.07547, avg_loss=0.07508]\n",
      "Step 511535  [5.414 sec/step, loss=0.07704, avg_loss=0.07509]\n",
      "Step 511536  [5.399 sec/step, loss=0.07627, avg_loss=0.07508]\n",
      "Step 511537  [5.394 sec/step, loss=0.07407, avg_loss=0.07505]\n",
      "Step 511538  [5.409 sec/step, loss=0.07706, avg_loss=0.07506]\n",
      "Step 511539  [5.397 sec/step, loss=0.07376, avg_loss=0.07503]\n",
      "Step 511540  [5.394 sec/step, loss=0.07677, avg_loss=0.07503]\n",
      "Step 511541  [5.379 sec/step, loss=0.07501, avg_loss=0.07504]\n",
      "Step 511542  [5.338 sec/step, loss=0.07243, avg_loss=0.07502]\n",
      "Step 511543  [5.320 sec/step, loss=0.06763, avg_loss=0.07493]\n",
      "Step 511544  [5.313 sec/step, loss=0.07239, avg_loss=0.07493]\n",
      "Step 511545  [5.316 sec/step, loss=0.07702, avg_loss=0.07493]\n",
      "Generated 32 batches of size 32 in 2.340 sec\n",
      "Step 511546  [5.339 sec/step, loss=0.07601, avg_loss=0.07495]\n",
      "Step 511547  [5.374 sec/step, loss=0.07462, avg_loss=0.07495]\n",
      "Step 511548  [5.377 sec/step, loss=0.07304, avg_loss=0.07493]\n",
      "Step 511549  [5.373 sec/step, loss=0.07550, avg_loss=0.07493]\n",
      "Step 511550  [5.371 sec/step, loss=0.07492, avg_loss=0.07494]\n",
      "Step 511551  [5.351 sec/step, loss=0.07563, avg_loss=0.07495]\n",
      "Step 511552  [5.337 sec/step, loss=0.07443, avg_loss=0.07492]\n",
      "Step 511553  [5.383 sec/step, loss=0.06741, avg_loss=0.07483]\n",
      "Step 511554  [5.399 sec/step, loss=0.07588, avg_loss=0.07488]\n",
      "Step 511555  [5.405 sec/step, loss=0.07600, avg_loss=0.07489]\n",
      "Step 511556  [5.396 sec/step, loss=0.07179, avg_loss=0.07484]\n",
      "Step 511557  [5.396 sec/step, loss=0.07730, avg_loss=0.07485]\n",
      "Step 511558  [5.395 sec/step, loss=0.07299, avg_loss=0.07484]\n",
      "Step 511559  [5.388 sec/step, loss=0.07538, avg_loss=0.07484]\n",
      "Step 511560  [5.399 sec/step, loss=0.07474, avg_loss=0.07483]\n",
      "Step 511561  [5.415 sec/step, loss=0.07534, avg_loss=0.07491]\n",
      "Step 511562  [5.416 sec/step, loss=0.07673, avg_loss=0.07492]\n",
      "Step 511563  [5.427 sec/step, loss=0.07201, avg_loss=0.07488]\n",
      "Step 511564  [5.460 sec/step, loss=0.06585, avg_loss=0.07478]\n",
      "Step 511565  [5.451 sec/step, loss=0.07435, avg_loss=0.07477]\n",
      "Step 511566  [5.429 sec/step, loss=0.06709, avg_loss=0.07469]\n",
      "Step 511567  [5.454 sec/step, loss=0.07658, avg_loss=0.07471]\n",
      "Step 511568  [5.419 sec/step, loss=0.07507, avg_loss=0.07470]\n",
      "Step 511569  [5.430 sec/step, loss=0.07451, avg_loss=0.07471]\n",
      "Step 511570  [5.430 sec/step, loss=0.07519, avg_loss=0.07471]\n",
      "Step 511571  [5.427 sec/step, loss=0.07444, avg_loss=0.07469]\n",
      "Step 511572  [5.436 sec/step, loss=0.07685, avg_loss=0.07470]\n",
      "Step 511573  [5.441 sec/step, loss=0.07664, avg_loss=0.07471]\n",
      "Step 511574  [5.412 sec/step, loss=0.07295, avg_loss=0.07468]\n",
      "Step 511575  [5.408 sec/step, loss=0.07614, avg_loss=0.07470]\n",
      "Step 511576  [5.416 sec/step, loss=0.07481, avg_loss=0.07470]\n",
      "Step 511577  [5.419 sec/step, loss=0.07700, avg_loss=0.07472]\n",
      "Generated 32 batches of size 32 in 2.299 sec\n",
      "Step 511578  [5.426 sec/step, loss=0.07583, avg_loss=0.07472]\n",
      "Step 511579  [5.441 sec/step, loss=0.07595, avg_loss=0.07476]\n",
      "Step 511580  [5.432 sec/step, loss=0.07565, avg_loss=0.07474]\n",
      "Step 511581  [5.437 sec/step, loss=0.07681, avg_loss=0.07475]\n",
      "Step 511582  [5.452 sec/step, loss=0.07633, avg_loss=0.07474]\n",
      "Step 511583  [5.459 sec/step, loss=0.07532, avg_loss=0.07474]\n",
      "Step 511584  [5.436 sec/step, loss=0.07258, avg_loss=0.07469]\n",
      "Step 511585  [5.435 sec/step, loss=0.07582, avg_loss=0.07468]\n",
      "Step 511586  [5.448 sec/step, loss=0.07774, avg_loss=0.07473]\n",
      "Step 511587  [5.452 sec/step, loss=0.07572, avg_loss=0.07471]\n",
      "Step 511588  [5.445 sec/step, loss=0.07534, avg_loss=0.07472]\n",
      "Step 511589  [5.453 sec/step, loss=0.07562, avg_loss=0.07473]\n",
      "Step 511590  [5.417 sec/step, loss=0.07552, avg_loss=0.07482]\n",
      "Step 511591  [5.421 sec/step, loss=0.07559, avg_loss=0.07482]\n",
      "Step 511592  [5.417 sec/step, loss=0.07432, avg_loss=0.07483]\n",
      "Step 511593  [5.420 sec/step, loss=0.07479, avg_loss=0.07482]\n",
      "Step 511594  [5.366 sec/step, loss=0.07380, avg_loss=0.07490]\n",
      "Step 511595  [5.387 sec/step, loss=0.07586, avg_loss=0.07499]\n",
      "Step 511596  [5.386 sec/step, loss=0.07517, avg_loss=0.07498]\n",
      "Step 511597  [5.401 sec/step, loss=0.07727, avg_loss=0.07499]\n",
      "Step 511598  [5.399 sec/step, loss=0.07630, avg_loss=0.07500]\n",
      "Step 511599  [5.373 sec/step, loss=0.06706, avg_loss=0.07490]\n",
      "Step 511600  [5.364 sec/step, loss=0.07655, avg_loss=0.07490]\n",
      "Writing summary at step: 511600\n",
      "Step 511601  [5.350 sec/step, loss=0.07472, avg_loss=0.07491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 511602  [5.361 sec/step, loss=0.07425, avg_loss=0.07491]\n",
      "Step 511603  [5.357 sec/step, loss=0.07623, avg_loss=0.07492]\n",
      "Step 511604  [5.343 sec/step, loss=0.07355, avg_loss=0.07489]\n",
      "Step 511605  [5.347 sec/step, loss=0.07598, avg_loss=0.07492]\n",
      "Step 511606  [5.371 sec/step, loss=0.06797, avg_loss=0.07486]\n",
      "Step 511607  [5.370 sec/step, loss=0.07483, avg_loss=0.07484]\n",
      "Step 511608  [5.373 sec/step, loss=0.07696, avg_loss=0.07485]\n",
      "Generated 32 batches of size 32 in 2.416 sec\n",
      "Step 511609  [5.407 sec/step, loss=0.07302, avg_loss=0.07486]\n",
      "Step 511610  [5.406 sec/step, loss=0.07592, avg_loss=0.07487]\n",
      "Step 511611  [5.420 sec/step, loss=0.07562, avg_loss=0.07487]\n",
      "Step 511612  [5.429 sec/step, loss=0.07491, avg_loss=0.07485]\n",
      "Step 511613  [5.425 sec/step, loss=0.07683, avg_loss=0.07485]\n",
      "Step 511614  [5.406 sec/step, loss=0.07421, avg_loss=0.07482]\n",
      "Step 511615  [5.392 sec/step, loss=0.07376, avg_loss=0.07479]\n",
      "Step 511616  [5.365 sec/step, loss=0.07259, avg_loss=0.07474]\n",
      "Step 511617  [5.362 sec/step, loss=0.07711, avg_loss=0.07477]\n",
      "Step 511618  [5.366 sec/step, loss=0.07251, avg_loss=0.07474]\n",
      "Step 511619  [5.351 sec/step, loss=0.07227, avg_loss=0.07472]\n",
      "Step 511620  [5.366 sec/step, loss=0.07654, avg_loss=0.07476]\n",
      "Step 511621  [5.378 sec/step, loss=0.07660, avg_loss=0.07480]\n",
      "Step 511622  [5.399 sec/step, loss=0.07750, avg_loss=0.07482]\n",
      "Step 511623  [5.375 sec/step, loss=0.07543, avg_loss=0.07481]\n",
      "Step 511624  [5.392 sec/step, loss=0.07451, avg_loss=0.07481]\n",
      "Step 511625  [5.406 sec/step, loss=0.07712, avg_loss=0.07483]\n",
      "Step 511626  [5.411 sec/step, loss=0.07614, avg_loss=0.07483]\n",
      "Step 511627  [5.409 sec/step, loss=0.07714, avg_loss=0.07483]\n",
      "Step 511628  [5.427 sec/step, loss=0.07514, avg_loss=0.07484]\n",
      "Step 511629  [5.413 sec/step, loss=0.07413, avg_loss=0.07482]\n",
      "Step 511630  [5.416 sec/step, loss=0.07702, avg_loss=0.07484]\n",
      "Step 511631  [5.434 sec/step, loss=0.07392, avg_loss=0.07481]\n",
      "Step 511632  [5.423 sec/step, loss=0.07619, avg_loss=0.07480]\n",
      "Step 511633  [5.410 sec/step, loss=0.07183, avg_loss=0.07475]\n",
      "Step 511634  [5.414 sec/step, loss=0.07568, avg_loss=0.07476]\n",
      "Step 511635  [5.396 sec/step, loss=0.07435, avg_loss=0.07473]\n",
      "Step 511636  [5.414 sec/step, loss=0.07749, avg_loss=0.07474]\n",
      "Step 511637  [5.393 sec/step, loss=0.06725, avg_loss=0.07467]\n",
      "Step 511638  [5.375 sec/step, loss=0.07466, avg_loss=0.07465]\n",
      "Step 511639  [5.380 sec/step, loss=0.07450, avg_loss=0.07466]\n",
      "Step 511640  [5.380 sec/step, loss=0.07703, avg_loss=0.07466]\n",
      "Generated 32 batches of size 32 in 2.606 sec\n",
      "Step 511641  [5.444 sec/step, loss=0.06675, avg_loss=0.07458]\n",
      "Step 511642  [5.443 sec/step, loss=0.07281, avg_loss=0.07458]\n",
      "Step 511643  [5.461 sec/step, loss=0.07566, avg_loss=0.07466]\n",
      "Step 511644  [5.466 sec/step, loss=0.07445, avg_loss=0.07468]\n",
      "Step 511645  [5.462 sec/step, loss=0.07330, avg_loss=0.07465]\n",
      "Step 511646  [5.448 sec/step, loss=0.07563, avg_loss=0.07464]\n",
      "Step 511647  [5.429 sec/step, loss=0.07689, avg_loss=0.07466]\n",
      "Step 511648  [5.437 sec/step, loss=0.07361, avg_loss=0.07467]\n",
      "Step 511649  [5.460 sec/step, loss=0.07471, avg_loss=0.07466]\n",
      "Step 511650  [5.461 sec/step, loss=0.07606, avg_loss=0.07467]\n",
      "Step 511651  [5.443 sec/step, loss=0.07253, avg_loss=0.07464]\n",
      "Step 511652  [5.452 sec/step, loss=0.07597, avg_loss=0.07466]\n",
      "Step 511653  [5.402 sec/step, loss=0.07508, avg_loss=0.07473]\n",
      "Step 511654  [5.406 sec/step, loss=0.07712, avg_loss=0.07475]\n",
      "Step 511655  [5.408 sec/step, loss=0.07212, avg_loss=0.07471]\n",
      "Step 511656  [5.413 sec/step, loss=0.07606, avg_loss=0.07475]\n",
      "Step 511657  [5.390 sec/step, loss=0.07501, avg_loss=0.07473]\n",
      "Step 511658  [5.446 sec/step, loss=0.06762, avg_loss=0.07467]\n",
      "Step 511659  [5.431 sec/step, loss=0.07174, avg_loss=0.07464]\n",
      "Step 511660  [5.420 sec/step, loss=0.07391, avg_loss=0.07463]\n",
      "Step 511661  [5.418 sec/step, loss=0.07421, avg_loss=0.07462]\n",
      "Step 511662  [5.429 sec/step, loss=0.07427, avg_loss=0.07459]\n",
      "Step 511663  [5.437 sec/step, loss=0.07722, avg_loss=0.07465]\n",
      "Step 511664  [5.396 sec/step, loss=0.07540, avg_loss=0.07474]\n",
      "Step 511665  [5.399 sec/step, loss=0.07684, avg_loss=0.07477]\n",
      "Step 511666  [5.398 sec/step, loss=0.06728, avg_loss=0.07477]\n",
      "Step 511667  [5.383 sec/step, loss=0.07660, avg_loss=0.07477]\n",
      "Step 511668  [5.397 sec/step, loss=0.07538, avg_loss=0.07477]\n",
      "Step 511669  [5.393 sec/step, loss=0.07521, avg_loss=0.07478]\n",
      "Step 511670  [5.408 sec/step, loss=0.07748, avg_loss=0.07480]\n",
      "Step 511671  [5.409 sec/step, loss=0.07335, avg_loss=0.07479]\n",
      "Step 511672  [5.407 sec/step, loss=0.07547, avg_loss=0.07478]\n",
      "Generated 32 batches of size 32 in 2.419 sec\n",
      "Step 511673  [5.436 sec/step, loss=0.07406, avg_loss=0.07475]\n",
      "Step 511674  [5.453 sec/step, loss=0.07347, avg_loss=0.07476]\n",
      "Step 511675  [5.470 sec/step, loss=0.07721, avg_loss=0.07477]\n",
      "Step 511676  [5.458 sec/step, loss=0.07644, avg_loss=0.07478]\n",
      "Step 511677  [5.457 sec/step, loss=0.07740, avg_loss=0.07479]\n",
      "Step 511678  [5.445 sec/step, loss=0.07479, avg_loss=0.07478]\n",
      "Step 511679  [5.433 sec/step, loss=0.07452, avg_loss=0.07476]\n",
      "Step 511680  [5.446 sec/step, loss=0.07664, avg_loss=0.07477]\n",
      "Step 511681  [5.455 sec/step, loss=0.07725, avg_loss=0.07478]\n",
      "Step 511682  [5.420 sec/step, loss=0.07491, avg_loss=0.07476]\n",
      "Step 511683  [5.417 sec/step, loss=0.07505, avg_loss=0.07476]\n",
      "Step 511684  [5.428 sec/step, loss=0.07643, avg_loss=0.07480]\n",
      "Step 511685  [5.412 sec/step, loss=0.07442, avg_loss=0.07478]\n",
      "Step 511686  [5.411 sec/step, loss=0.07737, avg_loss=0.07478]\n",
      "Step 511687  [5.400 sec/step, loss=0.07531, avg_loss=0.07478]\n",
      "Step 511688  [5.420 sec/step, loss=0.07496, avg_loss=0.07477]\n",
      "Step 511689  [5.402 sec/step, loss=0.07334, avg_loss=0.07475]\n",
      "Step 511690  [5.438 sec/step, loss=0.06690, avg_loss=0.07466]\n",
      "Step 511691  [5.446 sec/step, loss=0.07688, avg_loss=0.07468]\n",
      "Step 511692  [5.452 sec/step, loss=0.07622, avg_loss=0.07470]\n",
      "Step 511693  [5.446 sec/step, loss=0.07316, avg_loss=0.07468]\n",
      "Step 511694  [5.435 sec/step, loss=0.07347, avg_loss=0.07468]\n",
      "Step 511695  [5.433 sec/step, loss=0.07555, avg_loss=0.07467]\n",
      "Step 511696  [5.429 sec/step, loss=0.07607, avg_loss=0.07468]\n",
      "Step 511697  [5.428 sec/step, loss=0.07726, avg_loss=0.07468]\n",
      "Step 511698  [5.435 sec/step, loss=0.07500, avg_loss=0.07467]\n",
      "Step 511699  [5.472 sec/step, loss=0.07441, avg_loss=0.07474]\n",
      "Step 511700  [5.461 sec/step, loss=0.07481, avg_loss=0.07472]\n",
      "Writing summary at step: 511700\n",
      "Step 511701  [5.467 sec/step, loss=0.07357, avg_loss=0.07471]\n",
      "Step 511702  [5.492 sec/step, loss=0.07510, avg_loss=0.07472]\n",
      "Step 511703  [5.505 sec/step, loss=0.07757, avg_loss=0.07474]\n",
      "Generated 32 batches of size 32 in 2.417 sec\n",
      "Step 511704  [5.525 sec/step, loss=0.07451, avg_loss=0.07474]\n",
      "Step 511705  [5.531 sec/step, loss=0.07623, avg_loss=0.07475]\n",
      "Step 511706  [5.478 sec/step, loss=0.07681, avg_loss=0.07484]\n",
      "Step 511707  [5.460 sec/step, loss=0.07544, avg_loss=0.07484]\n",
      "Step 511708  [5.460 sec/step, loss=0.07470, avg_loss=0.07482]\n",
      "Step 511709  [5.436 sec/step, loss=0.07611, avg_loss=0.07485]\n",
      "Step 511710  [5.421 sec/step, loss=0.06645, avg_loss=0.07476]\n",
      "Step 511711  [5.410 sec/step, loss=0.07692, avg_loss=0.07477]\n",
      "Step 511712  [5.400 sec/step, loss=0.07586, avg_loss=0.07478]\n",
      "Step 511713  [5.393 sec/step, loss=0.07523, avg_loss=0.07476]\n",
      "Step 511714  [5.405 sec/step, loss=0.07445, avg_loss=0.07476]\n",
      "Step 511715  [5.412 sec/step, loss=0.07693, avg_loss=0.07480]\n",
      "Step 511716  [5.422 sec/step, loss=0.07483, avg_loss=0.07482]\n",
      "Step 511717  [5.417 sec/step, loss=0.07643, avg_loss=0.07481]\n",
      "Step 511718  [5.419 sec/step, loss=0.07668, avg_loss=0.07485]\n",
      "Step 511719  [5.436 sec/step, loss=0.07435, avg_loss=0.07487]\n",
      "Step 511720  [5.442 sec/step, loss=0.07397, avg_loss=0.07485]\n",
      "Step 511721  [5.434 sec/step, loss=0.07416, avg_loss=0.07482]\n",
      "Step 511722  [5.431 sec/step, loss=0.07720, avg_loss=0.07482]\n",
      "Step 511723  [5.422 sec/step, loss=0.06508, avg_loss=0.07472]\n",
      "Step 511724  [5.462 sec/step, loss=0.06638, avg_loss=0.07464]\n",
      "Step 511725  [5.452 sec/step, loss=0.07314, avg_loss=0.07460]\n",
      "Step 511726  [5.445 sec/step, loss=0.07645, avg_loss=0.07460]\n",
      "Step 511727  [5.435 sec/step, loss=0.07357, avg_loss=0.07456]\n",
      "Step 511728  [5.411 sec/step, loss=0.07526, avg_loss=0.07457]\n",
      "Step 511729  [5.430 sec/step, loss=0.07619, avg_loss=0.07459]\n",
      "Step 511730  [5.427 sec/step, loss=0.07589, avg_loss=0.07457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 511731  [5.389 sec/step, loss=0.07090, avg_loss=0.07454]\n",
      "Step 511732  [5.398 sec/step, loss=0.07675, avg_loss=0.07455]\n",
      "Step 511733  [5.407 sec/step, loss=0.07549, avg_loss=0.07459]\n",
      "Step 511734  [5.412 sec/step, loss=0.07766, avg_loss=0.07461]\n",
      "Step 511735  [5.434 sec/step, loss=0.07608, avg_loss=0.07462]\n",
      "Generated 32 batches of size 32 in 2.408 sec\n",
      "Step 511736  [5.435 sec/step, loss=0.07675, avg_loss=0.07462]\n",
      "Step 511737  [5.454 sec/step, loss=0.07670, avg_loss=0.07471]\n",
      "Step 511738  [5.451 sec/step, loss=0.07445, avg_loss=0.07471]\n",
      "Step 511739  [5.451 sec/step, loss=0.07639, avg_loss=0.07473]\n",
      "Step 511740  [5.439 sec/step, loss=0.07250, avg_loss=0.07468]\n",
      "Step 511741  [5.372 sec/step, loss=0.07187, avg_loss=0.07473]\n",
      "Step 511742  [5.383 sec/step, loss=0.07627, avg_loss=0.07477]\n",
      "Step 511743  [5.409 sec/step, loss=0.07477, avg_loss=0.07476]\n",
      "Step 511744  [5.427 sec/step, loss=0.07645, avg_loss=0.07478]\n",
      "Step 511745  [5.420 sec/step, loss=0.07591, avg_loss=0.07481]\n",
      "Step 511746  [5.406 sec/step, loss=0.07316, avg_loss=0.07478]\n",
      "Step 511747  [5.407 sec/step, loss=0.07747, avg_loss=0.07479]\n",
      "Step 511748  [5.396 sec/step, loss=0.07511, avg_loss=0.07480]\n",
      "Step 511749  [5.386 sec/step, loss=0.07700, avg_loss=0.07482]\n",
      "Step 511750  [5.396 sec/step, loss=0.07514, avg_loss=0.07481]\n",
      "Step 511751  [5.411 sec/step, loss=0.07243, avg_loss=0.07481]\n",
      "Step 511752  [5.396 sec/step, loss=0.07508, avg_loss=0.07480]\n",
      "Step 511753  [5.394 sec/step, loss=0.07200, avg_loss=0.07477]\n",
      "Step 511754  [5.377 sec/step, loss=0.07289, avg_loss=0.07473]\n",
      "Step 511755  [5.376 sec/step, loss=0.07620, avg_loss=0.07477]\n",
      "Step 511756  [5.383 sec/step, loss=0.07723, avg_loss=0.07478]\n",
      "Step 511757  [5.387 sec/step, loss=0.07382, avg_loss=0.07477]\n",
      "Step 511758  [5.340 sec/step, loss=0.07602, avg_loss=0.07486]\n",
      "Step 511759  [5.355 sec/step, loss=0.07613, avg_loss=0.07490]\n",
      "Step 511760  [5.351 sec/step, loss=0.07457, avg_loss=0.07491]\n",
      "Step 511761  [5.355 sec/step, loss=0.07684, avg_loss=0.07493]\n",
      "Step 511762  [5.333 sec/step, loss=0.07484, avg_loss=0.07494]\n",
      "Step 511763  [5.330 sec/step, loss=0.07724, avg_loss=0.07494]\n",
      "Step 511764  [5.369 sec/step, loss=0.06800, avg_loss=0.07487]\n",
      "Step 511765  [5.380 sec/step, loss=0.07694, avg_loss=0.07487]\n",
      "Step 511766  [5.399 sec/step, loss=0.07656, avg_loss=0.07496]\n",
      "Step 511767  [5.403 sec/step, loss=0.07673, avg_loss=0.07496]\n",
      "Generated 32 batches of size 32 in 2.733 sec\n",
      "Step 511768  [5.388 sec/step, loss=0.06787, avg_loss=0.07489]\n",
      "Step 511769  [5.395 sec/step, loss=0.07570, avg_loss=0.07489]\n",
      "Step 511770  [5.413 sec/step, loss=0.07430, avg_loss=0.07486]\n",
      "Step 511771  [5.418 sec/step, loss=0.07594, avg_loss=0.07488]\n",
      "Step 511772  [5.417 sec/step, loss=0.07497, avg_loss=0.07488]\n",
      "Step 511773  [5.380 sec/step, loss=0.07477, avg_loss=0.07489]\n",
      "Step 511774  [5.400 sec/step, loss=0.07489, avg_loss=0.07490]\n",
      "Step 511775  [5.401 sec/step, loss=0.07728, avg_loss=0.07490]\n",
      "Step 511776  [5.404 sec/step, loss=0.07315, avg_loss=0.07487]\n",
      "Step 511777  [5.387 sec/step, loss=0.07436, avg_loss=0.07484]\n",
      "Step 511778  [5.399 sec/step, loss=0.07707, avg_loss=0.07486]\n",
      "Step 511779  [5.399 sec/step, loss=0.07425, avg_loss=0.07486]\n",
      "Step 511780  [5.386 sec/step, loss=0.07592, avg_loss=0.07485]\n",
      "Step 511781  [5.376 sec/step, loss=0.07666, avg_loss=0.07484]\n",
      "Step 511782  [5.382 sec/step, loss=0.07447, avg_loss=0.07484]\n",
      "Step 511783  [5.372 sec/step, loss=0.07576, avg_loss=0.07485]\n",
      "Step 511784  [5.384 sec/step, loss=0.07667, avg_loss=0.07485]\n",
      "Step 511785  [5.404 sec/step, loss=0.07708, avg_loss=0.07488]\n",
      "Step 511786  [5.437 sec/step, loss=0.07013, avg_loss=0.07480]\n",
      "Step 511787  [5.428 sec/step, loss=0.07539, avg_loss=0.07480]\n",
      "Step 511788  [5.417 sec/step, loss=0.07552, avg_loss=0.07481]\n",
      "Step 511789  [5.430 sec/step, loss=0.07551, avg_loss=0.07483]\n",
      "Step 511790  [5.376 sec/step, loss=0.07622, avg_loss=0.07493]\n",
      "Step 511791  [5.370 sec/step, loss=0.07704, avg_loss=0.07493]\n",
      "Step 511792  [5.383 sec/step, loss=0.07539, avg_loss=0.07492]\n",
      "Step 511793  [5.376 sec/step, loss=0.06691, avg_loss=0.07486]\n",
      "Step 511794  [5.386 sec/step, loss=0.07329, avg_loss=0.07485]\n",
      "Step 511795  [5.373 sec/step, loss=0.07229, avg_loss=0.07482]\n",
      "Step 511796  [5.368 sec/step, loss=0.07513, avg_loss=0.07481]\n",
      "Step 511797  [5.366 sec/step, loss=0.07697, avg_loss=0.07481]\n",
      "Step 511798  [5.369 sec/step, loss=0.07564, avg_loss=0.07482]\n",
      "Step 511799  [5.336 sec/step, loss=0.07213, avg_loss=0.07479]\n",
      "Generated 32 batches of size 32 in 2.379 sec\n",
      "Step 511800  [5.353 sec/step, loss=0.07710, avg_loss=0.07482]\n",
      "Writing summary at step: 511800\n",
      "Step 511801  [5.354 sec/step, loss=0.07458, avg_loss=0.07483]\n",
      "Step 511802  [5.331 sec/step, loss=0.07553, avg_loss=0.07483]\n",
      "Step 511803  [5.321 sec/step, loss=0.07588, avg_loss=0.07481]\n",
      "Step 511804  [5.306 sec/step, loss=0.07626, avg_loss=0.07483]\n",
      "Step 511805  [5.299 sec/step, loss=0.07638, avg_loss=0.07483]\n",
      "Step 511806  [5.329 sec/step, loss=0.07394, avg_loss=0.07480]\n",
      "Step 511807  [5.339 sec/step, loss=0.07621, avg_loss=0.07481]\n",
      "Step 511808  [5.323 sec/step, loss=0.07276, avg_loss=0.07479]\n",
      "Step 511809  [5.315 sec/step, loss=0.07530, avg_loss=0.07478]\n",
      "Step 511810  [5.320 sec/step, loss=0.07150, avg_loss=0.07483]\n",
      "Step 511811  [5.304 sec/step, loss=0.07483, avg_loss=0.07481]\n",
      "Step 511812  [5.300 sec/step, loss=0.07647, avg_loss=0.07482]\n",
      "Step 511813  [5.303 sec/step, loss=0.07336, avg_loss=0.07480]\n",
      "Step 511814  [5.298 sec/step, loss=0.07488, avg_loss=0.07480]\n",
      "Step 511815  [5.293 sec/step, loss=0.07530, avg_loss=0.07479]\n",
      "Step 511816  [5.305 sec/step, loss=0.07716, avg_loss=0.07481]\n",
      "Step 511817  [5.327 sec/step, loss=0.07397, avg_loss=0.07479]\n",
      "Step 511818  [5.321 sec/step, loss=0.07491, avg_loss=0.07477]\n",
      "Step 511819  [5.328 sec/step, loss=0.07604, avg_loss=0.07479]\n",
      "Step 511820  [5.327 sec/step, loss=0.07656, avg_loss=0.07481]\n",
      "Step 511821  [5.335 sec/step, loss=0.07243, avg_loss=0.07480]\n",
      "Step 511822  [5.318 sec/step, loss=0.07432, avg_loss=0.07477]\n",
      "Step 511823  [5.348 sec/step, loss=0.07812, avg_loss=0.07490]\n",
      "Step 511824  [5.348 sec/step, loss=0.06703, avg_loss=0.07490]\n",
      "Step 511825  [5.357 sec/step, loss=0.07691, avg_loss=0.07494]\n",
      "Step 511826  [5.365 sec/step, loss=0.07589, avg_loss=0.07494]\n",
      "Step 511827  [5.360 sec/step, loss=0.07512, avg_loss=0.07495]\n",
      "Step 511828  [5.366 sec/step, loss=0.07370, avg_loss=0.07494]\n",
      "Step 511829  [5.333 sec/step, loss=0.06673, avg_loss=0.07484]\n",
      "Step 511830  [5.338 sec/step, loss=0.07612, avg_loss=0.07484]\n",
      "Generated 32 batches of size 32 in 2.404 sec\n",
      "Step 511831  [5.374 sec/step, loss=0.07428, avg_loss=0.07488]\n",
      "Step 511832  [5.365 sec/step, loss=0.07648, avg_loss=0.07487]\n",
      "Step 511833  [5.373 sec/step, loss=0.07711, avg_loss=0.07489]\n",
      "Step 511834  [5.359 sec/step, loss=0.07307, avg_loss=0.07484]\n",
      "Step 511835  [5.345 sec/step, loss=0.07665, avg_loss=0.07485]\n",
      "Step 511836  [5.331 sec/step, loss=0.07712, avg_loss=0.07485]\n",
      "Step 511837  [5.340 sec/step, loss=0.07746, avg_loss=0.07486]\n",
      "Step 511838  [5.359 sec/step, loss=0.07713, avg_loss=0.07489]\n",
      "Step 511839  [5.364 sec/step, loss=0.07399, avg_loss=0.07486]\n",
      "Step 511840  [5.370 sec/step, loss=0.07688, avg_loss=0.07491]\n",
      "Step 511841  [5.383 sec/step, loss=0.07607, avg_loss=0.07495]\n",
      "Step 511842  [5.417 sec/step, loss=0.07398, avg_loss=0.07493]\n",
      "Step 511843  [5.400 sec/step, loss=0.07726, avg_loss=0.07495]\n",
      "Step 511844  [5.399 sec/step, loss=0.07702, avg_loss=0.07496]\n",
      "Step 511845  [5.406 sec/step, loss=0.07631, avg_loss=0.07496]\n",
      "Step 511846  [5.422 sec/step, loss=0.07514, avg_loss=0.07498]\n",
      "Step 511847  [5.423 sec/step, loss=0.07585, avg_loss=0.07497]\n",
      "Step 511848  [5.445 sec/step, loss=0.07462, avg_loss=0.07496]\n",
      "Step 511849  [5.481 sec/step, loss=0.06648, avg_loss=0.07485]\n",
      "Step 511850  [5.473 sec/step, loss=0.07673, avg_loss=0.07487]\n",
      "Step 511851  [5.482 sec/step, loss=0.07662, avg_loss=0.07491]\n",
      "Step 511852  [5.491 sec/step, loss=0.07646, avg_loss=0.07493]\n",
      "Step 511853  [5.491 sec/step, loss=0.07276, avg_loss=0.07493]\n",
      "Step 511854  [5.510 sec/step, loss=0.07747, avg_loss=0.07498]\n",
      "Step 511855  [5.506 sec/step, loss=0.07507, avg_loss=0.07497]\n",
      "Step 511856  [5.494 sec/step, loss=0.07475, avg_loss=0.07494]\n",
      "Step 511857  [5.502 sec/step, loss=0.07678, avg_loss=0.07497]\n",
      "Step 511858  [5.489 sec/step, loss=0.07216, avg_loss=0.07493]\n",
      "Step 511859  [5.484 sec/step, loss=0.07564, avg_loss=0.07493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 511860  [5.480 sec/step, loss=0.07577, avg_loss=0.07494]\n",
      "Step 511861  [5.464 sec/step, loss=0.06668, avg_loss=0.07484]\n",
      "Step 511862  [5.466 sec/step, loss=0.07318, avg_loss=0.07482]\n",
      "Generated 32 batches of size 32 in 2.439 sec\n",
      "Step 511863  [5.467 sec/step, loss=0.07679, avg_loss=0.07482]\n",
      "Step 511864  [5.422 sec/step, loss=0.07638, avg_loss=0.07490]\n",
      "Step 511865  [5.412 sec/step, loss=0.07497, avg_loss=0.07488]\n",
      "Step 511866  [5.425 sec/step, loss=0.07675, avg_loss=0.07489]\n",
      "Step 511867  [5.410 sec/step, loss=0.07500, avg_loss=0.07487]\n",
      "Step 511868  [5.434 sec/step, loss=0.07726, avg_loss=0.07496]\n",
      "Step 511869  [5.421 sec/step, loss=0.07346, avg_loss=0.07494]\n",
      "Step 511870  [5.408 sec/step, loss=0.07726, avg_loss=0.07497]\n",
      "Step 511871  [5.407 sec/step, loss=0.07503, avg_loss=0.07496]\n",
      "Step 511872  [5.393 sec/step, loss=0.07275, avg_loss=0.07494]\n",
      "Step 511873  [5.409 sec/step, loss=0.07762, avg_loss=0.07497]\n",
      "Step 511874  [5.388 sec/step, loss=0.07538, avg_loss=0.07497]\n",
      "Step 511875  [5.363 sec/step, loss=0.07317, avg_loss=0.07493]\n",
      "Step 511876  [5.361 sec/step, loss=0.07665, avg_loss=0.07497]\n",
      "Step 511877  [5.369 sec/step, loss=0.07535, avg_loss=0.07498]\n",
      "Step 511878  [5.368 sec/step, loss=0.07650, avg_loss=0.07497]\n",
      "Step 511879  [5.384 sec/step, loss=0.07602, avg_loss=0.07499]\n",
      "Step 511880  [5.396 sec/step, loss=0.07673, avg_loss=0.07500]\n",
      "Step 511881  [5.407 sec/step, loss=0.07731, avg_loss=0.07500]\n",
      "Step 511882  [5.411 sec/step, loss=0.07463, avg_loss=0.07500]\n",
      "Step 511883  [5.416 sec/step, loss=0.07653, avg_loss=0.07501]\n",
      "Step 511884  [5.403 sec/step, loss=0.07494, avg_loss=0.07499]\n",
      "Step 511885  [5.388 sec/step, loss=0.07543, avg_loss=0.07498]\n",
      "Step 511886  [5.350 sec/step, loss=0.07562, avg_loss=0.07503]\n",
      "Step 511887  [5.353 sec/step, loss=0.07482, avg_loss=0.07503]\n",
      "Step 511888  [5.364 sec/step, loss=0.07546, avg_loss=0.07503]\n",
      "Step 511889  [5.353 sec/step, loss=0.07597, avg_loss=0.07503]\n",
      "Step 511890  [5.369 sec/step, loss=0.07499, avg_loss=0.07502]\n",
      "Step 511891  [5.359 sec/step, loss=0.07615, avg_loss=0.07501]\n",
      "Step 511892  [5.370 sec/step, loss=0.07398, avg_loss=0.07500]\n",
      "Step 511893  [5.387 sec/step, loss=0.07574, avg_loss=0.07508]\n",
      "Step 511894  [5.376 sec/step, loss=0.07339, avg_loss=0.07508]\n",
      "Generated 32 batches of size 32 in 2.369 sec\n",
      "Step 511895  [5.404 sec/step, loss=0.07730, avg_loss=0.07513]\n",
      "Step 511896  [5.405 sec/step, loss=0.07449, avg_loss=0.07513]\n",
      "Step 511897  [5.414 sec/step, loss=0.07594, avg_loss=0.07512]\n",
      "Step 511898  [5.395 sec/step, loss=0.07528, avg_loss=0.07511]\n",
      "Step 511899  [5.417 sec/step, loss=0.07687, avg_loss=0.07516]\n",
      "Step 511900  [5.454 sec/step, loss=0.06794, avg_loss=0.07507]\n",
      "Writing summary at step: 511900\n",
      "Step 511901  [5.451 sec/step, loss=0.07183, avg_loss=0.07504]\n",
      "Step 511902  [5.429 sec/step, loss=0.06730, avg_loss=0.07496]\n",
      "Step 511903  [5.424 sec/step, loss=0.07579, avg_loss=0.07496]\n",
      "Step 511904  [5.420 sec/step, loss=0.07355, avg_loss=0.07493]\n",
      "Step 511905  [5.423 sec/step, loss=0.07595, avg_loss=0.07493]\n",
      "Step 511906  [5.412 sec/step, loss=0.07598, avg_loss=0.07495]\n",
      "Step 511907  [5.405 sec/step, loss=0.07637, avg_loss=0.07495]\n",
      "Step 511908  [5.431 sec/step, loss=0.07645, avg_loss=0.07499]\n",
      "Step 511909  [5.458 sec/step, loss=0.07593, avg_loss=0.07499]\n",
      "Step 511910  [5.480 sec/step, loss=0.07708, avg_loss=0.07505]\n",
      "Step 511911  [5.482 sec/step, loss=0.07402, avg_loss=0.07504]\n",
      "Step 511912  [5.493 sec/step, loss=0.07718, avg_loss=0.07505]\n",
      "Step 511913  [5.494 sec/step, loss=0.07565, avg_loss=0.07507]\n",
      "Step 511914  [5.484 sec/step, loss=0.07505, avg_loss=0.07507]\n",
      "Step 511915  [5.481 sec/step, loss=0.07672, avg_loss=0.07509]\n",
      "Step 511916  [5.469 sec/step, loss=0.07631, avg_loss=0.07508]\n",
      "Step 511917  [5.443 sec/step, loss=0.07321, avg_loss=0.07507]\n",
      "Step 511918  [5.430 sec/step, loss=0.06665, avg_loss=0.07499]\n",
      "Step 511919  [5.425 sec/step, loss=0.07565, avg_loss=0.07498]\n",
      "Step 511920  [5.467 sec/step, loss=0.06636, avg_loss=0.07488]\n",
      "Step 511921  [5.457 sec/step, loss=0.07099, avg_loss=0.07487]\n",
      "Step 511922  [5.460 sec/step, loss=0.07478, avg_loss=0.07487]\n",
      "Step 511923  [5.444 sec/step, loss=0.07544, avg_loss=0.07485]\n",
      "Step 511924  [5.401 sec/step, loss=0.07645, avg_loss=0.07494]\n",
      "Step 511925  [5.388 sec/step, loss=0.07442, avg_loss=0.07491]\n",
      "Generated 32 batches of size 32 in 2.387 sec\n",
      "Step 511926  [5.399 sec/step, loss=0.07727, avg_loss=0.07493]\n",
      "Step 511927  [5.424 sec/step, loss=0.07653, avg_loss=0.07494]\n",
      "Step 511928  [5.419 sec/step, loss=0.07516, avg_loss=0.07496]\n",
      "Step 511929  [5.444 sec/step, loss=0.07689, avg_loss=0.07506]\n",
      "Step 511930  [5.443 sec/step, loss=0.07614, avg_loss=0.07506]\n",
      "Step 511931  [5.431 sec/step, loss=0.07465, avg_loss=0.07506]\n",
      "Step 511932  [5.434 sec/step, loss=0.07537, avg_loss=0.07505]\n",
      "Step 511933  [5.430 sec/step, loss=0.07588, avg_loss=0.07504]\n",
      "Step 511934  [5.420 sec/step, loss=0.07271, avg_loss=0.07504]\n",
      "Step 511935  [5.416 sec/step, loss=0.07404, avg_loss=0.07501]\n",
      "Step 511936  [5.405 sec/step, loss=0.07131, avg_loss=0.07495]\n",
      "Step 511937  [5.395 sec/step, loss=0.07287, avg_loss=0.07491]\n",
      "Step 511938  [5.397 sec/step, loss=0.07678, avg_loss=0.07490]\n",
      "Step 511939  [5.402 sec/step, loss=0.07561, avg_loss=0.07492]\n",
      "Step 511940  [5.422 sec/step, loss=0.07416, avg_loss=0.07489]\n",
      "Step 511941  [5.416 sec/step, loss=0.07502, avg_loss=0.07488]\n",
      "Step 511942  [5.397 sec/step, loss=0.07770, avg_loss=0.07492]\n",
      "Step 511943  [5.385 sec/step, loss=0.07613, avg_loss=0.07491]\n",
      "Step 511944  [5.356 sec/step, loss=0.06621, avg_loss=0.07480]\n",
      "Step 511945  [5.357 sec/step, loss=0.07621, avg_loss=0.07480]\n",
      "Step 511946  [5.355 sec/step, loss=0.07636, avg_loss=0.07481]\n",
      "Step 511947  [5.354 sec/step, loss=0.07489, avg_loss=0.07480]\n",
      "Step 511948  [5.344 sec/step, loss=0.07757, avg_loss=0.07483]\n",
      "Step 511949  [5.298 sec/step, loss=0.07571, avg_loss=0.07492]\n",
      "Step 511950  [5.291 sec/step, loss=0.07618, avg_loss=0.07492]\n",
      "Step 511951  [5.293 sec/step, loss=0.07685, avg_loss=0.07492]\n",
      "Step 511952  [5.287 sec/step, loss=0.07463, avg_loss=0.07490]\n",
      "Step 511953  [5.279 sec/step, loss=0.07298, avg_loss=0.07490]\n",
      "Step 511954  [5.284 sec/step, loss=0.07494, avg_loss=0.07488]\n",
      "Step 511955  [5.341 sec/step, loss=0.06736, avg_loss=0.07480]\n",
      "Step 511956  [5.349 sec/step, loss=0.07583, avg_loss=0.07481]\n",
      "Step 511957  [5.345 sec/step, loss=0.07295, avg_loss=0.07477]\n",
      "Generated 32 batches of size 32 in 2.524 sec\n",
      "Step 511958  [5.360 sec/step, loss=0.07559, avg_loss=0.07481]\n",
      "Step 511959  [5.362 sec/step, loss=0.07646, avg_loss=0.07482]\n",
      "Step 511960  [5.374 sec/step, loss=0.07704, avg_loss=0.07483]\n",
      "Step 511961  [5.385 sec/step, loss=0.07506, avg_loss=0.07491]\n",
      "Step 511962  [5.393 sec/step, loss=0.07769, avg_loss=0.07496]\n",
      "Step 511963  [5.394 sec/step, loss=0.07507, avg_loss=0.07494]\n",
      "Step 511964  [5.388 sec/step, loss=0.07533, avg_loss=0.07493]\n",
      "Step 511965  [5.378 sec/step, loss=0.07331, avg_loss=0.07491]\n",
      "Step 511966  [5.382 sec/step, loss=0.07679, avg_loss=0.07491]\n",
      "Step 511967  [5.387 sec/step, loss=0.07214, avg_loss=0.07488]\n",
      "Step 511968  [5.389 sec/step, loss=0.07745, avg_loss=0.07489]\n",
      "Step 511969  [5.400 sec/step, loss=0.07474, avg_loss=0.07490]\n",
      "Step 511970  [5.401 sec/step, loss=0.07507, avg_loss=0.07488]\n",
      "Step 511971  [5.402 sec/step, loss=0.07625, avg_loss=0.07489]\n",
      "Step 511972  [5.401 sec/step, loss=0.07304, avg_loss=0.07489]\n",
      "Step 511973  [5.390 sec/step, loss=0.07641, avg_loss=0.07488]\n",
      "Step 511974  [5.375 sec/step, loss=0.07360, avg_loss=0.07486]\n",
      "Step 511975  [5.387 sec/step, loss=0.07684, avg_loss=0.07490]\n",
      "Step 511976  [5.392 sec/step, loss=0.07574, avg_loss=0.07489]\n",
      "Step 511977  [5.398 sec/step, loss=0.07532, avg_loss=0.07489]\n",
      "Step 511978  [5.396 sec/step, loss=0.07567, avg_loss=0.07488]\n",
      "Step 511979  [5.379 sec/step, loss=0.07402, avg_loss=0.07486]\n",
      "Step 511980  [5.363 sec/step, loss=0.07647, avg_loss=0.07486]\n",
      "Step 511981  [5.345 sec/step, loss=0.07554, avg_loss=0.07484]\n",
      "Step 511982  [5.354 sec/step, loss=0.07746, avg_loss=0.07487]\n",
      "Step 511983  [5.332 sec/step, loss=0.06705, avg_loss=0.07477]\n",
      "Step 511984  [5.338 sec/step, loss=0.07660, avg_loss=0.07479]\n",
      "Step 511985  [5.341 sec/step, loss=0.07590, avg_loss=0.07480]\n",
      "Step 511986  [5.336 sec/step, loss=0.07607, avg_loss=0.07480]\n",
      "Step 511987  [5.333 sec/step, loss=0.07201, avg_loss=0.07477]\n",
      "Step 511988  [5.322 sec/step, loss=0.07674, avg_loss=0.07478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 511989  [5.335 sec/step, loss=0.07808, avg_loss=0.07481]\n",
      "Generated 32 batches of size 32 in 2.425 sec\n",
      "Step 511990  [5.351 sec/step, loss=0.07553, avg_loss=0.07481]\n",
      "Step 511991  [5.406 sec/step, loss=0.06758, avg_loss=0.07473]\n",
      "Step 511992  [5.389 sec/step, loss=0.07516, avg_loss=0.07474]\n",
      "Step 511993  [5.379 sec/step, loss=0.07563, avg_loss=0.07474]\n",
      "Step 511994  [5.417 sec/step, loss=0.07491, avg_loss=0.07475]\n",
      "Step 511995  [5.413 sec/step, loss=0.07765, avg_loss=0.07476]\n",
      "Step 511996  [5.422 sec/step, loss=0.07527, avg_loss=0.07476]\n",
      "Step 511997  [5.401 sec/step, loss=0.07520, avg_loss=0.07476]\n",
      "Step 511998  [5.420 sec/step, loss=0.07785, avg_loss=0.07478]\n",
      "Step 511999  [5.410 sec/step, loss=0.07324, avg_loss=0.07474]\n",
      "Step 512000  [5.364 sec/step, loss=0.07645, avg_loss=0.07483]\n",
      "Writing summary at step: 512000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-512000\n",
      "Saving audio and alignment...\n",
      "Input: ddaehhlii miin tsaaay or phaphuunddii zaddddaa kaandzii kii gaanthoon sae sovaan giiddarr bhii dhalak gijaa~________\n",
      "Step 512001  [5.384 sec/step, loss=0.07688, avg_loss=0.07488]\n",
      "Step 512002  [5.396 sec/step, loss=0.07463, avg_loss=0.07495]\n",
      "Step 512003  [5.385 sec/step, loss=0.07558, avg_loss=0.07495]\n",
      "Step 512004  [5.384 sec/step, loss=0.07651, avg_loss=0.07498]\n",
      "Step 512005  [5.390 sec/step, loss=0.07765, avg_loss=0.07500]\n",
      "Step 512006  [5.398 sec/step, loss=0.07471, avg_loss=0.07499]\n",
      "Step 512007  [5.409 sec/step, loss=0.07765, avg_loss=0.07500]\n",
      "Step 512008  [5.405 sec/step, loss=0.07751, avg_loss=0.07501]\n",
      "Step 512009  [5.377 sec/step, loss=0.07656, avg_loss=0.07502]\n",
      "Step 512010  [5.356 sec/step, loss=0.07530, avg_loss=0.07500]\n",
      "Step 512011  [5.364 sec/step, loss=0.07563, avg_loss=0.07501]\n",
      "Step 512012  [5.362 sec/step, loss=0.07546, avg_loss=0.07500]\n",
      "Step 512013  [5.356 sec/step, loss=0.07594, avg_loss=0.07500]\n",
      "Step 512014  [5.374 sec/step, loss=0.07547, avg_loss=0.07500]\n",
      "Step 512015  [5.383 sec/step, loss=0.07726, avg_loss=0.07501]\n",
      "Step 512016  [5.436 sec/step, loss=0.06714, avg_loss=0.07492]\n",
      "Step 512017  [5.445 sec/step, loss=0.07730, avg_loss=0.07496]\n",
      "Step 512018  [5.477 sec/step, loss=0.07757, avg_loss=0.07507]\n",
      "Step 512019  [5.469 sec/step, loss=0.07443, avg_loss=0.07506]\n",
      "Generated 32 batches of size 32 in 2.859 sec\n",
      "Step 512020  [5.406 sec/step, loss=0.06709, avg_loss=0.07506]\n",
      "Step 512021  [5.418 sec/step, loss=0.07705, avg_loss=0.07512]\n",
      "Step 512022  [5.424 sec/step, loss=0.07685, avg_loss=0.07514]\n",
      "Step 512023  [5.423 sec/step, loss=0.07672, avg_loss=0.07516]\n",
      "Step 512024  [5.412 sec/step, loss=0.07535, avg_loss=0.07515]\n",
      "Step 512025  [5.408 sec/step, loss=0.07218, avg_loss=0.07512]\n",
      "Step 512026  [5.397 sec/step, loss=0.07627, avg_loss=0.07511]\n",
      "Step 512027  [5.368 sec/step, loss=0.07230, avg_loss=0.07507]\n",
      "Step 512028  [5.370 sec/step, loss=0.07497, avg_loss=0.07507]\n",
      "Step 512029  [5.371 sec/step, loss=0.07786, avg_loss=0.07508]\n",
      "Step 512030  [5.357 sec/step, loss=0.07647, avg_loss=0.07508]\n",
      "Step 512031  [5.329 sec/step, loss=0.06768, avg_loss=0.07501]\n",
      "Step 512032  [5.320 sec/step, loss=0.07535, avg_loss=0.07501]\n",
      "Step 512033  [5.320 sec/step, loss=0.07593, avg_loss=0.07501]\n",
      "Step 512034  [5.383 sec/step, loss=0.06767, avg_loss=0.07496]\n",
      "Step 512035  [5.394 sec/step, loss=0.07693, avg_loss=0.07499]\n",
      "Step 512036  [5.404 sec/step, loss=0.07724, avg_loss=0.07505]\n",
      "Step 512037  [5.400 sec/step, loss=0.07657, avg_loss=0.07509]\n",
      "Step 512038  [5.388 sec/step, loss=0.07665, avg_loss=0.07509]\n",
      "Step 512039  [5.369 sec/step, loss=0.07477, avg_loss=0.07508]\n",
      "Step 512040  [5.335 sec/step, loss=0.07507, avg_loss=0.07509]\n",
      "Step 512041  [5.338 sec/step, loss=0.07271, avg_loss=0.07506]\n",
      "Step 512042  [5.336 sec/step, loss=0.07531, avg_loss=0.07504]\n",
      "Step 512043  [5.350 sec/step, loss=0.07508, avg_loss=0.07503]\n",
      "Step 512044  [5.352 sec/step, loss=0.07281, avg_loss=0.07510]\n",
      "Step 512045  [5.353 sec/step, loss=0.07712, avg_loss=0.07510]\n",
      "Step 512046  [5.367 sec/step, loss=0.07704, avg_loss=0.07511]\n",
      "Step 512047  [5.360 sec/step, loss=0.07597, avg_loss=0.07512]\n",
      "Step 512048  [5.362 sec/step, loss=0.07589, avg_loss=0.07511]\n",
      "Step 512049  [5.346 sec/step, loss=0.07255, avg_loss=0.07507]\n",
      "Step 512050  [5.338 sec/step, loss=0.07520, avg_loss=0.07506]\n",
      "Step 512051  [5.338 sec/step, loss=0.07730, avg_loss=0.07507]\n",
      "Generated 32 batches of size 32 in 2.334 sec\n",
      "Step 512052  [5.350 sec/step, loss=0.07659, avg_loss=0.07509]\n",
      "Step 512053  [5.364 sec/step, loss=0.07623, avg_loss=0.07512]\n",
      "Step 512054  [5.347 sec/step, loss=0.07334, avg_loss=0.07510]\n",
      "Step 512055  [5.297 sec/step, loss=0.07545, avg_loss=0.07519]\n",
      "Step 512056  [5.292 sec/step, loss=0.07479, avg_loss=0.07518]\n",
      "Step 512057  [5.294 sec/step, loss=0.07301, avg_loss=0.07518]\n",
      "Step 512058  [5.316 sec/step, loss=0.07416, avg_loss=0.07516]\n",
      "Step 512059  [5.332 sec/step, loss=0.07657, avg_loss=0.07516]\n",
      "Step 512060  [5.338 sec/step, loss=0.07729, avg_loss=0.07517]\n",
      "Step 512061  [5.346 sec/step, loss=0.07480, avg_loss=0.07516]\n",
      "Step 512062  [5.338 sec/step, loss=0.07638, avg_loss=0.07515]\n",
      "Step 512063  [5.345 sec/step, loss=0.07650, avg_loss=0.07516]\n",
      "Step 512064  [5.337 sec/step, loss=0.07371, avg_loss=0.07515]\n",
      "Step 512065  [5.346 sec/step, loss=0.07207, avg_loss=0.07514]\n",
      "Step 512066  [5.333 sec/step, loss=0.07643, avg_loss=0.07513]\n",
      "Step 512067  [5.361 sec/step, loss=0.07302, avg_loss=0.07514]\n",
      "Step 512068  [5.358 sec/step, loss=0.07676, avg_loss=0.07513]\n",
      "Step 512069  [5.361 sec/step, loss=0.07237, avg_loss=0.07511]\n",
      "Step 512070  [5.330 sec/step, loss=0.06795, avg_loss=0.07504]\n",
      "Step 512071  [5.331 sec/step, loss=0.07545, avg_loss=0.07503]\n",
      "Step 512072  [5.323 sec/step, loss=0.07284, avg_loss=0.07503]\n",
      "Step 512073  [5.335 sec/step, loss=0.07715, avg_loss=0.07504]\n",
      "Step 512074  [5.348 sec/step, loss=0.07568, avg_loss=0.07506]\n",
      "Step 512075  [5.353 sec/step, loss=0.07551, avg_loss=0.07504]\n",
      "Step 512076  [5.345 sec/step, loss=0.07635, avg_loss=0.07505]\n",
      "Step 512077  [5.385 sec/step, loss=0.06670, avg_loss=0.07496]\n",
      "Step 512078  [5.383 sec/step, loss=0.07681, avg_loss=0.07498]\n",
      "Step 512079  [5.387 sec/step, loss=0.07372, avg_loss=0.07497]\n",
      "Step 512080  [5.388 sec/step, loss=0.07435, avg_loss=0.07495]\n",
      "Step 512081  [5.404 sec/step, loss=0.07759, avg_loss=0.07497]\n",
      "Step 512082  [5.395 sec/step, loss=0.07640, avg_loss=0.07496]\n",
      "Step 512083  [5.403 sec/step, loss=0.07547, avg_loss=0.07505]\n",
      "Generated 32 batches of size 32 in 2.517 sec\n",
      "Step 512084  [5.399 sec/step, loss=0.07608, avg_loss=0.07504]\n",
      "Step 512085  [5.393 sec/step, loss=0.07548, avg_loss=0.07504]\n",
      "Step 512086  [5.395 sec/step, loss=0.07701, avg_loss=0.07505]\n",
      "Step 512087  [5.413 sec/step, loss=0.07674, avg_loss=0.07509]\n",
      "Step 512088  [5.407 sec/step, loss=0.07513, avg_loss=0.07508]\n",
      "Step 512089  [5.411 sec/step, loss=0.07518, avg_loss=0.07505]\n",
      "Step 512090  [5.388 sec/step, loss=0.07733, avg_loss=0.07507]\n",
      "Step 512091  [5.355 sec/step, loss=0.07644, avg_loss=0.07515]\n",
      "Step 512092  [5.334 sec/step, loss=0.07256, avg_loss=0.07513]\n",
      "Step 512093  [5.356 sec/step, loss=0.07662, avg_loss=0.07514]\n",
      "Step 512094  [5.341 sec/step, loss=0.07729, avg_loss=0.07516]\n",
      "Step 512095  [5.327 sec/step, loss=0.07663, avg_loss=0.07515]\n",
      "Step 512096  [5.337 sec/step, loss=0.07531, avg_loss=0.07515]\n",
      "Step 512097  [5.333 sec/step, loss=0.07488, avg_loss=0.07515]\n",
      "Step 512098  [5.330 sec/step, loss=0.07702, avg_loss=0.07514]\n",
      "Step 512099  [5.332 sec/step, loss=0.07626, avg_loss=0.07517]\n",
      "Step 512100  [5.322 sec/step, loss=0.07633, avg_loss=0.07517]\n",
      "Writing summary at step: 512100\n",
      "Step 512101  [5.302 sec/step, loss=0.07445, avg_loss=0.07515]\n",
      "Step 512102  [5.294 sec/step, loss=0.07335, avg_loss=0.07513]\n",
      "Step 512103  [5.308 sec/step, loss=0.07664, avg_loss=0.07514]\n",
      "Step 512104  [5.331 sec/step, loss=0.07439, avg_loss=0.07512]\n",
      "Step 512105  [5.327 sec/step, loss=0.07514, avg_loss=0.07510]\n",
      "Step 512106  [5.294 sec/step, loss=0.07119, avg_loss=0.07506]\n",
      "Step 512107  [5.283 sec/step, loss=0.07447, avg_loss=0.07503]\n",
      "Step 512108  [5.299 sec/step, loss=0.07451, avg_loss=0.07500]\n",
      "Step 512109  [5.305 sec/step, loss=0.07551, avg_loss=0.07499]\n",
      "Step 512110  [5.325 sec/step, loss=0.07720, avg_loss=0.07501]\n",
      "Step 512111  [5.305 sec/step, loss=0.06672, avg_loss=0.07492]\n",
      "Step 512112  [5.296 sec/step, loss=0.07636, avg_loss=0.07493]\n",
      "Step 512113  [5.308 sec/step, loss=0.07634, avg_loss=0.07493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 512114  [5.309 sec/step, loss=0.07710, avg_loss=0.07495]\n",
      "Generated 32 batches of size 32 in 2.476 sec\n",
      "Step 512115  [5.300 sec/step, loss=0.07454, avg_loss=0.07492]\n",
      "Step 512116  [5.261 sec/step, loss=0.07545, avg_loss=0.07500]\n",
      "Step 512117  [5.253 sec/step, loss=0.07592, avg_loss=0.07499]\n",
      "Step 512118  [5.239 sec/step, loss=0.07255, avg_loss=0.07494]\n",
      "Step 512119  [5.229 sec/step, loss=0.07234, avg_loss=0.07492]\n",
      "Step 512120  [5.256 sec/step, loss=0.07795, avg_loss=0.07503]\n",
      "Step 512121  [5.247 sec/step, loss=0.07462, avg_loss=0.07500]\n",
      "Step 512122  [5.246 sec/step, loss=0.07683, avg_loss=0.07500]\n",
      "Step 512123  [5.249 sec/step, loss=0.07599, avg_loss=0.07500]\n",
      "Step 512124  [5.247 sec/step, loss=0.07440, avg_loss=0.07499]\n",
      "Step 512125  [5.257 sec/step, loss=0.07503, avg_loss=0.07502]\n",
      "Step 512126  [5.243 sec/step, loss=0.07220, avg_loss=0.07497]\n",
      "Step 512127  [5.271 sec/step, loss=0.07597, avg_loss=0.07501]\n",
      "Step 512128  [5.280 sec/step, loss=0.07511, avg_loss=0.07501]\n",
      "Step 512129  [5.271 sec/step, loss=0.07654, avg_loss=0.07500]\n",
      "Step 512130  [5.281 sec/step, loss=0.07521, avg_loss=0.07499]\n",
      "Step 512131  [5.293 sec/step, loss=0.07454, avg_loss=0.07506]\n",
      "Step 512132  [5.304 sec/step, loss=0.07549, avg_loss=0.07506]\n",
      "Step 512133  [5.298 sec/step, loss=0.07530, avg_loss=0.07505]\n",
      "Step 512134  [5.238 sec/step, loss=0.07470, avg_loss=0.07512]\n",
      "Step 512135  [5.234 sec/step, loss=0.07648, avg_loss=0.07512]\n",
      "Step 512136  [5.256 sec/step, loss=0.07605, avg_loss=0.07510]\n",
      "Step 512137  [5.263 sec/step, loss=0.07533, avg_loss=0.07509]\n",
      "Step 512138  [5.270 sec/step, loss=0.07680, avg_loss=0.07509]\n",
      "Step 512139  [5.285 sec/step, loss=0.07435, avg_loss=0.07509]\n",
      "Step 512140  [5.341 sec/step, loss=0.06571, avg_loss=0.07500]\n",
      "Step 512141  [5.356 sec/step, loss=0.07742, avg_loss=0.07504]\n",
      "Step 512142  [5.339 sec/step, loss=0.07485, avg_loss=0.07504]\n",
      "Step 512143  [5.319 sec/step, loss=0.07407, avg_loss=0.07503]\n",
      "Step 512144  [5.330 sec/step, loss=0.07674, avg_loss=0.07507]\n",
      "Step 512145  [5.308 sec/step, loss=0.06603, avg_loss=0.07496]\n",
      "Step 512146  [5.305 sec/step, loss=0.07693, avg_loss=0.07496]\n",
      "Generated 32 batches of size 32 in 2.451 sec\n",
      "Step 512147  [5.304 sec/step, loss=0.07403, avg_loss=0.07494]\n",
      "Step 512148  [5.307 sec/step, loss=0.07656, avg_loss=0.07494]\n",
      "Step 512149  [5.328 sec/step, loss=0.07726, avg_loss=0.07499]\n",
      "Step 512150  [5.336 sec/step, loss=0.07561, avg_loss=0.07499]\n",
      "Step 512151  [5.341 sec/step, loss=0.07611, avg_loss=0.07498]\n",
      "Step 512152  [5.322 sec/step, loss=0.07243, avg_loss=0.07494]\n",
      "Step 512153  [5.312 sec/step, loss=0.07269, avg_loss=0.07490]\n",
      "Step 512154  [5.326 sec/step, loss=0.07694, avg_loss=0.07494]\n",
      "Step 512155  [5.327 sec/step, loss=0.07655, avg_loss=0.07495]\n",
      "Step 512156  [5.324 sec/step, loss=0.07302, avg_loss=0.07493]\n",
      "Step 512157  [5.311 sec/step, loss=0.07276, avg_loss=0.07493]\n",
      "Step 512158  [5.297 sec/step, loss=0.07744, avg_loss=0.07496]\n",
      "Step 512159  [5.306 sec/step, loss=0.07419, avg_loss=0.07494]\n",
      "Step 512160  [5.303 sec/step, loss=0.07538, avg_loss=0.07492]\n",
      "Step 512161  [5.298 sec/step, loss=0.07593, avg_loss=0.07493]\n",
      "Step 512162  [5.282 sec/step, loss=0.06734, avg_loss=0.07484]\n",
      "Step 512163  [5.277 sec/step, loss=0.07690, avg_loss=0.07485]\n",
      "Step 512164  [5.283 sec/step, loss=0.07529, avg_loss=0.07486]\n",
      "Step 512165  [5.336 sec/step, loss=0.06735, avg_loss=0.07481]\n",
      "Step 512166  [5.328 sec/step, loss=0.07669, avg_loss=0.07482]\n",
      "Step 512167  [5.310 sec/step, loss=0.07736, avg_loss=0.07486]\n",
      "Step 512168  [5.300 sec/step, loss=0.07671, avg_loss=0.07486]\n",
      "Step 512169  [5.300 sec/step, loss=0.07591, avg_loss=0.07490]\n",
      "Step 512170  [5.309 sec/step, loss=0.07443, avg_loss=0.07496]\n",
      "Step 512171  [5.314 sec/step, loss=0.07772, avg_loss=0.07498]\n",
      "Step 512172  [5.344 sec/step, loss=0.07619, avg_loss=0.07502]\n",
      "Step 512173  [5.346 sec/step, loss=0.07547, avg_loss=0.07500]\n",
      "Step 512174  [5.337 sec/step, loss=0.07201, avg_loss=0.07496]\n",
      "Step 512175  [5.342 sec/step, loss=0.07768, avg_loss=0.07498]\n",
      "Step 512176  [5.342 sec/step, loss=0.07632, avg_loss=0.07498]\n",
      "Step 512177  [5.300 sec/step, loss=0.07593, avg_loss=0.07508]\n",
      "Step 512178  [5.286 sec/step, loss=0.07530, avg_loss=0.07506]\n",
      "Generated 32 batches of size 32 in 2.372 sec\n",
      "Step 512179  [5.298 sec/step, loss=0.07661, avg_loss=0.07509]\n",
      "Step 512180  [5.307 sec/step, loss=0.07790, avg_loss=0.07513]\n",
      "Step 512181  [5.290 sec/step, loss=0.07548, avg_loss=0.07510]\n",
      "Step 512182  [5.300 sec/step, loss=0.07362, avg_loss=0.07508]\n",
      "Step 512183  [5.306 sec/step, loss=0.07452, avg_loss=0.07507]\n",
      "Step 512184  [5.299 sec/step, loss=0.07453, avg_loss=0.07505]\n",
      "Step 512185  [5.305 sec/step, loss=0.07618, avg_loss=0.07506]\n",
      "Step 512186  [5.305 sec/step, loss=0.07697, avg_loss=0.07506]\n",
      "Step 512187  [5.301 sec/step, loss=0.07562, avg_loss=0.07505]\n",
      "Step 512188  [5.307 sec/step, loss=0.07704, avg_loss=0.07507]\n",
      "Step 512189  [5.341 sec/step, loss=0.06825, avg_loss=0.07500]\n",
      "Step 512190  [5.352 sec/step, loss=0.07373, avg_loss=0.07496]\n",
      "Step 512191  [5.348 sec/step, loss=0.07672, avg_loss=0.07496]\n",
      "Step 512192  [5.348 sec/step, loss=0.07513, avg_loss=0.07499]\n",
      "Step 512193  [5.348 sec/step, loss=0.07504, avg_loss=0.07497]\n",
      "Step 512194  [5.340 sec/step, loss=0.07361, avg_loss=0.07494]\n",
      "Step 512195  [5.365 sec/step, loss=0.07490, avg_loss=0.07492]\n",
      "Step 512196  [5.356 sec/step, loss=0.07570, avg_loss=0.07492]\n",
      "Step 512197  [5.367 sec/step, loss=0.07326, avg_loss=0.07491]\n",
      "Step 512198  [5.345 sec/step, loss=0.06733, avg_loss=0.07481]\n",
      "Step 512199  [5.356 sec/step, loss=0.07714, avg_loss=0.07482]\n",
      "Step 512200  [5.372 sec/step, loss=0.07558, avg_loss=0.07481]\n",
      "Writing summary at step: 512200\n",
      "Step 512201  [5.385 sec/step, loss=0.07696, avg_loss=0.07484]\n",
      "Step 512202  [5.405 sec/step, loss=0.07718, avg_loss=0.07487]\n",
      "Step 512203  [5.393 sec/step, loss=0.07502, avg_loss=0.07486]\n",
      "Step 512204  [5.369 sec/step, loss=0.07452, avg_loss=0.07486]\n",
      "Step 512205  [5.370 sec/step, loss=0.07558, avg_loss=0.07486]\n",
      "Step 512206  [5.374 sec/step, loss=0.07295, avg_loss=0.07488]\n",
      "Step 512207  [5.378 sec/step, loss=0.07603, avg_loss=0.07490]\n",
      "Step 512208  [5.349 sec/step, loss=0.07444, avg_loss=0.07490]\n",
      "Step 512209  [5.342 sec/step, loss=0.07593, avg_loss=0.07490]\n",
      "Generated 32 batches of size 32 in 2.759 sec\n",
      "Step 512210  [5.322 sec/step, loss=0.07213, avg_loss=0.07485]\n",
      "Step 512211  [5.343 sec/step, loss=0.07576, avg_loss=0.07494]\n",
      "Step 512212  [5.340 sec/step, loss=0.07507, avg_loss=0.07493]\n",
      "Step 512213  [5.337 sec/step, loss=0.07682, avg_loss=0.07493]\n",
      "Step 512214  [5.327 sec/step, loss=0.07438, avg_loss=0.07491]\n",
      "Step 512215  [5.329 sec/step, loss=0.07626, avg_loss=0.07492]\n",
      "Step 512216  [5.326 sec/step, loss=0.07641, avg_loss=0.07493]\n",
      "Step 512217  [5.315 sec/step, loss=0.07222, avg_loss=0.07490]\n",
      "Step 512218  [5.324 sec/step, loss=0.07677, avg_loss=0.07494]\n",
      "Step 512219  [5.337 sec/step, loss=0.07587, avg_loss=0.07497]\n",
      "Step 512220  [5.312 sec/step, loss=0.07503, avg_loss=0.07494]\n",
      "Step 512221  [5.321 sec/step, loss=0.07638, avg_loss=0.07496]\n",
      "Step 512222  [5.345 sec/step, loss=0.07466, avg_loss=0.07494]\n",
      "Step 512223  [5.340 sec/step, loss=0.07439, avg_loss=0.07492]\n",
      "Step 512224  [5.342 sec/step, loss=0.07246, avg_loss=0.07490]\n",
      "Step 512225  [5.362 sec/step, loss=0.07492, avg_loss=0.07490]\n",
      "Step 512226  [5.366 sec/step, loss=0.07383, avg_loss=0.07492]\n",
      "Step 512227  [5.340 sec/step, loss=0.07168, avg_loss=0.07488]\n",
      "Step 512228  [5.388 sec/step, loss=0.06720, avg_loss=0.07480]\n",
      "Step 512229  [5.387 sec/step, loss=0.07209, avg_loss=0.07475]\n",
      "Step 512230  [5.380 sec/step, loss=0.07437, avg_loss=0.07474]\n",
      "Step 512231  [5.387 sec/step, loss=0.07616, avg_loss=0.07476]\n",
      "Step 512232  [5.395 sec/step, loss=0.07690, avg_loss=0.07477]\n",
      "Step 512233  [5.381 sec/step, loss=0.06687, avg_loss=0.07469]\n",
      "Step 512234  [5.392 sec/step, loss=0.07653, avg_loss=0.07471]\n",
      "Step 512235  [5.397 sec/step, loss=0.07707, avg_loss=0.07471]\n",
      "Step 512236  [5.379 sec/step, loss=0.07584, avg_loss=0.07471]\n",
      "Step 512237  [5.383 sec/step, loss=0.07703, avg_loss=0.07473]\n",
      "Step 512238  [5.388 sec/step, loss=0.07728, avg_loss=0.07473]\n",
      "Step 512239  [5.367 sec/step, loss=0.07218, avg_loss=0.07471]\n",
      "Step 512240  [5.310 sec/step, loss=0.07467, avg_loss=0.07480]\n",
      "Step 512241  [5.308 sec/step, loss=0.07723, avg_loss=0.07480]\n",
      "Generated 32 batches of size 32 in 2.295 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 512242  [5.331 sec/step, loss=0.07671, avg_loss=0.07482]\n",
      "Step 512243  [5.345 sec/step, loss=0.07523, avg_loss=0.07483]\n",
      "Step 512244  [5.354 sec/step, loss=0.07520, avg_loss=0.07481]\n",
      "Step 512245  [5.366 sec/step, loss=0.07608, avg_loss=0.07492]\n",
      "Step 512246  [5.346 sec/step, loss=0.07426, avg_loss=0.07489]\n",
      "Step 512247  [5.344 sec/step, loss=0.07531, avg_loss=0.07490]\n",
      "Step 512248  [5.334 sec/step, loss=0.07551, avg_loss=0.07489]\n",
      "Step 512249  [5.320 sec/step, loss=0.07634, avg_loss=0.07488]\n",
      "Step 512250  [5.335 sec/step, loss=0.07718, avg_loss=0.07490]\n",
      "Step 512251  [5.325 sec/step, loss=0.07682, avg_loss=0.07490]\n",
      "Step 512252  [5.347 sec/step, loss=0.07558, avg_loss=0.07494]\n",
      "Step 512253  [5.359 sec/step, loss=0.07666, avg_loss=0.07498]\n",
      "Step 512254  [5.347 sec/step, loss=0.07542, avg_loss=0.07496]\n",
      "Step 512255  [5.337 sec/step, loss=0.07182, avg_loss=0.07491]\n",
      "Step 512256  [5.335 sec/step, loss=0.07612, avg_loss=0.07494]\n",
      "Step 512257  [5.350 sec/step, loss=0.07590, avg_loss=0.07498]\n",
      "Step 512258  [5.387 sec/step, loss=0.06588, avg_loss=0.07486]\n",
      "Step 512259  [5.361 sec/step, loss=0.07343, avg_loss=0.07485]\n",
      "Step 512260  [5.362 sec/step, loss=0.07580, avg_loss=0.07486]\n",
      "Step 512261  [5.363 sec/step, loss=0.07646, avg_loss=0.07486]\n",
      "Step 512262  [5.390 sec/step, loss=0.07740, avg_loss=0.07496]\n",
      "Step 512263  [5.408 sec/step, loss=0.07629, avg_loss=0.07496]\n",
      "Step 512264  [5.400 sec/step, loss=0.07260, avg_loss=0.07493]\n",
      "Step 512265  [5.334 sec/step, loss=0.06794, avg_loss=0.07494]\n",
      "Step 512266  [5.345 sec/step, loss=0.07739, avg_loss=0.07494]\n",
      "Step 512267  [5.332 sec/step, loss=0.07513, avg_loss=0.07492]\n",
      "Step 512268  [5.351 sec/step, loss=0.07439, avg_loss=0.07490]\n",
      "Step 512269  [5.363 sec/step, loss=0.07459, avg_loss=0.07488]\n",
      "Step 512270  [5.370 sec/step, loss=0.07498, avg_loss=0.07489]\n",
      "Step 512271  [5.356 sec/step, loss=0.07423, avg_loss=0.07485]\n",
      "Step 512272  [5.340 sec/step, loss=0.07676, avg_loss=0.07486]\n",
      "Step 512273  [5.341 sec/step, loss=0.07654, avg_loss=0.07487]\n",
      "Generated 32 batches of size 32 in 2.376 sec\n",
      "Step 512274  [5.360 sec/step, loss=0.07604, avg_loss=0.07491]\n",
      "Step 512275  [5.334 sec/step, loss=0.07399, avg_loss=0.07487]\n",
      "Step 512276  [5.335 sec/step, loss=0.07636, avg_loss=0.07487]\n",
      "Step 512277  [5.331 sec/step, loss=0.07339, avg_loss=0.07485]\n",
      "Step 512278  [5.332 sec/step, loss=0.07486, avg_loss=0.07484]\n",
      "Step 512279  [5.340 sec/step, loss=0.07695, avg_loss=0.07485]\n",
      "Step 512280  [5.325 sec/step, loss=0.07474, avg_loss=0.07482]\n",
      "Step 512281  [5.342 sec/step, loss=0.07644, avg_loss=0.07483]\n",
      "Step 512282  [5.329 sec/step, loss=0.07320, avg_loss=0.07482]\n",
      "Step 512283  [5.340 sec/step, loss=0.07510, avg_loss=0.07483]\n",
      "Step 512284  [5.345 sec/step, loss=0.07371, avg_loss=0.07482]\n",
      "Step 512285  [5.343 sec/step, loss=0.07158, avg_loss=0.07477]\n",
      "Step 512286  [5.351 sec/step, loss=0.07657, avg_loss=0.07477]\n",
      "Step 512287  [5.398 sec/step, loss=0.06625, avg_loss=0.07468]\n",
      "Step 512288  [5.398 sec/step, loss=0.07537, avg_loss=0.07466]\n",
      "Step 512289  [5.346 sec/step, loss=0.07489, avg_loss=0.07473]\n",
      "Step 512290  [5.317 sec/step, loss=0.07191, avg_loss=0.07471]\n",
      "Step 512291  [5.290 sec/step, loss=0.07247, avg_loss=0.07466]\n",
      "Step 512292  [5.284 sec/step, loss=0.06725, avg_loss=0.07459]\n",
      "Step 512293  [5.275 sec/step, loss=0.07483, avg_loss=0.07458]\n",
      "Step 512294  [5.288 sec/step, loss=0.07666, avg_loss=0.07461]\n",
      "Step 512295  [5.260 sec/step, loss=0.07621, avg_loss=0.07463]\n",
      "Step 512296  [5.255 sec/step, loss=0.07329, avg_loss=0.07460]\n",
      "Step 512297  [5.251 sec/step, loss=0.07486, avg_loss=0.07462]\n",
      "Step 512298  [5.260 sec/step, loss=0.07434, avg_loss=0.07469]\n",
      "Step 512299  [5.252 sec/step, loss=0.07640, avg_loss=0.07468]\n",
      "Step 512300  [5.250 sec/step, loss=0.07687, avg_loss=0.07470]\n",
      "Writing summary at step: 512300\n",
      "Step 512301  [5.247 sec/step, loss=0.07417, avg_loss=0.07467]\n",
      "Step 512302  [5.252 sec/step, loss=0.07768, avg_loss=0.07467]\n",
      "Step 512303  [5.259 sec/step, loss=0.07713, avg_loss=0.07469]\n",
      "Step 512304  [5.261 sec/step, loss=0.07613, avg_loss=0.07471]\n",
      "Generated 32 batches of size 32 in 2.393 sec\n",
      "Step 512305  [5.278 sec/step, loss=0.07638, avg_loss=0.07472]\n",
      "Step 512306  [5.272 sec/step, loss=0.07462, avg_loss=0.07473]\n",
      "Step 512307  [5.273 sec/step, loss=0.07664, avg_loss=0.07474]\n",
      "Step 512308  [5.305 sec/step, loss=0.07190, avg_loss=0.07471]\n",
      "Step 512309  [5.317 sec/step, loss=0.07686, avg_loss=0.07472]\n",
      "Step 512310  [5.320 sec/step, loss=0.07433, avg_loss=0.07475]\n",
      "Step 512311  [5.332 sec/step, loss=0.07629, avg_loss=0.07475]\n",
      "Step 512312  [5.348 sec/step, loss=0.07751, avg_loss=0.07478]\n",
      "Step 512313  [5.351 sec/step, loss=0.07467, avg_loss=0.07475]\n",
      "Step 512314  [5.348 sec/step, loss=0.07644, avg_loss=0.07477]\n",
      "Step 512315  [5.345 sec/step, loss=0.07366, avg_loss=0.07475]\n",
      "Step 512316  [5.342 sec/step, loss=0.07644, avg_loss=0.07475]\n",
      "Step 512317  [5.364 sec/step, loss=0.07693, avg_loss=0.07480]\n",
      "Step 512318  [5.380 sec/step, loss=0.07450, avg_loss=0.07477]\n",
      "Step 512319  [5.376 sec/step, loss=0.07468, avg_loss=0.07476]\n",
      "Step 512320  [5.400 sec/step, loss=0.07707, avg_loss=0.07478]\n",
      "Step 512321  [5.397 sec/step, loss=0.07253, avg_loss=0.07474]\n",
      "Step 512322  [5.379 sec/step, loss=0.07567, avg_loss=0.07475]\n",
      "Step 512323  [5.389 sec/step, loss=0.07635, avg_loss=0.07477]\n",
      "Step 512324  [5.397 sec/step, loss=0.07586, avg_loss=0.07481]\n",
      "Step 512325  [5.378 sec/step, loss=0.07516, avg_loss=0.07481]\n",
      "Step 512326  [5.368 sec/step, loss=0.06575, avg_loss=0.07473]\n",
      "Step 512327  [5.375 sec/step, loss=0.07487, avg_loss=0.07476]\n",
      "Step 512328  [5.345 sec/step, loss=0.07424, avg_loss=0.07483]\n",
      "Step 512329  [5.332 sec/step, loss=0.07185, avg_loss=0.07483]\n",
      "Step 512330  [5.325 sec/step, loss=0.07478, avg_loss=0.07483]\n",
      "Step 512331  [5.319 sec/step, loss=0.07597, avg_loss=0.07483]\n",
      "Step 512332  [5.309 sec/step, loss=0.07659, avg_loss=0.07483]\n",
      "Step 512333  [5.333 sec/step, loss=0.07713, avg_loss=0.07493]\n",
      "Step 512334  [5.343 sec/step, loss=0.07484, avg_loss=0.07491]\n",
      "Step 512335  [5.384 sec/step, loss=0.06811, avg_loss=0.07482]\n",
      "Step 512336  [5.374 sec/step, loss=0.07342, avg_loss=0.07480]\n",
      "Generated 32 batches of size 32 in 2.548 sec\n",
      "Step 512337  [5.359 sec/step, loss=0.07204, avg_loss=0.07475]\n",
      "Step 512338  [5.351 sec/step, loss=0.07593, avg_loss=0.07474]\n",
      "Step 512339  [5.379 sec/step, loss=0.07468, avg_loss=0.07476]\n",
      "Step 512340  [5.400 sec/step, loss=0.07682, avg_loss=0.07478]\n",
      "Step 512341  [5.381 sec/step, loss=0.07521, avg_loss=0.07476]\n",
      "Step 512342  [5.370 sec/step, loss=0.07297, avg_loss=0.07473]\n",
      "Step 512343  [5.371 sec/step, loss=0.07729, avg_loss=0.07475]\n",
      "Step 512344  [5.368 sec/step, loss=0.07655, avg_loss=0.07476]\n",
      "Step 512345  [5.365 sec/step, loss=0.07487, avg_loss=0.07475]\n",
      "Step 512346  [5.379 sec/step, loss=0.07417, avg_loss=0.07475]\n",
      "Step 512347  [5.364 sec/step, loss=0.07323, avg_loss=0.07473]\n",
      "Step 512348  [5.351 sec/step, loss=0.07295, avg_loss=0.07470]\n",
      "Step 512349  [5.364 sec/step, loss=0.07677, avg_loss=0.07470]\n",
      "Step 512350  [5.359 sec/step, loss=0.07484, avg_loss=0.07468]\n",
      "Step 512351  [5.367 sec/step, loss=0.07676, avg_loss=0.07468]\n",
      "Step 512352  [5.371 sec/step, loss=0.07696, avg_loss=0.07469]\n",
      "Step 512353  [5.348 sec/step, loss=0.06696, avg_loss=0.07460]\n",
      "Step 512354  [5.359 sec/step, loss=0.07476, avg_loss=0.07459]\n",
      "Step 512355  [5.373 sec/step, loss=0.07609, avg_loss=0.07463]\n",
      "Step 512356  [5.386 sec/step, loss=0.07740, avg_loss=0.07465]\n",
      "Step 512357  [5.390 sec/step, loss=0.07686, avg_loss=0.07466]\n",
      "Step 512358  [5.337 sec/step, loss=0.07597, avg_loss=0.07476]\n",
      "Step 512359  [5.329 sec/step, loss=0.07503, avg_loss=0.07477]\n",
      "Step 512360  [5.321 sec/step, loss=0.07428, avg_loss=0.07476]\n",
      "Step 512361  [5.371 sec/step, loss=0.06752, avg_loss=0.07467]\n",
      "Step 512362  [5.362 sec/step, loss=0.07677, avg_loss=0.07466]\n",
      "Step 512363  [5.353 sec/step, loss=0.07614, avg_loss=0.07466]\n",
      "Step 512364  [5.363 sec/step, loss=0.07659, avg_loss=0.07470]\n",
      "Step 512365  [5.376 sec/step, loss=0.07624, avg_loss=0.07478]\n",
      "Step 512366  [5.365 sec/step, loss=0.07442, avg_loss=0.07475]\n",
      "Step 512367  [5.368 sec/step, loss=0.07573, avg_loss=0.07476]\n",
      "Step 512368  [5.359 sec/step, loss=0.07724, avg_loss=0.07479]\n",
      "Generated 32 batches of size 32 in 2.360 sec\n",
      "Step 512369  [5.352 sec/step, loss=0.07499, avg_loss=0.07479]\n",
      "Step 512370  [5.358 sec/step, loss=0.07521, avg_loss=0.07479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 512371  [5.377 sec/step, loss=0.07601, avg_loss=0.07481]\n",
      "Step 512372  [5.403 sec/step, loss=0.07406, avg_loss=0.07478]\n",
      "Step 512373  [5.379 sec/step, loss=0.07534, avg_loss=0.07477]\n",
      "Step 512374  [5.370 sec/step, loss=0.07424, avg_loss=0.07475]\n",
      "Step 512375  [5.382 sec/step, loss=0.07238, avg_loss=0.07474]\n",
      "Step 512376  [5.378 sec/step, loss=0.07498, avg_loss=0.07472]\n",
      "Step 512377  [5.376 sec/step, loss=0.07511, avg_loss=0.07474]\n",
      "Step 512378  [5.385 sec/step, loss=0.07511, avg_loss=0.07474]\n",
      "Step 512379  [5.366 sec/step, loss=0.07492, avg_loss=0.07472]\n",
      "Step 512380  [5.371 sec/step, loss=0.07619, avg_loss=0.07474]\n",
      "Step 512381  [5.371 sec/step, loss=0.07699, avg_loss=0.07474]\n",
      "Step 512382  [5.375 sec/step, loss=0.07644, avg_loss=0.07478]\n",
      "Step 512383  [5.370 sec/step, loss=0.07391, avg_loss=0.07476]\n",
      "Step 512384  [5.377 sec/step, loss=0.07696, avg_loss=0.07480]\n",
      "Step 512385  [5.392 sec/step, loss=0.07651, avg_loss=0.07485]\n",
      "Step 512386  [5.365 sec/step, loss=0.06659, avg_loss=0.07475]\n",
      "Step 512387  [5.319 sec/step, loss=0.07403, avg_loss=0.07482]\n",
      "Step 512388  [5.316 sec/step, loss=0.07598, avg_loss=0.07483]\n",
      "Step 512389  [5.338 sec/step, loss=0.07427, avg_loss=0.07482]\n",
      "Step 512390  [5.357 sec/step, loss=0.07679, avg_loss=0.07487]\n",
      "Step 512391  [5.379 sec/step, loss=0.07404, avg_loss=0.07489]\n",
      "Step 512392  [5.384 sec/step, loss=0.07133, avg_loss=0.07493]\n",
      "Step 512393  [5.372 sec/step, loss=0.07544, avg_loss=0.07494]\n",
      "Step 512394  [5.373 sec/step, loss=0.07706, avg_loss=0.07494]\n",
      "Step 512395  [5.424 sec/step, loss=0.06680, avg_loss=0.07485]\n",
      "Step 512396  [5.424 sec/step, loss=0.07306, avg_loss=0.07484]\n",
      "Step 512397  [5.430 sec/step, loss=0.07659, avg_loss=0.07486]\n",
      "Step 512398  [5.463 sec/step, loss=0.07450, avg_loss=0.07486]\n",
      "Step 512399  [5.471 sec/step, loss=0.07571, avg_loss=0.07486]\n",
      "Step 512400  [5.470 sec/step, loss=0.07721, avg_loss=0.07486]\n",
      "Writing summary at step: 512400\n",
      "Generated 32 batches of size 32 in 2.604 sec\n",
      "Step 512401  [5.470 sec/step, loss=0.07624, avg_loss=0.07488]\n",
      "Step 512402  [5.453 sec/step, loss=0.07429, avg_loss=0.07485]\n",
      "Step 512403  [5.454 sec/step, loss=0.07644, avg_loss=0.07484]\n",
      "Step 512404  [5.454 sec/step, loss=0.07526, avg_loss=0.07483]\n",
      "Step 512405  [5.445 sec/step, loss=0.07717, avg_loss=0.07484]\n",
      "Step 512406  [5.454 sec/step, loss=0.07525, avg_loss=0.07484]\n",
      "Step 512407  [5.445 sec/step, loss=0.07494, avg_loss=0.07483]\n",
      "Step 512408  [5.429 sec/step, loss=0.07610, avg_loss=0.07487]\n",
      "Step 512409  [5.406 sec/step, loss=0.07315, avg_loss=0.07483]\n",
      "Step 512410  [5.421 sec/step, loss=0.07686, avg_loss=0.07486]\n",
      "Step 512411  [5.417 sec/step, loss=0.07699, avg_loss=0.07486]\n",
      "Step 512412  [5.415 sec/step, loss=0.07719, avg_loss=0.07486]\n",
      "Step 512413  [5.401 sec/step, loss=0.07483, avg_loss=0.07486]\n",
      "Step 512414  [5.408 sec/step, loss=0.07331, avg_loss=0.07483]\n",
      "Step 512415  [5.410 sec/step, loss=0.07628, avg_loss=0.07486]\n",
      "Step 512416  [5.406 sec/step, loss=0.07281, avg_loss=0.07482]\n",
      "Step 512417  [5.410 sec/step, loss=0.07818, avg_loss=0.07483]\n",
      "Step 512418  [5.376 sec/step, loss=0.07445, avg_loss=0.07483]\n",
      "Step 512419  [5.377 sec/step, loss=0.07501, avg_loss=0.07484]\n",
      "Step 512420  [5.357 sec/step, loss=0.07494, avg_loss=0.07482]\n",
      "Step 512421  [5.361 sec/step, loss=0.07624, avg_loss=0.07485]\n",
      "Step 512422  [5.352 sec/step, loss=0.07242, avg_loss=0.07482]\n",
      "Step 512423  [5.344 sec/step, loss=0.07385, avg_loss=0.07480]\n",
      "Step 512424  [5.333 sec/step, loss=0.07347, avg_loss=0.07477]\n",
      "Step 512425  [5.337 sec/step, loss=0.07553, avg_loss=0.07478]\n",
      "Step 512426  [5.361 sec/step, loss=0.07722, avg_loss=0.07489]\n",
      "Step 512427  [5.354 sec/step, loss=0.07312, avg_loss=0.07487]\n",
      "Step 512428  [5.357 sec/step, loss=0.07445, avg_loss=0.07487]\n",
      "Step 512429  [5.386 sec/step, loss=0.07424, avg_loss=0.07490]\n",
      "Step 512430  [5.406 sec/step, loss=0.07605, avg_loss=0.07491]\n",
      "Step 512431  [5.409 sec/step, loss=0.07597, avg_loss=0.07491]\n",
      "Generated 32 batches of size 32 in 2.390 sec\n",
      "Step 512432  [5.424 sec/step, loss=0.07628, avg_loss=0.07491]\n",
      "Step 512433  [5.465 sec/step, loss=0.06644, avg_loss=0.07480]\n",
      "Step 512434  [5.479 sec/step, loss=0.07497, avg_loss=0.07480]\n",
      "Step 512435  [5.436 sec/step, loss=0.07443, avg_loss=0.07487]\n",
      "Step 512436  [5.446 sec/step, loss=0.07601, avg_loss=0.07489]\n",
      "Step 512437  [5.436 sec/step, loss=0.06764, avg_loss=0.07485]\n",
      "Step 512438  [5.429 sec/step, loss=0.07625, avg_loss=0.07485]\n",
      "Step 512439  [5.405 sec/step, loss=0.07518, avg_loss=0.07486]\n",
      "Step 512440  [5.398 sec/step, loss=0.07713, avg_loss=0.07486]\n",
      "Step 512441  [5.405 sec/step, loss=0.07629, avg_loss=0.07487]\n",
      "Step 512442  [5.415 sec/step, loss=0.07598, avg_loss=0.07490]\n",
      "Step 512443  [5.408 sec/step, loss=0.07643, avg_loss=0.07489]\n",
      "Step 512444  [5.417 sec/step, loss=0.07713, avg_loss=0.07490]\n",
      "Step 512445  [5.439 sec/step, loss=0.07533, avg_loss=0.07490]\n",
      "Step 512446  [5.426 sec/step, loss=0.07518, avg_loss=0.07491]\n",
      "Step 512447  [5.457 sec/step, loss=0.07597, avg_loss=0.07494]\n",
      "Step 512448  [5.479 sec/step, loss=0.07592, avg_loss=0.07497]\n",
      "Step 512449  [5.461 sec/step, loss=0.07209, avg_loss=0.07492]\n",
      "Step 512450  [5.453 sec/step, loss=0.07484, avg_loss=0.07492]\n",
      "Step 512451  [5.447 sec/step, loss=0.07728, avg_loss=0.07493]\n",
      "Step 512452  [5.432 sec/step, loss=0.07270, avg_loss=0.07488]\n",
      "Step 512453  [5.453 sec/step, loss=0.07649, avg_loss=0.07498]\n",
      "Step 512454  [5.453 sec/step, loss=0.07678, avg_loss=0.07500]\n",
      "Step 512455  [5.448 sec/step, loss=0.07458, avg_loss=0.07499]\n",
      "Step 512456  [5.465 sec/step, loss=0.07573, avg_loss=0.07497]\n",
      "Step 512457  [5.462 sec/step, loss=0.07695, avg_loss=0.07497]\n",
      "Step 512458  [5.516 sec/step, loss=0.06674, avg_loss=0.07488]\n",
      "Step 512459  [5.506 sec/step, loss=0.06676, avg_loss=0.07479]\n",
      "Step 512460  [5.508 sec/step, loss=0.07503, avg_loss=0.07480]\n",
      "Step 512461  [5.443 sec/step, loss=0.07277, avg_loss=0.07485]\n",
      "Step 512462  [5.447 sec/step, loss=0.07579, avg_loss=0.07485]\n",
      "Step 512463  [5.428 sec/step, loss=0.07616, avg_loss=0.07485]\n",
      "Generated 32 batches of size 32 in 2.358 sec\n",
      "Step 512464  [5.434 sec/step, loss=0.07700, avg_loss=0.07485]\n",
      "Step 512465  [5.446 sec/step, loss=0.07709, avg_loss=0.07486]\n",
      "Step 512466  [5.460 sec/step, loss=0.07470, avg_loss=0.07486]\n",
      "Step 512467  [5.454 sec/step, loss=0.07478, avg_loss=0.07485]\n",
      "Step 512468  [5.446 sec/step, loss=0.07421, avg_loss=0.07482]\n",
      "Step 512469  [5.433 sec/step, loss=0.07491, avg_loss=0.07482]\n",
      "Step 512470  [5.436 sec/step, loss=0.07486, avg_loss=0.07482]\n",
      "Step 512471  [5.424 sec/step, loss=0.07653, avg_loss=0.07482]\n",
      "Step 512472  [5.393 sec/step, loss=0.07419, avg_loss=0.07482]\n",
      "Step 512473  [5.414 sec/step, loss=0.07555, avg_loss=0.07483]\n",
      "Step 512474  [5.405 sec/step, loss=0.07386, avg_loss=0.07482]\n",
      "Step 512475  [5.410 sec/step, loss=0.07622, avg_loss=0.07486]\n",
      "Step 512476  [5.427 sec/step, loss=0.07785, avg_loss=0.07489]\n",
      "Step 512477  [5.421 sec/step, loss=0.07593, avg_loss=0.07490]\n",
      "Step 512478  [5.421 sec/step, loss=0.07521, avg_loss=0.07490]\n",
      "Step 512479  [5.432 sec/step, loss=0.07733, avg_loss=0.07492]\n",
      "Step 512480  [5.425 sec/step, loss=0.07513, avg_loss=0.07491]\n",
      "Step 512481  [5.466 sec/step, loss=0.06731, avg_loss=0.07481]\n",
      "Step 512482  [5.463 sec/step, loss=0.07603, avg_loss=0.07481]\n",
      "Step 512483  [5.468 sec/step, loss=0.07634, avg_loss=0.07483]\n",
      "Step 512484  [5.467 sec/step, loss=0.07528, avg_loss=0.07482]\n",
      "Step 512485  [5.461 sec/step, loss=0.07695, avg_loss=0.07482]\n",
      "Step 512486  [5.487 sec/step, loss=0.07716, avg_loss=0.07493]\n",
      "Step 512487  [5.480 sec/step, loss=0.07502, avg_loss=0.07494]\n",
      "Step 512488  [5.497 sec/step, loss=0.07601, avg_loss=0.07494]\n",
      "Step 512489  [5.482 sec/step, loss=0.07486, avg_loss=0.07494]\n",
      "Step 512490  [5.474 sec/step, loss=0.07498, avg_loss=0.07493]\n",
      "Step 512491  [5.477 sec/step, loss=0.07694, avg_loss=0.07495]\n",
      "Step 512492  [5.470 sec/step, loss=0.06494, avg_loss=0.07489]\n",
      "Step 512493  [5.479 sec/step, loss=0.07678, avg_loss=0.07490]\n",
      "Step 512494  [5.461 sec/step, loss=0.07264, avg_loss=0.07486]\n",
      "Step 512495  [5.398 sec/step, loss=0.07129, avg_loss=0.07490]\n",
      "Generated 32 batches of size 32 in 2.404 sec\n",
      "Step 512496  [5.408 sec/step, loss=0.07638, avg_loss=0.07494]\n",
      "Step 512497  [5.430 sec/step, loss=0.07344, avg_loss=0.07491]\n",
      "Step 512498  [5.406 sec/step, loss=0.07649, avg_loss=0.07493]\n",
      "Step 512499  [5.412 sec/step, loss=0.07634, avg_loss=0.07493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 512500  [5.419 sec/step, loss=0.07618, avg_loss=0.07492]\n",
      "Writing summary at step: 512500\n",
      "Step 512501  [5.410 sec/step, loss=0.07463, avg_loss=0.07491]\n",
      "Step 512502  [5.409 sec/step, loss=0.07498, avg_loss=0.07491]\n",
      "Step 512503  [5.409 sec/step, loss=0.07385, avg_loss=0.07489]\n",
      "Step 512504  [5.425 sec/step, loss=0.07723, avg_loss=0.07491]\n",
      "Step 512505  [5.421 sec/step, loss=0.07686, avg_loss=0.07490]\n",
      "Step 512506  [5.434 sec/step, loss=0.07451, avg_loss=0.07490]\n",
      "Step 512507  [5.444 sec/step, loss=0.07689, avg_loss=0.07492]\n",
      "Step 512508  [5.430 sec/step, loss=0.07393, avg_loss=0.07489]\n",
      "Step 512509  [5.448 sec/step, loss=0.07279, avg_loss=0.07489]\n",
      "Step 512510  [5.437 sec/step, loss=0.07460, avg_loss=0.07487]\n",
      "Step 512511  [5.409 sec/step, loss=0.06815, avg_loss=0.07478]\n",
      "Step 512512  [5.389 sec/step, loss=0.07414, avg_loss=0.07475]\n",
      "Step 512513  [5.402 sec/step, loss=0.07711, avg_loss=0.07477]\n",
      "Step 512514  [5.391 sec/step, loss=0.07488, avg_loss=0.07479]\n",
      "Step 512515  [5.388 sec/step, loss=0.07561, avg_loss=0.07478]\n",
      "Step 512516  [5.391 sec/step, loss=0.07646, avg_loss=0.07482]\n",
      "Step 512517  [5.377 sec/step, loss=0.07585, avg_loss=0.07479]\n",
      "Step 512518  [5.434 sec/step, loss=0.06784, avg_loss=0.07473]\n",
      "Step 512519  [5.442 sec/step, loss=0.07880, avg_loss=0.07477]\n",
      "Step 512520  [5.449 sec/step, loss=0.07344, avg_loss=0.07475]\n",
      "Step 512521  [5.448 sec/step, loss=0.07647, avg_loss=0.07475]\n",
      "Step 512522  [5.441 sec/step, loss=0.07202, avg_loss=0.07475]\n",
      "Step 512523  [5.440 sec/step, loss=0.07684, avg_loss=0.07478]\n",
      "Step 512524  [5.436 sec/step, loss=0.07415, avg_loss=0.07479]\n",
      "Step 512525  [5.445 sec/step, loss=0.07643, avg_loss=0.07480]\n",
      "Step 512526  [5.451 sec/step, loss=0.07832, avg_loss=0.07481]\n",
      "Generated 32 batches of size 32 in 2.370 sec\n",
      "Step 512527  [5.466 sec/step, loss=0.07323, avg_loss=0.07481]\n",
      "Step 512528  [5.470 sec/step, loss=0.07488, avg_loss=0.07481]\n",
      "Step 512529  [5.447 sec/step, loss=0.07599, avg_loss=0.07483]\n",
      "Step 512530  [5.440 sec/step, loss=0.07671, avg_loss=0.07484]\n",
      "Step 512531  [5.438 sec/step, loss=0.07413, avg_loss=0.07482]\n",
      "Step 512532  [5.441 sec/step, loss=0.07761, avg_loss=0.07483]\n",
      "Step 512533  [5.400 sec/step, loss=0.07672, avg_loss=0.07493]\n",
      "Step 512534  [5.381 sec/step, loss=0.07673, avg_loss=0.07495]\n",
      "Step 512535  [5.387 sec/step, loss=0.07862, avg_loss=0.07499]\n",
      "Step 512536  [5.387 sec/step, loss=0.07664, avg_loss=0.07500]\n",
      "Step 512537  [5.412 sec/step, loss=0.07621, avg_loss=0.07508]\n",
      "Step 512538  [5.404 sec/step, loss=0.07558, avg_loss=0.07508]\n",
      "Step 512539  [5.410 sec/step, loss=0.07474, avg_loss=0.07507]\n",
      "Step 512540  [5.402 sec/step, loss=0.07566, avg_loss=0.07506]\n",
      "Step 512541  [5.412 sec/step, loss=0.07692, avg_loss=0.07507]\n",
      "Step 512542  [5.388 sec/step, loss=0.07158, avg_loss=0.07502]\n",
      "Step 512543  [5.384 sec/step, loss=0.07632, avg_loss=0.07502]\n",
      "Step 512544  [5.374 sec/step, loss=0.07730, avg_loss=0.07502]\n",
      "Step 512545  [5.368 sec/step, loss=0.07781, avg_loss=0.07505]\n",
      "Step 512546  [5.364 sec/step, loss=0.07241, avg_loss=0.07502]\n",
      "Step 512547  [5.358 sec/step, loss=0.07725, avg_loss=0.07503]\n",
      "Step 512548  [5.355 sec/step, loss=0.07793, avg_loss=0.07505]\n",
      "Step 512549  [5.360 sec/step, loss=0.07385, avg_loss=0.07507]\n",
      "Step 512550  [5.359 sec/step, loss=0.07193, avg_loss=0.07504]\n",
      "Step 512551  [5.354 sec/step, loss=0.07446, avg_loss=0.07501]\n",
      "Step 512552  [5.373 sec/step, loss=0.07684, avg_loss=0.07505]\n",
      "Step 512553  [5.351 sec/step, loss=0.06737, avg_loss=0.07496]\n",
      "Step 512554  [5.348 sec/step, loss=0.07567, avg_loss=0.07495]\n",
      "Step 512555  [5.360 sec/step, loss=0.07725, avg_loss=0.07498]\n",
      "Step 512556  [5.353 sec/step, loss=0.07663, avg_loss=0.07499]\n",
      "Step 512557  [5.347 sec/step, loss=0.07351, avg_loss=0.07495]\n",
      "Step 512558  [5.298 sec/step, loss=0.07631, avg_loss=0.07505]\n",
      "Generated 32 batches of size 32 in 2.517 sec\n",
      "Step 512559  [5.312 sec/step, loss=0.07563, avg_loss=0.07514]\n",
      "Step 512560  [5.316 sec/step, loss=0.07602, avg_loss=0.07515]\n",
      "Step 512561  [5.330 sec/step, loss=0.07579, avg_loss=0.07518]\n",
      "Step 512562  [5.377 sec/step, loss=0.06629, avg_loss=0.07508]\n",
      "Step 512563  [5.393 sec/step, loss=0.07664, avg_loss=0.07509]\n",
      "Step 512564  [5.415 sec/step, loss=0.07431, avg_loss=0.07506]\n",
      "Step 512565  [5.404 sec/step, loss=0.07660, avg_loss=0.07505]\n",
      "Step 512566  [5.400 sec/step, loss=0.07602, avg_loss=0.07507]\n",
      "Step 512567  [5.401 sec/step, loss=0.07385, avg_loss=0.07506]\n",
      "Step 512568  [5.398 sec/step, loss=0.07720, avg_loss=0.07509]\n",
      "Step 512569  [5.410 sec/step, loss=0.07610, avg_loss=0.07510]\n",
      "Step 512570  [5.393 sec/step, loss=0.07181, avg_loss=0.07507]\n",
      "Step 512571  [5.442 sec/step, loss=0.06762, avg_loss=0.07498]\n",
      "Step 512572  [5.443 sec/step, loss=0.07651, avg_loss=0.07500]\n",
      "Step 512573  [5.434 sec/step, loss=0.07661, avg_loss=0.07501]\n",
      "Step 512574  [5.451 sec/step, loss=0.07680, avg_loss=0.07504]\n",
      "Step 512575  [5.447 sec/step, loss=0.07462, avg_loss=0.07503]\n",
      "Step 512576  [5.442 sec/step, loss=0.07611, avg_loss=0.07501]\n",
      "Step 512577  [5.455 sec/step, loss=0.07774, avg_loss=0.07503]\n",
      "Step 512578  [5.469 sec/step, loss=0.07517, avg_loss=0.07503]\n",
      "Step 512579  [5.478 sec/step, loss=0.07715, avg_loss=0.07503]\n",
      "Step 512580  [5.491 sec/step, loss=0.07526, avg_loss=0.07503]\n",
      "Step 512581  [5.449 sec/step, loss=0.07344, avg_loss=0.07509]\n",
      "Step 512582  [5.435 sec/step, loss=0.06645, avg_loss=0.07499]\n",
      "Step 512583  [5.425 sec/step, loss=0.07051, avg_loss=0.07493]\n",
      "Step 512584  [5.433 sec/step, loss=0.07745, avg_loss=0.07496]\n",
      "Step 512585  [5.431 sec/step, loss=0.07354, avg_loss=0.07492]\n",
      "Step 512586  [5.416 sec/step, loss=0.07473, avg_loss=0.07490]\n",
      "Step 512587  [5.415 sec/step, loss=0.07488, avg_loss=0.07490]\n",
      "Step 512588  [5.428 sec/step, loss=0.07408, avg_loss=0.07488]\n",
      "Step 512589  [5.425 sec/step, loss=0.07428, avg_loss=0.07487]\n",
      "Step 512590  [5.441 sec/step, loss=0.07658, avg_loss=0.07489]\n",
      "Generated 32 batches of size 32 in 2.970 sec\n",
      "Step 512591  [5.420 sec/step, loss=0.07419, avg_loss=0.07486]\n",
      "Step 512592  [5.428 sec/step, loss=0.07381, avg_loss=0.07495]\n",
      "Step 512593  [5.426 sec/step, loss=0.07514, avg_loss=0.07493]\n",
      "Step 512594  [5.430 sec/step, loss=0.07282, avg_loss=0.07493]\n",
      "Step 512595  [5.452 sec/step, loss=0.07707, avg_loss=0.07499]\n",
      "Step 512596  [5.444 sec/step, loss=0.07565, avg_loss=0.07498]\n",
      "Step 512597  [5.429 sec/step, loss=0.07541, avg_loss=0.07500]\n",
      "Step 512598  [5.451 sec/step, loss=0.07434, avg_loss=0.07498]\n",
      "Step 512599  [5.443 sec/step, loss=0.07706, avg_loss=0.07499]\n",
      "Step 512600  [5.437 sec/step, loss=0.07729, avg_loss=0.07500]\n",
      "Writing summary at step: 512600\n",
      "Step 512601  [5.431 sec/step, loss=0.07181, avg_loss=0.07497]\n",
      "Step 512602  [5.466 sec/step, loss=0.07416, avg_loss=0.07496]\n",
      "Step 512603  [5.487 sec/step, loss=0.07346, avg_loss=0.07496]\n",
      "Step 512604  [5.472 sec/step, loss=0.07643, avg_loss=0.07495]\n",
      "Step 512605  [5.468 sec/step, loss=0.07657, avg_loss=0.07495]\n",
      "Step 512606  [5.442 sec/step, loss=0.07195, avg_loss=0.07492]\n",
      "Step 512607  [5.423 sec/step, loss=0.06704, avg_loss=0.07483]\n",
      "Step 512608  [5.440 sec/step, loss=0.07481, avg_loss=0.07483]\n",
      "Step 512609  [5.439 sec/step, loss=0.07629, avg_loss=0.07487]\n",
      "Step 512610  [5.454 sec/step, loss=0.07738, avg_loss=0.07490]\n",
      "Step 512611  [5.466 sec/step, loss=0.07476, avg_loss=0.07496]\n",
      "Step 512612  [5.476 sec/step, loss=0.07266, avg_loss=0.07495]\n",
      "Step 512613  [5.466 sec/step, loss=0.07632, avg_loss=0.07494]\n",
      "Step 512614  [5.470 sec/step, loss=0.07465, avg_loss=0.07494]\n",
      "Step 512615  [5.485 sec/step, loss=0.07721, avg_loss=0.07495]\n",
      "Step 512616  [5.484 sec/step, loss=0.07619, avg_loss=0.07495]\n",
      "Step 512617  [5.491 sec/step, loss=0.07790, avg_loss=0.07497]\n",
      "Step 512618  [5.435 sec/step, loss=0.07574, avg_loss=0.07505]\n",
      "Step 512619  [5.441 sec/step, loss=0.07815, avg_loss=0.07504]\n",
      "Step 512620  [5.445 sec/step, loss=0.07472, avg_loss=0.07506]\n",
      "Step 512621  [5.447 sec/step, loss=0.07398, avg_loss=0.07503]\n",
      "Generated 32 batches of size 32 in 2.558 sec\n",
      "Step 512622  [5.449 sec/step, loss=0.07487, avg_loss=0.07506]\n",
      "Step 512623  [5.504 sec/step, loss=0.06762, avg_loss=0.07497]\n",
      "Step 512624  [5.516 sec/step, loss=0.07522, avg_loss=0.07498]\n",
      "Step 512625  [5.509 sec/step, loss=0.07584, avg_loss=0.07497]\n",
      "Step 512626  [5.503 sec/step, loss=0.07476, avg_loss=0.07494]\n",
      "Step 512627  [5.505 sec/step, loss=0.07642, avg_loss=0.07497]\n",
      "Step 512628  [5.471 sec/step, loss=0.07477, avg_loss=0.07497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 512629  [5.481 sec/step, loss=0.07581, avg_loss=0.07497]\n",
      "Step 512630  [5.487 sec/step, loss=0.07414, avg_loss=0.07494]\n",
      "Step 512631  [5.495 sec/step, loss=0.07579, avg_loss=0.07496]\n",
      "Step 512632  [5.479 sec/step, loss=0.07659, avg_loss=0.07495]\n",
      "Step 512633  [5.466 sec/step, loss=0.07418, avg_loss=0.07492]\n",
      "Step 512634  [5.460 sec/step, loss=0.07445, avg_loss=0.07490]\n",
      "Step 512635  [5.452 sec/step, loss=0.07527, avg_loss=0.07487]\n",
      "Step 512636  [5.444 sec/step, loss=0.07617, avg_loss=0.07486]\n",
      "Step 512637  [5.437 sec/step, loss=0.07339, avg_loss=0.07483]\n",
      "Step 512638  [5.437 sec/step, loss=0.07200, avg_loss=0.07480]\n",
      "Step 512639  [5.458 sec/step, loss=0.07642, avg_loss=0.07481]\n",
      "Step 512640  [5.473 sec/step, loss=0.07565, avg_loss=0.07481]\n",
      "Step 512641  [5.474 sec/step, loss=0.07695, avg_loss=0.07481]\n",
      "Step 512642  [5.487 sec/step, loss=0.07636, avg_loss=0.07486]\n",
      "Step 512643  [5.494 sec/step, loss=0.07264, avg_loss=0.07483]\n",
      "Step 512644  [5.504 sec/step, loss=0.07727, avg_loss=0.07483]\n",
      "Step 512645  [5.481 sec/step, loss=0.07246, avg_loss=0.07477]\n",
      "Step 512646  [5.491 sec/step, loss=0.07446, avg_loss=0.07479]\n",
      "Step 512647  [5.533 sec/step, loss=0.06751, avg_loss=0.07469]\n",
      "Step 512648  [5.552 sec/step, loss=0.07387, avg_loss=0.07465]\n",
      "Step 512649  [5.554 sec/step, loss=0.07631, avg_loss=0.07468]\n",
      "Step 512650  [5.546 sec/step, loss=0.07508, avg_loss=0.07471]\n",
      "Step 512651  [5.525 sec/step, loss=0.06771, avg_loss=0.07464]\n",
      "Step 512652  [5.508 sec/step, loss=0.07231, avg_loss=0.07460]\n",
      "Step 512653  [5.531 sec/step, loss=0.07646, avg_loss=0.07469]\n",
      "Generated 32 batches of size 32 in 2.523 sec\n",
      "Step 512654  [5.523 sec/step, loss=0.07484, avg_loss=0.07468]\n",
      "Step 512655  [5.518 sec/step, loss=0.07679, avg_loss=0.07468]\n",
      "Step 512656  [5.494 sec/step, loss=0.07463, avg_loss=0.07466]\n",
      "Step 512657  [5.509 sec/step, loss=0.07717, avg_loss=0.07469]\n",
      "Step 512658  [5.516 sec/step, loss=0.07694, avg_loss=0.07470]\n",
      "Step 512659  [5.533 sec/step, loss=0.07648, avg_loss=0.07471]\n",
      "Step 512660  [5.539 sec/step, loss=0.07572, avg_loss=0.07470]\n",
      "Step 512661  [5.541 sec/step, loss=0.07577, avg_loss=0.07470]\n",
      "Step 512662  [5.500 sec/step, loss=0.07431, avg_loss=0.07478]\n",
      "Step 512663  [5.504 sec/step, loss=0.07636, avg_loss=0.07478]\n",
      "Step 512664  [5.466 sec/step, loss=0.07176, avg_loss=0.07476]\n",
      "Step 512665  [5.473 sec/step, loss=0.07555, avg_loss=0.07475]\n",
      "Step 512666  [5.478 sec/step, loss=0.07462, avg_loss=0.07473]\n",
      "Step 512667  [5.488 sec/step, loss=0.07682, avg_loss=0.07476]\n",
      "Step 512668  [5.490 sec/step, loss=0.07594, avg_loss=0.07475]\n",
      "Step 512669  [5.502 sec/step, loss=0.07767, avg_loss=0.07476]\n",
      "Step 512670  [5.503 sec/step, loss=0.07414, avg_loss=0.07479]\n",
      "Step 512671  [5.457 sec/step, loss=0.07508, avg_loss=0.07486]\n",
      "Step 512672  [5.451 sec/step, loss=0.07500, avg_loss=0.07485]\n",
      "Step 512673  [5.458 sec/step, loss=0.07665, avg_loss=0.07485]\n",
      "Step 512674  [5.447 sec/step, loss=0.07603, avg_loss=0.07484]\n",
      "Step 512675  [5.456 sec/step, loss=0.07707, avg_loss=0.07486]\n",
      "Step 512676  [5.451 sec/step, loss=0.07196, avg_loss=0.07482]\n",
      "Step 512677  [5.427 sec/step, loss=0.07217, avg_loss=0.07477]\n",
      "Step 512678  [5.414 sec/step, loss=0.07522, avg_loss=0.07477]\n",
      "Step 512679  [5.403 sec/step, loss=0.07687, avg_loss=0.07476]\n",
      "Step 512680  [5.400 sec/step, loss=0.07601, avg_loss=0.07477]\n",
      "Step 512681  [5.393 sec/step, loss=0.07379, avg_loss=0.07478]\n",
      "Step 512682  [5.405 sec/step, loss=0.07458, avg_loss=0.07486]\n",
      "Step 512683  [5.406 sec/step, loss=0.07356, avg_loss=0.07489]\n",
      "Step 512684  [5.391 sec/step, loss=0.07653, avg_loss=0.07488]\n",
      "Step 512685  [5.395 sec/step, loss=0.07557, avg_loss=0.07490]\n",
      "Generated 32 batches of size 32 in 2.348 sec\n",
      "Step 512686  [5.411 sec/step, loss=0.07601, avg_loss=0.07491]\n",
      "Step 512687  [5.408 sec/step, loss=0.07539, avg_loss=0.07492]\n",
      "Step 512688  [5.434 sec/step, loss=0.06717, avg_loss=0.07485]\n",
      "Step 512689  [5.433 sec/step, loss=0.07581, avg_loss=0.07486]\n",
      "Step 512690  [5.402 sec/step, loss=0.06670, avg_loss=0.07476]\n",
      "Step 512691  [5.439 sec/step, loss=0.07403, avg_loss=0.07476]\n",
      "Step 512692  [5.463 sec/step, loss=0.07487, avg_loss=0.07477]\n",
      "Step 512693  [5.479 sec/step, loss=0.07713, avg_loss=0.07479]\n",
      "Step 512694  [5.493 sec/step, loss=0.07707, avg_loss=0.07484]\n",
      "Step 512695  [5.494 sec/step, loss=0.07729, avg_loss=0.07484]\n",
      "Step 512696  [5.493 sec/step, loss=0.07477, avg_loss=0.07483]\n",
      "Step 512697  [5.490 sec/step, loss=0.07644, avg_loss=0.07484]\n",
      "Step 512698  [5.459 sec/step, loss=0.07477, avg_loss=0.07484]\n",
      "Step 512699  [5.446 sec/step, loss=0.07642, avg_loss=0.07484]\n",
      "Step 512700  [5.435 sec/step, loss=0.07610, avg_loss=0.07482]\n",
      "Writing summary at step: 512700\n",
      "Step 512701  [5.447 sec/step, loss=0.07441, avg_loss=0.07485]\n",
      "Step 512702  [5.422 sec/step, loss=0.07589, avg_loss=0.07487]\n",
      "Step 512703  [5.391 sec/step, loss=0.07532, avg_loss=0.07489]\n",
      "Step 512704  [5.397 sec/step, loss=0.07574, avg_loss=0.07488]\n",
      "Step 512705  [5.398 sec/step, loss=0.07539, avg_loss=0.07487]\n",
      "Step 512706  [5.414 sec/step, loss=0.07702, avg_loss=0.07492]\n",
      "Step 512707  [5.445 sec/step, loss=0.07747, avg_loss=0.07502]\n",
      "Step 512708  [5.417 sec/step, loss=0.07350, avg_loss=0.07501]\n",
      "Step 512709  [5.423 sec/step, loss=0.07716, avg_loss=0.07502]\n",
      "Step 512710  [5.423 sec/step, loss=0.07691, avg_loss=0.07501]\n",
      "Step 512711  [5.424 sec/step, loss=0.07351, avg_loss=0.07500]\n",
      "Step 512712  [5.439 sec/step, loss=0.07642, avg_loss=0.07504]\n",
      "Step 512713  [5.450 sec/step, loss=0.07718, avg_loss=0.07505]\n",
      "Step 512714  [5.447 sec/step, loss=0.07492, avg_loss=0.07505]\n",
      "Step 512715  [5.445 sec/step, loss=0.07740, avg_loss=0.07505]\n",
      "Step 512716  [5.433 sec/step, loss=0.07326, avg_loss=0.07502]\n",
      "Generated 32 batches of size 32 in 2.543 sec\n",
      "Step 512717  [5.429 sec/step, loss=0.07600, avg_loss=0.07500]\n",
      "Step 512718  [5.464 sec/step, loss=0.07311, avg_loss=0.07498]\n",
      "Step 512719  [5.450 sec/step, loss=0.07517, avg_loss=0.07495]\n",
      "Step 512720  [5.450 sec/step, loss=0.07613, avg_loss=0.07496]\n",
      "Step 512721  [5.453 sec/step, loss=0.07251, avg_loss=0.07495]\n",
      "Step 512722  [5.479 sec/step, loss=0.07525, avg_loss=0.07495]\n",
      "Step 512723  [5.480 sec/step, loss=0.06740, avg_loss=0.07495]\n",
      "Step 512724  [5.490 sec/step, loss=0.07588, avg_loss=0.07496]\n",
      "Step 512725  [5.482 sec/step, loss=0.07307, avg_loss=0.07493]\n",
      "Step 512726  [5.486 sec/step, loss=0.07687, avg_loss=0.07495]\n",
      "Step 512727  [5.494 sec/step, loss=0.07613, avg_loss=0.07495]\n",
      "Step 512728  [5.504 sec/step, loss=0.07600, avg_loss=0.07496]\n",
      "Step 512729  [5.505 sec/step, loss=0.07442, avg_loss=0.07494]\n",
      "Step 512730  [5.493 sec/step, loss=0.07348, avg_loss=0.07494]\n",
      "Step 512731  [5.485 sec/step, loss=0.07643, avg_loss=0.07494]\n",
      "Step 512732  [5.479 sec/step, loss=0.07424, avg_loss=0.07492]\n",
      "Step 512733  [5.491 sec/step, loss=0.07595, avg_loss=0.07494]\n",
      "Step 512734  [5.498 sec/step, loss=0.07545, avg_loss=0.07495]\n",
      "Step 512735  [5.491 sec/step, loss=0.07591, avg_loss=0.07495]\n",
      "Step 512736  [5.497 sec/step, loss=0.07645, avg_loss=0.07496]\n",
      "Step 512737  [5.504 sec/step, loss=0.07742, avg_loss=0.07500]\n",
      "Step 512738  [5.514 sec/step, loss=0.07474, avg_loss=0.07502]\n",
      "Step 512739  [5.490 sec/step, loss=0.07489, avg_loss=0.07501]\n",
      "Step 512740  [5.478 sec/step, loss=0.07690, avg_loss=0.07502]\n",
      "Step 512741  [5.480 sec/step, loss=0.07386, avg_loss=0.07499]\n",
      "Step 512742  [5.493 sec/step, loss=0.07673, avg_loss=0.07499]\n",
      "Step 512743  [5.484 sec/step, loss=0.07459, avg_loss=0.07501]\n",
      "Step 512744  [5.465 sec/step, loss=0.07524, avg_loss=0.07499]\n",
      "Step 512745  [5.484 sec/step, loss=0.07654, avg_loss=0.07503]\n",
      "Step 512746  [5.513 sec/step, loss=0.07580, avg_loss=0.07505]\n",
      "Step 512747  [5.467 sec/step, loss=0.07695, avg_loss=0.07514]\n",
      "Step 512748  [5.439 sec/step, loss=0.07533, avg_loss=0.07516]\n",
      "Generated 32 batches of size 32 in 2.381 sec\n",
      "Step 512749  [5.452 sec/step, loss=0.07508, avg_loss=0.07515]\n",
      "Step 512750  [5.473 sec/step, loss=0.07759, avg_loss=0.07517]\n",
      "Step 512751  [5.478 sec/step, loss=0.07190, avg_loss=0.07521]\n",
      "Step 512752  [5.498 sec/step, loss=0.07629, avg_loss=0.07525]\n",
      "Step 512753  [5.503 sec/step, loss=0.07695, avg_loss=0.07526]\n",
      "Step 512754  [5.496 sec/step, loss=0.07476, avg_loss=0.07526]\n",
      "Step 512755  [5.471 sec/step, loss=0.06573, avg_loss=0.07515]\n",
      "Step 512756  [5.530 sec/step, loss=0.06741, avg_loss=0.07507]\n",
      "Step 512757  [5.504 sec/step, loss=0.07143, avg_loss=0.07502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 512758  [5.504 sec/step, loss=0.07709, avg_loss=0.07502]\n",
      "Step 512759  [5.497 sec/step, loss=0.07391, avg_loss=0.07499]\n",
      "Step 512760  [5.494 sec/step, loss=0.07588, avg_loss=0.07499]\n",
      "Step 512761  [5.493 sec/step, loss=0.07540, avg_loss=0.07499]\n",
      "Step 512762  [5.479 sec/step, loss=0.07448, avg_loss=0.07499]\n",
      "Step 512763  [5.447 sec/step, loss=0.07253, avg_loss=0.07495]\n",
      "Step 512764  [5.447 sec/step, loss=0.07511, avg_loss=0.07499]\n",
      "Step 512765  [5.435 sec/step, loss=0.07483, avg_loss=0.07498]\n",
      "Step 512766  [5.449 sec/step, loss=0.07617, avg_loss=0.07499]\n",
      "Step 512767  [5.465 sec/step, loss=0.07554, avg_loss=0.07498]\n",
      "Step 512768  [5.464 sec/step, loss=0.07597, avg_loss=0.07498]\n",
      "Step 512769  [5.459 sec/step, loss=0.07648, avg_loss=0.07497]\n",
      "Step 512770  [5.449 sec/step, loss=0.06793, avg_loss=0.07491]\n",
      "Step 512771  [5.456 sec/step, loss=0.07739, avg_loss=0.07493]\n",
      "Step 512772  [5.461 sec/step, loss=0.07389, avg_loss=0.07492]\n",
      "Step 512773  [5.450 sec/step, loss=0.07625, avg_loss=0.07492]\n",
      "Step 512774  [5.467 sec/step, loss=0.07626, avg_loss=0.07492]\n",
      "Step 512775  [5.456 sec/step, loss=0.07634, avg_loss=0.07491]\n",
      "Step 512776  [5.458 sec/step, loss=0.07563, avg_loss=0.07495]\n",
      "Step 512777  [5.523 sec/step, loss=0.06649, avg_loss=0.07489]\n",
      "Step 512778  [5.524 sec/step, loss=0.07700, avg_loss=0.07491]\n",
      "Step 512779  [5.524 sec/step, loss=0.07575, avg_loss=0.07490]\n",
      "Step 512780  [5.512 sec/step, loss=0.07098, avg_loss=0.07485]\n",
      "Generated 32 batches of size 32 in 2.420 sec\n",
      "Step 512781  [5.529 sec/step, loss=0.07626, avg_loss=0.07487]\n",
      "Step 512782  [5.535 sec/step, loss=0.07574, avg_loss=0.07488]\n",
      "Step 512783  [5.547 sec/step, loss=0.07694, avg_loss=0.07492]\n",
      "Step 512784  [5.544 sec/step, loss=0.07391, avg_loss=0.07489]\n",
      "Step 512785  [5.532 sec/step, loss=0.07238, avg_loss=0.07486]\n",
      "Step 512786  [5.530 sec/step, loss=0.07744, avg_loss=0.07487]\n",
      "Step 512787  [5.543 sec/step, loss=0.07720, avg_loss=0.07489]\n",
      "Step 512788  [5.491 sec/step, loss=0.07502, avg_loss=0.07497]\n",
      "Step 512789  [5.494 sec/step, loss=0.07563, avg_loss=0.07497]\n",
      "Step 512790  [5.492 sec/step, loss=0.06544, avg_loss=0.07496]\n",
      "Step 512791  [5.470 sec/step, loss=0.07576, avg_loss=0.07497]\n",
      "Step 512792  [5.481 sec/step, loss=0.07420, avg_loss=0.07497]\n",
      "Step 512793  [5.469 sec/step, loss=0.07580, avg_loss=0.07495]\n",
      "Step 512794  [5.505 sec/step, loss=0.06694, avg_loss=0.07485]\n",
      "Step 512795  [5.506 sec/step, loss=0.07600, avg_loss=0.07484]\n",
      "Step 512796  [5.509 sec/step, loss=0.07595, avg_loss=0.07485]\n",
      "Step 512797  [5.513 sec/step, loss=0.07748, avg_loss=0.07486]\n",
      "Step 512798  [5.525 sec/step, loss=0.07456, avg_loss=0.07486]\n",
      "Step 512799  [5.518 sec/step, loss=0.07240, avg_loss=0.07482]\n",
      "Step 512800  [5.524 sec/step, loss=0.07546, avg_loss=0.07481]\n",
      "Writing summary at step: 512800\n",
      "Step 512801  [5.532 sec/step, loss=0.07496, avg_loss=0.07482]\n",
      "Step 512802  [5.547 sec/step, loss=0.07700, avg_loss=0.07483]\n",
      "Step 512803  [5.558 sec/step, loss=0.07312, avg_loss=0.07481]\n",
      "Step 512804  [5.556 sec/step, loss=0.07629, avg_loss=0.07481]\n",
      "Step 512805  [5.544 sec/step, loss=0.07432, avg_loss=0.07480]\n",
      "Step 512806  [5.544 sec/step, loss=0.07667, avg_loss=0.07480]\n",
      "Step 512807  [5.533 sec/step, loss=0.07521, avg_loss=0.07478]\n",
      "Step 512808  [5.545 sec/step, loss=0.07169, avg_loss=0.07476]\n",
      "Step 512809  [5.529 sec/step, loss=0.07442, avg_loss=0.07473]\n",
      "Step 512810  [5.543 sec/step, loss=0.07493, avg_loss=0.07471]\n",
      "Step 512811  [5.545 sec/step, loss=0.07624, avg_loss=0.07474]\n",
      "Generated 32 batches of size 32 in 2.552 sec\n",
      "Step 512812  [5.535 sec/step, loss=0.07299, avg_loss=0.07470]\n",
      "Step 512813  [5.535 sec/step, loss=0.07710, avg_loss=0.07470]\n",
      "Step 512814  [5.554 sec/step, loss=0.07692, avg_loss=0.07472]\n",
      "Step 512815  [5.555 sec/step, loss=0.07683, avg_loss=0.07472]\n",
      "Step 512816  [5.552 sec/step, loss=0.07248, avg_loss=0.07471]\n",
      "Step 512817  [5.546 sec/step, loss=0.07640, avg_loss=0.07471]\n",
      "Step 512818  [5.514 sec/step, loss=0.07407, avg_loss=0.07472]\n",
      "Step 512819  [5.525 sec/step, loss=0.07393, avg_loss=0.07471]\n",
      "Step 512820  [5.528 sec/step, loss=0.07699, avg_loss=0.07472]\n",
      "Step 512821  [5.520 sec/step, loss=0.07451, avg_loss=0.07474]\n",
      "Step 512822  [5.502 sec/step, loss=0.07525, avg_loss=0.07474]\n",
      "Step 512823  [5.444 sec/step, loss=0.07450, avg_loss=0.07481]\n",
      "Step 512824  [5.432 sec/step, loss=0.07488, avg_loss=0.07480]\n",
      "Step 512825  [5.484 sec/step, loss=0.06663, avg_loss=0.07474]\n",
      "Step 512826  [5.488 sec/step, loss=0.07675, avg_loss=0.07473]\n",
      "Step 512827  [5.466 sec/step, loss=0.07530, avg_loss=0.07473]\n",
      "Step 512828  [5.474 sec/step, loss=0.07649, avg_loss=0.07473]\n",
      "Step 512829  [5.470 sec/step, loss=0.07287, avg_loss=0.07472]\n",
      "Step 512830  [5.477 sec/step, loss=0.07606, avg_loss=0.07474]\n",
      "Step 512831  [5.469 sec/step, loss=0.07485, avg_loss=0.07473]\n",
      "Step 512832  [5.491 sec/step, loss=0.07623, avg_loss=0.07475]\n",
      "Step 512833  [5.492 sec/step, loss=0.07721, avg_loss=0.07476]\n",
      "Step 512834  [5.493 sec/step, loss=0.07700, avg_loss=0.07477]\n",
      "Step 512835  [5.506 sec/step, loss=0.07724, avg_loss=0.07479]\n",
      "Step 512836  [5.512 sec/step, loss=0.07726, avg_loss=0.07480]\n",
      "Step 512837  [5.488 sec/step, loss=0.06718, avg_loss=0.07469]\n",
      "Step 512838  [5.492 sec/step, loss=0.07407, avg_loss=0.07469]\n",
      "Step 512839  [5.496 sec/step, loss=0.07607, avg_loss=0.07470]\n",
      "Step 512840  [5.506 sec/step, loss=0.07491, avg_loss=0.07468]\n",
      "Step 512841  [5.485 sec/step, loss=0.07173, avg_loss=0.07466]\n",
      "Step 512842  [5.467 sec/step, loss=0.07604, avg_loss=0.07465]\n",
      "Step 512843  [5.468 sec/step, loss=0.07402, avg_loss=0.07464]\n",
      "Generated 32 batches of size 32 in 2.453 sec\n",
      "Step 512844  [5.481 sec/step, loss=0.07565, avg_loss=0.07465]\n",
      "Step 512845  [5.501 sec/step, loss=0.07565, avg_loss=0.07464]\n",
      "Step 512846  [5.483 sec/step, loss=0.07349, avg_loss=0.07462]\n",
      "Step 512847  [5.479 sec/step, loss=0.07442, avg_loss=0.07459]\n",
      "Step 512848  [5.484 sec/step, loss=0.07534, avg_loss=0.07459]\n",
      "Step 512849  [5.488 sec/step, loss=0.07687, avg_loss=0.07461]\n",
      "Step 512850  [5.489 sec/step, loss=0.07701, avg_loss=0.07460]\n",
      "Step 512851  [5.502 sec/step, loss=0.07694, avg_loss=0.07465]\n",
      "Step 512852  [5.470 sec/step, loss=0.07396, avg_loss=0.07463]\n",
      "Step 512853  [5.450 sec/step, loss=0.07361, avg_loss=0.07460]\n",
      "Step 512854  [5.461 sec/step, loss=0.07667, avg_loss=0.07462]\n",
      "Step 512855  [5.505 sec/step, loss=0.07399, avg_loss=0.07470]\n",
      "Step 512856  [5.450 sec/step, loss=0.07487, avg_loss=0.07477]\n",
      "Step 512857  [5.470 sec/step, loss=0.07719, avg_loss=0.07483]\n",
      "Step 512858  [5.470 sec/step, loss=0.07721, avg_loss=0.07483]\n",
      "Step 512859  [5.463 sec/step, loss=0.07583, avg_loss=0.07485]\n",
      "Step 512860  [5.468 sec/step, loss=0.07487, avg_loss=0.07484]\n",
      "Step 512861  [5.473 sec/step, loss=0.07613, avg_loss=0.07485]\n",
      "Step 512862  [5.477 sec/step, loss=0.07340, avg_loss=0.07484]\n",
      "Step 512863  [5.496 sec/step, loss=0.07612, avg_loss=0.07487]\n",
      "Step 512864  [5.526 sec/step, loss=0.07474, avg_loss=0.07487]\n",
      "Step 512865  [5.584 sec/step, loss=0.06764, avg_loss=0.07480]\n",
      "Step 512866  [5.552 sec/step, loss=0.07499, avg_loss=0.07479]\n",
      "Step 512867  [5.539 sec/step, loss=0.07564, avg_loss=0.07479]\n",
      "Step 512868  [5.526 sec/step, loss=0.07527, avg_loss=0.07478]\n",
      "Step 512869  [5.532 sec/step, loss=0.07569, avg_loss=0.07477]\n",
      "Step 512870  [5.546 sec/step, loss=0.07633, avg_loss=0.07486]\n",
      "Step 512871  [5.532 sec/step, loss=0.07415, avg_loss=0.07482]\n",
      "Step 512872  [5.539 sec/step, loss=0.07636, avg_loss=0.07485]\n",
      "Step 512873  [5.553 sec/step, loss=0.07734, avg_loss=0.07486]\n",
      "Step 512874  [5.521 sec/step, loss=0.06754, avg_loss=0.07477]\n",
      "Step 512875  [5.529 sec/step, loss=0.07529, avg_loss=0.07476]\n",
      "Generated 32 batches of size 32 in 2.876 sec\n",
      "Step 512876  [5.519 sec/step, loss=0.07297, avg_loss=0.07474]\n",
      "Step 512877  [5.465 sec/step, loss=0.07386, avg_loss=0.07481]\n",
      "Step 512878  [5.462 sec/step, loss=0.07540, avg_loss=0.07479]\n",
      "Step 512879  [5.472 sec/step, loss=0.07535, avg_loss=0.07479]\n",
      "Step 512880  [5.486 sec/step, loss=0.07382, avg_loss=0.07482]\n",
      "Step 512881  [5.469 sec/step, loss=0.07231, avg_loss=0.07478]\n",
      "Step 512882  [5.459 sec/step, loss=0.07465, avg_loss=0.07477]\n",
      "Step 512883  [5.460 sec/step, loss=0.07618, avg_loss=0.07476]\n",
      "Step 512884  [5.482 sec/step, loss=0.07372, avg_loss=0.07476]\n",
      "Step 512885  [5.494 sec/step, loss=0.07666, avg_loss=0.07480]\n",
      "Step 512886  [5.498 sec/step, loss=0.07705, avg_loss=0.07480]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 512887  [5.492 sec/step, loss=0.07527, avg_loss=0.07478]\n",
      "Step 512888  [5.503 sec/step, loss=0.07482, avg_loss=0.07478]\n",
      "Step 512889  [5.525 sec/step, loss=0.07348, avg_loss=0.07475]\n",
      "Step 512890  [5.525 sec/step, loss=0.06713, avg_loss=0.07477]\n",
      "Step 512891  [5.517 sec/step, loss=0.07462, avg_loss=0.07476]\n",
      "Step 512892  [5.496 sec/step, loss=0.07606, avg_loss=0.07478]\n",
      "Step 512893  [5.500 sec/step, loss=0.07656, avg_loss=0.07479]\n",
      "Step 512894  [5.451 sec/step, loss=0.07539, avg_loss=0.07487]\n",
      "Step 512895  [5.439 sec/step, loss=0.07131, avg_loss=0.07482]\n",
      "Step 512896  [5.443 sec/step, loss=0.07448, avg_loss=0.07481]\n",
      "Step 512897  [5.417 sec/step, loss=0.07315, avg_loss=0.07477]\n",
      "Step 512898  [5.418 sec/step, loss=0.07662, avg_loss=0.07479]\n",
      "Step 512899  [5.417 sec/step, loss=0.07259, avg_loss=0.07479]\n",
      "Step 512900  [5.421 sec/step, loss=0.07471, avg_loss=0.07478]\n",
      "Writing summary at step: 512900\n",
      "Step 512901  [5.412 sec/step, loss=0.07403, avg_loss=0.07477]\n",
      "Step 512902  [5.392 sec/step, loss=0.07584, avg_loss=0.07476]\n",
      "Step 512903  [5.388 sec/step, loss=0.07634, avg_loss=0.07479]\n",
      "Step 512904  [5.376 sec/step, loss=0.07495, avg_loss=0.07478]\n",
      "Step 512905  [5.376 sec/step, loss=0.07513, avg_loss=0.07479]\n",
      "Step 512906  [5.424 sec/step, loss=0.06656, avg_loss=0.07468]\n",
      "Generated 32 batches of size 32 in 2.397 sec\n",
      "Step 512907  [5.429 sec/step, loss=0.07507, avg_loss=0.07468]\n",
      "Step 512908  [5.426 sec/step, loss=0.07311, avg_loss=0.07470]\n",
      "Step 512909  [5.442 sec/step, loss=0.07759, avg_loss=0.07473]\n",
      "Step 512910  [5.429 sec/step, loss=0.07731, avg_loss=0.07475]\n",
      "Step 512911  [5.449 sec/step, loss=0.07652, avg_loss=0.07476]\n",
      "Step 512912  [5.462 sec/step, loss=0.07647, avg_loss=0.07479]\n",
      "Step 512913  [5.469 sec/step, loss=0.07554, avg_loss=0.07478]\n",
      "Step 512914  [5.467 sec/step, loss=0.07626, avg_loss=0.07477]\n",
      "Step 512915  [5.456 sec/step, loss=0.07438, avg_loss=0.07474]\n",
      "Step 512916  [5.473 sec/step, loss=0.07300, avg_loss=0.07475]\n",
      "Step 512917  [5.468 sec/step, loss=0.07515, avg_loss=0.07474]\n",
      "Step 512918  [5.484 sec/step, loss=0.07477, avg_loss=0.07474]\n",
      "Step 512919  [5.469 sec/step, loss=0.07403, avg_loss=0.07474]\n",
      "Step 512920  [5.459 sec/step, loss=0.07262, avg_loss=0.07470]\n",
      "Step 512921  [5.469 sec/step, loss=0.07582, avg_loss=0.07471]\n",
      "Step 512922  [5.471 sec/step, loss=0.07583, avg_loss=0.07472]\n",
      "Step 512923  [5.529 sec/step, loss=0.06766, avg_loss=0.07465]\n",
      "Step 512924  [5.536 sec/step, loss=0.07682, avg_loss=0.07467]\n",
      "Step 512925  [5.476 sec/step, loss=0.07483, avg_loss=0.07475]\n",
      "Step 512926  [5.462 sec/step, loss=0.07744, avg_loss=0.07476]\n",
      "Step 512927  [5.497 sec/step, loss=0.07436, avg_loss=0.07475]\n",
      "Step 512928  [5.500 sec/step, loss=0.07701, avg_loss=0.07476]\n",
      "Step 512929  [5.517 sec/step, loss=0.07763, avg_loss=0.07480]\n",
      "Step 512930  [5.501 sec/step, loss=0.07149, avg_loss=0.07476]\n",
      "Step 512931  [5.508 sec/step, loss=0.07454, avg_loss=0.07475]\n",
      "Step 512932  [5.492 sec/step, loss=0.07333, avg_loss=0.07473]\n",
      "Step 512933  [5.491 sec/step, loss=0.07621, avg_loss=0.07472]\n",
      "Step 512934  [5.494 sec/step, loss=0.07753, avg_loss=0.07472]\n",
      "Step 512935  [5.494 sec/step, loss=0.07525, avg_loss=0.07470]\n",
      "Step 512936  [5.484 sec/step, loss=0.07600, avg_loss=0.07469]\n",
      "Step 512937  [5.506 sec/step, loss=0.07604, avg_loss=0.07478]\n",
      "Step 512938  [5.512 sec/step, loss=0.07688, avg_loss=0.07481]\n",
      "Generated 32 batches of size 32 in 2.509 sec\n",
      "Step 512939  [5.519 sec/step, loss=0.07614, avg_loss=0.07481]\n",
      "Step 512940  [5.511 sec/step, loss=0.07653, avg_loss=0.07482]\n",
      "Step 512941  [5.507 sec/step, loss=0.07264, avg_loss=0.07483]\n",
      "Step 512942  [5.523 sec/step, loss=0.07748, avg_loss=0.07485]\n",
      "Step 512943  [5.508 sec/step, loss=0.06605, avg_loss=0.07477]\n",
      "Step 512944  [5.503 sec/step, loss=0.07504, avg_loss=0.07476]\n",
      "Step 512945  [5.475 sec/step, loss=0.07484, avg_loss=0.07475]\n",
      "Step 512946  [5.492 sec/step, loss=0.07456, avg_loss=0.07476]\n",
      "Step 512947  [5.482 sec/step, loss=0.07442, avg_loss=0.07476]\n",
      "Step 512948  [5.477 sec/step, loss=0.07296, avg_loss=0.07474]\n",
      "Step 512949  [5.471 sec/step, loss=0.07460, avg_loss=0.07472]\n",
      "Step 512950  [5.469 sec/step, loss=0.07761, avg_loss=0.07472]\n",
      "Step 512951  [5.472 sec/step, loss=0.07498, avg_loss=0.07470]\n",
      "Step 512952  [5.476 sec/step, loss=0.07504, avg_loss=0.07471]\n",
      "Step 512953  [5.500 sec/step, loss=0.07453, avg_loss=0.07472]\n",
      "Step 512954  [5.492 sec/step, loss=0.07490, avg_loss=0.07470]\n",
      "Step 512955  [5.481 sec/step, loss=0.07651, avg_loss=0.07473]\n",
      "Step 512956  [5.488 sec/step, loss=0.07519, avg_loss=0.07473]\n",
      "Step 512957  [5.471 sec/step, loss=0.07229, avg_loss=0.07468]\n",
      "Step 512958  [5.462 sec/step, loss=0.07548, avg_loss=0.07467]\n",
      "Step 512959  [5.456 sec/step, loss=0.07439, avg_loss=0.07465]\n",
      "Step 512960  [5.453 sec/step, loss=0.07580, avg_loss=0.07466]\n",
      "Step 512961  [5.449 sec/step, loss=0.07654, avg_loss=0.07467]\n",
      "Step 512962  [5.448 sec/step, loss=0.07561, avg_loss=0.07469]\n",
      "Step 512963  [5.426 sec/step, loss=0.06700, avg_loss=0.07460]\n",
      "Step 512964  [5.401 sec/step, loss=0.07307, avg_loss=0.07458]\n",
      "Step 512965  [5.348 sec/step, loss=0.07652, avg_loss=0.07467]\n",
      "Step 512966  [5.373 sec/step, loss=0.07635, avg_loss=0.07468]\n",
      "Step 512967  [5.364 sec/step, loss=0.07448, avg_loss=0.07467]\n",
      "Step 512968  [5.383 sec/step, loss=0.07730, avg_loss=0.07469]\n",
      "Step 512969  [5.351 sec/step, loss=0.07300, avg_loss=0.07466]\n",
      "Step 512970  [5.403 sec/step, loss=0.06673, avg_loss=0.07457]\n",
      "Generated 32 batches of size 32 in 2.393 sec\n",
      "Step 512971  [5.415 sec/step, loss=0.07655, avg_loss=0.07459]\n",
      "Step 512972  [5.423 sec/step, loss=0.07447, avg_loss=0.07457]\n",
      "Step 512973  [5.426 sec/step, loss=0.07657, avg_loss=0.07457]\n",
      "Step 512974  [5.454 sec/step, loss=0.07697, avg_loss=0.07466]\n",
      "Step 512975  [5.455 sec/step, loss=0.07427, avg_loss=0.07465]\n",
      "Step 512976  [5.466 sec/step, loss=0.07681, avg_loss=0.07469]\n",
      "Step 512977  [5.497 sec/step, loss=0.07376, avg_loss=0.07469]\n",
      "Step 512978  [5.493 sec/step, loss=0.07536, avg_loss=0.07469]\n",
      "Step 512979  [5.478 sec/step, loss=0.07554, avg_loss=0.07469]\n",
      "Step 512980  [5.471 sec/step, loss=0.07340, avg_loss=0.07468]\n",
      "Step 512981  [5.480 sec/step, loss=0.07677, avg_loss=0.07473]\n",
      "Step 512982  [5.488 sec/step, loss=0.07474, avg_loss=0.07473]\n",
      "Step 512983  [5.482 sec/step, loss=0.07600, avg_loss=0.07473]\n",
      "Step 512984  [5.476 sec/step, loss=0.07528, avg_loss=0.07474]\n",
      "Step 512985  [5.457 sec/step, loss=0.07517, avg_loss=0.07473]\n",
      "Step 512986  [5.445 sec/step, loss=0.07568, avg_loss=0.07471]\n",
      "Step 512987  [5.441 sec/step, loss=0.07455, avg_loss=0.07471]\n",
      "Step 512988  [5.414 sec/step, loss=0.06666, avg_loss=0.07463]\n",
      "Step 512989  [5.381 sec/step, loss=0.07458, avg_loss=0.07464]\n",
      "Step 512990  [5.388 sec/step, loss=0.07224, avg_loss=0.07469]\n",
      "Step 512991  [5.388 sec/step, loss=0.07586, avg_loss=0.07470]\n",
      "Step 512992  [5.395 sec/step, loss=0.07405, avg_loss=0.07468]\n",
      "Step 512993  [5.400 sec/step, loss=0.07718, avg_loss=0.07469]\n",
      "Step 512994  [5.412 sec/step, loss=0.07639, avg_loss=0.07470]\n",
      "Step 512995  [5.415 sec/step, loss=0.07529, avg_loss=0.07474]\n",
      "Step 512996  [5.426 sec/step, loss=0.07584, avg_loss=0.07475]\n",
      "Step 512997  [5.432 sec/step, loss=0.07530, avg_loss=0.07477]\n",
      "Step 512998  [5.428 sec/step, loss=0.07644, avg_loss=0.07477]\n",
      "Step 512999  [5.455 sec/step, loss=0.07589, avg_loss=0.07480]\n",
      "Step 513000  [5.458 sec/step, loss=0.07724, avg_loss=0.07483]\n",
      "Writing summary at step: 513000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-513000\n",
      "Saving audio and alignment...\n",
      "Input: matshayroon kay aazaadd garvaa nay payshqaddmii kurtday huuay siikoortii kaarvaaiioon kay barxalaaf dzaaiiddaadd vaaguzaar karnay kaa diimaand kajjaa~_____________________________________________________________________\n",
      "Generated 32 batches of size 32 in 2.652 sec\n",
      "Step 513001  [5.449 sec/step, loss=0.07221, avg_loss=0.07481]\n",
      "Step 513002  [5.452 sec/step, loss=0.07600, avg_loss=0.07481]\n",
      "Step 513003  [5.452 sec/step, loss=0.07272, avg_loss=0.07477]\n",
      "Step 513004  [5.466 sec/step, loss=0.07542, avg_loss=0.07478]\n",
      "Step 513005  [5.478 sec/step, loss=0.07605, avg_loss=0.07479]\n",
      "Step 513006  [5.424 sec/step, loss=0.07632, avg_loss=0.07489]\n",
      "Step 513007  [5.441 sec/step, loss=0.07304, avg_loss=0.07487]\n",
      "Step 513008  [5.450 sec/step, loss=0.07579, avg_loss=0.07489]\n",
      "Step 513009  [5.451 sec/step, loss=0.07704, avg_loss=0.07489]\n",
      "Step 513010  [5.437 sec/step, loss=0.07212, avg_loss=0.07484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 513011  [5.431 sec/step, loss=0.07543, avg_loss=0.07482]\n",
      "Step 513012  [5.405 sec/step, loss=0.07499, avg_loss=0.07481]\n",
      "Step 513013  [5.382 sec/step, loss=0.07449, avg_loss=0.07480]\n",
      "Step 513014  [5.385 sec/step, loss=0.07707, avg_loss=0.07481]\n",
      "Step 513015  [5.391 sec/step, loss=0.07423, avg_loss=0.07481]\n",
      "Step 513016  [5.395 sec/step, loss=0.07763, avg_loss=0.07485]\n",
      "Step 513017  [5.409 sec/step, loss=0.07481, avg_loss=0.07485]\n",
      "Step 513018  [5.392 sec/step, loss=0.07480, avg_loss=0.07485]\n",
      "Step 513019  [5.404 sec/step, loss=0.07734, avg_loss=0.07488]\n",
      "Step 513020  [5.418 sec/step, loss=0.07689, avg_loss=0.07492]\n",
      "Step 513021  [5.399 sec/step, loss=0.07290, avg_loss=0.07490]\n",
      "Step 513022  [5.394 sec/step, loss=0.07555, avg_loss=0.07489]\n",
      "Step 513023  [5.343 sec/step, loss=0.07643, avg_loss=0.07498]\n",
      "Step 513024  [5.331 sec/step, loss=0.07384, avg_loss=0.07495]\n",
      "Step 513025  [5.391 sec/step, loss=0.06728, avg_loss=0.07488]\n",
      "Step 513026  [5.391 sec/step, loss=0.07664, avg_loss=0.07487]\n",
      "Step 513027  [5.361 sec/step, loss=0.07604, avg_loss=0.07488]\n",
      "Step 513028  [5.358 sec/step, loss=0.07663, avg_loss=0.07488]\n",
      "Step 513029  [5.348 sec/step, loss=0.07591, avg_loss=0.07486]\n",
      "Step 513030  [5.363 sec/step, loss=0.07565, avg_loss=0.07490]\n",
      "Step 513031  [5.387 sec/step, loss=0.07475, avg_loss=0.07491]\n",
      "Step 513032  [5.389 sec/step, loss=0.07622, avg_loss=0.07494]\n",
      "Generated 32 batches of size 32 in 2.381 sec\n",
      "Step 513033  [5.388 sec/step, loss=0.07638, avg_loss=0.07494]\n",
      "Step 513034  [5.362 sec/step, loss=0.06646, avg_loss=0.07483]\n",
      "Step 513035  [5.379 sec/step, loss=0.07369, avg_loss=0.07481]\n",
      "Step 513036  [5.369 sec/step, loss=0.07219, avg_loss=0.07477]\n",
      "Step 513037  [5.362 sec/step, loss=0.07633, avg_loss=0.07478]\n",
      "Step 513038  [5.345 sec/step, loss=0.07514, avg_loss=0.07476]\n",
      "Step 513039  [5.349 sec/step, loss=0.07739, avg_loss=0.07477]\n",
      "Step 513040  [5.346 sec/step, loss=0.07470, avg_loss=0.07475]\n",
      "Step 513041  [5.359 sec/step, loss=0.07514, avg_loss=0.07478]\n",
      "Step 513042  [5.339 sec/step, loss=0.07457, avg_loss=0.07475]\n",
      "Step 513043  [5.353 sec/step, loss=0.07465, avg_loss=0.07483]\n",
      "Step 513044  [5.356 sec/step, loss=0.07558, avg_loss=0.07484]\n",
      "Step 513045  [5.355 sec/step, loss=0.07590, avg_loss=0.07485]\n",
      "Step 513046  [5.323 sec/step, loss=0.07452, avg_loss=0.07485]\n",
      "Step 513047  [5.339 sec/step, loss=0.07694, avg_loss=0.07488]\n",
      "Step 513048  [5.352 sec/step, loss=0.07688, avg_loss=0.07491]\n",
      "Step 513049  [5.355 sec/step, loss=0.07445, avg_loss=0.07491]\n",
      "Step 513050  [5.345 sec/step, loss=0.07623, avg_loss=0.07490]\n",
      "Step 513051  [5.345 sec/step, loss=0.07727, avg_loss=0.07492]\n",
      "Step 513052  [5.355 sec/step, loss=0.07551, avg_loss=0.07493]\n",
      "Step 513053  [5.339 sec/step, loss=0.07278, avg_loss=0.07491]\n",
      "Step 513054  [5.360 sec/step, loss=0.07778, avg_loss=0.07494]\n",
      "Step 513055  [5.349 sec/step, loss=0.07560, avg_loss=0.07493]\n",
      "Step 513056  [5.336 sec/step, loss=0.07235, avg_loss=0.07490]\n",
      "Step 513057  [5.357 sec/step, loss=0.07564, avg_loss=0.07493]\n",
      "Step 513058  [5.344 sec/step, loss=0.07259, avg_loss=0.07491]\n",
      "Step 513059  [5.355 sec/step, loss=0.07541, avg_loss=0.07492]\n",
      "Step 513060  [5.332 sec/step, loss=0.06568, avg_loss=0.07481]\n",
      "Step 513061  [5.319 sec/step, loss=0.07495, avg_loss=0.07480]\n",
      "Step 513062  [5.319 sec/step, loss=0.07616, avg_loss=0.07480]\n",
      "Step 513063  [5.338 sec/step, loss=0.07632, avg_loss=0.07490]\n",
      "Step 513064  [5.368 sec/step, loss=0.07400, avg_loss=0.07491]\n",
      "Generated 32 batches of size 32 in 2.342 sec\n",
      "Step 513065  [5.385 sec/step, loss=0.07701, avg_loss=0.07491]\n",
      "Step 513066  [5.363 sec/step, loss=0.07358, avg_loss=0.07488]\n",
      "Step 513067  [5.417 sec/step, loss=0.06808, avg_loss=0.07482]\n",
      "Step 513068  [5.427 sec/step, loss=0.07632, avg_loss=0.07481]\n",
      "Step 513069  [5.453 sec/step, loss=0.07626, avg_loss=0.07484]\n",
      "Step 513070  [5.405 sec/step, loss=0.07475, avg_loss=0.07492]\n",
      "Step 513071  [5.405 sec/step, loss=0.07680, avg_loss=0.07493]\n",
      "Step 513072  [5.395 sec/step, loss=0.07589, avg_loss=0.07494]\n",
      "Step 513073  [5.379 sec/step, loss=0.07646, avg_loss=0.07494]\n",
      "Step 513074  [5.369 sec/step, loss=0.07227, avg_loss=0.07489]\n",
      "Step 513075  [5.354 sec/step, loss=0.07314, avg_loss=0.07488]\n",
      "Step 513076  [5.336 sec/step, loss=0.06978, avg_loss=0.07481]\n",
      "Step 513077  [5.303 sec/step, loss=0.07354, avg_loss=0.07481]\n",
      "Step 513078  [5.306 sec/step, loss=0.07563, avg_loss=0.07481]\n",
      "Step 513079  [5.294 sec/step, loss=0.07391, avg_loss=0.07479]\n",
      "Step 513080  [5.306 sec/step, loss=0.07545, avg_loss=0.07481]\n",
      "Step 513081  [5.298 sec/step, loss=0.07645, avg_loss=0.07481]\n",
      "Step 513082  [5.294 sec/step, loss=0.07244, avg_loss=0.07479]\n",
      "Step 513083  [5.306 sec/step, loss=0.07580, avg_loss=0.07479]\n",
      "Step 513084  [5.347 sec/step, loss=0.06610, avg_loss=0.07469]\n",
      "Step 513085  [5.361 sec/step, loss=0.07633, avg_loss=0.07471]\n",
      "Step 513086  [5.372 sec/step, loss=0.07746, avg_loss=0.07472]\n",
      "Step 513087  [5.368 sec/step, loss=0.07124, avg_loss=0.07469]\n",
      "Step 513088  [5.388 sec/step, loss=0.07671, avg_loss=0.07479]\n",
      "Step 513089  [5.390 sec/step, loss=0.07498, avg_loss=0.07480]\n",
      "Step 513090  [5.393 sec/step, loss=0.07502, avg_loss=0.07482]\n",
      "Step 513091  [5.401 sec/step, loss=0.07555, avg_loss=0.07482]\n",
      "Step 513092  [5.398 sec/step, loss=0.07688, avg_loss=0.07485]\n",
      "Step 513093  [5.385 sec/step, loss=0.07620, avg_loss=0.07484]\n",
      "Step 513094  [5.394 sec/step, loss=0.07652, avg_loss=0.07484]\n",
      "Step 513095  [5.393 sec/step, loss=0.07415, avg_loss=0.07483]\n",
      "Step 513096  [5.385 sec/step, loss=0.07672, avg_loss=0.07484]\n",
      "Generated 32 batches of size 32 in 2.494 sec\n",
      "Step 513097  [5.398 sec/step, loss=0.07588, avg_loss=0.07484]\n",
      "Step 513098  [5.399 sec/step, loss=0.07487, avg_loss=0.07483]\n",
      "Step 513099  [5.394 sec/step, loss=0.07682, avg_loss=0.07484]\n",
      "Step 513100  [5.400 sec/step, loss=0.07666, avg_loss=0.07483]\n",
      "Writing summary at step: 513100\n",
      "Step 513101  [5.406 sec/step, loss=0.07622, avg_loss=0.07487]\n",
      "Step 513102  [5.408 sec/step, loss=0.07645, avg_loss=0.07488]\n",
      "Step 513103  [5.404 sec/step, loss=0.07485, avg_loss=0.07490]\n",
      "Step 513104  [5.408 sec/step, loss=0.07756, avg_loss=0.07492]\n",
      "Step 513105  [5.408 sec/step, loss=0.07576, avg_loss=0.07492]\n",
      "Step 513106  [5.413 sec/step, loss=0.07681, avg_loss=0.07492]\n",
      "Step 513107  [5.401 sec/step, loss=0.07679, avg_loss=0.07496]\n",
      "Step 513108  [5.401 sec/step, loss=0.07495, avg_loss=0.07495]\n",
      "Step 513109  [5.377 sec/step, loss=0.07095, avg_loss=0.07489]\n",
      "Step 513110  [5.361 sec/step, loss=0.06372, avg_loss=0.07480]\n",
      "Step 513111  [5.357 sec/step, loss=0.07452, avg_loss=0.07480]\n",
      "Step 513112  [5.365 sec/step, loss=0.07634, avg_loss=0.07481]\n",
      "Step 513113  [5.367 sec/step, loss=0.07465, avg_loss=0.07481]\n",
      "Step 513114  [5.353 sec/step, loss=0.07435, avg_loss=0.07478]\n",
      "Step 513115  [5.333 sec/step, loss=0.07177, avg_loss=0.07476]\n",
      "Step 513116  [5.335 sec/step, loss=0.07437, avg_loss=0.07473]\n",
      "Step 513117  [5.320 sec/step, loss=0.07489, avg_loss=0.07473]\n",
      "Step 513118  [5.342 sec/step, loss=0.07573, avg_loss=0.07474]\n",
      "Step 513119  [5.361 sec/step, loss=0.07406, avg_loss=0.07470]\n",
      "Step 513120  [5.357 sec/step, loss=0.07728, avg_loss=0.07471]\n",
      "Step 513121  [5.368 sec/step, loss=0.07213, avg_loss=0.07470]\n",
      "Step 513122  [5.377 sec/step, loss=0.07700, avg_loss=0.07471]\n",
      "Step 513123  [5.376 sec/step, loss=0.07573, avg_loss=0.07471]\n",
      "Step 513124  [5.382 sec/step, loss=0.07264, avg_loss=0.07469]\n",
      "Step 513125  [5.339 sec/step, loss=0.07621, avg_loss=0.07478]\n",
      "Step 513126  [5.338 sec/step, loss=0.07692, avg_loss=0.07479]\n",
      "Step 513127  [5.333 sec/step, loss=0.07392, avg_loss=0.07477]\n",
      "Generated 32 batches of size 32 in 2.373 sec\n",
      "Step 513128  [5.330 sec/step, loss=0.07566, avg_loss=0.07476]\n",
      "Step 513129  [5.320 sec/step, loss=0.07334, avg_loss=0.07473]\n",
      "Step 513130  [5.317 sec/step, loss=0.07591, avg_loss=0.07473]\n",
      "Step 513131  [5.287 sec/step, loss=0.07482, avg_loss=0.07473]\n",
      "Step 513132  [5.279 sec/step, loss=0.07583, avg_loss=0.07473]\n",
      "Step 513133  [5.283 sec/step, loss=0.07715, avg_loss=0.07474]\n",
      "Step 513134  [5.314 sec/step, loss=0.07684, avg_loss=0.07484]\n",
      "Step 513135  [5.300 sec/step, loss=0.07520, avg_loss=0.07486]\n",
      "Step 513136  [5.363 sec/step, loss=0.06723, avg_loss=0.07481]\n",
      "Step 513137  [5.364 sec/step, loss=0.07556, avg_loss=0.07480]\n",
      "Step 513138  [5.355 sec/step, loss=0.06756, avg_loss=0.07472]\n",
      "Step 513139  [5.337 sec/step, loss=0.07424, avg_loss=0.07469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 513140  [5.336 sec/step, loss=0.07494, avg_loss=0.07469]\n",
      "Step 513141  [5.337 sec/step, loss=0.07699, avg_loss=0.07471]\n",
      "Step 513142  [5.333 sec/step, loss=0.07131, avg_loss=0.07468]\n",
      "Step 513143  [5.334 sec/step, loss=0.07248, avg_loss=0.07466]\n",
      "Step 513144  [5.338 sec/step, loss=0.07698, avg_loss=0.07467]\n",
      "Step 513145  [5.328 sec/step, loss=0.07221, avg_loss=0.07464]\n",
      "Step 513146  [5.384 sec/step, loss=0.06691, avg_loss=0.07456]\n",
      "Step 513147  [5.390 sec/step, loss=0.07664, avg_loss=0.07456]\n",
      "Step 513148  [5.382 sec/step, loss=0.07543, avg_loss=0.07454]\n",
      "Step 513149  [5.367 sec/step, loss=0.07651, avg_loss=0.07456]\n",
      "Step 513150  [5.377 sec/step, loss=0.07458, avg_loss=0.07455]\n",
      "Step 513151  [5.395 sec/step, loss=0.07421, avg_loss=0.07452]\n",
      "Step 513152  [5.395 sec/step, loss=0.07194, avg_loss=0.07448]\n",
      "Step 513153  [5.390 sec/step, loss=0.07495, avg_loss=0.07450]\n",
      "Step 513154  [5.377 sec/step, loss=0.07591, avg_loss=0.07448]\n",
      "Step 513155  [5.385 sec/step, loss=0.07656, avg_loss=0.07449]\n",
      "Step 513156  [5.402 sec/step, loss=0.07753, avg_loss=0.07454]\n",
      "Step 513157  [5.387 sec/step, loss=0.07662, avg_loss=0.07455]\n",
      "Step 513158  [5.408 sec/step, loss=0.07630, avg_loss=0.07459]\n",
      "Step 513159  [5.430 sec/step, loss=0.07413, avg_loss=0.07458]\n",
      "Generated 32 batches of size 32 in 2.400 sec\n",
      "Step 513160  [5.455 sec/step, loss=0.07574, avg_loss=0.07468]\n",
      "Step 513161  [5.455 sec/step, loss=0.07499, avg_loss=0.07468]\n",
      "Step 513162  [5.467 sec/step, loss=0.07714, avg_loss=0.07469]\n",
      "Step 513163  [5.479 sec/step, loss=0.07745, avg_loss=0.07470]\n",
      "Step 513164  [5.450 sec/step, loss=0.07427, avg_loss=0.07470]\n",
      "Step 513165  [5.437 sec/step, loss=0.07616, avg_loss=0.07469]\n",
      "Step 513166  [5.446 sec/step, loss=0.07640, avg_loss=0.07472]\n",
      "Step 513167  [5.393 sec/step, loss=0.07338, avg_loss=0.07478]\n",
      "Step 513168  [5.383 sec/step, loss=0.07727, avg_loss=0.07478]\n",
      "Step 513169  [5.373 sec/step, loss=0.07697, avg_loss=0.07479]\n",
      "Step 513170  [5.358 sec/step, loss=0.07280, avg_loss=0.07477]\n",
      "Step 513171  [5.344 sec/step, loss=0.07484, avg_loss=0.07475]\n",
      "Step 513172  [5.337 sec/step, loss=0.07498, avg_loss=0.07474]\n",
      "Step 513173  [5.344 sec/step, loss=0.07699, avg_loss=0.07475]\n",
      "Step 513174  [5.352 sec/step, loss=0.07711, avg_loss=0.07480]\n",
      "Step 513175  [5.359 sec/step, loss=0.07226, avg_loss=0.07479]\n",
      "Step 513176  [5.384 sec/step, loss=0.07697, avg_loss=0.07486]\n",
      "Step 513177  [5.402 sec/step, loss=0.07618, avg_loss=0.07489]\n",
      "Step 513178  [5.430 sec/step, loss=0.07369, avg_loss=0.07487]\n",
      "Step 513179  [5.449 sec/step, loss=0.07617, avg_loss=0.07489]\n",
      "Step 513180  [5.441 sec/step, loss=0.07593, avg_loss=0.07489]\n",
      "Step 513181  [5.429 sec/step, loss=0.07267, avg_loss=0.07486]\n",
      "Step 513182  [5.432 sec/step, loss=0.07547, avg_loss=0.07489]\n",
      "Step 513183  [5.421 sec/step, loss=0.07499, avg_loss=0.07488]\n",
      "Step 513184  [5.367 sec/step, loss=0.07621, avg_loss=0.07498]\n",
      "Step 513185  [5.374 sec/step, loss=0.07686, avg_loss=0.07499]\n",
      "Step 513186  [5.356 sec/step, loss=0.07486, avg_loss=0.07496]\n",
      "Step 513187  [5.362 sec/step, loss=0.07619, avg_loss=0.07501]\n",
      "Step 513188  [5.362 sec/step, loss=0.07575, avg_loss=0.07500]\n",
      "Step 513189  [5.359 sec/step, loss=0.07143, avg_loss=0.07496]\n",
      "Step 513190  [5.365 sec/step, loss=0.07578, avg_loss=0.07497]\n",
      "Step 513191  [5.358 sec/step, loss=0.07477, avg_loss=0.07496]\n",
      "Generated 32 batches of size 32 in 2.445 sec\n",
      "Step 513192  [5.367 sec/step, loss=0.07761, avg_loss=0.07497]\n",
      "Step 513193  [5.422 sec/step, loss=0.06749, avg_loss=0.07488]\n",
      "Step 513194  [5.411 sec/step, loss=0.07519, avg_loss=0.07487]\n",
      "Step 513195  [5.405 sec/step, loss=0.07446, avg_loss=0.07487]\n",
      "Step 513196  [5.401 sec/step, loss=0.07551, avg_loss=0.07486]\n",
      "Step 513197  [5.403 sec/step, loss=0.07726, avg_loss=0.07488]\n",
      "Step 513198  [5.418 sec/step, loss=0.07672, avg_loss=0.07489]\n",
      "Step 513199  [5.412 sec/step, loss=0.07493, avg_loss=0.07488]\n",
      "Step 513200  [5.379 sec/step, loss=0.06680, avg_loss=0.07478]\n",
      "Writing summary at step: 513200\n",
      "Step 513201  [5.390 sec/step, loss=0.07681, avg_loss=0.07478]\n",
      "Step 513202  [5.399 sec/step, loss=0.07723, avg_loss=0.07479]\n",
      "Step 513203  [5.431 sec/step, loss=0.07413, avg_loss=0.07478]\n",
      "Step 513204  [5.429 sec/step, loss=0.07741, avg_loss=0.07478]\n",
      "Step 513205  [5.429 sec/step, loss=0.07600, avg_loss=0.07478]\n",
      "Step 513206  [5.409 sec/step, loss=0.06722, avg_loss=0.07469]\n",
      "Step 513207  [5.446 sec/step, loss=0.06662, avg_loss=0.07459]\n",
      "Step 513208  [5.456 sec/step, loss=0.07805, avg_loss=0.07462]\n",
      "Step 513209  [5.483 sec/step, loss=0.07684, avg_loss=0.07468]\n",
      "Step 513210  [5.502 sec/step, loss=0.07685, avg_loss=0.07481]\n",
      "Step 513211  [5.491 sec/step, loss=0.07470, avg_loss=0.07481]\n",
      "Step 513212  [5.502 sec/step, loss=0.07648, avg_loss=0.07481]\n",
      "Step 513213  [5.507 sec/step, loss=0.07522, avg_loss=0.07482]\n",
      "Step 513214  [5.528 sec/step, loss=0.07544, avg_loss=0.07483]\n",
      "Step 513215  [5.534 sec/step, loss=0.07515, avg_loss=0.07486]\n",
      "Step 513216  [5.526 sec/step, loss=0.07602, avg_loss=0.07488]\n",
      "Step 513217  [5.531 sec/step, loss=0.07603, avg_loss=0.07489]\n",
      "Step 513218  [5.501 sec/step, loss=0.07368, avg_loss=0.07487]\n",
      "Step 513219  [5.474 sec/step, loss=0.07575, avg_loss=0.07489]\n",
      "Step 513220  [5.466 sec/step, loss=0.07507, avg_loss=0.07486]\n",
      "Step 513221  [5.462 sec/step, loss=0.07512, avg_loss=0.07489]\n",
      "Step 513222  [5.453 sec/step, loss=0.07347, avg_loss=0.07486]\n",
      "Generated 32 batches of size 32 in 2.454 sec\n",
      "Step 513223  [5.455 sec/step, loss=0.07392, avg_loss=0.07484]\n",
      "Step 513224  [5.458 sec/step, loss=0.07635, avg_loss=0.07488]\n",
      "Step 513225  [5.457 sec/step, loss=0.07384, avg_loss=0.07485]\n",
      "Step 513226  [5.450 sec/step, loss=0.07126, avg_loss=0.07480]\n",
      "Step 513227  [5.461 sec/step, loss=0.07298, avg_loss=0.07479]\n",
      "Step 513228  [5.464 sec/step, loss=0.07607, avg_loss=0.07479]\n",
      "Step 513229  [5.477 sec/step, loss=0.07749, avg_loss=0.07483]\n",
      "Step 513230  [5.467 sec/step, loss=0.07155, avg_loss=0.07479]\n",
      "Step 513231  [5.490 sec/step, loss=0.07550, avg_loss=0.07480]\n",
      "Step 513232  [5.496 sec/step, loss=0.07579, avg_loss=0.07480]\n",
      "Step 513233  [5.499 sec/step, loss=0.07684, avg_loss=0.07479]\n",
      "Step 513234  [5.535 sec/step, loss=0.06681, avg_loss=0.07469]\n",
      "Step 513235  [5.549 sec/step, loss=0.07249, avg_loss=0.07467]\n",
      "Step 513236  [5.497 sec/step, loss=0.07570, avg_loss=0.07475]\n",
      "Step 513237  [5.502 sec/step, loss=0.07616, avg_loss=0.07476]\n",
      "Step 513238  [5.522 sec/step, loss=0.07670, avg_loss=0.07485]\n",
      "Step 513239  [5.536 sec/step, loss=0.07496, avg_loss=0.07485]\n",
      "Step 513240  [5.546 sec/step, loss=0.07525, avg_loss=0.07486]\n",
      "Step 513241  [5.544 sec/step, loss=0.07622, avg_loss=0.07485]\n",
      "Step 513242  [5.557 sec/step, loss=0.07527, avg_loss=0.07489]\n",
      "Step 513243  [5.558 sec/step, loss=0.07215, avg_loss=0.07489]\n",
      "Step 513244  [5.542 sec/step, loss=0.07408, avg_loss=0.07486]\n",
      "Step 513245  [5.539 sec/step, loss=0.06751, avg_loss=0.07481]\n",
      "Step 513246  [5.505 sec/step, loss=0.07635, avg_loss=0.07490]\n",
      "Step 513247  [5.488 sec/step, loss=0.07460, avg_loss=0.07488]\n",
      "Step 513248  [5.493 sec/step, loss=0.07728, avg_loss=0.07490]\n",
      "Step 513249  [5.508 sec/step, loss=0.07699, avg_loss=0.07491]\n",
      "Step 513250  [5.499 sec/step, loss=0.07296, avg_loss=0.07489]\n",
      "Step 513251  [5.466 sec/step, loss=0.07181, avg_loss=0.07487]\n",
      "Step 513252  [5.466 sec/step, loss=0.07546, avg_loss=0.07490]\n",
      "Step 513253  [5.476 sec/step, loss=0.07635, avg_loss=0.07492]\n",
      "Step 513254  [5.465 sec/step, loss=0.07513, avg_loss=0.07491]\n",
      "Generated 32 batches of size 32 in 2.395 sec\n",
      "Step 513255  [5.466 sec/step, loss=0.07490, avg_loss=0.07489]\n",
      "Step 513256  [5.468 sec/step, loss=0.07699, avg_loss=0.07489]\n",
      "Step 513257  [5.476 sec/step, loss=0.07600, avg_loss=0.07488]\n",
      "Step 513258  [5.464 sec/step, loss=0.07460, avg_loss=0.07486]\n",
      "Step 513259  [5.430 sec/step, loss=0.07510, avg_loss=0.07487]\n",
      "Step 513260  [5.441 sec/step, loss=0.07670, avg_loss=0.07488]\n",
      "Step 513261  [5.465 sec/step, loss=0.07653, avg_loss=0.07490]\n",
      "Step 513262  [5.462 sec/step, loss=0.07708, avg_loss=0.07490]\n",
      "Step 513263  [5.435 sec/step, loss=0.07268, avg_loss=0.07485]\n",
      "Step 513264  [5.429 sec/step, loss=0.07205, avg_loss=0.07483]\n",
      "Step 513265  [5.427 sec/step, loss=0.07554, avg_loss=0.07482]\n",
      "Step 513266  [5.430 sec/step, loss=0.07678, avg_loss=0.07483]\n",
      "Step 513267  [5.426 sec/step, loss=0.07532, avg_loss=0.07484]\n",
      "Step 513268  [5.435 sec/step, loss=0.07630, avg_loss=0.07484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 513269  [5.461 sec/step, loss=0.07376, avg_loss=0.07480]\n",
      "Step 513270  [5.485 sec/step, loss=0.07653, avg_loss=0.07484]\n",
      "Step 513271  [5.490 sec/step, loss=0.07523, avg_loss=0.07484]\n",
      "Step 513272  [5.510 sec/step, loss=0.07436, avg_loss=0.07484]\n",
      "Step 513273  [5.497 sec/step, loss=0.07437, avg_loss=0.07481]\n",
      "Step 513274  [5.490 sec/step, loss=0.07660, avg_loss=0.07481]\n",
      "Step 513275  [5.489 sec/step, loss=0.07303, avg_loss=0.07481]\n",
      "Step 513276  [5.489 sec/step, loss=0.07721, avg_loss=0.07482]\n",
      "Step 513277  [5.488 sec/step, loss=0.07726, avg_loss=0.07483]\n",
      "Step 513278  [5.474 sec/step, loss=0.07680, avg_loss=0.07486]\n",
      "Step 513279  [5.474 sec/step, loss=0.07625, avg_loss=0.07486]\n",
      "Step 513280  [5.472 sec/step, loss=0.07261, avg_loss=0.07483]\n",
      "Step 513281  [5.481 sec/step, loss=0.07700, avg_loss=0.07487]\n",
      "Step 513282  [5.491 sec/step, loss=0.07381, avg_loss=0.07485]\n",
      "Step 513283  [5.490 sec/step, loss=0.07637, avg_loss=0.07487]\n",
      "Step 513284  [5.491 sec/step, loss=0.07587, avg_loss=0.07486]\n",
      "Step 513285  [5.496 sec/step, loss=0.07719, avg_loss=0.07487]\n",
      "Step 513286  [5.508 sec/step, loss=0.07559, avg_loss=0.07487]\n",
      "Generated 32 batches of size 32 in 2.401 sec\n",
      "Step 513287  [5.520 sec/step, loss=0.07249, avg_loss=0.07484]\n",
      "Step 513288  [5.497 sec/step, loss=0.06696, avg_loss=0.07475]\n",
      "Step 513289  [5.502 sec/step, loss=0.07521, avg_loss=0.07479]\n",
      "Step 513290  [5.512 sec/step, loss=0.07721, avg_loss=0.07480]\n",
      "Step 513291  [5.566 sec/step, loss=0.06766, avg_loss=0.07473]\n",
      "Step 513292  [5.539 sec/step, loss=0.07526, avg_loss=0.07471]\n",
      "Step 513293  [5.499 sec/step, loss=0.07752, avg_loss=0.07481]\n",
      "Step 513294  [5.491 sec/step, loss=0.07291, avg_loss=0.07478]\n",
      "Step 513295  [5.484 sec/step, loss=0.07234, avg_loss=0.07476]\n",
      "Step 513296  [5.488 sec/step, loss=0.07492, avg_loss=0.07476]\n",
      "Step 513297  [5.482 sec/step, loss=0.07563, avg_loss=0.07474]\n",
      "Step 513298  [5.467 sec/step, loss=0.07258, avg_loss=0.07470]\n",
      "Step 513299  [5.457 sec/step, loss=0.07586, avg_loss=0.07471]\n",
      "Step 513300  [5.484 sec/step, loss=0.07650, avg_loss=0.07481]\n",
      "Writing summary at step: 513300\n",
      "Step 513301  [5.486 sec/step, loss=0.07694, avg_loss=0.07481]\n",
      "Step 513302  [5.478 sec/step, loss=0.07621, avg_loss=0.07480]\n",
      "Step 513303  [5.449 sec/step, loss=0.07484, avg_loss=0.07480]\n",
      "Step 513304  [5.440 sec/step, loss=0.07596, avg_loss=0.07479]\n",
      "Step 513305  [5.428 sec/step, loss=0.07511, avg_loss=0.07478]\n",
      "Step 513306  [5.439 sec/step, loss=0.07507, avg_loss=0.07486]\n",
      "Step 513307  [5.395 sec/step, loss=0.07294, avg_loss=0.07492]\n",
      "Step 513308  [5.389 sec/step, loss=0.07677, avg_loss=0.07491]\n",
      "Step 513309  [5.390 sec/step, loss=0.07709, avg_loss=0.07491]\n",
      "Step 513310  [5.393 sec/step, loss=0.07578, avg_loss=0.07490]\n",
      "Step 513311  [5.407 sec/step, loss=0.07696, avg_loss=0.07492]\n",
      "Step 513312  [5.396 sec/step, loss=0.07395, avg_loss=0.07490]\n",
      "Step 513313  [5.447 sec/step, loss=0.06626, avg_loss=0.07481]\n",
      "Step 513314  [5.428 sec/step, loss=0.07452, avg_loss=0.07480]\n",
      "Step 513315  [5.422 sec/step, loss=0.07278, avg_loss=0.07478]\n",
      "Step 513316  [5.418 sec/step, loss=0.07200, avg_loss=0.07474]\n",
      "Step 513317  [5.410 sec/step, loss=0.07519, avg_loss=0.07473]\n",
      "Generated 32 batches of size 32 in 2.414 sec\n",
      "Step 513318  [5.435 sec/step, loss=0.07664, avg_loss=0.07476]\n",
      "Step 513319  [5.438 sec/step, loss=0.07718, avg_loss=0.07477]\n",
      "Step 513320  [5.421 sec/step, loss=0.06627, avg_loss=0.07468]\n",
      "Step 513321  [5.437 sec/step, loss=0.07688, avg_loss=0.07470]\n",
      "Step 513322  [5.458 sec/step, loss=0.07473, avg_loss=0.07471]\n",
      "Step 513323  [5.460 sec/step, loss=0.07671, avg_loss=0.07474]\n",
      "Step 513324  [5.471 sec/step, loss=0.07651, avg_loss=0.07474]\n",
      "Step 513325  [5.473 sec/step, loss=0.07623, avg_loss=0.07477]\n",
      "Step 513326  [5.508 sec/step, loss=0.07364, avg_loss=0.07479]\n",
      "Step 513327  [5.504 sec/step, loss=0.07605, avg_loss=0.07482]\n",
      "Step 513328  [5.508 sec/step, loss=0.07690, avg_loss=0.07483]\n",
      "Step 513329  [5.498 sec/step, loss=0.07150, avg_loss=0.07477]\n",
      "Step 513330  [5.504 sec/step, loss=0.07621, avg_loss=0.07482]\n",
      "Step 513331  [5.505 sec/step, loss=0.07485, avg_loss=0.07481]\n",
      "Step 513332  [5.502 sec/step, loss=0.07578, avg_loss=0.07481]\n",
      "Step 513333  [5.474 sec/step, loss=0.07253, avg_loss=0.07477]\n",
      "Step 513334  [5.420 sec/step, loss=0.07479, avg_loss=0.07485]\n",
      "Step 513335  [5.401 sec/step, loss=0.07684, avg_loss=0.07489]\n",
      "Step 513336  [5.407 sec/step, loss=0.07591, avg_loss=0.07489]\n",
      "Step 513337  [5.412 sec/step, loss=0.07685, avg_loss=0.07490]\n",
      "Step 513338  [5.409 sec/step, loss=0.07484, avg_loss=0.07488]\n",
      "Step 513339  [5.399 sec/step, loss=0.07291, avg_loss=0.07486]\n",
      "Step 513340  [5.382 sec/step, loss=0.07352, avg_loss=0.07484]\n",
      "Step 513341  [5.390 sec/step, loss=0.07733, avg_loss=0.07485]\n",
      "Step 513342  [5.416 sec/step, loss=0.07418, avg_loss=0.07484]\n",
      "Step 513343  [5.409 sec/step, loss=0.07557, avg_loss=0.07488]\n",
      "Step 513344  [5.428 sec/step, loss=0.07708, avg_loss=0.07491]\n",
      "Step 513345  [5.430 sec/step, loss=0.06666, avg_loss=0.07490]\n",
      "Step 513346  [5.466 sec/step, loss=0.06793, avg_loss=0.07481]\n",
      "Step 513347  [5.474 sec/step, loss=0.07629, avg_loss=0.07483]\n",
      "Step 513348  [5.474 sec/step, loss=0.07705, avg_loss=0.07483]\n",
      "Step 513349  [5.463 sec/step, loss=0.07480, avg_loss=0.07481]\n",
      "Generated 32 batches of size 32 in 2.362 sec\n",
      "Step 513350  [5.472 sec/step, loss=0.07581, avg_loss=0.07484]\n",
      "Step 513351  [5.501 sec/step, loss=0.07643, avg_loss=0.07488]\n",
      "Step 513352  [5.489 sec/step, loss=0.07224, avg_loss=0.07485]\n",
      "Step 513353  [5.501 sec/step, loss=0.07419, avg_loss=0.07483]\n",
      "Step 513354  [5.528 sec/step, loss=0.07739, avg_loss=0.07485]\n",
      "Step 513355  [5.519 sec/step, loss=0.07646, avg_loss=0.07487]\n",
      "Step 513356  [5.513 sec/step, loss=0.07635, avg_loss=0.07486]\n",
      "Step 513357  [5.513 sec/step, loss=0.07559, avg_loss=0.07486]\n",
      "Step 513358  [5.511 sec/step, loss=0.07469, avg_loss=0.07486]\n",
      "Step 513359  [5.538 sec/step, loss=0.07600, avg_loss=0.07487]\n",
      "Step 513360  [5.532 sec/step, loss=0.07686, avg_loss=0.07487]\n",
      "Step 513361  [5.504 sec/step, loss=0.07233, avg_loss=0.07483]\n",
      "Step 513362  [5.492 sec/step, loss=0.07443, avg_loss=0.07480]\n",
      "Step 513363  [5.504 sec/step, loss=0.07621, avg_loss=0.07483]\n",
      "Step 513364  [5.524 sec/step, loss=0.07709, avg_loss=0.07488]\n",
      "Step 513365  [5.528 sec/step, loss=0.07636, avg_loss=0.07489]\n",
      "Step 513366  [5.520 sec/step, loss=0.07319, avg_loss=0.07486]\n",
      "Step 513367  [5.524 sec/step, loss=0.07422, avg_loss=0.07485]\n",
      "Step 513368  [5.508 sec/step, loss=0.07671, avg_loss=0.07485]\n",
      "Step 513369  [5.463 sec/step, loss=0.06738, avg_loss=0.07479]\n",
      "Step 513370  [5.461 sec/step, loss=0.07483, avg_loss=0.07477]\n",
      "Step 513371  [5.459 sec/step, loss=0.07383, avg_loss=0.07476]\n",
      "Step 513372  [5.459 sec/step, loss=0.07687, avg_loss=0.07478]\n",
      "Step 513373  [5.456 sec/step, loss=0.07502, avg_loss=0.07479]\n",
      "Step 513374  [5.458 sec/step, loss=0.07483, avg_loss=0.07477]\n",
      "Step 513375  [5.514 sec/step, loss=0.06777, avg_loss=0.07472]\n",
      "Step 513376  [5.498 sec/step, loss=0.07483, avg_loss=0.07469]\n",
      "Step 513377  [5.490 sec/step, loss=0.07588, avg_loss=0.07468]\n",
      "Step 513378  [5.470 sec/step, loss=0.07496, avg_loss=0.07466]\n",
      "Step 513379  [5.454 sec/step, loss=0.07191, avg_loss=0.07462]\n",
      "Step 513380  [5.452 sec/step, loss=0.07624, avg_loss=0.07465]\n",
      "Step 513381  [5.469 sec/step, loss=0.07635, avg_loss=0.07465]\n",
      "Generated 32 batches of size 32 in 2.427 sec\n",
      "Step 513382  [5.465 sec/step, loss=0.07535, avg_loss=0.07466]\n",
      "Step 513383  [5.472 sec/step, loss=0.07530, avg_loss=0.07465]\n",
      "Step 513384  [5.479 sec/step, loss=0.07579, avg_loss=0.07465]\n",
      "Step 513385  [5.465 sec/step, loss=0.07720, avg_loss=0.07465]\n",
      "Step 513386  [5.469 sec/step, loss=0.07709, avg_loss=0.07467]\n",
      "Step 513387  [5.471 sec/step, loss=0.07761, avg_loss=0.07472]\n",
      "Step 513388  [5.518 sec/step, loss=0.07390, avg_loss=0.07479]\n",
      "Step 513389  [5.522 sec/step, loss=0.07518, avg_loss=0.07479]\n",
      "Step 513390  [5.525 sec/step, loss=0.07489, avg_loss=0.07476]\n",
      "Step 513391  [5.474 sec/step, loss=0.07487, avg_loss=0.07483]\n",
      "Step 513392  [5.488 sec/step, loss=0.07663, avg_loss=0.07485]\n",
      "Step 513393  [5.478 sec/step, loss=0.07512, avg_loss=0.07482]\n",
      "Step 513394  [5.480 sec/step, loss=0.07490, avg_loss=0.07484]\n",
      "Step 513395  [5.503 sec/step, loss=0.07528, avg_loss=0.07487]\n",
      "Step 513396  [5.495 sec/step, loss=0.07594, avg_loss=0.07488]\n",
      "Step 513397  [5.492 sec/step, loss=0.07609, avg_loss=0.07489]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 513398  [5.480 sec/step, loss=0.07247, avg_loss=0.07489]\n",
      "Step 513399  [5.472 sec/step, loss=0.07542, avg_loss=0.07488]\n",
      "Step 513400  [5.462 sec/step, loss=0.07561, avg_loss=0.07487]\n",
      "Writing summary at step: 513400\n",
      "Step 513401  [5.456 sec/step, loss=0.07555, avg_loss=0.07486]\n",
      "Step 513402  [5.463 sec/step, loss=0.07702, avg_loss=0.07487]\n",
      "Step 513403  [5.452 sec/step, loss=0.07318, avg_loss=0.07485]\n",
      "Step 513404  [5.451 sec/step, loss=0.07494, avg_loss=0.07484]\n",
      "Step 513405  [5.463 sec/step, loss=0.07605, avg_loss=0.07485]\n",
      "Step 513406  [5.482 sec/step, loss=0.07692, avg_loss=0.07487]\n",
      "Step 513407  [5.482 sec/step, loss=0.07589, avg_loss=0.07490]\n",
      "Step 513408  [5.471 sec/step, loss=0.07423, avg_loss=0.07487]\n",
      "Step 513409  [5.508 sec/step, loss=0.06538, avg_loss=0.07476]\n",
      "Step 513410  [5.502 sec/step, loss=0.07235, avg_loss=0.07472]\n",
      "Step 513411  [5.517 sec/step, loss=0.07501, avg_loss=0.07470]\n",
      "Step 513412  [5.513 sec/step, loss=0.07460, avg_loss=0.07471]\n",
      "Generated 32 batches of size 32 in 2.470 sec\n",
      "Step 513413  [5.477 sec/step, loss=0.07721, avg_loss=0.07482]\n",
      "Step 513414  [5.478 sec/step, loss=0.07605, avg_loss=0.07483]\n",
      "Step 513415  [5.497 sec/step, loss=0.07700, avg_loss=0.07488]\n",
      "Step 513416  [5.482 sec/step, loss=0.06634, avg_loss=0.07482]\n",
      "Step 513417  [5.502 sec/step, loss=0.07500, avg_loss=0.07482]\n",
      "Step 513418  [5.507 sec/step, loss=0.07639, avg_loss=0.07482]\n",
      "Step 513419  [5.524 sec/step, loss=0.07594, avg_loss=0.07480]\n",
      "Step 513420  [5.548 sec/step, loss=0.07656, avg_loss=0.07491]\n",
      "Step 513421  [5.552 sec/step, loss=0.07693, avg_loss=0.07491]\n",
      "Step 513422  [5.531 sec/step, loss=0.07186, avg_loss=0.07488]\n",
      "Step 513423  [5.530 sec/step, loss=0.07597, avg_loss=0.07487]\n",
      "Step 513424  [5.527 sec/step, loss=0.07443, avg_loss=0.07485]\n",
      "Step 513425  [5.529 sec/step, loss=0.07695, avg_loss=0.07486]\n",
      "Step 513426  [5.483 sec/step, loss=0.06743, avg_loss=0.07479]\n",
      "Step 513427  [5.504 sec/step, loss=0.07537, avg_loss=0.07479]\n",
      "Step 513428  [5.495 sec/step, loss=0.07596, avg_loss=0.07478]\n",
      "Step 513429  [5.505 sec/step, loss=0.07596, avg_loss=0.07482]\n",
      "Step 513430  [5.522 sec/step, loss=0.07673, avg_loss=0.07483]\n",
      "Step 513431  [5.522 sec/step, loss=0.07509, avg_loss=0.07483]\n",
      "Step 513432  [5.512 sec/step, loss=0.07138, avg_loss=0.07479]\n",
      "Step 513433  [5.533 sec/step, loss=0.07667, avg_loss=0.07483]\n",
      "Step 513434  [5.566 sec/step, loss=0.07242, avg_loss=0.07480]\n",
      "Step 513435  [5.547 sec/step, loss=0.07481, avg_loss=0.07478]\n",
      "Step 513436  [5.551 sec/step, loss=0.07680, avg_loss=0.07479]\n",
      "Step 513437  [5.538 sec/step, loss=0.07447, avg_loss=0.07477]\n",
      "Step 513438  [5.541 sec/step, loss=0.07316, avg_loss=0.07475]\n",
      "Step 513439  [5.533 sec/step, loss=0.07237, avg_loss=0.07475]\n",
      "Step 513440  [5.552 sec/step, loss=0.07719, avg_loss=0.07478]\n",
      "Step 513441  [5.550 sec/step, loss=0.07564, avg_loss=0.07477]\n",
      "Step 513442  [5.538 sec/step, loss=0.07792, avg_loss=0.07480]\n",
      "Step 513443  [5.541 sec/step, loss=0.07471, avg_loss=0.07480]\n",
      "Step 513444  [5.525 sec/step, loss=0.07332, avg_loss=0.07476]\n",
      "Generated 32 batches of size 32 in 2.445 sec\n",
      "Step 513445  [5.548 sec/step, loss=0.07367, avg_loss=0.07483]\n",
      "Step 513446  [5.490 sec/step, loss=0.07524, avg_loss=0.07490]\n",
      "Step 513447  [5.487 sec/step, loss=0.07660, avg_loss=0.07490]\n",
      "Step 513448  [5.529 sec/step, loss=0.06880, avg_loss=0.07482]\n",
      "Step 513449  [5.530 sec/step, loss=0.07459, avg_loss=0.07482]\n",
      "Step 513450  [5.528 sec/step, loss=0.07277, avg_loss=0.07479]\n",
      "Step 513451  [5.508 sec/step, loss=0.07613, avg_loss=0.07479]\n",
      "Step 513452  [5.520 sec/step, loss=0.07568, avg_loss=0.07482]\n",
      "Step 513453  [5.504 sec/step, loss=0.07579, avg_loss=0.07484]\n",
      "Step 513454  [5.484 sec/step, loss=0.07393, avg_loss=0.07480]\n",
      "Step 513455  [5.499 sec/step, loss=0.07639, avg_loss=0.07480]\n",
      "Step 513456  [5.502 sec/step, loss=0.07391, avg_loss=0.07478]\n",
      "Step 513457  [5.496 sec/step, loss=0.07492, avg_loss=0.07477]\n",
      "Step 513458  [5.488 sec/step, loss=0.07215, avg_loss=0.07474]\n",
      "Step 513459  [5.466 sec/step, loss=0.07585, avg_loss=0.07474]\n",
      "Step 513460  [5.451 sec/step, loss=0.07601, avg_loss=0.07473]\n",
      "Step 513461  [5.474 sec/step, loss=0.07694, avg_loss=0.07478]\n",
      "Step 513462  [5.483 sec/step, loss=0.07633, avg_loss=0.07480]\n",
      "Step 513463  [5.474 sec/step, loss=0.07263, avg_loss=0.07476]\n",
      "Step 513464  [5.476 sec/step, loss=0.07687, avg_loss=0.07476]\n",
      "Step 513465  [5.477 sec/step, loss=0.07358, avg_loss=0.07473]\n",
      "Step 513466  [5.489 sec/step, loss=0.07702, avg_loss=0.07477]\n",
      "Step 513467  [5.504 sec/step, loss=0.07459, avg_loss=0.07478]\n",
      "Step 513468  [5.494 sec/step, loss=0.07527, avg_loss=0.07476]\n",
      "Step 513469  [5.529 sec/step, loss=0.07595, avg_loss=0.07485]\n",
      "Step 513470  [5.505 sec/step, loss=0.06649, avg_loss=0.07476]\n",
      "Step 513471  [5.510 sec/step, loss=0.07602, avg_loss=0.07479]\n",
      "Step 513472  [5.498 sec/step, loss=0.07633, avg_loss=0.07478]\n",
      "Step 513473  [5.534 sec/step, loss=0.07538, avg_loss=0.07478]\n",
      "Step 513474  [5.519 sec/step, loss=0.07501, avg_loss=0.07479]\n",
      "Step 513475  [5.466 sec/step, loss=0.07254, avg_loss=0.07483]\n",
      "Step 513476  [5.474 sec/step, loss=0.07584, avg_loss=0.07484]\n",
      "Generated 32 batches of size 32 in 2.607 sec\n",
      "Step 513477  [5.524 sec/step, loss=0.06653, avg_loss=0.07475]\n",
      "Step 513478  [5.536 sec/step, loss=0.07578, avg_loss=0.07476]\n",
      "Step 513479  [5.559 sec/step, loss=0.07692, avg_loss=0.07481]\n",
      "Step 513480  [5.566 sec/step, loss=0.07636, avg_loss=0.07481]\n",
      "Step 513481  [5.553 sec/step, loss=0.07535, avg_loss=0.07480]\n",
      "Step 513482  [5.557 sec/step, loss=0.07744, avg_loss=0.07482]\n",
      "Step 513483  [5.548 sec/step, loss=0.07500, avg_loss=0.07482]\n",
      "Step 513484  [5.536 sec/step, loss=0.07456, avg_loss=0.07481]\n",
      "Step 513485  [5.543 sec/step, loss=0.07694, avg_loss=0.07480]\n",
      "Step 513486  [5.546 sec/step, loss=0.07707, avg_loss=0.07480]\n",
      "Step 513487  [5.537 sec/step, loss=0.07634, avg_loss=0.07479]\n",
      "Step 513488  [5.510 sec/step, loss=0.07471, avg_loss=0.07480]\n",
      "Step 513489  [5.502 sec/step, loss=0.07492, avg_loss=0.07480]\n",
      "Step 513490  [5.498 sec/step, loss=0.07669, avg_loss=0.07481]\n",
      "Step 513491  [5.507 sec/step, loss=0.07714, avg_loss=0.07484]\n",
      "Step 513492  [5.490 sec/step, loss=0.07356, avg_loss=0.07481]\n",
      "Step 513493  [5.516 sec/step, loss=0.07411, avg_loss=0.07479]\n",
      "Step 513494  [5.515 sec/step, loss=0.07550, avg_loss=0.07480]\n",
      "Step 513495  [5.504 sec/step, loss=0.07626, avg_loss=0.07481]\n",
      "Step 513496  [5.502 sec/step, loss=0.07427, avg_loss=0.07479]\n",
      "Step 513497  [5.504 sec/step, loss=0.07543, avg_loss=0.07479]\n",
      "Step 513498  [5.565 sec/step, loss=0.06558, avg_loss=0.07472]\n",
      "Step 513499  [5.584 sec/step, loss=0.07680, avg_loss=0.07473]\n",
      "Step 513500  [5.595 sec/step, loss=0.07728, avg_loss=0.07475]\n",
      "Writing summary at step: 513500\n",
      "Step 513501  [5.582 sec/step, loss=0.07524, avg_loss=0.07475]\n",
      "Step 513502  [5.576 sec/step, loss=0.07600, avg_loss=0.07474]\n",
      "Step 513503  [5.605 sec/step, loss=0.07707, avg_loss=0.07477]\n",
      "Step 513504  [5.598 sec/step, loss=0.07220, avg_loss=0.07475]\n",
      "Step 513505  [5.595 sec/step, loss=0.07575, avg_loss=0.07474]\n",
      "Step 513506  [5.576 sec/step, loss=0.07518, avg_loss=0.07473]\n",
      "Step 513507  [5.584 sec/step, loss=0.07520, avg_loss=0.07472]\n",
      "Generated 32 batches of size 32 in 2.539 sec\n",
      "Step 513508  [5.595 sec/step, loss=0.07638, avg_loss=0.07474]\n",
      "Step 513509  [5.550 sec/step, loss=0.07554, avg_loss=0.07484]\n",
      "Step 513510  [5.533 sec/step, loss=0.06719, avg_loss=0.07479]\n",
      "Step 513511  [5.503 sec/step, loss=0.07535, avg_loss=0.07479]\n",
      "Step 513512  [5.532 sec/step, loss=0.07444, avg_loss=0.07479]\n",
      "Step 513513  [5.515 sec/step, loss=0.07637, avg_loss=0.07478]\n",
      "Step 513514  [5.517 sec/step, loss=0.07638, avg_loss=0.07479]\n",
      "Step 513515  [5.505 sec/step, loss=0.07253, avg_loss=0.07474]\n",
      "Step 513516  [5.534 sec/step, loss=0.07523, avg_loss=0.07483]\n",
      "Step 513517  [5.522 sec/step, loss=0.07632, avg_loss=0.07485]\n",
      "Step 513518  [5.498 sec/step, loss=0.07494, avg_loss=0.07483]\n",
      "Step 513519  [5.480 sec/step, loss=0.07521, avg_loss=0.07482]\n",
      "Step 513520  [5.499 sec/step, loss=0.07364, avg_loss=0.07479]\n",
      "Step 513521  [5.475 sec/step, loss=0.07107, avg_loss=0.07474]\n",
      "Step 513522  [5.478 sec/step, loss=0.07646, avg_loss=0.07478]\n",
      "Step 513523  [5.490 sec/step, loss=0.07495, avg_loss=0.07477]\n",
      "Step 513524  [5.496 sec/step, loss=0.07594, avg_loss=0.07479]\n",
      "Step 513525  [5.483 sec/step, loss=0.07270, avg_loss=0.07474]\n",
      "Step 513526  [5.505 sec/step, loss=0.07568, avg_loss=0.07483]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 513527  [5.485 sec/step, loss=0.07435, avg_loss=0.07482]\n",
      "Step 513528  [5.480 sec/step, loss=0.07293, avg_loss=0.07479]\n",
      "Step 513529  [5.477 sec/step, loss=0.07494, avg_loss=0.07478]\n",
      "Step 513530  [5.458 sec/step, loss=0.07486, avg_loss=0.07476]\n",
      "Step 513531  [5.460 sec/step, loss=0.07691, avg_loss=0.07478]\n",
      "Step 513532  [5.472 sec/step, loss=0.07628, avg_loss=0.07482]\n",
      "Step 513533  [5.456 sec/step, loss=0.07363, avg_loss=0.07479]\n",
      "Step 513534  [5.428 sec/step, loss=0.07656, avg_loss=0.07484]\n",
      "Step 513535  [5.446 sec/step, loss=0.07718, avg_loss=0.07486]\n",
      "Step 513536  [5.490 sec/step, loss=0.06828, avg_loss=0.07477]\n",
      "Step 513537  [5.509 sec/step, loss=0.07678, avg_loss=0.07480]\n",
      "Step 513538  [5.489 sec/step, loss=0.06550, avg_loss=0.07472]\n",
      "Step 513539  [5.509 sec/step, loss=0.07688, avg_loss=0.07477]\n",
      "Generated 32 batches of size 32 in 2.775 sec\n",
      "Step 513540  [5.493 sec/step, loss=0.07479, avg_loss=0.07474]\n",
      "Step 513541  [5.484 sec/step, loss=0.07603, avg_loss=0.07475]\n",
      "Step 513542  [5.455 sec/step, loss=0.07176, avg_loss=0.07468]\n",
      "Step 513543  [5.474 sec/step, loss=0.07445, avg_loss=0.07468]\n",
      "Step 513544  [5.485 sec/step, loss=0.07606, avg_loss=0.07471]\n",
      "Step 513545  [5.476 sec/step, loss=0.07164, avg_loss=0.07469]\n",
      "Step 513546  [5.486 sec/step, loss=0.07646, avg_loss=0.07470]\n",
      "Step 513547  [5.496 sec/step, loss=0.07730, avg_loss=0.07471]\n",
      "Step 513548  [5.456 sec/step, loss=0.07553, avg_loss=0.07477]\n",
      "Step 513549  [5.464 sec/step, loss=0.07674, avg_loss=0.07480]\n",
      "Step 513550  [5.470 sec/step, loss=0.07710, avg_loss=0.07484]\n",
      "Step 513551  [5.486 sec/step, loss=0.07672, avg_loss=0.07485]\n",
      "Step 513552  [5.487 sec/step, loss=0.07527, avg_loss=0.07484]\n",
      "Step 513553  [5.502 sec/step, loss=0.07517, avg_loss=0.07484]\n",
      "Step 513554  [5.498 sec/step, loss=0.07222, avg_loss=0.07482]\n",
      "Step 513555  [5.489 sec/step, loss=0.07653, avg_loss=0.07482]\n",
      "Step 513556  [5.466 sec/step, loss=0.06760, avg_loss=0.07476]\n",
      "Step 513557  [5.479 sec/step, loss=0.07743, avg_loss=0.07478]\n",
      "Step 513558  [5.494 sec/step, loss=0.07663, avg_loss=0.07483]\n",
      "Step 513559  [5.483 sec/step, loss=0.07265, avg_loss=0.07479]\n",
      "Step 513560  [5.486 sec/step, loss=0.07433, avg_loss=0.07478]\n",
      "Step 513561  [5.504 sec/step, loss=0.07459, avg_loss=0.07475]\n",
      "Step 513562  [5.500 sec/step, loss=0.07550, avg_loss=0.07475]\n",
      "Step 513563  [5.507 sec/step, loss=0.07691, avg_loss=0.07479]\n",
      "Step 513564  [5.517 sec/step, loss=0.07533, avg_loss=0.07477]\n",
      "Step 513565  [5.512 sec/step, loss=0.07346, avg_loss=0.07477]\n",
      "Step 513566  [5.496 sec/step, loss=0.07505, avg_loss=0.07475]\n",
      "Step 513567  [5.536 sec/step, loss=0.06768, avg_loss=0.07468]\n",
      "Step 513568  [5.546 sec/step, loss=0.07645, avg_loss=0.07470]\n",
      "Step 513569  [5.534 sec/step, loss=0.07441, avg_loss=0.07468]\n",
      "Step 513570  [5.562 sec/step, loss=0.07569, avg_loss=0.07477]\n",
      "Step 513571  [5.550 sec/step, loss=0.07500, avg_loss=0.07476]\n",
      "Generated 32 batches of size 32 in 2.388 sec\n",
      "Step 513572  [5.559 sec/step, loss=0.07587, avg_loss=0.07476]\n",
      "Step 513573  [5.524 sec/step, loss=0.07311, avg_loss=0.07473]\n",
      "Step 513574  [5.540 sec/step, loss=0.07553, avg_loss=0.07474]\n",
      "Step 513575  [5.549 sec/step, loss=0.07283, avg_loss=0.07474]\n",
      "Step 513576  [5.545 sec/step, loss=0.07432, avg_loss=0.07473]\n",
      "Step 513577  [5.485 sec/step, loss=0.07144, avg_loss=0.07478]\n",
      "Step 513578  [5.488 sec/step, loss=0.07737, avg_loss=0.07479]\n",
      "Step 513579  [5.486 sec/step, loss=0.07597, avg_loss=0.07478]\n",
      "Step 513580  [5.481 sec/step, loss=0.07582, avg_loss=0.07478]\n",
      "Step 513581  [5.481 sec/step, loss=0.07564, avg_loss=0.07478]\n",
      "Step 513582  [5.468 sec/step, loss=0.07432, avg_loss=0.07475]\n",
      "Step 513583  [5.493 sec/step, loss=0.07526, avg_loss=0.07475]\n",
      "Step 513584  [5.491 sec/step, loss=0.07374, avg_loss=0.07474]\n",
      "Step 513585  [5.490 sec/step, loss=0.07699, avg_loss=0.07474]\n",
      "Step 513586  [5.476 sec/step, loss=0.07594, avg_loss=0.07473]\n",
      "Step 513587  [5.470 sec/step, loss=0.07458, avg_loss=0.07471]\n",
      "Step 513588  [5.471 sec/step, loss=0.07670, avg_loss=0.07473]\n",
      "Step 513589  [5.473 sec/step, loss=0.07416, avg_loss=0.07473]\n",
      "Step 513590  [5.484 sec/step, loss=0.07575, avg_loss=0.07472]\n",
      "Step 513591  [5.484 sec/step, loss=0.07718, avg_loss=0.07472]\n",
      "Step 513592  [5.501 sec/step, loss=0.07473, avg_loss=0.07473]\n",
      "Step 513593  [5.486 sec/step, loss=0.07675, avg_loss=0.07476]\n",
      "Step 513594  [5.486 sec/step, loss=0.07534, avg_loss=0.07475]\n",
      "Step 513595  [5.495 sec/step, loss=0.07343, avg_loss=0.07473]\n",
      "Step 513596  [5.507 sec/step, loss=0.07485, avg_loss=0.07473]\n",
      "Step 513597  [5.513 sec/step, loss=0.07613, avg_loss=0.07474]\n",
      "Step 513598  [5.451 sec/step, loss=0.07511, avg_loss=0.07483]\n",
      "Step 513599  [5.442 sec/step, loss=0.07292, avg_loss=0.07480]\n",
      "Step 513600  [5.433 sec/step, loss=0.07644, avg_loss=0.07479]\n",
      "Writing summary at step: 513600\n",
      "Step 513601  [5.453 sec/step, loss=0.07524, avg_loss=0.07479]\n",
      "Step 513602  [5.458 sec/step, loss=0.07672, avg_loss=0.07479]\n",
      "Generated 32 batches of size 32 in 2.554 sec\n",
      "Step 513603  [5.448 sec/step, loss=0.07573, avg_loss=0.07478]\n",
      "Step 513604  [5.454 sec/step, loss=0.07174, avg_loss=0.07478]\n",
      "Step 513605  [5.464 sec/step, loss=0.07701, avg_loss=0.07479]\n",
      "Step 513606  [5.464 sec/step, loss=0.07457, avg_loss=0.07478]\n",
      "Step 513607  [5.441 sec/step, loss=0.07201, avg_loss=0.07475]\n",
      "Step 513608  [5.414 sec/step, loss=0.06655, avg_loss=0.07465]\n",
      "Step 513609  [5.459 sec/step, loss=0.06645, avg_loss=0.07456]\n",
      "Step 513610  [5.491 sec/step, loss=0.07717, avg_loss=0.07466]\n",
      "Step 513611  [5.510 sec/step, loss=0.07589, avg_loss=0.07467]\n",
      "Step 513612  [5.479 sec/step, loss=0.07536, avg_loss=0.07468]\n",
      "Step 513613  [5.495 sec/step, loss=0.07613, avg_loss=0.07467]\n",
      "Step 513614  [5.542 sec/step, loss=0.06657, avg_loss=0.07458]\n",
      "Step 513615  [5.549 sec/step, loss=0.07662, avg_loss=0.07462]\n",
      "Step 513616  [5.524 sec/step, loss=0.07289, avg_loss=0.07459]\n",
      "Step 513617  [5.523 sec/step, loss=0.07437, avg_loss=0.07457]\n",
      "Step 513618  [5.541 sec/step, loss=0.07682, avg_loss=0.07459]\n",
      "Step 513619  [5.547 sec/step, loss=0.07537, avg_loss=0.07459]\n",
      "Step 513620  [5.521 sec/step, loss=0.07572, avg_loss=0.07461]\n",
      "Step 513621  [5.521 sec/step, loss=0.07201, avg_loss=0.07462]\n",
      "Step 513622  [5.532 sec/step, loss=0.07694, avg_loss=0.07463]\n",
      "Step 513623  [5.529 sec/step, loss=0.07590, avg_loss=0.07464]\n",
      "Step 513624  [5.505 sec/step, loss=0.07482, avg_loss=0.07463]\n",
      "Step 513625  [5.508 sec/step, loss=0.07454, avg_loss=0.07465]\n",
      "Step 513626  [5.495 sec/step, loss=0.07423, avg_loss=0.07463]\n",
      "Step 513627  [5.497 sec/step, loss=0.07580, avg_loss=0.07465]\n",
      "Step 513628  [5.480 sec/step, loss=0.06542, avg_loss=0.07457]\n",
      "Step 513629  [5.484 sec/step, loss=0.07726, avg_loss=0.07459]\n",
      "Step 513630  [5.485 sec/step, loss=0.07492, avg_loss=0.07459]\n",
      "Step 513631  [5.466 sec/step, loss=0.07326, avg_loss=0.07456]\n",
      "Step 513632  [5.462 sec/step, loss=0.07665, avg_loss=0.07456]\n",
      "Step 513633  [5.496 sec/step, loss=0.07618, avg_loss=0.07459]\n",
      "Step 513634  [5.501 sec/step, loss=0.07664, avg_loss=0.07459]\n",
      "Generated 32 batches of size 32 in 2.431 sec\n",
      "Step 513635  [5.501 sec/step, loss=0.07616, avg_loss=0.07458]\n",
      "Step 513636  [5.448 sec/step, loss=0.07417, avg_loss=0.07464]\n",
      "Step 513637  [5.442 sec/step, loss=0.07587, avg_loss=0.07463]\n",
      "Step 513638  [5.472 sec/step, loss=0.07718, avg_loss=0.07474]\n",
      "Step 513639  [5.470 sec/step, loss=0.07329, avg_loss=0.07471]\n",
      "Step 513640  [5.473 sec/step, loss=0.07595, avg_loss=0.07472]\n",
      "Step 513641  [5.485 sec/step, loss=0.07707, avg_loss=0.07473]\n",
      "Step 513642  [5.505 sec/step, loss=0.07616, avg_loss=0.07477]\n",
      "Step 513643  [5.512 sec/step, loss=0.07573, avg_loss=0.07479]\n",
      "Step 513644  [5.491 sec/step, loss=0.07297, avg_loss=0.07476]\n",
      "Step 513645  [5.502 sec/step, loss=0.07529, avg_loss=0.07479]\n",
      "Step 513646  [5.487 sec/step, loss=0.07157, avg_loss=0.07474]\n",
      "Step 513647  [5.472 sec/step, loss=0.07335, avg_loss=0.07470]\n",
      "Step 513648  [5.470 sec/step, loss=0.07674, avg_loss=0.07472]\n",
      "Step 513649  [5.474 sec/step, loss=0.07727, avg_loss=0.07472]\n",
      "Step 513650  [5.443 sec/step, loss=0.06715, avg_loss=0.07462]\n",
      "Step 513651  [5.426 sec/step, loss=0.07413, avg_loss=0.07460]\n",
      "Step 513652  [5.438 sec/step, loss=0.07725, avg_loss=0.07462]\n",
      "Step 513653  [5.423 sec/step, loss=0.07632, avg_loss=0.07463]\n",
      "Step 513654  [5.421 sec/step, loss=0.07557, avg_loss=0.07466]\n",
      "Step 513655  [5.413 sec/step, loss=0.07550, avg_loss=0.07465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 513656  [5.422 sec/step, loss=0.07414, avg_loss=0.07472]\n",
      "Step 513657  [5.414 sec/step, loss=0.07673, avg_loss=0.07471]\n",
      "Step 513658  [5.418 sec/step, loss=0.07615, avg_loss=0.07470]\n",
      "Step 513659  [5.446 sec/step, loss=0.07706, avg_loss=0.07475]\n",
      "Step 513660  [5.464 sec/step, loss=0.07419, avg_loss=0.07475]\n",
      "Step 513661  [5.439 sec/step, loss=0.07465, avg_loss=0.07475]\n",
      "Step 513662  [5.450 sec/step, loss=0.07751, avg_loss=0.07477]\n",
      "Step 513663  [5.452 sec/step, loss=0.07177, avg_loss=0.07472]\n",
      "Step 513664  [5.431 sec/step, loss=0.07534, avg_loss=0.07472]\n",
      "Step 513665  [5.432 sec/step, loss=0.07695, avg_loss=0.07475]\n",
      "Step 513666  [5.428 sec/step, loss=0.07491, avg_loss=0.07475]\n",
      "Generated 32 batches of size 32 in 2.705 sec\n",
      "Step 513667  [5.432 sec/step, loss=0.06744, avg_loss=0.07475]\n",
      "Step 513668  [5.439 sec/step, loss=0.07551, avg_loss=0.07474]\n",
      "Step 513669  [5.440 sec/step, loss=0.07615, avg_loss=0.07476]\n",
      "Step 513670  [5.430 sec/step, loss=0.07596, avg_loss=0.07476]\n",
      "Step 513671  [5.436 sec/step, loss=0.07630, avg_loss=0.07477]\n",
      "Step 513672  [5.452 sec/step, loss=0.07454, avg_loss=0.07476]\n",
      "Step 513673  [5.471 sec/step, loss=0.07697, avg_loss=0.07480]\n",
      "Step 513674  [5.473 sec/step, loss=0.07693, avg_loss=0.07481]\n",
      "Step 513675  [5.471 sec/step, loss=0.07472, avg_loss=0.07483]\n",
      "Step 513676  [5.480 sec/step, loss=0.07527, avg_loss=0.07484]\n",
      "Step 513677  [5.485 sec/step, loss=0.07453, avg_loss=0.07487]\n",
      "Step 513678  [5.490 sec/step, loss=0.07449, avg_loss=0.07484]\n",
      "Step 513679  [5.486 sec/step, loss=0.07527, avg_loss=0.07483]\n",
      "Step 513680  [5.490 sec/step, loss=0.07656, avg_loss=0.07484]\n",
      "Step 513681  [5.486 sec/step, loss=0.07116, avg_loss=0.07480]\n",
      "Step 513682  [5.491 sec/step, loss=0.07636, avg_loss=0.07482]\n",
      "Step 513683  [5.480 sec/step, loss=0.07653, avg_loss=0.07483]\n",
      "Step 513684  [5.499 sec/step, loss=0.07411, avg_loss=0.07483]\n",
      "Step 513685  [5.489 sec/step, loss=0.07604, avg_loss=0.07482]\n",
      "Step 513686  [5.519 sec/step, loss=0.07474, avg_loss=0.07481]\n",
      "Step 513687  [5.519 sec/step, loss=0.07489, avg_loss=0.07482]\n",
      "Step 513688  [5.522 sec/step, loss=0.07595, avg_loss=0.07481]\n",
      "Step 513689  [5.539 sec/step, loss=0.07686, avg_loss=0.07484]\n",
      "Step 513690  [5.535 sec/step, loss=0.07674, avg_loss=0.07485]\n",
      "Step 513691  [5.531 sec/step, loss=0.07395, avg_loss=0.07481]\n",
      "Step 513692  [5.540 sec/step, loss=0.07534, avg_loss=0.07482]\n",
      "Step 513693  [5.527 sec/step, loss=0.07585, avg_loss=0.07481]\n",
      "Step 513694  [5.523 sec/step, loss=0.07513, avg_loss=0.07481]\n",
      "Step 513695  [5.517 sec/step, loss=0.07487, avg_loss=0.07482]\n",
      "Step 513696  [5.502 sec/step, loss=0.07488, avg_loss=0.07482]\n",
      "Step 513697  [5.486 sec/step, loss=0.07485, avg_loss=0.07481]\n",
      "Step 513698  [5.547 sec/step, loss=0.06716, avg_loss=0.07473]\n",
      "Generated 32 batches of size 32 in 2.457 sec\n",
      "Step 513699  [5.578 sec/step, loss=0.07338, avg_loss=0.07473]\n",
      "Step 513700  [5.558 sec/step, loss=0.06878, avg_loss=0.07466]\n",
      "Writing summary at step: 513700\n",
      "Step 513701  [5.535 sec/step, loss=0.07225, avg_loss=0.07463]\n",
      "Step 513702  [5.528 sec/step, loss=0.07537, avg_loss=0.07461]\n",
      "Step 513703  [5.511 sec/step, loss=0.07204, avg_loss=0.07458]\n",
      "Step 513704  [5.522 sec/step, loss=0.07718, avg_loss=0.07463]\n",
      "Step 513705  [5.519 sec/step, loss=0.07726, avg_loss=0.07463]\n",
      "Step 513706  [5.519 sec/step, loss=0.07550, avg_loss=0.07464]\n",
      "Step 513707  [5.522 sec/step, loss=0.07465, avg_loss=0.07467]\n",
      "Step 513708  [5.554 sec/step, loss=0.07459, avg_loss=0.07475]\n",
      "Step 513709  [5.516 sec/step, loss=0.07682, avg_loss=0.07485]\n",
      "Step 513710  [5.503 sec/step, loss=0.07424, avg_loss=0.07483]\n",
      "Step 513711  [5.498 sec/step, loss=0.07432, avg_loss=0.07481]\n",
      "Step 513712  [5.532 sec/step, loss=0.07563, avg_loss=0.07481]\n",
      "Step 513713  [5.511 sec/step, loss=0.07429, avg_loss=0.07479]\n",
      "Step 513714  [5.468 sec/step, loss=0.07681, avg_loss=0.07490]\n",
      "Step 513715  [5.463 sec/step, loss=0.07085, avg_loss=0.07484]\n",
      "Step 513716  [5.527 sec/step, loss=0.06623, avg_loss=0.07477]\n",
      "Step 513717  [5.532 sec/step, loss=0.07578, avg_loss=0.07479]\n",
      "Step 513718  [5.528 sec/step, loss=0.07675, avg_loss=0.07479]\n",
      "Step 513719  [5.520 sec/step, loss=0.07630, avg_loss=0.07479]\n",
      "Step 513720  [5.527 sec/step, loss=0.07594, avg_loss=0.07480]\n",
      "Step 513721  [5.543 sec/step, loss=0.07622, avg_loss=0.07484]\n",
      "Step 513722  [5.529 sec/step, loss=0.07614, avg_loss=0.07483]\n",
      "Step 513723  [5.519 sec/step, loss=0.07579, avg_loss=0.07483]\n",
      "Step 513724  [5.542 sec/step, loss=0.07685, avg_loss=0.07485]\n",
      "Step 513725  [5.545 sec/step, loss=0.07634, avg_loss=0.07487]\n",
      "Step 513726  [5.549 sec/step, loss=0.07383, avg_loss=0.07486]\n",
      "Step 513727  [5.561 sec/step, loss=0.07709, avg_loss=0.07488]\n",
      "Step 513728  [5.567 sec/step, loss=0.07213, avg_loss=0.07494]\n",
      "Step 513729  [5.563 sec/step, loss=0.07635, avg_loss=0.07494]\n",
      "Generated 32 batches of size 32 in 2.546 sec\n",
      "Step 513730  [5.576 sec/step, loss=0.07639, avg_loss=0.07495]\n",
      "Step 513731  [5.565 sec/step, loss=0.07193, avg_loss=0.07494]\n",
      "Step 513732  [5.551 sec/step, loss=0.06617, avg_loss=0.07483]\n",
      "Step 513733  [5.534 sec/step, loss=0.07606, avg_loss=0.07483]\n",
      "Step 513734  [5.534 sec/step, loss=0.07587, avg_loss=0.07482]\n",
      "Step 513735  [5.544 sec/step, loss=0.07650, avg_loss=0.07483]\n",
      "Step 513736  [5.544 sec/step, loss=0.07527, avg_loss=0.07484]\n",
      "Step 513737  [5.533 sec/step, loss=0.07518, avg_loss=0.07483]\n",
      "Step 513738  [5.509 sec/step, loss=0.07495, avg_loss=0.07481]\n",
      "Step 513739  [5.505 sec/step, loss=0.07511, avg_loss=0.07483]\n",
      "Step 513740  [5.501 sec/step, loss=0.07100, avg_loss=0.07478]\n",
      "Step 513741  [5.475 sec/step, loss=0.06735, avg_loss=0.07468]\n",
      "Step 513742  [5.476 sec/step, loss=0.07693, avg_loss=0.07469]\n",
      "Step 513743  [5.462 sec/step, loss=0.07550, avg_loss=0.07468]\n",
      "Step 513744  [5.489 sec/step, loss=0.07564, avg_loss=0.07471]\n",
      "Step 513745  [5.486 sec/step, loss=0.07555, avg_loss=0.07471]\n",
      "Step 513746  [5.496 sec/step, loss=0.07319, avg_loss=0.07473]\n",
      "Step 513747  [5.528 sec/step, loss=0.07435, avg_loss=0.07474]\n",
      "Step 513748  [5.516 sec/step, loss=0.07491, avg_loss=0.07472]\n",
      "Step 513749  [5.503 sec/step, loss=0.07447, avg_loss=0.07469]\n",
      "Step 513750  [5.522 sec/step, loss=0.07510, avg_loss=0.07477]\n",
      "Step 513751  [5.518 sec/step, loss=0.07544, avg_loss=0.07479]\n",
      "Step 513752  [5.499 sec/step, loss=0.07454, avg_loss=0.07476]\n",
      "Step 513753  [5.510 sec/step, loss=0.07684, avg_loss=0.07476]\n",
      "Step 513754  [5.506 sec/step, loss=0.07342, avg_loss=0.07474]\n",
      "Step 513755  [5.507 sec/step, loss=0.07654, avg_loss=0.07475]\n",
      "Step 513756  [5.524 sec/step, loss=0.07682, avg_loss=0.07478]\n",
      "Step 513757  [5.514 sec/step, loss=0.07173, avg_loss=0.07473]\n",
      "Step 513758  [5.499 sec/step, loss=0.07397, avg_loss=0.07471]\n",
      "Step 513759  [5.497 sec/step, loss=0.07693, avg_loss=0.07471]\n",
      "Step 513760  [5.474 sec/step, loss=0.07632, avg_loss=0.07473]\n",
      "Step 513761  [5.485 sec/step, loss=0.07746, avg_loss=0.07476]\n",
      "Generated 32 batches of size 32 in 2.627 sec\n",
      "Step 513762  [5.529 sec/step, loss=0.06720, avg_loss=0.07465]\n",
      "Step 513763  [5.530 sec/step, loss=0.07566, avg_loss=0.07469]\n",
      "Step 513764  [5.532 sec/step, loss=0.07611, avg_loss=0.07470]\n",
      "Step 513765  [5.544 sec/step, loss=0.07468, avg_loss=0.07468]\n",
      "Step 513766  [5.560 sec/step, loss=0.07719, avg_loss=0.07470]\n",
      "Step 513767  [5.509 sec/step, loss=0.07358, avg_loss=0.07476]\n",
      "Step 513768  [5.514 sec/step, loss=0.07753, avg_loss=0.07478]\n",
      "Step 513769  [5.534 sec/step, loss=0.07507, avg_loss=0.07477]\n",
      "Step 513770  [5.537 sec/step, loss=0.07349, avg_loss=0.07475]\n",
      "Step 513771  [5.544 sec/step, loss=0.07489, avg_loss=0.07473]\n",
      "Step 513772  [5.520 sec/step, loss=0.07581, avg_loss=0.07474]\n",
      "Step 513773  [5.530 sec/step, loss=0.07601, avg_loss=0.07474]\n",
      "Step 513774  [5.523 sec/step, loss=0.07584, avg_loss=0.07472]\n",
      "Step 513775  [5.503 sec/step, loss=0.07168, avg_loss=0.07469]\n",
      "Step 513776  [5.508 sec/step, loss=0.07361, avg_loss=0.07468]\n",
      "Step 513777  [5.521 sec/step, loss=0.07683, avg_loss=0.07470]\n",
      "Step 513778  [5.511 sec/step, loss=0.07609, avg_loss=0.07472]\n",
      "Step 513779  [5.512 sec/step, loss=0.07507, avg_loss=0.07471]\n",
      "Step 513780  [5.518 sec/step, loss=0.07719, avg_loss=0.07472]\n",
      "Step 513781  [5.514 sec/step, loss=0.07498, avg_loss=0.07476]\n",
      "Step 513782  [5.513 sec/step, loss=0.07514, avg_loss=0.07475]\n",
      "Step 513783  [5.495 sec/step, loss=0.07514, avg_loss=0.07473]\n",
      "Step 513784  [5.468 sec/step, loss=0.06603, avg_loss=0.07465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 513785  [5.476 sec/step, loss=0.07577, avg_loss=0.07465]\n",
      "Step 513786  [5.438 sec/step, loss=0.07503, avg_loss=0.07465]\n",
      "Step 513787  [5.442 sec/step, loss=0.07552, avg_loss=0.07466]\n",
      "Step 513788  [5.435 sec/step, loss=0.07583, avg_loss=0.07466]\n",
      "Step 513789  [5.416 sec/step, loss=0.07287, avg_loss=0.07462]\n",
      "Step 513790  [5.407 sec/step, loss=0.07593, avg_loss=0.07461]\n",
      "Step 513791  [5.400 sec/step, loss=0.07426, avg_loss=0.07461]\n",
      "Step 513792  [5.397 sec/step, loss=0.07718, avg_loss=0.07463]\n",
      "Step 513793  [5.413 sec/step, loss=0.07555, avg_loss=0.07463]\n",
      "Generated 32 batches of size 32 in 2.530 sec\n",
      "Step 513794  [5.405 sec/step, loss=0.07167, avg_loss=0.07459]\n",
      "Step 513795  [5.414 sec/step, loss=0.07480, avg_loss=0.07459]\n",
      "Step 513796  [5.432 sec/step, loss=0.07760, avg_loss=0.07462]\n",
      "Step 513797  [5.453 sec/step, loss=0.07709, avg_loss=0.07464]\n",
      "Step 513798  [5.427 sec/step, loss=0.07519, avg_loss=0.07472]\n",
      "Step 513799  [5.393 sec/step, loss=0.07325, avg_loss=0.07472]\n",
      "Step 513800  [5.459 sec/step, loss=0.06585, avg_loss=0.07469]\n",
      "Writing summary at step: 513800\n",
      "Step 513801  [5.470 sec/step, loss=0.07632, avg_loss=0.07473]\n",
      "Step 513802  [5.469 sec/step, loss=0.07581, avg_loss=0.07474]\n",
      "Step 513803  [5.500 sec/step, loss=0.07417, avg_loss=0.07476]\n",
      "Step 513804  [5.508 sec/step, loss=0.07708, avg_loss=0.07476]\n",
      "Step 513805  [5.499 sec/step, loss=0.07416, avg_loss=0.07473]\n",
      "Step 513806  [5.533 sec/step, loss=0.07419, avg_loss=0.07471]\n",
      "Step 513807  [5.538 sec/step, loss=0.07413, avg_loss=0.07471]\n",
      "Step 513808  [5.527 sec/step, loss=0.07579, avg_loss=0.07472]\n",
      "Step 513809  [5.512 sec/step, loss=0.07435, avg_loss=0.07470]\n",
      "Step 513810  [5.520 sec/step, loss=0.07690, avg_loss=0.07472]\n",
      "Step 513811  [5.502 sec/step, loss=0.07498, avg_loss=0.07473]\n",
      "Step 513812  [5.489 sec/step, loss=0.07384, avg_loss=0.07471]\n",
      "Step 513813  [5.486 sec/step, loss=0.07269, avg_loss=0.07469]\n",
      "Step 513814  [5.474 sec/step, loss=0.07299, avg_loss=0.07466]\n",
      "Step 513815  [5.469 sec/step, loss=0.07461, avg_loss=0.07469]\n",
      "Step 513816  [5.423 sec/step, loss=0.07529, avg_loss=0.07478]\n",
      "Step 513817  [5.407 sec/step, loss=0.07322, avg_loss=0.07476]\n",
      "Step 513818  [5.451 sec/step, loss=0.06812, avg_loss=0.07467]\n",
      "Step 513819  [5.450 sec/step, loss=0.07622, avg_loss=0.07467]\n",
      "Step 513820  [5.447 sec/step, loss=0.07616, avg_loss=0.07467]\n",
      "Step 513821  [5.453 sec/step, loss=0.07673, avg_loss=0.07468]\n",
      "Step 513822  [5.469 sec/step, loss=0.07502, avg_loss=0.07467]\n",
      "Step 513823  [5.473 sec/step, loss=0.07593, avg_loss=0.07467]\n",
      "Step 513824  [5.455 sec/step, loss=0.07638, avg_loss=0.07466]\n",
      "Generated 32 batches of size 32 in 2.433 sec\n",
      "Step 513825  [5.466 sec/step, loss=0.07603, avg_loss=0.07466]\n",
      "Step 513826  [5.478 sec/step, loss=0.07546, avg_loss=0.07468]\n",
      "Step 513827  [5.465 sec/step, loss=0.07494, avg_loss=0.07466]\n",
      "Step 513828  [5.485 sec/step, loss=0.07712, avg_loss=0.07471]\n",
      "Step 513829  [5.483 sec/step, loss=0.07563, avg_loss=0.07470]\n",
      "Step 513830  [5.466 sec/step, loss=0.07137, avg_loss=0.07465]\n",
      "Step 513831  [5.494 sec/step, loss=0.07691, avg_loss=0.07470]\n",
      "Step 513832  [5.515 sec/step, loss=0.07668, avg_loss=0.07480]\n",
      "Step 513833  [5.490 sec/step, loss=0.06653, avg_loss=0.07471]\n",
      "Step 513834  [5.490 sec/step, loss=0.07512, avg_loss=0.07470]\n",
      "Step 513835  [5.472 sec/step, loss=0.07501, avg_loss=0.07469]\n",
      "Step 513836  [5.480 sec/step, loss=0.07573, avg_loss=0.07469]\n",
      "Step 513837  [5.473 sec/step, loss=0.07485, avg_loss=0.07469]\n",
      "Step 513838  [5.484 sec/step, loss=0.07401, avg_loss=0.07468]\n",
      "Step 513839  [5.495 sec/step, loss=0.07695, avg_loss=0.07470]\n",
      "Step 513840  [5.495 sec/step, loss=0.07368, avg_loss=0.07472]\n",
      "Step 513841  [5.522 sec/step, loss=0.07472, avg_loss=0.07480]\n",
      "Step 513842  [5.567 sec/step, loss=0.06740, avg_loss=0.07470]\n",
      "Step 513843  [5.563 sec/step, loss=0.07666, avg_loss=0.07471]\n",
      "Step 513844  [5.579 sec/step, loss=0.07408, avg_loss=0.07470]\n",
      "Step 513845  [5.560 sec/step, loss=0.07148, avg_loss=0.07466]\n",
      "Step 513846  [5.560 sec/step, loss=0.07543, avg_loss=0.07468]\n",
      "Step 513847  [5.529 sec/step, loss=0.07610, avg_loss=0.07470]\n",
      "Step 513848  [5.535 sec/step, loss=0.07616, avg_loss=0.07471]\n",
      "Step 513849  [5.560 sec/step, loss=0.07428, avg_loss=0.07471]\n",
      "Step 513850  [5.565 sec/step, loss=0.07764, avg_loss=0.07473]\n",
      "Step 513851  [5.586 sec/step, loss=0.07687, avg_loss=0.07475]\n",
      "Step 513852  [5.589 sec/step, loss=0.07119, avg_loss=0.07471]\n",
      "Step 513853  [5.587 sec/step, loss=0.07742, avg_loss=0.07472]\n",
      "Step 513854  [5.607 sec/step, loss=0.07521, avg_loss=0.07474]\n",
      "Step 513855  [5.598 sec/step, loss=0.07500, avg_loss=0.07472]\n",
      "Step 513856  [5.586 sec/step, loss=0.07489, avg_loss=0.07470]\n",
      "Generated 32 batches of size 32 in 2.436 sec\n",
      "Step 513857  [5.604 sec/step, loss=0.07558, avg_loss=0.07474]\n",
      "Step 513858  [5.596 sec/step, loss=0.06698, avg_loss=0.07467]\n",
      "Step 513859  [5.596 sec/step, loss=0.07653, avg_loss=0.07467]\n",
      "Step 513860  [5.594 sec/step, loss=0.07232, avg_loss=0.07463]\n",
      "Step 513861  [5.591 sec/step, loss=0.07546, avg_loss=0.07461]\n",
      "Step 513862  [5.549 sec/step, loss=0.07715, avg_loss=0.07471]\n",
      "Step 513863  [5.550 sec/step, loss=0.07567, avg_loss=0.07471]\n",
      "Step 513864  [5.537 sec/step, loss=0.07074, avg_loss=0.07465]\n",
      "Step 513865  [5.523 sec/step, loss=0.07627, avg_loss=0.07467]\n",
      "Step 513866  [5.532 sec/step, loss=0.07582, avg_loss=0.07466]\n",
      "Step 513867  [5.530 sec/step, loss=0.07342, avg_loss=0.07465]\n",
      "Step 513868  [5.565 sec/step, loss=0.06697, avg_loss=0.07455]\n",
      "Step 513869  [5.552 sec/step, loss=0.07529, avg_loss=0.07455]\n",
      "Step 513870  [5.549 sec/step, loss=0.07651, avg_loss=0.07458]\n",
      "Step 513871  [5.545 sec/step, loss=0.07369, avg_loss=0.07457]\n",
      "Step 513872  [5.553 sec/step, loss=0.07695, avg_loss=0.07458]\n",
      "Step 513873  [5.540 sec/step, loss=0.07622, avg_loss=0.07458]\n",
      "Step 513874  [5.567 sec/step, loss=0.07384, avg_loss=0.07456]\n",
      "Step 513875  [5.580 sec/step, loss=0.07581, avg_loss=0.07460]\n",
      "Step 513876  [5.587 sec/step, loss=0.07775, avg_loss=0.07464]\n",
      "Step 513877  [5.563 sec/step, loss=0.07289, avg_loss=0.07461]\n",
      "Step 513878  [5.553 sec/step, loss=0.07537, avg_loss=0.07460]\n",
      "Step 513879  [5.557 sec/step, loss=0.07701, avg_loss=0.07462]\n",
      "Step 513880  [5.532 sec/step, loss=0.06655, avg_loss=0.07451]\n",
      "Step 513881  [5.544 sec/step, loss=0.07558, avg_loss=0.07452]\n",
      "Step 513882  [5.548 sec/step, loss=0.07565, avg_loss=0.07452]\n",
      "Step 513883  [5.544 sec/step, loss=0.07195, avg_loss=0.07449]\n",
      "Step 513884  [5.556 sec/step, loss=0.07288, avg_loss=0.07456]\n",
      "Step 513885  [5.546 sec/step, loss=0.07468, avg_loss=0.07455]\n",
      "Step 513886  [5.563 sec/step, loss=0.07678, avg_loss=0.07457]\n",
      "Step 513887  [5.562 sec/step, loss=0.07230, avg_loss=0.07453]\n",
      "Step 513888  [5.575 sec/step, loss=0.07483, avg_loss=0.07452]\n",
      "Generated 32 batches of size 32 in 2.576 sec\n",
      "Step 513889  [5.587 sec/step, loss=0.07545, avg_loss=0.07455]\n",
      "Step 513890  [5.594 sec/step, loss=0.07489, avg_loss=0.07454]\n",
      "Step 513891  [5.607 sec/step, loss=0.07768, avg_loss=0.07457]\n",
      "Step 513892  [5.596 sec/step, loss=0.07483, avg_loss=0.07455]\n",
      "Step 513893  [5.568 sec/step, loss=0.07369, avg_loss=0.07453]\n",
      "Step 513894  [5.576 sec/step, loss=0.07612, avg_loss=0.07458]\n",
      "Step 513895  [5.571 sec/step, loss=0.07569, avg_loss=0.07458]\n",
      "Step 513896  [5.574 sec/step, loss=0.07662, avg_loss=0.07457]\n",
      "Step 513897  [5.556 sec/step, loss=0.07457, avg_loss=0.07455]\n",
      "Step 513898  [5.523 sec/step, loss=0.07121, avg_loss=0.07451]\n",
      "Step 513899  [5.539 sec/step, loss=0.07684, avg_loss=0.07455]\n",
      "Step 513900  [5.505 sec/step, loss=0.07622, avg_loss=0.07465]\n",
      "Writing summary at step: 513900\n",
      "Step 513901  [5.558 sec/step, loss=0.06708, avg_loss=0.07456]\n",
      "Step 513902  [5.541 sec/step, loss=0.06690, avg_loss=0.07447]\n",
      "Step 513903  [5.514 sec/step, loss=0.07519, avg_loss=0.07448]\n",
      "Step 513904  [5.485 sec/step, loss=0.07276, avg_loss=0.07443]\n",
      "Step 513905  [5.475 sec/step, loss=0.07253, avg_loss=0.07442]\n",
      "Step 513906  [5.450 sec/step, loss=0.07611, avg_loss=0.07444]\n",
      "Step 513907  [5.454 sec/step, loss=0.07658, avg_loss=0.07446]\n",
      "Step 513908  [5.457 sec/step, loss=0.07616, avg_loss=0.07447]\n",
      "Step 513909  [5.474 sec/step, loss=0.07664, avg_loss=0.07449]\n",
      "Step 513910  [5.484 sec/step, loss=0.07635, avg_loss=0.07448]\n",
      "Step 513911  [5.490 sec/step, loss=0.07615, avg_loss=0.07449]\n",
      "Step 513912  [5.484 sec/step, loss=0.07742, avg_loss=0.07453]\n",
      "Step 513913  [5.502 sec/step, loss=0.07726, avg_loss=0.07458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 513914  [5.502 sec/step, loss=0.07605, avg_loss=0.07461]\n",
      "Step 513915  [5.506 sec/step, loss=0.07468, avg_loss=0.07461]\n",
      "Step 513916  [5.501 sec/step, loss=0.07517, avg_loss=0.07461]\n",
      "Step 513917  [5.527 sec/step, loss=0.07686, avg_loss=0.07464]\n",
      "Step 513918  [5.474 sec/step, loss=0.07323, avg_loss=0.07469]\n",
      "Step 513919  [5.478 sec/step, loss=0.07599, avg_loss=0.07469]\n",
      "Generated 32 batches of size 32 in 2.516 sec\n",
      "Step 513920  [5.481 sec/step, loss=0.07236, avg_loss=0.07465]\n",
      "Step 513921  [5.479 sec/step, loss=0.07585, avg_loss=0.07464]\n",
      "Step 513922  [5.468 sec/step, loss=0.07545, avg_loss=0.07465]\n",
      "Step 513923  [5.455 sec/step, loss=0.07499, avg_loss=0.07464]\n",
      "Step 513924  [5.469 sec/step, loss=0.07437, avg_loss=0.07462]\n",
      "Step 513925  [5.463 sec/step, loss=0.07546, avg_loss=0.07461]\n",
      "Step 513926  [5.451 sec/step, loss=0.07469, avg_loss=0.07461]\n",
      "Step 513927  [5.450 sec/step, loss=0.07525, avg_loss=0.07461]\n",
      "Step 513928  [5.466 sec/step, loss=0.07351, avg_loss=0.07457]\n",
      "Step 513929  [5.513 sec/step, loss=0.06770, avg_loss=0.07449]\n",
      "Step 513930  [5.509 sec/step, loss=0.07139, avg_loss=0.07449]\n",
      "Step 513931  [5.504 sec/step, loss=0.07671, avg_loss=0.07449]\n",
      "Step 513932  [5.497 sec/step, loss=0.07452, avg_loss=0.07447]\n",
      "Step 513933  [5.529 sec/step, loss=0.07710, avg_loss=0.07458]\n",
      "Step 513934  [5.536 sec/step, loss=0.07454, avg_loss=0.07457]\n",
      "Step 513935  [5.531 sec/step, loss=0.07606, avg_loss=0.07458]\n",
      "Step 513936  [5.525 sec/step, loss=0.07627, avg_loss=0.07459]\n",
      "Step 513937  [5.559 sec/step, loss=0.07496, avg_loss=0.07459]\n",
      "Step 513938  [5.563 sec/step, loss=0.07524, avg_loss=0.07460]\n",
      "Step 513939  [5.535 sec/step, loss=0.06677, avg_loss=0.07450]\n",
      "Step 513940  [5.551 sec/step, loss=0.07628, avg_loss=0.07452]\n",
      "Step 513941  [5.545 sec/step, loss=0.07565, avg_loss=0.07453]\n",
      "Step 513942  [5.500 sec/step, loss=0.07596, avg_loss=0.07462]\n",
      "Step 513943  [5.506 sec/step, loss=0.07698, avg_loss=0.07462]\n",
      "Step 513944  [5.471 sec/step, loss=0.07454, avg_loss=0.07463]\n",
      "Step 513945  [5.474 sec/step, loss=0.07524, avg_loss=0.07466]\n",
      "Step 513946  [5.467 sec/step, loss=0.07359, avg_loss=0.07465]\n",
      "Step 513947  [5.498 sec/step, loss=0.07381, avg_loss=0.07462]\n",
      "Step 513948  [5.499 sec/step, loss=0.07219, avg_loss=0.07458]\n",
      "Step 513949  [5.485 sec/step, loss=0.07670, avg_loss=0.07461]\n",
      "Step 513950  [5.478 sec/step, loss=0.07586, avg_loss=0.07459]\n",
      "Step 513951  [5.460 sec/step, loss=0.07402, avg_loss=0.07456]\n",
      "Generated 32 batches of size 32 in 2.727 sec\n",
      "Step 513952  [5.451 sec/step, loss=0.07304, avg_loss=0.07458]\n",
      "Step 513953  [5.442 sec/step, loss=0.07565, avg_loss=0.07456]\n",
      "Step 513954  [5.448 sec/step, loss=0.07742, avg_loss=0.07458]\n",
      "Step 513955  [5.468 sec/step, loss=0.07518, avg_loss=0.07459]\n",
      "Step 513956  [5.474 sec/step, loss=0.07291, avg_loss=0.07457]\n",
      "Step 513957  [5.466 sec/step, loss=0.07709, avg_loss=0.07458]\n",
      "Step 513958  [5.482 sec/step, loss=0.07201, avg_loss=0.07463]\n",
      "Step 513959  [5.482 sec/step, loss=0.07718, avg_loss=0.07464]\n",
      "Step 513960  [5.487 sec/step, loss=0.07544, avg_loss=0.07467]\n",
      "Step 513961  [5.477 sec/step, loss=0.07596, avg_loss=0.07467]\n",
      "Step 513962  [5.470 sec/step, loss=0.07683, avg_loss=0.07467]\n",
      "Step 513963  [5.471 sec/step, loss=0.07542, avg_loss=0.07467]\n",
      "Step 513964  [5.468 sec/step, loss=0.07212, avg_loss=0.07468]\n",
      "Step 513965  [5.452 sec/step, loss=0.06617, avg_loss=0.07458]\n",
      "Step 513966  [5.423 sec/step, loss=0.07127, avg_loss=0.07454]\n",
      "Step 513967  [5.413 sec/step, loss=0.07434, avg_loss=0.07454]\n",
      "Step 513968  [5.372 sec/step, loss=0.07472, avg_loss=0.07462]\n",
      "Step 513969  [5.353 sec/step, loss=0.07495, avg_loss=0.07462]\n",
      "Step 513970  [5.348 sec/step, loss=0.07639, avg_loss=0.07462]\n",
      "Step 513971  [5.357 sec/step, loss=0.07700, avg_loss=0.07465]\n",
      "Step 513972  [5.366 sec/step, loss=0.07613, avg_loss=0.07464]\n",
      "Step 513973  [5.361 sec/step, loss=0.07685, avg_loss=0.07465]\n",
      "Step 513974  [5.356 sec/step, loss=0.07578, avg_loss=0.07467]\n",
      "Step 513975  [5.366 sec/step, loss=0.07448, avg_loss=0.07465]\n",
      "Step 513976  [5.354 sec/step, loss=0.07609, avg_loss=0.07464]\n",
      "Step 513977  [5.414 sec/step, loss=0.06667, avg_loss=0.07458]\n",
      "Step 513978  [5.416 sec/step, loss=0.07162, avg_loss=0.07454]\n",
      "Step 513979  [5.417 sec/step, loss=0.07697, avg_loss=0.07454]\n",
      "Step 513980  [5.430 sec/step, loss=0.07479, avg_loss=0.07462]\n",
      "Step 513981  [5.435 sec/step, loss=0.07438, avg_loss=0.07461]\n",
      "Step 513982  [5.436 sec/step, loss=0.07443, avg_loss=0.07460]\n",
      "Step 513983  [5.445 sec/step, loss=0.07612, avg_loss=0.07464]\n",
      "Generated 32 batches of size 32 in 2.460 sec\n",
      "Step 513984  [5.467 sec/step, loss=0.07659, avg_loss=0.07468]\n",
      "Step 513985  [5.472 sec/step, loss=0.07398, avg_loss=0.07467]\n",
      "Step 513986  [5.474 sec/step, loss=0.07650, avg_loss=0.07467]\n",
      "Step 513987  [5.474 sec/step, loss=0.07180, avg_loss=0.07466]\n",
      "Step 513988  [5.466 sec/step, loss=0.07499, avg_loss=0.07466]\n",
      "Step 513989  [5.463 sec/step, loss=0.07529, avg_loss=0.07466]\n",
      "Step 513990  [5.456 sec/step, loss=0.07546, avg_loss=0.07467]\n",
      "Step 513991  [5.434 sec/step, loss=0.07496, avg_loss=0.07464]\n",
      "Step 513992  [5.452 sec/step, loss=0.07677, avg_loss=0.07466]\n",
      "Step 513993  [5.485 sec/step, loss=0.07444, avg_loss=0.07467]\n",
      "Step 513994  [5.479 sec/step, loss=0.07470, avg_loss=0.07465]\n",
      "Step 513995  [5.483 sec/step, loss=0.07777, avg_loss=0.07467]\n",
      "Step 513996  [5.473 sec/step, loss=0.07356, avg_loss=0.07464]\n",
      "Step 513997  [5.462 sec/step, loss=0.06677, avg_loss=0.07456]\n",
      "Step 513998  [5.467 sec/step, loss=0.07669, avg_loss=0.07462]\n",
      "Step 513999  [5.460 sec/step, loss=0.07629, avg_loss=0.07461]\n",
      "Step 514000  [5.456 sec/step, loss=0.07756, avg_loss=0.07463]\n",
      "Writing summary at step: 514000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-514000\n",
      "Saving audio and alignment...\n",
      "Input: kutsh hazraatd giraedzuuayshan kay liiay mumadd v muaavan kaanfaransoon miin uuluulazm hoo kur shuuurii shirkatd kurtday hiin~__________\n",
      "Step 514001  [5.415 sec/step, loss=0.07534, avg_loss=0.07471]\n",
      "Step 514002  [5.428 sec/step, loss=0.07484, avg_loss=0.07479]\n",
      "Step 514003  [5.433 sec/step, loss=0.07524, avg_loss=0.07479]\n",
      "Step 514004  [5.444 sec/step, loss=0.07613, avg_loss=0.07482]\n",
      "Step 514005  [5.496 sec/step, loss=0.07094, avg_loss=0.07481]\n",
      "Step 514006  [5.479 sec/step, loss=0.07297, avg_loss=0.07478]\n",
      "Step 514007  [5.474 sec/step, loss=0.07407, avg_loss=0.07475]\n",
      "Step 514008  [5.475 sec/step, loss=0.07672, avg_loss=0.07476]\n",
      "Step 514009  [5.461 sec/step, loss=0.07383, avg_loss=0.07473]\n",
      "Step 514010  [5.446 sec/step, loss=0.07541, avg_loss=0.07472]\n",
      "Step 514011  [5.461 sec/step, loss=0.07693, avg_loss=0.07473]\n",
      "Step 514012  [5.455 sec/step, loss=0.07546, avg_loss=0.07471]\n",
      "Step 514013  [5.459 sec/step, loss=0.07498, avg_loss=0.07468]\n",
      "Generated 32 batches of size 32 in 2.640 sec\n",
      "Step 514014  [5.456 sec/step, loss=0.07305, avg_loss=0.07465]\n",
      "Step 514015  [5.451 sec/step, loss=0.07528, avg_loss=0.07466]\n",
      "Step 514016  [5.461 sec/step, loss=0.07627, avg_loss=0.07467]\n",
      "Step 514017  [5.450 sec/step, loss=0.07616, avg_loss=0.07466]\n",
      "Step 514018  [5.455 sec/step, loss=0.07599, avg_loss=0.07469]\n",
      "Step 514019  [5.455 sec/step, loss=0.07633, avg_loss=0.07469]\n",
      "Step 514020  [5.456 sec/step, loss=0.07638, avg_loss=0.07473]\n",
      "Step 514021  [5.472 sec/step, loss=0.07448, avg_loss=0.07472]\n",
      "Step 514022  [5.460 sec/step, loss=0.07141, avg_loss=0.07468]\n",
      "Step 514023  [5.517 sec/step, loss=0.06638, avg_loss=0.07459]\n",
      "Step 514024  [5.491 sec/step, loss=0.07275, avg_loss=0.07458]\n",
      "Step 514025  [5.477 sec/step, loss=0.07312, avg_loss=0.07456]\n",
      "Step 514026  [5.477 sec/step, loss=0.07424, avg_loss=0.07455]\n",
      "Step 514027  [5.475 sec/step, loss=0.07384, avg_loss=0.07454]\n",
      "Step 514028  [5.449 sec/step, loss=0.07599, avg_loss=0.07456]\n",
      "Step 514029  [5.387 sec/step, loss=0.07175, avg_loss=0.07460]\n",
      "Step 514030  [5.411 sec/step, loss=0.07601, avg_loss=0.07465]\n",
      "Step 514031  [5.394 sec/step, loss=0.07440, avg_loss=0.07462]\n",
      "Step 514032  [5.410 sec/step, loss=0.07623, avg_loss=0.07464]\n",
      "Step 514033  [5.398 sec/step, loss=0.07562, avg_loss=0.07463]\n",
      "Step 514034  [5.410 sec/step, loss=0.07596, avg_loss=0.07464]\n",
      "Step 514035  [5.417 sec/step, loss=0.07581, avg_loss=0.07464]\n",
      "Step 514036  [5.420 sec/step, loss=0.07683, avg_loss=0.07464]\n",
      "Step 514037  [5.403 sec/step, loss=0.07691, avg_loss=0.07466]\n",
      "Step 514038  [5.399 sec/step, loss=0.07568, avg_loss=0.07467]\n",
      "Step 514039  [5.433 sec/step, loss=0.07623, avg_loss=0.07476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 514040  [5.408 sec/step, loss=0.06757, avg_loss=0.07468]\n",
      "Step 514041  [5.414 sec/step, loss=0.07551, avg_loss=0.07467]\n",
      "Step 514042  [5.398 sec/step, loss=0.07503, avg_loss=0.07467]\n",
      "Step 514043  [5.388 sec/step, loss=0.07543, avg_loss=0.07465]\n",
      "Step 514044  [5.389 sec/step, loss=0.07477, avg_loss=0.07465]\n",
      "Step 514045  [5.398 sec/step, loss=0.07548, avg_loss=0.07465]\n",
      "Generated 32 batches of size 32 in 2.366 sec\n",
      "Step 514046  [5.414 sec/step, loss=0.07583, avg_loss=0.07468]\n",
      "Step 514047  [5.394 sec/step, loss=0.07703, avg_loss=0.07471]\n",
      "Step 514048  [5.382 sec/step, loss=0.07494, avg_loss=0.07474]\n",
      "Step 514049  [5.383 sec/step, loss=0.07720, avg_loss=0.07474]\n",
      "Step 514050  [5.393 sec/step, loss=0.07722, avg_loss=0.07475]\n",
      "Step 514051  [5.398 sec/step, loss=0.07681, avg_loss=0.07478]\n",
      "Step 514052  [5.405 sec/step, loss=0.07575, avg_loss=0.07481]\n",
      "Step 514053  [5.411 sec/step, loss=0.07454, avg_loss=0.07480]\n",
      "Step 514054  [5.396 sec/step, loss=0.07644, avg_loss=0.07479]\n",
      "Step 514055  [5.390 sec/step, loss=0.07670, avg_loss=0.07480]\n",
      "Step 514056  [5.377 sec/step, loss=0.07110, avg_loss=0.07479]\n",
      "Step 514057  [5.380 sec/step, loss=0.07723, avg_loss=0.07479]\n",
      "Step 514058  [5.432 sec/step, loss=0.06736, avg_loss=0.07474]\n",
      "Step 514059  [5.415 sec/step, loss=0.07338, avg_loss=0.07470]\n",
      "Step 514060  [5.422 sec/step, loss=0.07549, avg_loss=0.07470]\n",
      "Step 514061  [5.413 sec/step, loss=0.07509, avg_loss=0.07469]\n",
      "Step 514062  [5.418 sec/step, loss=0.07531, avg_loss=0.07468]\n",
      "Step 514063  [5.417 sec/step, loss=0.07487, avg_loss=0.07467]\n",
      "Step 514064  [5.429 sec/step, loss=0.07493, avg_loss=0.07470]\n",
      "Step 514065  [5.450 sec/step, loss=0.07512, avg_loss=0.07479]\n",
      "Step 514066  [5.480 sec/step, loss=0.07397, avg_loss=0.07482]\n",
      "Step 514067  [5.489 sec/step, loss=0.07677, avg_loss=0.07484]\n",
      "Step 514068  [5.484 sec/step, loss=0.07570, avg_loss=0.07485]\n",
      "Step 514069  [5.477 sec/step, loss=0.07201, avg_loss=0.07482]\n",
      "Step 514070  [5.486 sec/step, loss=0.07724, avg_loss=0.07483]\n",
      "Step 514071  [5.472 sec/step, loss=0.07532, avg_loss=0.07482]\n",
      "Step 514072  [5.466 sec/step, loss=0.07372, avg_loss=0.07479]\n",
      "Step 514073  [5.476 sec/step, loss=0.07681, avg_loss=0.07479]\n",
      "Step 514074  [5.450 sec/step, loss=0.07594, avg_loss=0.07479]\n",
      "Step 514075  [5.439 sec/step, loss=0.07177, avg_loss=0.07477]\n",
      "Step 514076  [5.439 sec/step, loss=0.07557, avg_loss=0.07476]\n",
      "Step 514077  [5.381 sec/step, loss=0.07355, avg_loss=0.07483]\n",
      "Generated 32 batches of size 32 in 2.484 sec\n",
      "Step 514078  [5.388 sec/step, loss=0.07563, avg_loss=0.07487]\n",
      "Step 514079  [5.379 sec/step, loss=0.07636, avg_loss=0.07486]\n",
      "Step 514080  [5.383 sec/step, loss=0.07558, avg_loss=0.07487]\n",
      "Step 514081  [5.356 sec/step, loss=0.06658, avg_loss=0.07479]\n",
      "Step 514082  [5.365 sec/step, loss=0.07686, avg_loss=0.07482]\n",
      "Step 514083  [5.355 sec/step, loss=0.07482, avg_loss=0.07480]\n",
      "Step 514084  [5.338 sec/step, loss=0.07601, avg_loss=0.07480]\n",
      "Step 514085  [5.365 sec/step, loss=0.07419, avg_loss=0.07480]\n",
      "Step 514086  [5.371 sec/step, loss=0.07689, avg_loss=0.07480]\n",
      "Step 514087  [5.387 sec/step, loss=0.07671, avg_loss=0.07485]\n",
      "Step 514088  [5.381 sec/step, loss=0.07607, avg_loss=0.07486]\n",
      "Step 514089  [5.372 sec/step, loss=0.07471, avg_loss=0.07486]\n",
      "Step 514090  [5.351 sec/step, loss=0.07297, avg_loss=0.07483]\n",
      "Step 514091  [5.361 sec/step, loss=0.07174, avg_loss=0.07480]\n",
      "Step 514092  [5.371 sec/step, loss=0.07340, avg_loss=0.07477]\n",
      "Step 514093  [5.344 sec/step, loss=0.07486, avg_loss=0.07477]\n",
      "Step 514094  [5.359 sec/step, loss=0.07729, avg_loss=0.07480]\n",
      "Step 514095  [5.347 sec/step, loss=0.07444, avg_loss=0.07476]\n",
      "Step 514096  [5.351 sec/step, loss=0.07565, avg_loss=0.07479]\n",
      "Step 514097  [5.374 sec/step, loss=0.07727, avg_loss=0.07489]\n",
      "Step 514098  [5.378 sec/step, loss=0.07666, avg_loss=0.07489]\n",
      "Step 514099  [5.424 sec/step, loss=0.06710, avg_loss=0.07480]\n",
      "Step 514100  [5.409 sec/step, loss=0.07625, avg_loss=0.07479]\n",
      "Writing summary at step: 514100\n",
      "Step 514101  [5.398 sec/step, loss=0.07562, avg_loss=0.07479]\n",
      "Step 514102  [5.384 sec/step, loss=0.06655, avg_loss=0.07470]\n",
      "Step 514103  [5.376 sec/step, loss=0.07278, avg_loss=0.07468]\n",
      "Step 514104  [5.381 sec/step, loss=0.07559, avg_loss=0.07467]\n",
      "Step 514105  [5.343 sec/step, loss=0.07621, avg_loss=0.07473]\n",
      "Step 514106  [5.353 sec/step, loss=0.07348, avg_loss=0.07473]\n",
      "Step 514107  [5.367 sec/step, loss=0.07650, avg_loss=0.07476]\n",
      "Step 514108  [5.372 sec/step, loss=0.07441, avg_loss=0.07473]\n",
      "Generated 32 batches of size 32 in 2.373 sec\n",
      "Step 514109  [5.384 sec/step, loss=0.07692, avg_loss=0.07477]\n",
      "Step 514110  [5.380 sec/step, loss=0.07426, avg_loss=0.07475]\n",
      "Step 514111  [5.384 sec/step, loss=0.07608, avg_loss=0.07475]\n",
      "Step 514112  [5.389 sec/step, loss=0.07541, avg_loss=0.07474]\n",
      "Step 514113  [5.379 sec/step, loss=0.07613, avg_loss=0.07476]\n",
      "Step 514114  [5.396 sec/step, loss=0.07448, avg_loss=0.07477]\n",
      "Step 514115  [5.397 sec/step, loss=0.07230, avg_loss=0.07474]\n",
      "Step 514116  [5.392 sec/step, loss=0.07604, avg_loss=0.07474]\n",
      "Step 514117  [5.388 sec/step, loss=0.07491, avg_loss=0.07473]\n",
      "Step 514118  [5.397 sec/step, loss=0.07678, avg_loss=0.07473]\n",
      "Step 514119  [5.414 sec/step, loss=0.07622, avg_loss=0.07473]\n",
      "Step 514120  [5.402 sec/step, loss=0.07600, avg_loss=0.07473]\n",
      "Step 514121  [5.385 sec/step, loss=0.07717, avg_loss=0.07476]\n",
      "Step 514122  [5.385 sec/step, loss=0.07512, avg_loss=0.07479]\n",
      "Step 514123  [5.333 sec/step, loss=0.07505, avg_loss=0.07488]\n",
      "Step 514124  [5.357 sec/step, loss=0.07689, avg_loss=0.07492]\n",
      "Step 514125  [5.352 sec/step, loss=0.07484, avg_loss=0.07494]\n",
      "Step 514126  [5.357 sec/step, loss=0.07647, avg_loss=0.07496]\n",
      "Step 514127  [5.402 sec/step, loss=0.06954, avg_loss=0.07492]\n",
      "Step 514128  [5.385 sec/step, loss=0.06666, avg_loss=0.07482]\n",
      "Step 514129  [5.400 sec/step, loss=0.07533, avg_loss=0.07486]\n",
      "Step 514130  [5.414 sec/step, loss=0.07392, avg_loss=0.07484]\n",
      "Step 514131  [5.433 sec/step, loss=0.07528, avg_loss=0.07485]\n",
      "Step 514132  [5.422 sec/step, loss=0.07648, avg_loss=0.07485]\n",
      "Step 514133  [5.414 sec/step, loss=0.07527, avg_loss=0.07485]\n",
      "Step 514134  [5.387 sec/step, loss=0.07612, avg_loss=0.07485]\n",
      "Step 514135  [5.396 sec/step, loss=0.07480, avg_loss=0.07484]\n",
      "Step 514136  [5.390 sec/step, loss=0.07543, avg_loss=0.07482]\n",
      "Step 514137  [5.388 sec/step, loss=0.07570, avg_loss=0.07481]\n",
      "Step 514138  [5.377 sec/step, loss=0.07203, avg_loss=0.07478]\n",
      "Step 514139  [5.364 sec/step, loss=0.07553, avg_loss=0.07477]\n",
      "Step 514140  [5.390 sec/step, loss=0.07564, avg_loss=0.07485]\n",
      "Generated 32 batches of size 32 in 2.512 sec\n",
      "Step 514141  [5.385 sec/step, loss=0.07452, avg_loss=0.07484]\n",
      "Step 514142  [5.391 sec/step, loss=0.07295, avg_loss=0.07482]\n",
      "Step 514143  [5.409 sec/step, loss=0.07673, avg_loss=0.07483]\n",
      "Step 514144  [5.424 sec/step, loss=0.07664, avg_loss=0.07485]\n",
      "Step 514145  [5.419 sec/step, loss=0.07449, avg_loss=0.07484]\n",
      "Step 514146  [5.408 sec/step, loss=0.07212, avg_loss=0.07480]\n",
      "Step 514147  [5.402 sec/step, loss=0.07586, avg_loss=0.07479]\n",
      "Step 514148  [5.416 sec/step, loss=0.07614, avg_loss=0.07480]\n",
      "Step 514149  [5.391 sec/step, loss=0.07278, avg_loss=0.07476]\n",
      "Step 514150  [5.380 sec/step, loss=0.07616, avg_loss=0.07475]\n",
      "Step 514151  [5.375 sec/step, loss=0.07587, avg_loss=0.07474]\n",
      "Step 514152  [5.381 sec/step, loss=0.07626, avg_loss=0.07474]\n",
      "Step 514153  [5.377 sec/step, loss=0.07578, avg_loss=0.07476]\n",
      "Step 514154  [5.393 sec/step, loss=0.07681, avg_loss=0.07476]\n",
      "Step 514155  [5.386 sec/step, loss=0.07338, avg_loss=0.07473]\n",
      "Step 514156  [5.406 sec/step, loss=0.07421, avg_loss=0.07476]\n",
      "Step 514157  [5.399 sec/step, loss=0.07576, avg_loss=0.07474]\n",
      "Step 514158  [5.342 sec/step, loss=0.07440, avg_loss=0.07481]\n",
      "Step 514159  [5.356 sec/step, loss=0.07635, avg_loss=0.07484]\n",
      "Step 514160  [5.349 sec/step, loss=0.07686, avg_loss=0.07486]\n",
      "Step 514161  [5.357 sec/step, loss=0.07427, avg_loss=0.07485]\n",
      "Step 514162  [5.349 sec/step, loss=0.07563, avg_loss=0.07485]\n",
      "Step 514163  [5.355 sec/step, loss=0.07680, avg_loss=0.07487]\n",
      "Step 514164  [5.339 sec/step, loss=0.06626, avg_loss=0.07479]\n",
      "Step 514165  [5.328 sec/step, loss=0.07478, avg_loss=0.07478]\n",
      "Step 514166  [5.305 sec/step, loss=0.07373, avg_loss=0.07478]\n",
      "Step 514167  [5.318 sec/step, loss=0.07677, avg_loss=0.07478]\n",
      "Step 514168  [5.321 sec/step, loss=0.07642, avg_loss=0.07479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 514169  [5.362 sec/step, loss=0.07401, avg_loss=0.07481]\n",
      "Step 514170  [5.345 sec/step, loss=0.07235, avg_loss=0.07476]\n",
      "Step 514171  [5.352 sec/step, loss=0.07568, avg_loss=0.07476]\n",
      "Step 514172  [5.352 sec/step, loss=0.07439, avg_loss=0.07477]\n",
      "Generated 32 batches of size 32 in 2.619 sec\n",
      "Step 514173  [5.396 sec/step, loss=0.06763, avg_loss=0.07468]\n",
      "Step 514174  [5.401 sec/step, loss=0.07558, avg_loss=0.07467]\n",
      "Step 514175  [5.401 sec/step, loss=0.07616, avg_loss=0.07472]\n",
      "Step 514176  [5.406 sec/step, loss=0.07676, avg_loss=0.07473]\n",
      "Step 514177  [5.409 sec/step, loss=0.07476, avg_loss=0.07474]\n",
      "Step 514178  [5.418 sec/step, loss=0.07741, avg_loss=0.07476]\n",
      "Step 514179  [5.402 sec/step, loss=0.07295, avg_loss=0.07472]\n",
      "Step 514180  [5.423 sec/step, loss=0.07444, avg_loss=0.07471]\n",
      "Step 514181  [5.429 sec/step, loss=0.07502, avg_loss=0.07480]\n",
      "Step 514182  [5.426 sec/step, loss=0.07695, avg_loss=0.07480]\n",
      "Step 514183  [5.431 sec/step, loss=0.07334, avg_loss=0.07478]\n",
      "Step 514184  [5.431 sec/step, loss=0.07626, avg_loss=0.07479]\n",
      "Step 514185  [5.393 sec/step, loss=0.07152, avg_loss=0.07476]\n",
      "Step 514186  [5.378 sec/step, loss=0.07612, avg_loss=0.07475]\n",
      "Step 514187  [5.357 sec/step, loss=0.07547, avg_loss=0.07474]\n",
      "Step 514188  [5.375 sec/step, loss=0.07647, avg_loss=0.07474]\n",
      "Step 514189  [5.393 sec/step, loss=0.07686, avg_loss=0.07476]\n",
      "Step 514190  [5.412 sec/step, loss=0.07636, avg_loss=0.07480]\n",
      "Step 514191  [5.410 sec/step, loss=0.07635, avg_loss=0.07484]\n",
      "Step 514192  [5.386 sec/step, loss=0.07657, avg_loss=0.07488]\n",
      "Step 514193  [5.420 sec/step, loss=0.07382, avg_loss=0.07487]\n",
      "Step 514194  [5.417 sec/step, loss=0.07708, avg_loss=0.07486]\n",
      "Step 514195  [5.425 sec/step, loss=0.07591, avg_loss=0.07488]\n",
      "Step 514196  [5.414 sec/step, loss=0.07581, avg_loss=0.07488]\n",
      "Step 514197  [5.417 sec/step, loss=0.07682, avg_loss=0.07488]\n",
      "Step 514198  [5.406 sec/step, loss=0.07510, avg_loss=0.07486]\n",
      "Step 514199  [5.356 sec/step, loss=0.07554, avg_loss=0.07494]\n",
      "Step 514200  [5.343 sec/step, loss=0.06639, avg_loss=0.07485]\n",
      "Writing summary at step: 514200\n",
      "Step 514201  [5.396 sec/step, loss=0.06708, avg_loss=0.07476]\n",
      "Step 514202  [5.432 sec/step, loss=0.07645, avg_loss=0.07486]\n",
      "Step 514203  [5.428 sec/step, loss=0.07226, avg_loss=0.07485]\n",
      "Generated 32 batches of size 32 in 2.483 sec\n",
      "Step 514204  [5.442 sec/step, loss=0.07568, avg_loss=0.07485]\n",
      "Step 514205  [5.444 sec/step, loss=0.07603, avg_loss=0.07485]\n",
      "Step 514206  [5.446 sec/step, loss=0.07220, avg_loss=0.07484]\n",
      "Step 514207  [5.435 sec/step, loss=0.07447, avg_loss=0.07482]\n",
      "Step 514208  [5.430 sec/step, loss=0.07499, avg_loss=0.07483]\n",
      "Step 514209  [5.426 sec/step, loss=0.07665, avg_loss=0.07482]\n",
      "Step 514210  [5.420 sec/step, loss=0.07387, avg_loss=0.07482]\n",
      "Step 514211  [5.402 sec/step, loss=0.07565, avg_loss=0.07481]\n",
      "Step 514212  [5.393 sec/step, loss=0.07463, avg_loss=0.07481]\n",
      "Step 514213  [5.387 sec/step, loss=0.07632, avg_loss=0.07481]\n",
      "Step 514214  [5.379 sec/step, loss=0.07688, avg_loss=0.07483]\n",
      "Step 514215  [5.394 sec/step, loss=0.07565, avg_loss=0.07487]\n",
      "Step 514216  [5.393 sec/step, loss=0.07548, avg_loss=0.07486]\n",
      "Step 514217  [5.448 sec/step, loss=0.06685, avg_loss=0.07478]\n",
      "Step 514218  [5.436 sec/step, loss=0.07531, avg_loss=0.07477]\n",
      "Step 514219  [5.407 sec/step, loss=0.07214, avg_loss=0.07472]\n",
      "Step 514220  [5.412 sec/step, loss=0.07669, avg_loss=0.07473]\n",
      "Step 514221  [5.412 sec/step, loss=0.07715, avg_loss=0.07473]\n",
      "Step 514222  [5.422 sec/step, loss=0.07256, avg_loss=0.07471]\n",
      "Step 514223  [5.416 sec/step, loss=0.07106, avg_loss=0.07467]\n",
      "Step 514224  [5.387 sec/step, loss=0.06657, avg_loss=0.07456]\n",
      "Step 514225  [5.406 sec/step, loss=0.07725, avg_loss=0.07459]\n",
      "Step 514226  [5.403 sec/step, loss=0.07570, avg_loss=0.07458]\n",
      "Step 514227  [5.356 sec/step, loss=0.07390, avg_loss=0.07462]\n",
      "Step 514228  [5.379 sec/step, loss=0.07641, avg_loss=0.07472]\n",
      "Step 514229  [5.372 sec/step, loss=0.07330, avg_loss=0.07470]\n",
      "Step 514230  [5.338 sec/step, loss=0.07429, avg_loss=0.07470]\n",
      "Step 514231  [5.344 sec/step, loss=0.07693, avg_loss=0.07472]\n",
      "Step 514232  [5.357 sec/step, loss=0.07741, avg_loss=0.07473]\n",
      "Step 514233  [5.362 sec/step, loss=0.07576, avg_loss=0.07473]\n",
      "Step 514234  [5.378 sec/step, loss=0.07705, avg_loss=0.07474]\n",
      "Step 514235  [5.377 sec/step, loss=0.07451, avg_loss=0.07474]\n",
      "Generated 32 batches of size 32 in 2.462 sec\n",
      "Step 514236  [5.406 sec/step, loss=0.07344, avg_loss=0.07472]\n",
      "Step 514237  [5.406 sec/step, loss=0.07611, avg_loss=0.07473]\n",
      "Step 514238  [5.417 sec/step, loss=0.07475, avg_loss=0.07475]\n",
      "Step 514239  [5.406 sec/step, loss=0.07500, avg_loss=0.07475]\n",
      "Step 514240  [5.400 sec/step, loss=0.07532, avg_loss=0.07474]\n",
      "Step 514241  [5.391 sec/step, loss=0.07427, avg_loss=0.07474]\n",
      "Step 514242  [5.382 sec/step, loss=0.07315, avg_loss=0.07474]\n",
      "Step 514243  [5.377 sec/step, loss=0.07509, avg_loss=0.07473]\n",
      "Step 514244  [5.378 sec/step, loss=0.07686, avg_loss=0.07473]\n",
      "Step 514245  [5.391 sec/step, loss=0.07712, avg_loss=0.07476]\n",
      "Step 514246  [5.391 sec/step, loss=0.07630, avg_loss=0.07480]\n",
      "Step 514247  [5.390 sec/step, loss=0.07665, avg_loss=0.07481]\n",
      "Step 514248  [5.407 sec/step, loss=0.07369, avg_loss=0.07478]\n",
      "Step 514249  [5.416 sec/step, loss=0.07249, avg_loss=0.07478]\n",
      "Step 514250  [5.465 sec/step, loss=0.06640, avg_loss=0.07468]\n",
      "Step 514251  [5.477 sec/step, loss=0.07539, avg_loss=0.07468]\n",
      "Step 514252  [5.483 sec/step, loss=0.07428, avg_loss=0.07466]\n",
      "Step 514253  [5.484 sec/step, loss=0.07425, avg_loss=0.07464]\n",
      "Step 514254  [5.477 sec/step, loss=0.07669, avg_loss=0.07464]\n",
      "Step 514255  [5.482 sec/step, loss=0.07581, avg_loss=0.07466]\n",
      "Step 514256  [5.483 sec/step, loss=0.07713, avg_loss=0.07469]\n",
      "Step 514257  [5.484 sec/step, loss=0.07568, avg_loss=0.07469]\n",
      "Step 514258  [5.491 sec/step, loss=0.07502, avg_loss=0.07470]\n",
      "Step 514259  [5.485 sec/step, loss=0.07555, avg_loss=0.07469]\n",
      "Step 514260  [5.489 sec/step, loss=0.07564, avg_loss=0.07468]\n",
      "Step 514261  [5.490 sec/step, loss=0.07591, avg_loss=0.07469]\n",
      "Step 514262  [5.487 sec/step, loss=0.07566, avg_loss=0.07469]\n",
      "Step 514263  [5.496 sec/step, loss=0.07670, avg_loss=0.07469]\n",
      "Step 514264  [5.519 sec/step, loss=0.07502, avg_loss=0.07478]\n",
      "Step 514265  [5.516 sec/step, loss=0.07500, avg_loss=0.07478]\n",
      "Step 514266  [5.504 sec/step, loss=0.06755, avg_loss=0.07472]\n",
      "Step 514267  [5.504 sec/step, loss=0.07710, avg_loss=0.07472]\n",
      "Generated 32 batches of size 32 in 2.432 sec\n",
      "Step 514268  [5.512 sec/step, loss=0.07669, avg_loss=0.07473]\n",
      "Step 514269  [5.494 sec/step, loss=0.07677, avg_loss=0.07475]\n",
      "Step 514270  [5.491 sec/step, loss=0.07324, avg_loss=0.07476]\n",
      "Step 514271  [5.480 sec/step, loss=0.07458, avg_loss=0.07475]\n",
      "Step 514272  [5.495 sec/step, loss=0.07429, avg_loss=0.07475]\n",
      "Step 514273  [5.432 sec/step, loss=0.07472, avg_loss=0.07482]\n",
      "Step 514274  [5.427 sec/step, loss=0.07444, avg_loss=0.07481]\n",
      "Step 514275  [5.428 sec/step, loss=0.07216, avg_loss=0.07477]\n",
      "Step 514276  [5.407 sec/step, loss=0.07272, avg_loss=0.07473]\n",
      "Step 514277  [5.408 sec/step, loss=0.07441, avg_loss=0.07473]\n",
      "Step 514278  [5.447 sec/step, loss=0.06778, avg_loss=0.07463]\n",
      "Step 514279  [5.471 sec/step, loss=0.07513, avg_loss=0.07465]\n",
      "Step 514280  [5.433 sec/step, loss=0.06615, avg_loss=0.07457]\n",
      "Step 514281  [5.457 sec/step, loss=0.07489, avg_loss=0.07457]\n",
      "Step 514282  [5.439 sec/step, loss=0.07470, avg_loss=0.07455]\n",
      "Step 514283  [5.439 sec/step, loss=0.07354, avg_loss=0.07455]\n",
      "Step 514284  [5.440 sec/step, loss=0.07591, avg_loss=0.07454]\n",
      "Step 514285  [5.467 sec/step, loss=0.07674, avg_loss=0.07460]\n",
      "Step 514286  [5.471 sec/step, loss=0.07621, avg_loss=0.07460]\n",
      "Step 514287  [5.480 sec/step, loss=0.07585, avg_loss=0.07460]\n",
      "Step 514288  [5.451 sec/step, loss=0.07224, avg_loss=0.07456]\n",
      "Step 514289  [5.443 sec/step, loss=0.07626, avg_loss=0.07455]\n",
      "Step 514290  [5.444 sec/step, loss=0.07696, avg_loss=0.07456]\n",
      "Step 514291  [5.456 sec/step, loss=0.07589, avg_loss=0.07455]\n",
      "Step 514292  [5.444 sec/step, loss=0.07229, avg_loss=0.07451]\n",
      "Step 514293  [5.426 sec/step, loss=0.07504, avg_loss=0.07452]\n",
      "Step 514294  [5.414 sec/step, loss=0.07402, avg_loss=0.07449]\n",
      "Step 514295  [5.411 sec/step, loss=0.07751, avg_loss=0.07451]\n",
      "Step 514296  [5.422 sec/step, loss=0.07645, avg_loss=0.07452]\n",
      "Step 514297  [5.407 sec/step, loss=0.07482, avg_loss=0.07450]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 514298  [5.417 sec/step, loss=0.07435, avg_loss=0.07449]\n",
      "Step 514299  [5.419 sec/step, loss=0.07292, avg_loss=0.07446]\n",
      "Generated 32 batches of size 32 in 2.452 sec\n",
      "Step 514300  [5.457 sec/step, loss=0.07602, avg_loss=0.07456]\n",
      "Writing summary at step: 514300\n",
      "Step 514301  [5.417 sec/step, loss=0.07735, avg_loss=0.07466]\n",
      "Step 514302  [5.395 sec/step, loss=0.07607, avg_loss=0.07466]\n",
      "Step 514303  [5.437 sec/step, loss=0.07396, avg_loss=0.07467]\n",
      "Step 514304  [5.428 sec/step, loss=0.07481, avg_loss=0.07467]\n",
      "Step 514305  [5.411 sec/step, loss=0.07512, avg_loss=0.07466]\n",
      "Step 514306  [5.415 sec/step, loss=0.07626, avg_loss=0.07470]\n",
      "Step 514307  [5.429 sec/step, loss=0.07739, avg_loss=0.07473]\n",
      "Step 514308  [5.423 sec/step, loss=0.07621, avg_loss=0.07474]\n",
      "Step 514309  [5.404 sec/step, loss=0.07115, avg_loss=0.07468]\n",
      "Step 514310  [5.404 sec/step, loss=0.07397, avg_loss=0.07468]\n",
      "Step 514311  [5.393 sec/step, loss=0.07508, avg_loss=0.07468]\n",
      "Step 514312  [5.380 sec/step, loss=0.06657, avg_loss=0.07460]\n",
      "Step 514313  [5.398 sec/step, loss=0.07656, avg_loss=0.07460]\n",
      "Step 514314  [5.382 sec/step, loss=0.07281, avg_loss=0.07456]\n",
      "Step 514315  [5.391 sec/step, loss=0.07626, avg_loss=0.07457]\n",
      "Step 514316  [5.380 sec/step, loss=0.07542, avg_loss=0.07457]\n",
      "Step 514317  [5.338 sec/step, loss=0.07609, avg_loss=0.07466]\n",
      "Step 514318  [5.338 sec/step, loss=0.07231, avg_loss=0.07463]\n",
      "Step 514319  [5.359 sec/step, loss=0.07660, avg_loss=0.07467]\n",
      "Step 514320  [5.361 sec/step, loss=0.07626, avg_loss=0.07467]\n",
      "Step 514321  [5.351 sec/step, loss=0.07446, avg_loss=0.07464]\n",
      "Step 514322  [5.353 sec/step, loss=0.07532, avg_loss=0.07467]\n",
      "Step 514323  [5.412 sec/step, loss=0.06678, avg_loss=0.07463]\n",
      "Step 514324  [5.458 sec/step, loss=0.07459, avg_loss=0.07471]\n",
      "Step 514325  [5.462 sec/step, loss=0.07730, avg_loss=0.07471]\n",
      "Step 514326  [5.465 sec/step, loss=0.07599, avg_loss=0.07471]\n",
      "Step 514327  [5.470 sec/step, loss=0.07594, avg_loss=0.07473]\n",
      "Step 514328  [5.468 sec/step, loss=0.07564, avg_loss=0.07472]\n",
      "Step 514329  [5.485 sec/step, loss=0.07682, avg_loss=0.07476]\n",
      "Step 514330  [5.490 sec/step, loss=0.07233, avg_loss=0.07474]\n",
      "Generated 32 batches of size 32 in 2.409 sec\n",
      "Step 514331  [5.480 sec/step, loss=0.07608, avg_loss=0.07473]\n",
      "Step 514332  [5.463 sec/step, loss=0.07409, avg_loss=0.07470]\n",
      "Step 514333  [5.459 sec/step, loss=0.07649, avg_loss=0.07470]\n",
      "Step 514334  [5.457 sec/step, loss=0.07697, avg_loss=0.07470]\n",
      "Step 514335  [5.454 sec/step, loss=0.07670, avg_loss=0.07472]\n",
      "Step 514336  [5.432 sec/step, loss=0.07661, avg_loss=0.07476]\n",
      "Step 514337  [5.427 sec/step, loss=0.07489, avg_loss=0.07474]\n",
      "Step 514338  [5.429 sec/step, loss=0.07664, avg_loss=0.07476]\n",
      "Step 514339  [5.447 sec/step, loss=0.07531, avg_loss=0.07477]\n",
      "Step 514340  [5.428 sec/step, loss=0.07255, avg_loss=0.07474]\n",
      "Step 514341  [5.451 sec/step, loss=0.07739, avg_loss=0.07477]\n",
      "Step 514342  [5.449 sec/step, loss=0.06599, avg_loss=0.07470]\n",
      "Step 514343  [5.426 sec/step, loss=0.07183, avg_loss=0.07467]\n",
      "Step 514344  [5.421 sec/step, loss=0.07616, avg_loss=0.07466]\n",
      "Step 514345  [5.411 sec/step, loss=0.07486, avg_loss=0.07464]\n",
      "Step 514346  [5.425 sec/step, loss=0.07528, avg_loss=0.07463]\n",
      "Step 514347  [5.438 sec/step, loss=0.07700, avg_loss=0.07463]\n",
      "Step 514348  [5.413 sec/step, loss=0.07591, avg_loss=0.07465]\n",
      "Step 514349  [5.409 sec/step, loss=0.07477, avg_loss=0.07467]\n",
      "Step 514350  [5.370 sec/step, loss=0.07686, avg_loss=0.07478]\n",
      "Step 514351  [5.369 sec/step, loss=0.07674, avg_loss=0.07479]\n",
      "Step 514352  [5.370 sec/step, loss=0.07712, avg_loss=0.07482]\n",
      "Step 514353  [5.363 sec/step, loss=0.07433, avg_loss=0.07482]\n",
      "Step 514354  [5.359 sec/step, loss=0.07261, avg_loss=0.07478]\n",
      "Step 514355  [5.349 sec/step, loss=0.07401, avg_loss=0.07476]\n",
      "Step 514356  [5.338 sec/step, loss=0.07458, avg_loss=0.07474]\n",
      "Step 514357  [5.340 sec/step, loss=0.07673, avg_loss=0.07475]\n",
      "Step 514358  [5.337 sec/step, loss=0.07303, avg_loss=0.07473]\n",
      "Step 514359  [5.341 sec/step, loss=0.07559, avg_loss=0.07473]\n",
      "Step 514360  [5.337 sec/step, loss=0.07611, avg_loss=0.07473]\n",
      "Step 514361  [5.343 sec/step, loss=0.07560, avg_loss=0.07473]\n",
      "Step 514362  [5.349 sec/step, loss=0.07613, avg_loss=0.07473]\n",
      "Generated 32 batches of size 32 in 2.375 sec\n",
      "Step 514363  [5.347 sec/step, loss=0.07668, avg_loss=0.07473]\n",
      "Step 514364  [5.346 sec/step, loss=0.07721, avg_loss=0.07476]\n",
      "Step 514365  [5.346 sec/step, loss=0.07474, avg_loss=0.07475]\n",
      "Step 514366  [5.363 sec/step, loss=0.07554, avg_loss=0.07483]\n",
      "Step 514367  [5.347 sec/step, loss=0.07561, avg_loss=0.07482]\n",
      "Step 514368  [5.356 sec/step, loss=0.07600, avg_loss=0.07481]\n",
      "Step 514369  [5.360 sec/step, loss=0.07684, avg_loss=0.07481]\n",
      "Step 514370  [5.424 sec/step, loss=0.06763, avg_loss=0.07476]\n",
      "Step 514371  [5.436 sec/step, loss=0.07286, avg_loss=0.07474]\n",
      "Step 514372  [5.412 sec/step, loss=0.07650, avg_loss=0.07476]\n",
      "Step 514373  [5.404 sec/step, loss=0.07228, avg_loss=0.07474]\n",
      "Step 514374  [5.422 sec/step, loss=0.07716, avg_loss=0.07476]\n",
      "Step 514375  [5.429 sec/step, loss=0.07579, avg_loss=0.07480]\n",
      "Step 514376  [5.430 sec/step, loss=0.07497, avg_loss=0.07482]\n",
      "Step 514377  [5.431 sec/step, loss=0.07200, avg_loss=0.07480]\n",
      "Step 514378  [5.374 sec/step, loss=0.07461, avg_loss=0.07487]\n",
      "Step 514379  [5.365 sec/step, loss=0.07310, avg_loss=0.07485]\n",
      "Step 514380  [5.391 sec/step, loss=0.07597, avg_loss=0.07494]\n",
      "Step 514381  [5.382 sec/step, loss=0.07506, avg_loss=0.07495]\n",
      "Step 514382  [5.389 sec/step, loss=0.07490, avg_loss=0.07495]\n",
      "Step 514383  [5.380 sec/step, loss=0.07279, avg_loss=0.07494]\n",
      "Step 514384  [5.375 sec/step, loss=0.07701, avg_loss=0.07495]\n",
      "Step 514385  [5.371 sec/step, loss=0.07694, avg_loss=0.07495]\n",
      "Step 514386  [5.371 sec/step, loss=0.07561, avg_loss=0.07495]\n",
      "Step 514387  [5.388 sec/step, loss=0.07291, avg_loss=0.07492]\n",
      "Step 514388  [5.402 sec/step, loss=0.07395, avg_loss=0.07494]\n",
      "Step 514389  [5.406 sec/step, loss=0.07471, avg_loss=0.07492]\n",
      "Step 514390  [5.400 sec/step, loss=0.07571, avg_loss=0.07491]\n",
      "Step 514391  [5.441 sec/step, loss=0.06545, avg_loss=0.07480]\n",
      "Step 514392  [5.480 sec/step, loss=0.07392, avg_loss=0.07482]\n",
      "Step 514393  [5.468 sec/step, loss=0.07637, avg_loss=0.07483]\n",
      "Step 514394  [5.465 sec/step, loss=0.07494, avg_loss=0.07484]\n",
      "Generated 32 batches of size 32 in 2.502 sec\n",
      "Step 514395  [5.472 sec/step, loss=0.07677, avg_loss=0.07483]\n",
      "Step 514396  [5.474 sec/step, loss=0.07472, avg_loss=0.07482]\n",
      "Step 514397  [5.490 sec/step, loss=0.07683, avg_loss=0.07484]\n",
      "Step 514398  [5.504 sec/step, loss=0.07538, avg_loss=0.07485]\n",
      "Step 514399  [5.514 sec/step, loss=0.07691, avg_loss=0.07489]\n",
      "Step 514400  [5.474 sec/step, loss=0.06691, avg_loss=0.07480]\n",
      "Writing summary at step: 514400\n",
      "Step 514401  [5.458 sec/step, loss=0.07450, avg_loss=0.07477]\n",
      "Step 514402  [5.458 sec/step, loss=0.07351, avg_loss=0.07474]\n",
      "Step 514403  [5.458 sec/step, loss=0.07265, avg_loss=0.07473]\n",
      "Step 514404  [5.461 sec/step, loss=0.07724, avg_loss=0.07475]\n",
      "Step 514405  [5.457 sec/step, loss=0.07306, avg_loss=0.07473]\n",
      "Step 514406  [5.440 sec/step, loss=0.06771, avg_loss=0.07465]\n",
      "Step 514407  [5.422 sec/step, loss=0.07398, avg_loss=0.07461]\n",
      "Step 514408  [5.414 sec/step, loss=0.07174, avg_loss=0.07457]\n",
      "Step 514409  [5.438 sec/step, loss=0.07665, avg_loss=0.07462]\n",
      "Step 514410  [5.456 sec/step, loss=0.07754, avg_loss=0.07466]\n",
      "Step 514411  [5.458 sec/step, loss=0.07446, avg_loss=0.07465]\n",
      "Step 514412  [5.488 sec/step, loss=0.07664, avg_loss=0.07475]\n",
      "Step 514413  [5.475 sec/step, loss=0.07539, avg_loss=0.07474]\n",
      "Step 514414  [5.490 sec/step, loss=0.07676, avg_loss=0.07478]\n",
      "Step 514415  [5.484 sec/step, loss=0.07570, avg_loss=0.07478]\n",
      "Step 514416  [5.495 sec/step, loss=0.07610, avg_loss=0.07478]\n",
      "Step 514417  [5.483 sec/step, loss=0.07202, avg_loss=0.07474]\n",
      "Step 514418  [5.489 sec/step, loss=0.07664, avg_loss=0.07479]\n",
      "Step 514419  [5.476 sec/step, loss=0.07602, avg_loss=0.07478]\n",
      "Step 514420  [5.467 sec/step, loss=0.07462, avg_loss=0.07476]\n",
      "Step 514421  [5.472 sec/step, loss=0.07626, avg_loss=0.07478]\n",
      "Step 514422  [5.468 sec/step, loss=0.07515, avg_loss=0.07478]\n",
      "Step 514423  [5.468 sec/step, loss=0.06685, avg_loss=0.07478]\n",
      "Step 514424  [5.440 sec/step, loss=0.07523, avg_loss=0.07479]\n",
      "Step 514425  [5.429 sec/step, loss=0.07368, avg_loss=0.07475]\n",
      "Generated 32 batches of size 32 in 2.451 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 514426  [5.440 sec/step, loss=0.07558, avg_loss=0.07475]\n",
      "Step 514427  [5.445 sec/step, loss=0.07615, avg_loss=0.07475]\n",
      "Step 514428  [5.459 sec/step, loss=0.07578, avg_loss=0.07475]\n",
      "Step 514429  [5.450 sec/step, loss=0.07331, avg_loss=0.07471]\n",
      "Step 514430  [5.462 sec/step, loss=0.07691, avg_loss=0.07476]\n",
      "Step 514431  [5.448 sec/step, loss=0.07510, avg_loss=0.07475]\n",
      "Step 514432  [5.466 sec/step, loss=0.07513, avg_loss=0.07476]\n",
      "Step 514433  [5.466 sec/step, loss=0.07605, avg_loss=0.07476]\n",
      "Step 514434  [5.445 sec/step, loss=0.07522, avg_loss=0.07474]\n",
      "Step 514435  [5.428 sec/step, loss=0.07474, avg_loss=0.07472]\n",
      "Step 514436  [5.442 sec/step, loss=0.07477, avg_loss=0.07470]\n",
      "Step 514437  [5.448 sec/step, loss=0.07607, avg_loss=0.07471]\n",
      "Step 514438  [5.445 sec/step, loss=0.07574, avg_loss=0.07470]\n",
      "Step 514439  [5.415 sec/step, loss=0.06589, avg_loss=0.07461]\n",
      "Step 514440  [5.434 sec/step, loss=0.07399, avg_loss=0.07462]\n",
      "Step 514441  [5.405 sec/step, loss=0.07185, avg_loss=0.07457]\n",
      "Step 514442  [5.418 sec/step, loss=0.07616, avg_loss=0.07467]\n",
      "Step 514443  [5.431 sec/step, loss=0.07711, avg_loss=0.07472]\n",
      "Step 514444  [5.437 sec/step, loss=0.07634, avg_loss=0.07473]\n",
      "Step 514445  [5.454 sec/step, loss=0.07534, avg_loss=0.07473]\n",
      "Step 514446  [5.445 sec/step, loss=0.07552, avg_loss=0.07473]\n",
      "Step 514447  [5.438 sec/step, loss=0.07570, avg_loss=0.07472]\n",
      "Step 514448  [5.430 sec/step, loss=0.07474, avg_loss=0.07471]\n",
      "Step 514449  [5.439 sec/step, loss=0.07474, avg_loss=0.07471]\n",
      "Step 514450  [5.436 sec/step, loss=0.07543, avg_loss=0.07469]\n",
      "Step 514451  [5.444 sec/step, loss=0.07752, avg_loss=0.07470]\n",
      "Step 514452  [5.432 sec/step, loss=0.07501, avg_loss=0.07468]\n",
      "Step 514453  [5.452 sec/step, loss=0.07699, avg_loss=0.07471]\n",
      "Step 514454  [5.437 sec/step, loss=0.07277, avg_loss=0.07471]\n",
      "Step 514455  [5.446 sec/step, loss=0.07593, avg_loss=0.07473]\n",
      "Step 514456  [5.472 sec/step, loss=0.07435, avg_loss=0.07473]\n",
      "Step 514457  [5.472 sec/step, loss=0.07340, avg_loss=0.07469]\n",
      "Generated 32 batches of size 32 in 2.513 sec\n",
      "Step 514458  [5.471 sec/step, loss=0.07491, avg_loss=0.07471]\n",
      "Step 514459  [5.462 sec/step, loss=0.07224, avg_loss=0.07468]\n",
      "Step 514460  [5.457 sec/step, loss=0.07271, avg_loss=0.07464]\n",
      "Step 514461  [5.457 sec/step, loss=0.07603, avg_loss=0.07465]\n",
      "Step 514462  [5.458 sec/step, loss=0.07738, avg_loss=0.07466]\n",
      "Step 514463  [5.455 sec/step, loss=0.07490, avg_loss=0.07464]\n",
      "Step 514464  [5.455 sec/step, loss=0.07561, avg_loss=0.07463]\n",
      "Step 514465  [5.516 sec/step, loss=0.06750, avg_loss=0.07455]\n",
      "Step 514466  [5.511 sec/step, loss=0.07514, avg_loss=0.07455]\n",
      "Step 514467  [5.515 sec/step, loss=0.07426, avg_loss=0.07454]\n",
      "Step 514468  [5.496 sec/step, loss=0.07673, avg_loss=0.07454]\n",
      "Step 514469  [5.482 sec/step, loss=0.07518, avg_loss=0.07453]\n",
      "Step 514470  [5.441 sec/step, loss=0.07477, avg_loss=0.07460]\n",
      "Step 514471  [5.448 sec/step, loss=0.07669, avg_loss=0.07464]\n",
      "Step 514472  [5.445 sec/step, loss=0.07582, avg_loss=0.07463]\n",
      "Step 514473  [5.456 sec/step, loss=0.07606, avg_loss=0.07467]\n",
      "Step 514474  [5.440 sec/step, loss=0.07118, avg_loss=0.07461]\n",
      "Step 514475  [5.431 sec/step, loss=0.07389, avg_loss=0.07459]\n",
      "Step 514476  [5.459 sec/step, loss=0.07617, avg_loss=0.07460]\n",
      "Step 514477  [5.488 sec/step, loss=0.07334, avg_loss=0.07461]\n",
      "Step 514478  [5.506 sec/step, loss=0.07740, avg_loss=0.07464]\n",
      "Step 514479  [5.508 sec/step, loss=0.07638, avg_loss=0.07467]\n",
      "Step 514480  [5.514 sec/step, loss=0.07650, avg_loss=0.07468]\n",
      "Step 514481  [5.509 sec/step, loss=0.07402, avg_loss=0.07467]\n",
      "Step 514482  [5.499 sec/step, loss=0.07223, avg_loss=0.07464]\n",
      "Step 514483  [5.501 sec/step, loss=0.07354, avg_loss=0.07465]\n",
      "Step 514484  [5.509 sec/step, loss=0.07578, avg_loss=0.07464]\n",
      "Step 514485  [5.506 sec/step, loss=0.07540, avg_loss=0.07462]\n",
      "Step 514486  [5.485 sec/step, loss=0.06686, avg_loss=0.07454]\n",
      "Step 514487  [5.470 sec/step, loss=0.07457, avg_loss=0.07455]\n",
      "Step 514488  [5.460 sec/step, loss=0.07504, avg_loss=0.07456]\n",
      "Step 514489  [5.458 sec/step, loss=0.07615, avg_loss=0.07458]\n",
      "Generated 32 batches of size 32 in 2.673 sec\n",
      "Step 514490  [5.513 sec/step, loss=0.06716, avg_loss=0.07449]\n",
      "Step 514491  [5.460 sec/step, loss=0.07620, avg_loss=0.07460]\n",
      "Step 514492  [5.441 sec/step, loss=0.07691, avg_loss=0.07463]\n",
      "Step 514493  [5.430 sec/step, loss=0.07287, avg_loss=0.07459]\n",
      "Step 514494  [5.430 sec/step, loss=0.07500, avg_loss=0.07459]\n",
      "Step 514495  [5.429 sec/step, loss=0.07777, avg_loss=0.07460]\n",
      "Step 514496  [5.433 sec/step, loss=0.07642, avg_loss=0.07462]\n",
      "Step 514497  [5.418 sec/step, loss=0.07273, avg_loss=0.07458]\n",
      "Step 514498  [5.407 sec/step, loss=0.07688, avg_loss=0.07460]\n",
      "Step 514499  [5.407 sec/step, loss=0.07718, avg_loss=0.07460]\n",
      "Step 514500  [5.427 sec/step, loss=0.07650, avg_loss=0.07469]\n",
      "Writing summary at step: 514500\n",
      "Step 514501  [5.426 sec/step, loss=0.07129, avg_loss=0.07466]\n",
      "Step 514502  [5.446 sec/step, loss=0.07367, avg_loss=0.07466]\n",
      "Step 514503  [5.428 sec/step, loss=0.07552, avg_loss=0.07469]\n",
      "Step 514504  [5.431 sec/step, loss=0.07562, avg_loss=0.07468]\n",
      "Step 514505  [5.442 sec/step, loss=0.07628, avg_loss=0.07471]\n",
      "Step 514506  [5.464 sec/step, loss=0.07569, avg_loss=0.07479]\n",
      "Step 514507  [5.469 sec/step, loss=0.07555, avg_loss=0.07480]\n",
      "Step 514508  [5.477 sec/step, loss=0.07397, avg_loss=0.07483]\n",
      "Step 514509  [5.467 sec/step, loss=0.07312, avg_loss=0.07479]\n",
      "Step 514510  [5.448 sec/step, loss=0.07197, avg_loss=0.07474]\n",
      "Step 514511  [5.462 sec/step, loss=0.07594, avg_loss=0.07475]\n",
      "Step 514512  [5.451 sec/step, loss=0.07357, avg_loss=0.07472]\n",
      "Step 514513  [5.436 sec/step, loss=0.07302, avg_loss=0.07470]\n",
      "Step 514514  [5.484 sec/step, loss=0.06741, avg_loss=0.07460]\n",
      "Step 514515  [5.482 sec/step, loss=0.07665, avg_loss=0.07461]\n",
      "Step 514516  [5.487 sec/step, loss=0.07747, avg_loss=0.07463]\n",
      "Step 514517  [5.488 sec/step, loss=0.07238, avg_loss=0.07463]\n",
      "Step 514518  [5.487 sec/step, loss=0.07325, avg_loss=0.07459]\n",
      "Step 514519  [5.518 sec/step, loss=0.07453, avg_loss=0.07458]\n",
      "Step 514520  [5.531 sec/step, loss=0.07668, avg_loss=0.07460]\n",
      "Generated 32 batches of size 32 in 2.580 sec\n",
      "Step 514521  [5.531 sec/step, loss=0.07475, avg_loss=0.07459]\n",
      "Step 514522  [5.530 sec/step, loss=0.07520, avg_loss=0.07459]\n",
      "Step 514523  [5.470 sec/step, loss=0.07266, avg_loss=0.07464]\n",
      "Step 514524  [5.495 sec/step, loss=0.07552, avg_loss=0.07465]\n",
      "Step 514525  [5.488 sec/step, loss=0.07428, avg_loss=0.07465]\n",
      "Step 514526  [5.476 sec/step, loss=0.07533, avg_loss=0.07465]\n",
      "Step 514527  [5.455 sec/step, loss=0.06693, avg_loss=0.07456]\n",
      "Step 514528  [5.445 sec/step, loss=0.07634, avg_loss=0.07456]\n",
      "Step 514529  [5.435 sec/step, loss=0.07535, avg_loss=0.07458]\n",
      "Step 514530  [5.440 sec/step, loss=0.07677, avg_loss=0.07458]\n",
      "Step 514531  [5.444 sec/step, loss=0.07588, avg_loss=0.07459]\n",
      "Step 514532  [5.437 sec/step, loss=0.07672, avg_loss=0.07461]\n",
      "Step 514533  [5.422 sec/step, loss=0.06668, avg_loss=0.07451]\n",
      "Step 514534  [5.430 sec/step, loss=0.07405, avg_loss=0.07450]\n",
      "Step 514535  [5.432 sec/step, loss=0.07361, avg_loss=0.07449]\n",
      "Step 514536  [5.411 sec/step, loss=0.07570, avg_loss=0.07450]\n",
      "Step 514537  [5.422 sec/step, loss=0.07688, avg_loss=0.07451]\n",
      "Step 514538  [5.434 sec/step, loss=0.07669, avg_loss=0.07452]\n",
      "Step 514539  [5.450 sec/step, loss=0.07609, avg_loss=0.07462]\n",
      "Step 514540  [5.495 sec/step, loss=0.06550, avg_loss=0.07453]\n",
      "Step 514541  [5.507 sec/step, loss=0.07680, avg_loss=0.07458]\n",
      "Step 514542  [5.515 sec/step, loss=0.07651, avg_loss=0.07459]\n",
      "Step 514543  [5.532 sec/step, loss=0.07539, avg_loss=0.07457]\n",
      "Step 514544  [5.517 sec/step, loss=0.07484, avg_loss=0.07455]\n",
      "Step 514545  [5.530 sec/step, loss=0.07534, avg_loss=0.07455]\n",
      "Step 514546  [5.529 sec/step, loss=0.07501, avg_loss=0.07455]\n",
      "Step 514547  [5.535 sec/step, loss=0.07655, avg_loss=0.07456]\n",
      "Step 514548  [5.543 sec/step, loss=0.07253, avg_loss=0.07454]\n",
      "Step 514549  [5.536 sec/step, loss=0.07507, avg_loss=0.07454]\n",
      "Step 514550  [5.537 sec/step, loss=0.07585, avg_loss=0.07454]\n",
      "Step 514551  [5.525 sec/step, loss=0.07590, avg_loss=0.07453]\n",
      "Step 514552  [5.512 sec/step, loss=0.07203, avg_loss=0.07450]\n",
      "Generated 32 batches of size 32 in 2.411 sec\n",
      "Step 514553  [5.512 sec/step, loss=0.07370, avg_loss=0.07446]\n",
      "Step 514554  [5.516 sec/step, loss=0.07190, avg_loss=0.07446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 514555  [5.519 sec/step, loss=0.07676, avg_loss=0.07446]\n",
      "Step 514556  [5.496 sec/step, loss=0.07548, avg_loss=0.07448]\n",
      "Step 514557  [5.500 sec/step, loss=0.07534, avg_loss=0.07449]\n",
      "Step 514558  [5.509 sec/step, loss=0.07474, avg_loss=0.07449]\n",
      "Step 514559  [5.502 sec/step, loss=0.07494, avg_loss=0.07452]\n",
      "Step 514560  [5.513 sec/step, loss=0.07547, avg_loss=0.07455]\n",
      "Step 514561  [5.502 sec/step, loss=0.07452, avg_loss=0.07453]\n",
      "Step 514562  [5.490 sec/step, loss=0.07596, avg_loss=0.07452]\n",
      "Step 514563  [5.491 sec/step, loss=0.07729, avg_loss=0.07454]\n",
      "Step 514564  [5.475 sec/step, loss=0.07491, avg_loss=0.07454]\n",
      "Step 514565  [5.437 sec/step, loss=0.07531, avg_loss=0.07461]\n",
      "Step 514566  [5.441 sec/step, loss=0.07351, avg_loss=0.07460]\n",
      "Step 514567  [5.443 sec/step, loss=0.07628, avg_loss=0.07462]\n",
      "Step 514568  [5.439 sec/step, loss=0.07654, avg_loss=0.07462]\n",
      "Step 514569  [5.448 sec/step, loss=0.07517, avg_loss=0.07462]\n",
      "Step 514570  [5.458 sec/step, loss=0.07620, avg_loss=0.07463]\n",
      "Step 514571  [5.451 sec/step, loss=0.07580, avg_loss=0.07462]\n",
      "Step 514572  [5.448 sec/step, loss=0.07409, avg_loss=0.07460]\n",
      "Step 514573  [5.452 sec/step, loss=0.07500, avg_loss=0.07459]\n",
      "Step 514574  [5.443 sec/step, loss=0.07074, avg_loss=0.07459]\n",
      "Step 514575  [5.431 sec/step, loss=0.06664, avg_loss=0.07452]\n",
      "Step 514576  [5.417 sec/step, loss=0.07463, avg_loss=0.07450]\n",
      "Step 514577  [5.442 sec/step, loss=0.06799, avg_loss=0.07445]\n",
      "Step 514578  [5.438 sec/step, loss=0.07745, avg_loss=0.07445]\n",
      "Step 514579  [5.437 sec/step, loss=0.07593, avg_loss=0.07444]\n",
      "Step 514580  [5.423 sec/step, loss=0.07311, avg_loss=0.07441]\n",
      "Step 514581  [5.439 sec/step, loss=0.07652, avg_loss=0.07443]\n",
      "Step 514582  [5.442 sec/step, loss=0.07457, avg_loss=0.07446]\n",
      "Step 514583  [5.443 sec/step, loss=0.07371, avg_loss=0.07446]\n",
      "Step 514584  [5.453 sec/step, loss=0.07639, avg_loss=0.07447]\n",
      "Generated 32 batches of size 32 in 2.425 sec\n",
      "Step 514585  [5.461 sec/step, loss=0.07690, avg_loss=0.07448]\n",
      "Step 514586  [5.474 sec/step, loss=0.07629, avg_loss=0.07458]\n",
      "Step 514587  [5.455 sec/step, loss=0.07142, avg_loss=0.07454]\n",
      "Step 514588  [5.461 sec/step, loss=0.07490, avg_loss=0.07454]\n",
      "Step 514589  [5.452 sec/step, loss=0.07131, avg_loss=0.07449]\n",
      "Step 514590  [5.423 sec/step, loss=0.07368, avg_loss=0.07456]\n",
      "Step 514591  [5.434 sec/step, loss=0.07688, avg_loss=0.07457]\n",
      "Step 514592  [5.428 sec/step, loss=0.07616, avg_loss=0.07456]\n",
      "Step 514593  [5.454 sec/step, loss=0.07419, avg_loss=0.07457]\n",
      "Step 514594  [5.456 sec/step, loss=0.07120, avg_loss=0.07453]\n",
      "Step 514595  [5.454 sec/step, loss=0.07372, avg_loss=0.07449]\n",
      "Step 514596  [5.453 sec/step, loss=0.07471, avg_loss=0.07448]\n",
      "Step 514597  [5.456 sec/step, loss=0.07510, avg_loss=0.07450]\n",
      "Step 514598  [5.445 sec/step, loss=0.07233, avg_loss=0.07445]\n",
      "Step 514599  [5.434 sec/step, loss=0.07535, avg_loss=0.07444]\n",
      "Step 514600  [5.435 sec/step, loss=0.07601, avg_loss=0.07443]\n",
      "Writing summary at step: 514600\n",
      "Step 514601  [5.423 sec/step, loss=0.06755, avg_loss=0.07439]\n",
      "Step 514602  [5.403 sec/step, loss=0.07420, avg_loss=0.07440]\n",
      "Step 514603  [5.401 sec/step, loss=0.07678, avg_loss=0.07441]\n",
      "Step 514604  [5.400 sec/step, loss=0.07497, avg_loss=0.07440]\n",
      "Step 514605  [5.453 sec/step, loss=0.06662, avg_loss=0.07431]\n",
      "Step 514606  [5.440 sec/step, loss=0.07484, avg_loss=0.07430]\n",
      "Step 514607  [5.424 sec/step, loss=0.07328, avg_loss=0.07428]\n",
      "Step 514608  [5.432 sec/step, loss=0.07719, avg_loss=0.07431]\n",
      "Step 514609  [5.437 sec/step, loss=0.07710, avg_loss=0.07435]\n",
      "Step 514610  [5.447 sec/step, loss=0.07572, avg_loss=0.07439]\n",
      "Step 514611  [5.448 sec/step, loss=0.07337, avg_loss=0.07436]\n",
      "Step 514612  [5.448 sec/step, loss=0.07387, avg_loss=0.07436]\n",
      "Step 514613  [5.489 sec/step, loss=0.07448, avg_loss=0.07438]\n",
      "Step 514614  [5.435 sec/step, loss=0.07459, avg_loss=0.07445]\n",
      "Step 514615  [5.423 sec/step, loss=0.07631, avg_loss=0.07445]\n",
      "Generated 32 batches of size 32 in 2.387 sec\n",
      "Step 514616  [5.433 sec/step, loss=0.07754, avg_loss=0.07445]\n",
      "Step 514617  [5.433 sec/step, loss=0.07602, avg_loss=0.07448]\n",
      "Step 514618  [5.439 sec/step, loss=0.07745, avg_loss=0.07453]\n",
      "Step 514619  [5.425 sec/step, loss=0.07716, avg_loss=0.07455]\n",
      "Step 514620  [5.422 sec/step, loss=0.07616, avg_loss=0.07455]\n",
      "Step 514621  [5.419 sec/step, loss=0.07425, avg_loss=0.07454]\n",
      "Step 514622  [5.421 sec/step, loss=0.07628, avg_loss=0.07455]\n",
      "Step 514623  [5.421 sec/step, loss=0.07416, avg_loss=0.07457]\n",
      "Step 514624  [5.416 sec/step, loss=0.07394, avg_loss=0.07455]\n",
      "Step 514625  [5.439 sec/step, loss=0.07538, avg_loss=0.07456]\n",
      "Step 514626  [5.435 sec/step, loss=0.07572, avg_loss=0.07457]\n",
      "Step 514627  [5.457 sec/step, loss=0.07611, avg_loss=0.07466]\n",
      "Step 514628  [5.467 sec/step, loss=0.07640, avg_loss=0.07466]\n",
      "Step 514629  [5.472 sec/step, loss=0.07321, avg_loss=0.07464]\n",
      "Step 514630  [5.461 sec/step, loss=0.07493, avg_loss=0.07462]\n",
      "Step 514631  [5.462 sec/step, loss=0.07651, avg_loss=0.07463]\n",
      "Step 514632  [5.443 sec/step, loss=0.07086, avg_loss=0.07457]\n",
      "Step 514633  [5.454 sec/step, loss=0.07462, avg_loss=0.07465]\n",
      "Step 514634  [5.440 sec/step, loss=0.06612, avg_loss=0.07457]\n",
      "Step 514635  [5.447 sec/step, loss=0.07613, avg_loss=0.07459]\n",
      "Step 514636  [5.438 sec/step, loss=0.07488, avg_loss=0.07458]\n",
      "Step 514637  [5.432 sec/step, loss=0.07562, avg_loss=0.07457]\n",
      "Step 514638  [5.434 sec/step, loss=0.07635, avg_loss=0.07457]\n",
      "Step 514639  [5.445 sec/step, loss=0.07630, avg_loss=0.07457]\n",
      "Step 514640  [5.403 sec/step, loss=0.07681, avg_loss=0.07468]\n",
      "Step 514641  [5.397 sec/step, loss=0.07503, avg_loss=0.07467]\n",
      "Step 514642  [5.394 sec/step, loss=0.07638, avg_loss=0.07466]\n",
      "Step 514643  [5.384 sec/step, loss=0.07561, avg_loss=0.07467]\n",
      "Step 514644  [5.391 sec/step, loss=0.07438, avg_loss=0.07466]\n",
      "Step 514645  [5.361 sec/step, loss=0.07270, avg_loss=0.07464]\n",
      "Step 514646  [5.360 sec/step, loss=0.07544, avg_loss=0.07464]\n",
      "Step 514647  [5.396 sec/step, loss=0.06694, avg_loss=0.07454]\n",
      "Generated 32 batches of size 32 in 2.462 sec\n",
      "Step 514648  [5.413 sec/step, loss=0.07799, avg_loss=0.07460]\n",
      "Step 514649  [5.416 sec/step, loss=0.07427, avg_loss=0.07459]\n",
      "Step 514650  [5.418 sec/step, loss=0.07629, avg_loss=0.07460]\n",
      "Step 514651  [5.401 sec/step, loss=0.07140, avg_loss=0.07455]\n",
      "Step 514652  [5.415 sec/step, loss=0.07531, avg_loss=0.07458]\n",
      "Step 514653  [5.428 sec/step, loss=0.07585, avg_loss=0.07460]\n",
      "Step 514654  [5.433 sec/step, loss=0.07336, avg_loss=0.07462]\n",
      "Step 514655  [5.429 sec/step, loss=0.07568, avg_loss=0.07461]\n",
      "Step 514656  [5.439 sec/step, loss=0.07707, avg_loss=0.07462]\n",
      "Step 514657  [5.431 sec/step, loss=0.07650, avg_loss=0.07464]\n",
      "Step 514658  [5.428 sec/step, loss=0.07539, avg_loss=0.07464]\n",
      "Step 514659  [5.451 sec/step, loss=0.07695, avg_loss=0.07466]\n",
      "Step 514660  [5.446 sec/step, loss=0.07597, avg_loss=0.07467]\n",
      "Step 514661  [5.463 sec/step, loss=0.07578, avg_loss=0.07468]\n",
      "Step 514662  [5.459 sec/step, loss=0.07498, avg_loss=0.07467]\n",
      "Step 514663  [5.448 sec/step, loss=0.07494, avg_loss=0.07465]\n",
      "Step 514664  [5.458 sec/step, loss=0.07388, avg_loss=0.07464]\n",
      "Step 514665  [5.454 sec/step, loss=0.08028, avg_loss=0.07469]\n",
      "Step 514666  [5.466 sec/step, loss=0.07478, avg_loss=0.07470]\n",
      "Step 514667  [5.488 sec/step, loss=0.07455, avg_loss=0.07468]\n",
      "Step 514668  [5.537 sec/step, loss=0.06671, avg_loss=0.07458]\n",
      "Step 514669  [5.521 sec/step, loss=0.07490, avg_loss=0.07458]\n",
      "Step 514670  [5.522 sec/step, loss=0.07413, avg_loss=0.07456]\n",
      "Step 514671  [5.526 sec/step, loss=0.07728, avg_loss=0.07457]\n",
      "Step 514672  [5.530 sec/step, loss=0.07505, avg_loss=0.07458]\n",
      "Step 514673  [5.521 sec/step, loss=0.07181, avg_loss=0.07455]\n",
      "Step 514674  [5.528 sec/step, loss=0.07458, avg_loss=0.07459]\n",
      "Step 514675  [5.554 sec/step, loss=0.07509, avg_loss=0.07467]\n",
      "Step 514676  [5.562 sec/step, loss=0.07713, avg_loss=0.07470]\n",
      "Step 514677  [5.497 sec/step, loss=0.07261, avg_loss=0.07475]\n",
      "Step 514678  [5.505 sec/step, loss=0.07720, avg_loss=0.07474]\n",
      "Step 514679  [5.502 sec/step, loss=0.07398, avg_loss=0.07472]\n",
      "Generated 32 batches of size 32 in 2.503 sec\n",
      "Step 514680  [5.507 sec/step, loss=0.07651, avg_loss=0.07476]\n",
      "Step 514681  [5.490 sec/step, loss=0.07601, avg_loss=0.07475]\n",
      "Step 514682  [5.503 sec/step, loss=0.07567, avg_loss=0.07476]\n",
      "Step 514683  [5.516 sec/step, loss=0.07571, avg_loss=0.07478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 514684  [5.509 sec/step, loss=0.07522, avg_loss=0.07477]\n",
      "Step 514685  [5.491 sec/step, loss=0.07420, avg_loss=0.07475]\n",
      "Step 514686  [5.475 sec/step, loss=0.06697, avg_loss=0.07465]\n",
      "Step 514687  [5.492 sec/step, loss=0.07645, avg_loss=0.07470]\n",
      "Step 514688  [5.486 sec/step, loss=0.07215, avg_loss=0.07467]\n",
      "Step 514689  [5.506 sec/step, loss=0.07746, avg_loss=0.07474]\n",
      "Step 514690  [5.482 sec/step, loss=0.07448, avg_loss=0.07474]\n",
      "Step 514691  [5.471 sec/step, loss=0.07458, avg_loss=0.07472]\n",
      "Step 514692  [5.460 sec/step, loss=0.07440, avg_loss=0.07470]\n",
      "Step 514693  [5.469 sec/step, loss=0.07419, avg_loss=0.07470]\n",
      "Step 514694  [5.481 sec/step, loss=0.07722, avg_loss=0.07476]\n",
      "Step 514695  [5.481 sec/step, loss=0.07596, avg_loss=0.07479]\n",
      "Step 514696  [5.468 sec/step, loss=0.07430, avg_loss=0.07478]\n",
      "Step 514697  [5.482 sec/step, loss=0.07678, avg_loss=0.07480]\n",
      "Step 514698  [5.502 sec/step, loss=0.07713, avg_loss=0.07485]\n",
      "Step 514699  [5.513 sec/step, loss=0.07443, avg_loss=0.07484]\n",
      "Step 514700  [5.506 sec/step, loss=0.07452, avg_loss=0.07482]\n",
      "Writing summary at step: 514700\n",
      "Step 514701  [5.525 sec/step, loss=0.07599, avg_loss=0.07491]\n",
      "Step 514702  [5.526 sec/step, loss=0.07599, avg_loss=0.07493]\n",
      "Step 514703  [5.515 sec/step, loss=0.07200, avg_loss=0.07488]\n",
      "Step 514704  [5.513 sec/step, loss=0.07809, avg_loss=0.07491]\n",
      "Step 514705  [5.491 sec/step, loss=0.07408, avg_loss=0.07498]\n",
      "Step 514706  [5.488 sec/step, loss=0.07139, avg_loss=0.07495]\n",
      "Step 514707  [5.488 sec/step, loss=0.07322, avg_loss=0.07495]\n",
      "Step 514708  [5.484 sec/step, loss=0.07493, avg_loss=0.07493]\n",
      "Step 514709  [5.465 sec/step, loss=0.07512, avg_loss=0.07491]\n",
      "Step 514710  [5.468 sec/step, loss=0.07737, avg_loss=0.07492]\n",
      "Generated 32 batches of size 32 in 2.853 sec\n",
      "Step 514711  [5.451 sec/step, loss=0.06668, avg_loss=0.07486]\n",
      "Step 514712  [5.443 sec/step, loss=0.07493, avg_loss=0.07487]\n",
      "Step 514713  [5.425 sec/step, loss=0.07751, avg_loss=0.07490]\n",
      "Step 514714  [5.479 sec/step, loss=0.06829, avg_loss=0.07483]\n",
      "Step 514715  [5.486 sec/step, loss=0.07602, avg_loss=0.07483]\n",
      "Step 514716  [5.468 sec/step, loss=0.07588, avg_loss=0.07481]\n",
      "Step 514717  [5.466 sec/step, loss=0.07627, avg_loss=0.07482]\n",
      "Step 514718  [5.467 sec/step, loss=0.07490, avg_loss=0.07479]\n",
      "Step 514719  [5.456 sec/step, loss=0.07541, avg_loss=0.07477]\n",
      "Step 514720  [5.462 sec/step, loss=0.07709, avg_loss=0.07478]\n",
      "Step 514721  [5.469 sec/step, loss=0.07612, avg_loss=0.07480]\n",
      "Step 514722  [5.477 sec/step, loss=0.07684, avg_loss=0.07481]\n",
      "Step 514723  [5.477 sec/step, loss=0.07297, avg_loss=0.07480]\n",
      "Step 514724  [5.463 sec/step, loss=0.07626, avg_loss=0.07482]\n",
      "Step 514725  [5.448 sec/step, loss=0.07719, avg_loss=0.07484]\n",
      "Step 514726  [5.460 sec/step, loss=0.07693, avg_loss=0.07485]\n",
      "Step 514727  [5.454 sec/step, loss=0.07310, avg_loss=0.07482]\n",
      "Step 514728  [5.431 sec/step, loss=0.07650, avg_loss=0.07482]\n",
      "Step 514729  [5.438 sec/step, loss=0.07726, avg_loss=0.07486]\n",
      "Step 514730  [5.443 sec/step, loss=0.07594, avg_loss=0.07487]\n",
      "Step 514731  [5.445 sec/step, loss=0.07516, avg_loss=0.07486]\n",
      "Step 514732  [5.444 sec/step, loss=0.07500, avg_loss=0.07490]\n",
      "Step 514733  [5.448 sec/step, loss=0.07402, avg_loss=0.07489]\n",
      "Step 514734  [5.467 sec/step, loss=0.07357, avg_loss=0.07497]\n",
      "Step 514735  [5.466 sec/step, loss=0.07358, avg_loss=0.07494]\n",
      "Step 514736  [5.463 sec/step, loss=0.07226, avg_loss=0.07491]\n",
      "Step 514737  [5.503 sec/step, loss=0.06698, avg_loss=0.07483]\n",
      "Step 514738  [5.492 sec/step, loss=0.07620, avg_loss=0.07483]\n",
      "Step 514739  [5.501 sec/step, loss=0.07630, avg_loss=0.07483]\n",
      "Step 514740  [5.521 sec/step, loss=0.07414, avg_loss=0.07480]\n",
      "Step 514741  [5.517 sec/step, loss=0.07459, avg_loss=0.07480]\n",
      "Step 514742  [5.509 sec/step, loss=0.07450, avg_loss=0.07478]\n",
      "Generated 32 batches of size 32 in 2.548 sec\n",
      "Step 514743  [5.501 sec/step, loss=0.07492, avg_loss=0.07477]\n",
      "Step 514744  [5.509 sec/step, loss=0.07663, avg_loss=0.07479]\n",
      "Step 514745  [5.529 sec/step, loss=0.07665, avg_loss=0.07483]\n",
      "Step 514746  [5.527 sec/step, loss=0.07589, avg_loss=0.07484]\n",
      "Step 514747  [5.489 sec/step, loss=0.07735, avg_loss=0.07494]\n",
      "Step 514748  [5.480 sec/step, loss=0.07548, avg_loss=0.07492]\n",
      "Step 514749  [5.467 sec/step, loss=0.06690, avg_loss=0.07484]\n",
      "Step 514750  [5.457 sec/step, loss=0.07555, avg_loss=0.07483]\n",
      "Step 514751  [5.483 sec/step, loss=0.07443, avg_loss=0.07486]\n",
      "Step 514752  [5.475 sec/step, loss=0.07451, avg_loss=0.07486]\n",
      "Step 514753  [5.450 sec/step, loss=0.07471, avg_loss=0.07485]\n",
      "Step 514754  [5.459 sec/step, loss=0.07529, avg_loss=0.07486]\n",
      "Step 514755  [5.450 sec/step, loss=0.07639, avg_loss=0.07487]\n",
      "Step 514756  [5.446 sec/step, loss=0.07718, avg_loss=0.07487]\n",
      "Step 514757  [5.462 sec/step, loss=0.07495, avg_loss=0.07486]\n",
      "Step 514758  [5.463 sec/step, loss=0.07631, avg_loss=0.07487]\n",
      "Step 514759  [5.445 sec/step, loss=0.07455, avg_loss=0.07484]\n",
      "Step 514760  [5.444 sec/step, loss=0.07687, avg_loss=0.07485]\n",
      "Step 514761  [5.430 sec/step, loss=0.07379, avg_loss=0.07483]\n",
      "Step 514762  [5.452 sec/step, loss=0.07702, avg_loss=0.07485]\n",
      "Step 514763  [5.456 sec/step, loss=0.07582, avg_loss=0.07486]\n",
      "Step 514764  [5.507 sec/step, loss=0.06687, avg_loss=0.07479]\n",
      "Step 514765  [5.519 sec/step, loss=0.07641, avg_loss=0.07475]\n",
      "Step 514766  [5.507 sec/step, loss=0.07455, avg_loss=0.07475]\n",
      "Step 514767  [5.471 sec/step, loss=0.07220, avg_loss=0.07473]\n",
      "Step 514768  [5.418 sec/step, loss=0.07590, avg_loss=0.07482]\n",
      "Step 514769  [5.424 sec/step, loss=0.07163, avg_loss=0.07479]\n",
      "Step 514770  [5.414 sec/step, loss=0.07746, avg_loss=0.07482]\n",
      "Step 514771  [5.405 sec/step, loss=0.07645, avg_loss=0.07481]\n",
      "Step 514772  [5.387 sec/step, loss=0.06555, avg_loss=0.07472]\n",
      "Step 514773  [5.381 sec/step, loss=0.07233, avg_loss=0.07472]\n",
      "Step 514774  [5.412 sec/step, loss=0.07405, avg_loss=0.07472]\n",
      "Generated 32 batches of size 32 in 2.749 sec\n",
      "Step 514775  [5.395 sec/step, loss=0.07515, avg_loss=0.07472]\n",
      "Step 514776  [5.395 sec/step, loss=0.07714, avg_loss=0.07472]\n",
      "Step 514777  [5.408 sec/step, loss=0.07480, avg_loss=0.07474]\n",
      "Step 514778  [5.398 sec/step, loss=0.07511, avg_loss=0.07472]\n",
      "Step 514779  [5.410 sec/step, loss=0.07468, avg_loss=0.07472]\n",
      "Step 514780  [5.414 sec/step, loss=0.07719, avg_loss=0.07473]\n",
      "Step 514781  [5.409 sec/step, loss=0.07507, avg_loss=0.07472]\n",
      "Step 514782  [5.412 sec/step, loss=0.07572, avg_loss=0.07472]\n",
      "Step 514783  [5.408 sec/step, loss=0.07638, avg_loss=0.07473]\n",
      "Step 514784  [5.403 sec/step, loss=0.07658, avg_loss=0.07474]\n",
      "Step 514785  [5.421 sec/step, loss=0.07671, avg_loss=0.07477]\n",
      "Step 514786  [5.447 sec/step, loss=0.07667, avg_loss=0.07486]\n",
      "Step 514787  [5.450 sec/step, loss=0.07564, avg_loss=0.07486]\n",
      "Step 514788  [5.503 sec/step, loss=0.06987, avg_loss=0.07483]\n",
      "Step 514789  [5.481 sec/step, loss=0.07463, avg_loss=0.07480]\n",
      "Step 514790  [5.466 sec/step, loss=0.07503, avg_loss=0.07481]\n",
      "Step 514791  [5.457 sec/step, loss=0.07216, avg_loss=0.07479]\n",
      "Step 514792  [5.461 sec/step, loss=0.07462, avg_loss=0.07479]\n",
      "Step 514793  [5.466 sec/step, loss=0.07625, avg_loss=0.07481]\n",
      "Step 514794  [5.460 sec/step, loss=0.07601, avg_loss=0.07480]\n",
      "Step 514795  [5.468 sec/step, loss=0.07614, avg_loss=0.07480]\n",
      "Step 514796  [5.466 sec/step, loss=0.07176, avg_loss=0.07477]\n",
      "Step 514797  [5.446 sec/step, loss=0.07349, avg_loss=0.07474]\n",
      "Step 514798  [5.431 sec/step, loss=0.07280, avg_loss=0.07470]\n",
      "Step 514799  [5.428 sec/step, loss=0.07689, avg_loss=0.07472]\n",
      "Step 514800  [5.451 sec/step, loss=0.07631, avg_loss=0.07474]\n",
      "Writing summary at step: 514800\n",
      "Step 514801  [5.448 sec/step, loss=0.07398, avg_loss=0.07472]\n",
      "Step 514802  [5.454 sec/step, loss=0.07619, avg_loss=0.07472]\n",
      "Step 514803  [5.467 sec/step, loss=0.07698, avg_loss=0.07477]\n",
      "Step 514804  [5.463 sec/step, loss=0.07566, avg_loss=0.07475]\n",
      "Step 514805  [5.447 sec/step, loss=0.07694, avg_loss=0.07478]\n",
      "Generated 32 batches of size 32 in 2.585 sec\n",
      "Step 514806  [5.459 sec/step, loss=0.07597, avg_loss=0.07482]\n",
      "Step 514807  [5.478 sec/step, loss=0.07600, avg_loss=0.07485]\n",
      "Step 514808  [5.478 sec/step, loss=0.07612, avg_loss=0.07486]\n",
      "Step 514809  [5.497 sec/step, loss=0.07632, avg_loss=0.07487]\n",
      "Step 514810  [5.474 sec/step, loss=0.06757, avg_loss=0.07477]\n",
      "Step 514811  [5.483 sec/step, loss=0.07613, avg_loss=0.07487]\n",
      "Step 514812  [5.479 sec/step, loss=0.07162, avg_loss=0.07484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 514813  [5.481 sec/step, loss=0.07637, avg_loss=0.07482]\n",
      "Step 514814  [5.438 sec/step, loss=0.07515, avg_loss=0.07489]\n",
      "Step 514815  [5.444 sec/step, loss=0.07622, avg_loss=0.07490]\n",
      "Step 514816  [5.464 sec/step, loss=0.07502, avg_loss=0.07489]\n",
      "Step 514817  [5.466 sec/step, loss=0.07531, avg_loss=0.07488]\n",
      "Step 514818  [5.457 sec/step, loss=0.07580, avg_loss=0.07489]\n",
      "Step 514819  [5.453 sec/step, loss=0.07274, avg_loss=0.07486]\n",
      "Step 514820  [5.442 sec/step, loss=0.07597, avg_loss=0.07485]\n",
      "Step 514821  [5.437 sec/step, loss=0.07297, avg_loss=0.07482]\n",
      "Step 514822  [5.425 sec/step, loss=0.07480, avg_loss=0.07480]\n",
      "Step 514823  [5.427 sec/step, loss=0.07475, avg_loss=0.07481]\n",
      "Step 514824  [5.406 sec/step, loss=0.06592, avg_loss=0.07471]\n",
      "Step 514825  [5.455 sec/step, loss=0.06765, avg_loss=0.07462]\n",
      "Step 514826  [5.473 sec/step, loss=0.07404, avg_loss=0.07459]\n",
      "Step 514827  [5.479 sec/step, loss=0.07562, avg_loss=0.07461]\n",
      "Step 514828  [5.487 sec/step, loss=0.07585, avg_loss=0.07461]\n",
      "Step 514829  [5.475 sec/step, loss=0.07452, avg_loss=0.07458]\n",
      "Step 514830  [5.459 sec/step, loss=0.07165, avg_loss=0.07454]\n",
      "Step 514831  [5.471 sec/step, loss=0.07725, avg_loss=0.07456]\n",
      "Step 514832  [5.480 sec/step, loss=0.07568, avg_loss=0.07456]\n",
      "Step 514833  [5.485 sec/step, loss=0.07635, avg_loss=0.07459]\n",
      "Step 514834  [5.481 sec/step, loss=0.07564, avg_loss=0.07461]\n",
      "Step 514835  [5.488 sec/step, loss=0.07736, avg_loss=0.07464]\n",
      "Step 514836  [5.509 sec/step, loss=0.07674, avg_loss=0.07469]\n",
      "Step 514837  [5.448 sec/step, loss=0.07285, avg_loss=0.07475]\n",
      "Generated 32 batches of size 32 in 2.404 sec\n",
      "Step 514838  [5.463 sec/step, loss=0.07501, avg_loss=0.07474]\n",
      "Step 514839  [5.442 sec/step, loss=0.07480, avg_loss=0.07472]\n",
      "Step 514840  [5.425 sec/step, loss=0.07732, avg_loss=0.07475]\n",
      "Step 514841  [5.417 sec/step, loss=0.07288, avg_loss=0.07474]\n",
      "Step 514842  [5.419 sec/step, loss=0.07040, avg_loss=0.07469]\n",
      "Step 514843  [5.435 sec/step, loss=0.07786, avg_loss=0.07472]\n",
      "Step 514844  [5.429 sec/step, loss=0.07685, avg_loss=0.07473]\n",
      "Step 514845  [5.415 sec/step, loss=0.07644, avg_loss=0.07472]\n",
      "Step 514846  [5.431 sec/step, loss=0.07710, avg_loss=0.07474]\n",
      "Step 514847  [5.414 sec/step, loss=0.07294, avg_loss=0.07469]\n",
      "Step 514848  [5.409 sec/step, loss=0.07530, avg_loss=0.07469]\n",
      "Step 514849  [5.414 sec/step, loss=0.07239, avg_loss=0.07475]\n",
      "Step 514850  [5.427 sec/step, loss=0.07636, avg_loss=0.07475]\n",
      "Step 514851  [5.417 sec/step, loss=0.07613, avg_loss=0.07477]\n",
      "Step 514852  [5.432 sec/step, loss=0.07670, avg_loss=0.07479]\n",
      "Step 514853  [5.423 sec/step, loss=0.07493, avg_loss=0.07479]\n",
      "Step 514854  [5.426 sec/step, loss=0.07561, avg_loss=0.07480]\n",
      "Step 514855  [5.437 sec/step, loss=0.07686, avg_loss=0.07480]\n",
      "Step 514856  [5.440 sec/step, loss=0.07561, avg_loss=0.07479]\n",
      "Step 514857  [5.424 sec/step, loss=0.07623, avg_loss=0.07480]\n",
      "Step 514858  [5.404 sec/step, loss=0.06735, avg_loss=0.07471]\n",
      "Step 514859  [5.420 sec/step, loss=0.07671, avg_loss=0.07473]\n",
      "Step 514860  [5.425 sec/step, loss=0.07619, avg_loss=0.07472]\n",
      "Step 514861  [5.440 sec/step, loss=0.07688, avg_loss=0.07476]\n",
      "Step 514862  [5.443 sec/step, loss=0.07591, avg_loss=0.07474]\n",
      "Step 514863  [5.436 sec/step, loss=0.07401, avg_loss=0.07473]\n",
      "Step 514864  [5.387 sec/step, loss=0.07633, avg_loss=0.07482]\n",
      "Step 514865  [5.365 sec/step, loss=0.07435, avg_loss=0.07480]\n",
      "Step 514866  [5.378 sec/step, loss=0.07631, avg_loss=0.07482]\n",
      "Step 514867  [5.390 sec/step, loss=0.07583, avg_loss=0.07485]\n",
      "Step 514868  [5.385 sec/step, loss=0.07489, avg_loss=0.07484]\n",
      "Step 514869  [5.437 sec/step, loss=0.06652, avg_loss=0.07479]\n",
      "Generated 32 batches of size 32 in 2.544 sec\n",
      "Step 514870  [5.421 sec/step, loss=0.07499, avg_loss=0.07477]\n",
      "Step 514871  [5.427 sec/step, loss=0.07564, avg_loss=0.07476]\n",
      "Step 514872  [5.445 sec/step, loss=0.07292, avg_loss=0.07483]\n",
      "Step 514873  [5.458 sec/step, loss=0.07582, avg_loss=0.07487]\n",
      "Step 514874  [5.417 sec/step, loss=0.07292, avg_loss=0.07486]\n",
      "Step 514875  [5.450 sec/step, loss=0.07407, avg_loss=0.07485]\n",
      "Step 514876  [5.438 sec/step, loss=0.07180, avg_loss=0.07479]\n",
      "Step 514877  [5.443 sec/step, loss=0.07572, avg_loss=0.07480]\n",
      "Step 514878  [5.447 sec/step, loss=0.07751, avg_loss=0.07483]\n",
      "Step 514879  [5.430 sec/step, loss=0.07409, avg_loss=0.07482]\n",
      "Step 514880  [5.468 sec/step, loss=0.06734, avg_loss=0.07472]\n",
      "Step 514881  [5.474 sec/step, loss=0.07163, avg_loss=0.07469]\n",
      "Step 514882  [5.463 sec/step, loss=0.07583, avg_loss=0.07469]\n",
      "Step 514883  [5.472 sec/step, loss=0.07710, avg_loss=0.07470]\n",
      "Step 514884  [5.473 sec/step, loss=0.07400, avg_loss=0.07467]\n",
      "Step 514885  [5.458 sec/step, loss=0.07626, avg_loss=0.07467]\n",
      "Step 514886  [5.449 sec/step, loss=0.07517, avg_loss=0.07465]\n",
      "Step 514887  [5.456 sec/step, loss=0.07650, avg_loss=0.07466]\n",
      "Step 514888  [5.433 sec/step, loss=0.07351, avg_loss=0.07470]\n",
      "Step 514889  [5.446 sec/step, loss=0.07687, avg_loss=0.07472]\n",
      "Step 514890  [5.464 sec/step, loss=0.07677, avg_loss=0.07474]\n",
      "Step 514891  [5.502 sec/step, loss=0.07666, avg_loss=0.07478]\n",
      "Step 514892  [5.495 sec/step, loss=0.07171, avg_loss=0.07475]\n",
      "Step 514893  [5.480 sec/step, loss=0.07687, avg_loss=0.07476]\n",
      "Step 514894  [5.476 sec/step, loss=0.07593, avg_loss=0.07476]\n",
      "Step 514895  [5.461 sec/step, loss=0.07389, avg_loss=0.07473]\n",
      "Step 514896  [5.472 sec/step, loss=0.07693, avg_loss=0.07479]\n",
      "Step 514897  [5.489 sec/step, loss=0.07486, avg_loss=0.07480]\n",
      "Step 514898  [5.499 sec/step, loss=0.07576, avg_loss=0.07483]\n",
      "Step 514899  [5.475 sec/step, loss=0.06659, avg_loss=0.07473]\n",
      "Step 514900  [5.449 sec/step, loss=0.07391, avg_loss=0.07470]\n",
      "Writing summary at step: 514900\n",
      "Generated 32 batches of size 32 in 2.775 sec\n",
      "Step 514901  [5.440 sec/step, loss=0.07255, avg_loss=0.07469]\n",
      "Step 514902  [5.429 sec/step, loss=0.07533, avg_loss=0.07468]\n",
      "Step 514903  [5.427 sec/step, loss=0.07491, avg_loss=0.07466]\n",
      "Step 514904  [5.424 sec/step, loss=0.07624, avg_loss=0.07467]\n",
      "Step 514905  [5.411 sec/step, loss=0.07344, avg_loss=0.07463]\n",
      "Step 514906  [5.424 sec/step, loss=0.07641, avg_loss=0.07463]\n",
      "Step 514907  [5.424 sec/step, loss=0.07616, avg_loss=0.07464]\n",
      "Step 514908  [5.425 sec/step, loss=0.07523, avg_loss=0.07463]\n",
      "Step 514909  [5.417 sec/step, loss=0.07596, avg_loss=0.07462]\n",
      "Step 514910  [5.423 sec/step, loss=0.07487, avg_loss=0.07470]\n",
      "Step 514911  [5.435 sec/step, loss=0.07658, avg_loss=0.07470]\n",
      "Step 514912  [5.444 sec/step, loss=0.07532, avg_loss=0.07474]\n",
      "Step 514913  [5.418 sec/step, loss=0.06691, avg_loss=0.07464]\n",
      "Step 514914  [5.460 sec/step, loss=0.06716, avg_loss=0.07456]\n",
      "Step 514915  [5.446 sec/step, loss=0.07395, avg_loss=0.07454]\n",
      "Step 514916  [5.435 sec/step, loss=0.07698, avg_loss=0.07456]\n",
      "Step 514917  [5.429 sec/step, loss=0.07428, avg_loss=0.07455]\n",
      "Step 514918  [5.411 sec/step, loss=0.07271, avg_loss=0.07452]\n",
      "Step 514919  [5.431 sec/step, loss=0.07632, avg_loss=0.07455]\n",
      "Step 514920  [5.443 sec/step, loss=0.07514, avg_loss=0.07455]\n",
      "Step 514921  [5.435 sec/step, loss=0.07414, avg_loss=0.07456]\n",
      "Step 514922  [5.434 sec/step, loss=0.07481, avg_loss=0.07456]\n",
      "Step 514923  [5.449 sec/step, loss=0.07678, avg_loss=0.07458]\n",
      "Step 514924  [5.463 sec/step, loss=0.07250, avg_loss=0.07464]\n",
      "Step 514925  [5.414 sec/step, loss=0.07612, avg_loss=0.07473]\n",
      "Step 514926  [5.398 sec/step, loss=0.07561, avg_loss=0.07474]\n",
      "Step 514927  [5.397 sec/step, loss=0.07608, avg_loss=0.07475]\n",
      "Step 514928  [5.402 sec/step, loss=0.07677, avg_loss=0.07476]\n",
      "Step 514929  [5.420 sec/step, loss=0.07670, avg_loss=0.07478]\n",
      "Step 514930  [5.425 sec/step, loss=0.07588, avg_loss=0.07482]\n",
      "Step 514931  [5.427 sec/step, loss=0.07681, avg_loss=0.07482]\n",
      "Step 514932  [5.433 sec/step, loss=0.07557, avg_loss=0.07482]\n",
      "Generated 32 batches of size 32 in 2.403 sec\n",
      "Step 514933  [5.441 sec/step, loss=0.07567, avg_loss=0.07481]\n",
      "Step 514934  [5.442 sec/step, loss=0.07535, avg_loss=0.07481]\n",
      "Step 514935  [5.445 sec/step, loss=0.07691, avg_loss=0.07480]\n",
      "Step 514936  [5.465 sec/step, loss=0.07402, avg_loss=0.07478]\n",
      "Step 514937  [5.474 sec/step, loss=0.07420, avg_loss=0.07479]\n",
      "Step 514938  [5.447 sec/step, loss=0.07315, avg_loss=0.07477]\n",
      "Step 514939  [5.450 sec/step, loss=0.07701, avg_loss=0.07479]\n",
      "Step 514940  [5.442 sec/step, loss=0.07357, avg_loss=0.07476]\n",
      "Step 514941  [5.460 sec/step, loss=0.07367, avg_loss=0.07476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 514942  [5.467 sec/step, loss=0.07620, avg_loss=0.07482]\n",
      "Step 514943  [5.451 sec/step, loss=0.07517, avg_loss=0.07479]\n",
      "Step 514944  [5.455 sec/step, loss=0.07651, avg_loss=0.07479]\n",
      "Step 514945  [5.462 sec/step, loss=0.07681, avg_loss=0.07479]\n",
      "Step 514946  [5.450 sec/step, loss=0.07595, avg_loss=0.07478]\n",
      "Step 514947  [5.504 sec/step, loss=0.06609, avg_loss=0.07471]\n",
      "Step 514948  [5.532 sec/step, loss=0.07373, avg_loss=0.07470]\n",
      "Step 514949  [5.527 sec/step, loss=0.06527, avg_loss=0.07463]\n",
      "Step 514950  [5.522 sec/step, loss=0.07702, avg_loss=0.07463]\n",
      "Step 514951  [5.520 sec/step, loss=0.07539, avg_loss=0.07463]\n",
      "Step 514952  [5.517 sec/step, loss=0.07617, avg_loss=0.07462]\n",
      "Step 514953  [5.526 sec/step, loss=0.07528, avg_loss=0.07463]\n",
      "Step 514954  [5.527 sec/step, loss=0.07613, avg_loss=0.07463]\n",
      "Step 514955  [5.509 sec/step, loss=0.07485, avg_loss=0.07461]\n",
      "Step 514956  [5.507 sec/step, loss=0.07526, avg_loss=0.07461]\n",
      "Step 514957  [5.499 sec/step, loss=0.07125, avg_loss=0.07456]\n",
      "Step 514958  [5.530 sec/step, loss=0.07673, avg_loss=0.07465]\n",
      "Step 514959  [5.505 sec/step, loss=0.07216, avg_loss=0.07461]\n",
      "Step 514960  [5.504 sec/step, loss=0.07548, avg_loss=0.07460]\n",
      "Step 514961  [5.502 sec/step, loss=0.07709, avg_loss=0.07460]\n",
      "Step 514962  [5.477 sec/step, loss=0.07472, avg_loss=0.07459]\n",
      "Step 514963  [5.493 sec/step, loss=0.07686, avg_loss=0.07462]\n",
      "Step 514964  [5.487 sec/step, loss=0.07199, avg_loss=0.07457]\n",
      "Generated 32 batches of size 32 in 2.578 sec\n",
      "Step 514965  [5.496 sec/step, loss=0.07402, avg_loss=0.07457]\n",
      "Step 514966  [5.500 sec/step, loss=0.07576, avg_loss=0.07456]\n",
      "Step 514967  [5.504 sec/step, loss=0.07444, avg_loss=0.07455]\n",
      "Step 514968  [5.504 sec/step, loss=0.07502, avg_loss=0.07455]\n",
      "Step 514969  [5.465 sec/step, loss=0.07690, avg_loss=0.07466]\n",
      "Step 514970  [5.468 sec/step, loss=0.07595, avg_loss=0.07467]\n",
      "Step 514971  [5.461 sec/step, loss=0.07602, avg_loss=0.07467]\n",
      "Step 514972  [5.455 sec/step, loss=0.07448, avg_loss=0.07468]\n",
      "Step 514973  [5.453 sec/step, loss=0.07453, avg_loss=0.07467]\n",
      "Step 514974  [5.459 sec/step, loss=0.07420, avg_loss=0.07468]\n",
      "Step 514975  [5.437 sec/step, loss=0.07640, avg_loss=0.07471]\n",
      "Step 514976  [5.449 sec/step, loss=0.07588, avg_loss=0.07475]\n",
      "Step 514977  [5.445 sec/step, loss=0.07512, avg_loss=0.07474]\n",
      "Step 514978  [5.433 sec/step, loss=0.07590, avg_loss=0.07473]\n",
      "Step 514979  [5.450 sec/step, loss=0.07514, avg_loss=0.07474]\n",
      "Step 514980  [5.404 sec/step, loss=0.07574, avg_loss=0.07482]\n",
      "Step 514981  [5.424 sec/step, loss=0.07597, avg_loss=0.07486]\n",
      "Step 514982  [5.424 sec/step, loss=0.07596, avg_loss=0.07487]\n",
      "Step 514983  [5.418 sec/step, loss=0.07454, avg_loss=0.07484]\n",
      "Step 514984  [5.423 sec/step, loss=0.07694, avg_loss=0.07487]\n",
      "Step 514985  [5.424 sec/step, loss=0.07612, avg_loss=0.07487]\n",
      "Step 514986  [5.411 sec/step, loss=0.07225, avg_loss=0.07484]\n",
      "Step 514987  [5.383 sec/step, loss=0.06742, avg_loss=0.07475]\n",
      "Step 514988  [5.363 sec/step, loss=0.07481, avg_loss=0.07476]\n",
      "Step 514989  [5.353 sec/step, loss=0.07493, avg_loss=0.07474]\n",
      "Step 514990  [5.355 sec/step, loss=0.07557, avg_loss=0.07473]\n",
      "Step 514991  [5.381 sec/step, loss=0.06650, avg_loss=0.07463]\n",
      "Step 514992  [5.380 sec/step, loss=0.07518, avg_loss=0.07466]\n",
      "Step 514993  [5.376 sec/step, loss=0.07728, avg_loss=0.07467]\n",
      "Step 514994  [5.371 sec/step, loss=0.07455, avg_loss=0.07465]\n",
      "Step 514995  [5.369 sec/step, loss=0.07352, avg_loss=0.07465]\n",
      "Step 514996  [5.373 sec/step, loss=0.07696, avg_loss=0.07465]\n",
      "Generated 32 batches of size 32 in 2.538 sec\n",
      "Step 514997  [5.365 sec/step, loss=0.07408, avg_loss=0.07464]\n",
      "Step 514998  [5.360 sec/step, loss=0.07501, avg_loss=0.07464]\n",
      "Step 514999  [5.378 sec/step, loss=0.07635, avg_loss=0.07473]\n",
      "Step 515000  [5.397 sec/step, loss=0.07693, avg_loss=0.07476]\n",
      "Writing summary at step: 515000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-515000\n",
      "Saving audio and alignment...\n",
      "Input: ixlaaqii imiidz banaanay kayliay or dzhagrray sae batsnay kayliay rizvaan apnay payshroo xaan zaaddaa kii koothiioon kay ddoray pir gijaa~__\n",
      "Step 515001  [5.396 sec/step, loss=0.07098, avg_loss=0.07475]\n",
      "Step 515002  [5.431 sec/step, loss=0.07376, avg_loss=0.07473]\n",
      "Step 515003  [5.426 sec/step, loss=0.07675, avg_loss=0.07475]\n",
      "Step 515004  [5.424 sec/step, loss=0.07536, avg_loss=0.07474]\n",
      "Step 515005  [5.436 sec/step, loss=0.07711, avg_loss=0.07478]\n",
      "Step 515006  [5.412 sec/step, loss=0.07517, avg_loss=0.07477]\n",
      "Step 515007  [5.399 sec/step, loss=0.07448, avg_loss=0.07475]\n",
      "Step 515008  [5.414 sec/step, loss=0.07667, avg_loss=0.07476]\n",
      "Step 515009  [5.415 sec/step, loss=0.07680, avg_loss=0.07477]\n",
      "Step 515010  [5.436 sec/step, loss=0.07491, avg_loss=0.07477]\n",
      "Step 515011  [5.409 sec/step, loss=0.06719, avg_loss=0.07468]\n",
      "Step 515012  [5.400 sec/step, loss=0.07248, avg_loss=0.07465]\n",
      "Step 515013  [5.412 sec/step, loss=0.07352, avg_loss=0.07472]\n",
      "Step 515014  [5.368 sec/step, loss=0.07593, avg_loss=0.07480]\n",
      "Step 515015  [5.377 sec/step, loss=0.07674, avg_loss=0.07483]\n",
      "Step 515016  [5.370 sec/step, loss=0.07610, avg_loss=0.07482]\n",
      "Step 515017  [5.378 sec/step, loss=0.07539, avg_loss=0.07483]\n",
      "Step 515018  [5.390 sec/step, loss=0.07084, avg_loss=0.07481]\n",
      "Step 515019  [5.358 sec/step, loss=0.07241, avg_loss=0.07478]\n",
      "Step 515020  [5.343 sec/step, loss=0.07548, avg_loss=0.07478]\n",
      "Step 515021  [5.355 sec/step, loss=0.07605, avg_loss=0.07480]\n",
      "Step 515022  [5.363 sec/step, loss=0.07546, avg_loss=0.07480]\n",
      "Step 515023  [5.354 sec/step, loss=0.07470, avg_loss=0.07478]\n",
      "Step 515024  [5.360 sec/step, loss=0.07519, avg_loss=0.07481]\n",
      "Step 515025  [5.368 sec/step, loss=0.07410, avg_loss=0.07479]\n",
      "Step 515026  [5.349 sec/step, loss=0.07507, avg_loss=0.07479]\n",
      "Generated 32 batches of size 32 in 2.490 sec\n",
      "Step 515027  [5.376 sec/step, loss=0.07372, avg_loss=0.07476]\n",
      "Step 515028  [5.376 sec/step, loss=0.07680, avg_loss=0.07476]\n",
      "Step 515029  [5.375 sec/step, loss=0.07687, avg_loss=0.07476]\n",
      "Step 515030  [5.398 sec/step, loss=0.07392, avg_loss=0.07474]\n",
      "Step 515031  [5.435 sec/step, loss=0.06725, avg_loss=0.07465]\n",
      "Step 515032  [5.427 sec/step, loss=0.07487, avg_loss=0.07464]\n",
      "Step 515033  [5.426 sec/step, loss=0.07669, avg_loss=0.07465]\n",
      "Step 515034  [5.423 sec/step, loss=0.07629, avg_loss=0.07466]\n",
      "Step 515035  [5.409 sec/step, loss=0.07637, avg_loss=0.07466]\n",
      "Step 515036  [5.395 sec/step, loss=0.07436, avg_loss=0.07466]\n",
      "Step 515037  [5.447 sec/step, loss=0.06677, avg_loss=0.07458]\n",
      "Step 515038  [5.441 sec/step, loss=0.06636, avg_loss=0.07452]\n",
      "Step 515039  [5.453 sec/step, loss=0.07659, avg_loss=0.07451]\n",
      "Step 515040  [5.458 sec/step, loss=0.07597, avg_loss=0.07454]\n",
      "Step 515041  [5.468 sec/step, loss=0.07684, avg_loss=0.07457]\n",
      "Step 515042  [5.462 sec/step, loss=0.07353, avg_loss=0.07454]\n",
      "Step 515043  [5.460 sec/step, loss=0.07545, avg_loss=0.07454]\n",
      "Step 515044  [5.445 sec/step, loss=0.07612, avg_loss=0.07454]\n",
      "Step 515045  [5.434 sec/step, loss=0.07272, avg_loss=0.07450]\n",
      "Step 515046  [5.442 sec/step, loss=0.07514, avg_loss=0.07449]\n",
      "Step 515047  [5.392 sec/step, loss=0.07516, avg_loss=0.07458]\n",
      "Step 515048  [5.381 sec/step, loss=0.07577, avg_loss=0.07460]\n",
      "Step 515049  [5.400 sec/step, loss=0.07595, avg_loss=0.07471]\n",
      "Step 515050  [5.397 sec/step, loss=0.07577, avg_loss=0.07470]\n",
      "Step 515051  [5.405 sec/step, loss=0.07671, avg_loss=0.07471]\n",
      "Step 515052  [5.429 sec/step, loss=0.07354, avg_loss=0.07468]\n",
      "Step 515053  [5.437 sec/step, loss=0.07528, avg_loss=0.07468]\n",
      "Step 515054  [5.418 sec/step, loss=0.07211, avg_loss=0.07464]\n",
      "Step 515055  [5.417 sec/step, loss=0.07186, avg_loss=0.07461]\n",
      "Step 515056  [5.398 sec/step, loss=0.07524, avg_loss=0.07461]\n",
      "Step 515057  [5.405 sec/step, loss=0.07413, avg_loss=0.07464]\n",
      "Step 515058  [5.382 sec/step, loss=0.07481, avg_loss=0.07462]\n",
      "Generated 32 batches of size 32 in 2.404 sec\n",
      "Step 515059  [5.409 sec/step, loss=0.07719, avg_loss=0.07467]\n",
      "Step 515060  [5.410 sec/step, loss=0.07576, avg_loss=0.07468]\n",
      "Step 515061  [5.402 sec/step, loss=0.07218, avg_loss=0.07463]\n",
      "Step 515062  [5.411 sec/step, loss=0.07643, avg_loss=0.07464]\n",
      "Step 515063  [5.398 sec/step, loss=0.07470, avg_loss=0.07462]\n",
      "Step 515064  [5.407 sec/step, loss=0.07522, avg_loss=0.07466]\n",
      "Step 515065  [5.403 sec/step, loss=0.07587, avg_loss=0.07467]\n",
      "Step 515066  [5.396 sec/step, loss=0.07689, avg_loss=0.07468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 515067  [5.383 sec/step, loss=0.07433, avg_loss=0.07468]\n",
      "Step 515068  [5.387 sec/step, loss=0.07638, avg_loss=0.07470]\n",
      "Step 515069  [5.390 sec/step, loss=0.07639, avg_loss=0.07469]\n",
      "Step 515070  [5.393 sec/step, loss=0.07653, avg_loss=0.07470]\n",
      "Step 515071  [5.395 sec/step, loss=0.07247, avg_loss=0.07466]\n",
      "Step 515072  [5.408 sec/step, loss=0.07731, avg_loss=0.07469]\n",
      "Step 515073  [5.422 sec/step, loss=0.07491, avg_loss=0.07469]\n",
      "Step 515074  [5.435 sec/step, loss=0.07682, avg_loss=0.07472]\n",
      "Step 515075  [5.431 sec/step, loss=0.07587, avg_loss=0.07472]\n",
      "Step 515076  [5.447 sec/step, loss=0.07397, avg_loss=0.07470]\n",
      "Step 515077  [5.437 sec/step, loss=0.07498, avg_loss=0.07469]\n",
      "Step 515078  [5.443 sec/step, loss=0.07619, avg_loss=0.07470]\n",
      "Step 515079  [5.437 sec/step, loss=0.07533, avg_loss=0.07470]\n",
      "Step 515080  [5.445 sec/step, loss=0.07717, avg_loss=0.07471]\n",
      "Step 515081  [5.419 sec/step, loss=0.07430, avg_loss=0.07470]\n",
      "Step 515082  [5.432 sec/step, loss=0.07353, avg_loss=0.07467]\n",
      "Step 515083  [5.430 sec/step, loss=0.07262, avg_loss=0.07465]\n",
      "Step 515084  [5.410 sec/step, loss=0.07208, avg_loss=0.07461]\n",
      "Step 515085  [5.426 sec/step, loss=0.07703, avg_loss=0.07461]\n",
      "Step 515086  [5.435 sec/step, loss=0.07345, avg_loss=0.07463]\n",
      "Step 515087  [5.502 sec/step, loss=0.06694, avg_loss=0.07462]\n",
      "Step 515088  [5.502 sec/step, loss=0.07538, avg_loss=0.07463]\n",
      "Step 515089  [5.516 sec/step, loss=0.07760, avg_loss=0.07465]\n",
      "Step 515090  [5.511 sec/step, loss=0.07568, avg_loss=0.07465]\n",
      "Generated 32 batches of size 32 in 2.831 sec\n",
      "Step 515091  [5.450 sec/step, loss=0.07315, avg_loss=0.07472]\n",
      "Step 515092  [5.456 sec/step, loss=0.07475, avg_loss=0.07472]\n",
      "Step 515093  [5.446 sec/step, loss=0.07233, avg_loss=0.07467]\n",
      "Step 515094  [5.463 sec/step, loss=0.07674, avg_loss=0.07469]\n",
      "Step 515095  [5.461 sec/step, loss=0.07460, avg_loss=0.07470]\n",
      "Step 515096  [5.447 sec/step, loss=0.07475, avg_loss=0.07468]\n",
      "Step 515097  [5.455 sec/step, loss=0.07704, avg_loss=0.07471]\n",
      "Step 515098  [5.453 sec/step, loss=0.07609, avg_loss=0.07472]\n",
      "Step 515099  [5.434 sec/step, loss=0.06721, avg_loss=0.07463]\n",
      "Step 515100  [5.414 sec/step, loss=0.07174, avg_loss=0.07458]\n",
      "Writing summary at step: 515100\n",
      "Step 515101  [5.434 sec/step, loss=0.07726, avg_loss=0.07464]\n",
      "Step 515102  [5.414 sec/step, loss=0.07530, avg_loss=0.07465]\n",
      "Step 515103  [5.416 sec/step, loss=0.07581, avg_loss=0.07464]\n",
      "Step 515104  [5.419 sec/step, loss=0.07276, avg_loss=0.07462]\n",
      "Step 515105  [5.458 sec/step, loss=0.06637, avg_loss=0.07451]\n",
      "Step 515106  [5.471 sec/step, loss=0.07609, avg_loss=0.07452]\n",
      "Step 515107  [5.471 sec/step, loss=0.07495, avg_loss=0.07452]\n",
      "Step 515108  [5.452 sec/step, loss=0.07614, avg_loss=0.07452]\n",
      "Step 515109  [5.455 sec/step, loss=0.07587, avg_loss=0.07451]\n",
      "Step 515110  [5.464 sec/step, loss=0.07341, avg_loss=0.07449]\n",
      "Step 515111  [5.495 sec/step, loss=0.07674, avg_loss=0.07459]\n",
      "Step 515112  [5.504 sec/step, loss=0.07616, avg_loss=0.07463]\n",
      "Step 515113  [5.535 sec/step, loss=0.07354, avg_loss=0.07463]\n",
      "Step 515114  [5.512 sec/step, loss=0.06706, avg_loss=0.07454]\n",
      "Step 515115  [5.494 sec/step, loss=0.07214, avg_loss=0.07449]\n",
      "Step 515116  [5.494 sec/step, loss=0.07590, avg_loss=0.07449]\n",
      "Step 515117  [5.493 sec/step, loss=0.07402, avg_loss=0.07448]\n",
      "Step 515118  [5.485 sec/step, loss=0.07304, avg_loss=0.07450]\n",
      "Step 515119  [5.503 sec/step, loss=0.07546, avg_loss=0.07453]\n",
      "Step 515120  [5.500 sec/step, loss=0.07422, avg_loss=0.07452]\n",
      "Step 515121  [5.504 sec/step, loss=0.07686, avg_loss=0.07452]\n",
      "Generated 32 batches of size 32 in 2.489 sec\n",
      "Step 515122  [5.506 sec/step, loss=0.07505, avg_loss=0.07452]\n",
      "Step 515123  [5.504 sec/step, loss=0.07105, avg_loss=0.07448]\n",
      "Step 515124  [5.503 sec/step, loss=0.07610, avg_loss=0.07449]\n",
      "Step 515125  [5.504 sec/step, loss=0.07368, avg_loss=0.07449]\n",
      "Step 515126  [5.507 sec/step, loss=0.07426, avg_loss=0.07448]\n",
      "Step 515127  [5.476 sec/step, loss=0.07562, avg_loss=0.07450]\n",
      "Step 515128  [5.480 sec/step, loss=0.07627, avg_loss=0.07449]\n",
      "Step 515129  [5.482 sec/step, loss=0.07646, avg_loss=0.07449]\n",
      "Step 515130  [5.459 sec/step, loss=0.07484, avg_loss=0.07450]\n",
      "Step 515131  [5.413 sec/step, loss=0.07609, avg_loss=0.07459]\n",
      "Step 515132  [5.408 sec/step, loss=0.07321, avg_loss=0.07457]\n",
      "Step 515133  [5.393 sec/step, loss=0.07182, avg_loss=0.07452]\n",
      "Step 515134  [5.402 sec/step, loss=0.07728, avg_loss=0.07453]\n",
      "Step 515135  [5.403 sec/step, loss=0.07293, avg_loss=0.07450]\n",
      "Step 515136  [5.399 sec/step, loss=0.07502, avg_loss=0.07450]\n",
      "Step 515137  [5.353 sec/step, loss=0.07413, avg_loss=0.07458]\n",
      "Step 515138  [5.388 sec/step, loss=0.07465, avg_loss=0.07466]\n",
      "Step 515139  [5.360 sec/step, loss=0.07251, avg_loss=0.07462]\n",
      "Step 515140  [5.363 sec/step, loss=0.07743, avg_loss=0.07464]\n",
      "Step 515141  [5.348 sec/step, loss=0.07519, avg_loss=0.07462]\n",
      "Step 515142  [5.355 sec/step, loss=0.07556, avg_loss=0.07464]\n",
      "Step 515143  [5.370 sec/step, loss=0.07752, avg_loss=0.07466]\n",
      "Step 515144  [5.424 sec/step, loss=0.06779, avg_loss=0.07458]\n",
      "Step 515145  [5.426 sec/step, loss=0.07698, avg_loss=0.07462]\n",
      "Step 515146  [5.445 sec/step, loss=0.07479, avg_loss=0.07462]\n",
      "Step 515147  [5.456 sec/step, loss=0.07735, avg_loss=0.07464]\n",
      "Step 515148  [5.451 sec/step, loss=0.07423, avg_loss=0.07462]\n",
      "Step 515149  [5.446 sec/step, loss=0.07446, avg_loss=0.07461]\n",
      "Step 515150  [5.454 sec/step, loss=0.07499, avg_loss=0.07460]\n",
      "Step 515151  [5.455 sec/step, loss=0.07490, avg_loss=0.07458]\n",
      "Step 515152  [5.422 sec/step, loss=0.07431, avg_loss=0.07459]\n",
      "Step 515153  [5.415 sec/step, loss=0.07417, avg_loss=0.07458]\n",
      "Generated 32 batches of size 32 in 2.577 sec\n",
      "Step 515154  [5.423 sec/step, loss=0.07060, avg_loss=0.07456]\n",
      "Step 515155  [5.423 sec/step, loss=0.07471, avg_loss=0.07459]\n",
      "Step 515156  [5.434 sec/step, loss=0.07524, avg_loss=0.07459]\n",
      "Step 515157  [5.431 sec/step, loss=0.07542, avg_loss=0.07460]\n",
      "Step 515158  [5.453 sec/step, loss=0.07704, avg_loss=0.07463]\n",
      "Step 515159  [5.439 sec/step, loss=0.07541, avg_loss=0.07461]\n",
      "Step 515160  [5.417 sec/step, loss=0.06567, avg_loss=0.07451]\n",
      "Step 515161  [5.413 sec/step, loss=0.07627, avg_loss=0.07455]\n",
      "Step 515162  [5.413 sec/step, loss=0.07621, avg_loss=0.07455]\n",
      "Step 515163  [5.411 sec/step, loss=0.07611, avg_loss=0.07456]\n",
      "Step 515164  [5.409 sec/step, loss=0.07629, avg_loss=0.07457]\n",
      "Step 515165  [5.411 sec/step, loss=0.07182, avg_loss=0.07453]\n",
      "Step 515166  [5.418 sec/step, loss=0.07596, avg_loss=0.07452]\n",
      "Step 515167  [5.421 sec/step, loss=0.07273, avg_loss=0.07451]\n",
      "Step 515168  [5.414 sec/step, loss=0.07233, avg_loss=0.07446]\n",
      "Step 515169  [5.404 sec/step, loss=0.07500, avg_loss=0.07445]\n",
      "Step 515170  [5.396 sec/step, loss=0.07434, avg_loss=0.07443]\n",
      "Step 515171  [5.402 sec/step, loss=0.07663, avg_loss=0.07447]\n",
      "Step 515172  [5.378 sec/step, loss=0.06588, avg_loss=0.07436]\n",
      "Step 515173  [5.354 sec/step, loss=0.07179, avg_loss=0.07433]\n",
      "Step 515174  [5.357 sec/step, loss=0.07693, avg_loss=0.07433]\n",
      "Step 515175  [5.356 sec/step, loss=0.07522, avg_loss=0.07432]\n",
      "Step 515176  [5.330 sec/step, loss=0.07311, avg_loss=0.07431]\n",
      "Step 515177  [5.338 sec/step, loss=0.07303, avg_loss=0.07429]\n",
      "Step 515178  [5.357 sec/step, loss=0.07532, avg_loss=0.07428]\n",
      "Step 515179  [5.363 sec/step, loss=0.07718, avg_loss=0.07430]\n",
      "Step 515180  [5.363 sec/step, loss=0.07696, avg_loss=0.07430]\n",
      "Step 515181  [5.370 sec/step, loss=0.07534, avg_loss=0.07431]\n",
      "Step 515182  [5.369 sec/step, loss=0.07718, avg_loss=0.07435]\n",
      "Step 515183  [5.361 sec/step, loss=0.07474, avg_loss=0.07437]\n",
      "Step 515184  [5.375 sec/step, loss=0.07626, avg_loss=0.07441]\n",
      "Step 515185  [5.354 sec/step, loss=0.07628, avg_loss=0.07440]\n",
      "Generated 32 batches of size 32 in 2.396 sec\n",
      "Step 515186  [5.373 sec/step, loss=0.07696, avg_loss=0.07444]\n",
      "Step 515187  [5.373 sec/step, loss=0.06582, avg_loss=0.07443]\n",
      "Step 515188  [5.362 sec/step, loss=0.07464, avg_loss=0.07442]\n",
      "Step 515189  [5.367 sec/step, loss=0.07627, avg_loss=0.07441]\n",
      "Step 515190  [5.378 sec/step, loss=0.07480, avg_loss=0.07440]\n",
      "Step 515191  [5.383 sec/step, loss=0.07522, avg_loss=0.07442]\n",
      "Step 515192  [5.394 sec/step, loss=0.07314, avg_loss=0.07440]\n",
      "Step 515193  [5.405 sec/step, loss=0.07710, avg_loss=0.07445]\n",
      "Step 515194  [5.400 sec/step, loss=0.07621, avg_loss=0.07444]\n",
      "Step 515195  [5.434 sec/step, loss=0.07393, avg_loss=0.07444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 515196  [5.452 sec/step, loss=0.07638, avg_loss=0.07445]\n",
      "Step 515197  [5.436 sec/step, loss=0.07401, avg_loss=0.07442]\n",
      "Step 515198  [5.431 sec/step, loss=0.07500, avg_loss=0.07441]\n",
      "Step 515199  [5.431 sec/step, loss=0.06836, avg_loss=0.07442]\n",
      "Step 515200  [5.444 sec/step, loss=0.07515, avg_loss=0.07446]\n",
      "Writing summary at step: 515200\n",
      "Step 515201  [5.424 sec/step, loss=0.07178, avg_loss=0.07440]\n",
      "Step 515202  [5.428 sec/step, loss=0.07571, avg_loss=0.07441]\n",
      "Step 515203  [5.428 sec/step, loss=0.07572, avg_loss=0.07441]\n",
      "Step 515204  [5.475 sec/step, loss=0.06726, avg_loss=0.07435]\n",
      "Step 515205  [5.436 sec/step, loss=0.07739, avg_loss=0.07446]\n",
      "Step 515206  [5.439 sec/step, loss=0.07678, avg_loss=0.07447]\n",
      "Step 515207  [5.447 sec/step, loss=0.07691, avg_loss=0.07449]\n",
      "Step 515208  [5.461 sec/step, loss=0.07542, avg_loss=0.07448]\n",
      "Step 515209  [5.453 sec/step, loss=0.07157, avg_loss=0.07444]\n",
      "Step 515210  [5.441 sec/step, loss=0.07648, avg_loss=0.07447]\n",
      "Step 515211  [5.430 sec/step, loss=0.07613, avg_loss=0.07446]\n",
      "Step 515212  [5.440 sec/step, loss=0.07671, avg_loss=0.07447]\n",
      "Step 515213  [5.409 sec/step, loss=0.07627, avg_loss=0.07449]\n",
      "Step 515214  [5.425 sec/step, loss=0.07495, avg_loss=0.07457]\n",
      "Step 515215  [5.439 sec/step, loss=0.07529, avg_loss=0.07460]\n",
      "Step 515216  [5.438 sec/step, loss=0.07433, avg_loss=0.07459]\n",
      "Generated 32 batches of size 32 in 2.394 sec\n",
      "Step 515217  [5.451 sec/step, loss=0.07659, avg_loss=0.07461]\n",
      "Step 515218  [5.476 sec/step, loss=0.07578, avg_loss=0.07464]\n",
      "Step 515219  [5.479 sec/step, loss=0.07529, avg_loss=0.07464]\n",
      "Step 515220  [5.480 sec/step, loss=0.07616, avg_loss=0.07466]\n",
      "Step 515221  [5.460 sec/step, loss=0.07502, avg_loss=0.07464]\n",
      "Step 515222  [5.441 sec/step, loss=0.07182, avg_loss=0.07461]\n",
      "Step 515223  [5.435 sec/step, loss=0.07352, avg_loss=0.07463]\n",
      "Step 515224  [5.428 sec/step, loss=0.07339, avg_loss=0.07461]\n",
      "Step 515225  [5.420 sec/step, loss=0.07635, avg_loss=0.07463]\n",
      "Step 515226  [5.425 sec/step, loss=0.07637, avg_loss=0.07465]\n",
      "Step 515227  [5.438 sec/step, loss=0.07741, avg_loss=0.07467]\n",
      "Step 515228  [5.443 sec/step, loss=0.07380, avg_loss=0.07465]\n",
      "Step 515229  [5.429 sec/step, loss=0.07333, avg_loss=0.07462]\n",
      "Step 515230  [5.415 sec/step, loss=0.06600, avg_loss=0.07453]\n",
      "Step 515231  [5.423 sec/step, loss=0.07450, avg_loss=0.07451]\n",
      "Step 515232  [5.424 sec/step, loss=0.07464, avg_loss=0.07453]\n",
      "Step 515233  [5.435 sec/step, loss=0.07585, avg_loss=0.07457]\n",
      "Step 515234  [5.479 sec/step, loss=0.06659, avg_loss=0.07446]\n",
      "Step 515235  [5.492 sec/step, loss=0.07689, avg_loss=0.07450]\n",
      "Step 515236  [5.486 sec/step, loss=0.07641, avg_loss=0.07451]\n",
      "Step 515237  [5.468 sec/step, loss=0.07198, avg_loss=0.07449]\n",
      "Step 515238  [5.445 sec/step, loss=0.07449, avg_loss=0.07449]\n",
      "Step 515239  [5.464 sec/step, loss=0.07569, avg_loss=0.07452]\n",
      "Step 515240  [5.462 sec/step, loss=0.07587, avg_loss=0.07451]\n",
      "Step 515241  [5.459 sec/step, loss=0.07590, avg_loss=0.07451]\n",
      "Step 515242  [5.484 sec/step, loss=0.07400, avg_loss=0.07450]\n",
      "Step 515243  [5.484 sec/step, loss=0.07606, avg_loss=0.07448]\n",
      "Step 515244  [5.433 sec/step, loss=0.07605, avg_loss=0.07457]\n",
      "Step 515245  [5.433 sec/step, loss=0.07498, avg_loss=0.07455]\n",
      "Step 515246  [5.411 sec/step, loss=0.07727, avg_loss=0.07457]\n",
      "Step 515247  [5.395 sec/step, loss=0.07333, avg_loss=0.07453]\n",
      "Step 515248  [5.377 sec/step, loss=0.07538, avg_loss=0.07454]\n",
      "Generated 32 batches of size 32 in 2.596 sec\n",
      "Step 515249  [5.382 sec/step, loss=0.07245, avg_loss=0.07452]\n",
      "Step 515250  [5.374 sec/step, loss=0.07563, avg_loss=0.07453]\n",
      "Step 515251  [5.373 sec/step, loss=0.07672, avg_loss=0.07455]\n",
      "Step 515252  [5.387 sec/step, loss=0.07494, avg_loss=0.07455]\n",
      "Step 515253  [5.396 sec/step, loss=0.07737, avg_loss=0.07458]\n",
      "Step 515254  [5.394 sec/step, loss=0.07135, avg_loss=0.07459]\n",
      "Step 515255  [5.407 sec/step, loss=0.07599, avg_loss=0.07460]\n",
      "Step 515256  [5.397 sec/step, loss=0.07188, avg_loss=0.07457]\n",
      "Step 515257  [5.404 sec/step, loss=0.07509, avg_loss=0.07457]\n",
      "Step 515258  [5.379 sec/step, loss=0.07205, avg_loss=0.07452]\n",
      "Step 515259  [5.389 sec/step, loss=0.07701, avg_loss=0.07453]\n",
      "Step 515260  [5.409 sec/step, loss=0.07621, avg_loss=0.07464]\n",
      "Step 515261  [5.423 sec/step, loss=0.07683, avg_loss=0.07465]\n",
      "Step 515262  [5.425 sec/step, loss=0.07562, avg_loss=0.07464]\n",
      "Step 515263  [5.427 sec/step, loss=0.07463, avg_loss=0.07462]\n",
      "Step 515264  [5.421 sec/step, loss=0.07444, avg_loss=0.07461]\n",
      "Step 515265  [5.423 sec/step, loss=0.07670, avg_loss=0.07465]\n",
      "Step 515266  [5.398 sec/step, loss=0.07427, avg_loss=0.07464]\n",
      "Step 515267  [5.420 sec/step, loss=0.07650, avg_loss=0.07468]\n",
      "Step 515268  [5.435 sec/step, loss=0.07551, avg_loss=0.07471]\n",
      "Step 515269  [5.429 sec/step, loss=0.07126, avg_loss=0.07467]\n",
      "Step 515270  [5.446 sec/step, loss=0.07668, avg_loss=0.07469]\n",
      "Step 515271  [5.436 sec/step, loss=0.07332, avg_loss=0.07466]\n",
      "Step 515272  [5.452 sec/step, loss=0.07504, avg_loss=0.07475]\n",
      "Step 515273  [5.479 sec/step, loss=0.07675, avg_loss=0.07480]\n",
      "Step 515274  [5.498 sec/step, loss=0.07364, avg_loss=0.07477]\n",
      "Step 515275  [5.512 sec/step, loss=0.07621, avg_loss=0.07478]\n",
      "Step 515276  [5.509 sec/step, loss=0.07606, avg_loss=0.07481]\n",
      "Step 515277  [5.514 sec/step, loss=0.07578, avg_loss=0.07484]\n",
      "Step 515278  [5.492 sec/step, loss=0.07486, avg_loss=0.07483]\n",
      "Step 515279  [5.493 sec/step, loss=0.07436, avg_loss=0.07480]\n",
      "Step 515280  [5.486 sec/step, loss=0.07537, avg_loss=0.07479]\n",
      "Generated 32 batches of size 32 in 2.482 sec\n",
      "Step 515281  [5.495 sec/step, loss=0.07567, avg_loss=0.07479]\n",
      "Step 515282  [5.478 sec/step, loss=0.07531, avg_loss=0.07477]\n",
      "Step 515283  [5.480 sec/step, loss=0.07623, avg_loss=0.07479]\n",
      "Step 515284  [5.469 sec/step, loss=0.07524, avg_loss=0.07478]\n",
      "Step 515285  [5.486 sec/step, loss=0.07478, avg_loss=0.07476]\n",
      "Step 515286  [5.454 sec/step, loss=0.06716, avg_loss=0.07466]\n",
      "Step 515287  [5.390 sec/step, loss=0.07310, avg_loss=0.07474]\n",
      "Step 515288  [5.409 sec/step, loss=0.07621, avg_loss=0.07475]\n",
      "Step 515289  [5.444 sec/step, loss=0.06615, avg_loss=0.07465]\n",
      "Step 515290  [5.445 sec/step, loss=0.07653, avg_loss=0.07467]\n",
      "Step 515291  [5.456 sec/step, loss=0.07543, avg_loss=0.07467]\n",
      "Step 515292  [5.446 sec/step, loss=0.07397, avg_loss=0.07468]\n",
      "Step 515293  [5.426 sec/step, loss=0.07185, avg_loss=0.07463]\n",
      "Step 515294  [5.423 sec/step, loss=0.07569, avg_loss=0.07462]\n",
      "Step 515295  [5.399 sec/step, loss=0.07638, avg_loss=0.07464]\n",
      "Step 515296  [5.396 sec/step, loss=0.07649, avg_loss=0.07465]\n",
      "Step 515297  [5.409 sec/step, loss=0.07675, avg_loss=0.07467]\n",
      "Step 515298  [5.425 sec/step, loss=0.07538, avg_loss=0.07468]\n",
      "Step 515299  [5.434 sec/step, loss=0.07418, avg_loss=0.07474]\n",
      "Step 515300  [5.441 sec/step, loss=0.07685, avg_loss=0.07475]\n",
      "Writing summary at step: 515300\n",
      "Step 515301  [5.437 sec/step, loss=0.07262, avg_loss=0.07476]\n",
      "Step 515302  [5.422 sec/step, loss=0.07344, avg_loss=0.07474]\n",
      "Step 515303  [5.414 sec/step, loss=0.07448, avg_loss=0.07473]\n",
      "Step 515304  [5.362 sec/step, loss=0.07623, avg_loss=0.07482]\n",
      "Step 515305  [5.379 sec/step, loss=0.07385, avg_loss=0.07478]\n",
      "Step 515306  [5.362 sec/step, loss=0.07453, avg_loss=0.07476]\n",
      "Step 515307  [5.382 sec/step, loss=0.07338, avg_loss=0.07472]\n",
      "Step 515308  [5.380 sec/step, loss=0.07709, avg_loss=0.07474]\n",
      "Step 515309  [5.382 sec/step, loss=0.07511, avg_loss=0.07477]\n",
      "Step 515310  [5.368 sec/step, loss=0.07495, avg_loss=0.07476]\n",
      "Step 515311  [5.366 sec/step, loss=0.07546, avg_loss=0.07475]\n",
      "Generated 32 batches of size 32 in 2.424 sec\n",
      "Step 515312  [5.368 sec/step, loss=0.07542, avg_loss=0.07474]\n",
      "Step 515313  [5.355 sec/step, loss=0.06610, avg_loss=0.07464]\n",
      "Step 515314  [5.353 sec/step, loss=0.07554, avg_loss=0.07464]\n",
      "Step 515315  [5.360 sec/step, loss=0.07734, avg_loss=0.07466]\n",
      "Step 515316  [5.363 sec/step, loss=0.07261, avg_loss=0.07465]\n",
      "Step 515317  [5.350 sec/step, loss=0.07661, avg_loss=0.07465]\n",
      "Step 515318  [5.341 sec/step, loss=0.07621, avg_loss=0.07465]\n",
      "Step 515319  [5.383 sec/step, loss=0.06797, avg_loss=0.07458]\n",
      "Step 515320  [5.398 sec/step, loss=0.07697, avg_loss=0.07459]\n",
      "Step 515321  [5.404 sec/step, loss=0.07416, avg_loss=0.07458]\n",
      "Step 515322  [5.419 sec/step, loss=0.07572, avg_loss=0.07462]\n",
      "Step 515323  [5.451 sec/step, loss=0.07444, avg_loss=0.07463]\n",
      "Step 515324  [5.463 sec/step, loss=0.07673, avg_loss=0.07466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 515325  [5.459 sec/step, loss=0.07444, avg_loss=0.07464]\n",
      "Step 515326  [5.473 sec/step, loss=0.07654, avg_loss=0.07464]\n",
      "Step 515327  [5.452 sec/step, loss=0.07411, avg_loss=0.07461]\n",
      "Step 515328  [5.435 sec/step, loss=0.07590, avg_loss=0.07463]\n",
      "Step 515329  [5.438 sec/step, loss=0.07550, avg_loss=0.07465]\n",
      "Step 515330  [5.463 sec/step, loss=0.07539, avg_loss=0.07475]\n",
      "Step 515331  [5.452 sec/step, loss=0.07199, avg_loss=0.07472]\n",
      "Step 515332  [5.510 sec/step, loss=0.06671, avg_loss=0.07464]\n",
      "Step 515333  [5.510 sec/step, loss=0.07553, avg_loss=0.07464]\n",
      "Step 515334  [5.458 sec/step, loss=0.07161, avg_loss=0.07469]\n",
      "Step 515335  [5.445 sec/step, loss=0.07398, avg_loss=0.07466]\n",
      "Step 515336  [5.428 sec/step, loss=0.07211, avg_loss=0.07462]\n",
      "Step 515337  [5.444 sec/step, loss=0.07644, avg_loss=0.07466]\n",
      "Step 515338  [5.453 sec/step, loss=0.07554, avg_loss=0.07467]\n",
      "Step 515339  [5.460 sec/step, loss=0.07466, avg_loss=0.07466]\n",
      "Step 515340  [5.444 sec/step, loss=0.07554, avg_loss=0.07466]\n",
      "Step 515341  [5.472 sec/step, loss=0.07397, avg_loss=0.07464]\n",
      "Step 515342  [5.428 sec/step, loss=0.06703, avg_loss=0.07457]\n",
      "Step 515343  [5.413 sec/step, loss=0.07582, avg_loss=0.07457]\n",
      "Generated 32 batches of size 32 in 2.434 sec\n",
      "Step 515344  [5.428 sec/step, loss=0.07702, avg_loss=0.07458]\n",
      "Step 515345  [5.417 sec/step, loss=0.07199, avg_loss=0.07455]\n",
      "Step 515346  [5.425 sec/step, loss=0.07492, avg_loss=0.07452]\n",
      "Step 515347  [5.438 sec/step, loss=0.07570, avg_loss=0.07455]\n",
      "Step 515348  [5.443 sec/step, loss=0.07612, avg_loss=0.07455]\n",
      "Step 515349  [5.452 sec/step, loss=0.07696, avg_loss=0.07460]\n",
      "Step 515350  [5.449 sec/step, loss=0.07615, avg_loss=0.07460]\n",
      "Step 515351  [5.433 sec/step, loss=0.07477, avg_loss=0.07458]\n",
      "Step 515352  [5.436 sec/step, loss=0.07685, avg_loss=0.07460]\n",
      "Step 515353  [5.417 sec/step, loss=0.07399, avg_loss=0.07457]\n",
      "Step 515354  [5.422 sec/step, loss=0.07324, avg_loss=0.07459]\n",
      "Step 515355  [5.424 sec/step, loss=0.07590, avg_loss=0.07459]\n",
      "Step 515356  [5.443 sec/step, loss=0.07688, avg_loss=0.07464]\n",
      "Step 515357  [5.492 sec/step, loss=0.06756, avg_loss=0.07456]\n",
      "Step 515358  [5.503 sec/step, loss=0.07455, avg_loss=0.07459]\n",
      "Step 515359  [5.496 sec/step, loss=0.07585, avg_loss=0.07458]\n",
      "Step 515360  [5.497 sec/step, loss=0.07574, avg_loss=0.07457]\n",
      "Step 515361  [5.471 sec/step, loss=0.07236, avg_loss=0.07453]\n",
      "Step 515362  [5.464 sec/step, loss=0.07600, avg_loss=0.07453]\n",
      "Step 515363  [5.478 sec/step, loss=0.07688, avg_loss=0.07455]\n",
      "Step 515364  [5.482 sec/step, loss=0.07477, avg_loss=0.07456]\n",
      "Step 515365  [5.478 sec/step, loss=0.07611, avg_loss=0.07455]\n",
      "Step 515366  [5.499 sec/step, loss=0.07473, avg_loss=0.07456]\n",
      "Step 515367  [5.465 sec/step, loss=0.06633, avg_loss=0.07445]\n",
      "Step 515368  [5.448 sec/step, loss=0.07112, avg_loss=0.07441]\n",
      "Step 515369  [5.444 sec/step, loss=0.07425, avg_loss=0.07444]\n",
      "Step 515370  [5.439 sec/step, loss=0.07285, avg_loss=0.07440]\n",
      "Step 515371  [5.469 sec/step, loss=0.07334, avg_loss=0.07440]\n",
      "Step 515372  [5.483 sec/step, loss=0.07773, avg_loss=0.07443]\n",
      "Step 515373  [5.478 sec/step, loss=0.07640, avg_loss=0.07442]\n",
      "Step 515374  [5.456 sec/step, loss=0.07579, avg_loss=0.07445]\n",
      "Step 515375  [5.449 sec/step, loss=0.07570, avg_loss=0.07444]\n",
      "Generated 32 batches of size 32 in 2.439 sec\n",
      "Step 515376  [5.467 sec/step, loss=0.07722, avg_loss=0.07445]\n",
      "Step 515377  [5.457 sec/step, loss=0.07476, avg_loss=0.07444]\n",
      "Step 515378  [5.468 sec/step, loss=0.07523, avg_loss=0.07445]\n",
      "Step 515379  [5.447 sec/step, loss=0.07524, avg_loss=0.07446]\n",
      "Step 515380  [5.438 sec/step, loss=0.07317, avg_loss=0.07443]\n",
      "Step 515381  [5.448 sec/step, loss=0.07621, avg_loss=0.07444]\n",
      "Step 515382  [5.463 sec/step, loss=0.07676, avg_loss=0.07445]\n",
      "Step 515383  [5.470 sec/step, loss=0.07536, avg_loss=0.07444]\n",
      "Step 515384  [5.478 sec/step, loss=0.07609, avg_loss=0.07445]\n",
      "Step 515385  [5.472 sec/step, loss=0.07546, avg_loss=0.07446]\n",
      "Step 515386  [5.487 sec/step, loss=0.07615, avg_loss=0.07455]\n",
      "Step 515387  [5.509 sec/step, loss=0.07720, avg_loss=0.07459]\n",
      "Step 515388  [5.510 sec/step, loss=0.07658, avg_loss=0.07459]\n",
      "Step 515389  [5.475 sec/step, loss=0.07637, avg_loss=0.07470]\n",
      "Step 515390  [5.511 sec/step, loss=0.06605, avg_loss=0.07459]\n",
      "Step 515391  [5.510 sec/step, loss=0.07599, avg_loss=0.07460]\n",
      "Step 515392  [5.535 sec/step, loss=0.07579, avg_loss=0.07462]\n",
      "Step 515393  [5.550 sec/step, loss=0.07552, avg_loss=0.07465]\n",
      "Step 515394  [5.537 sec/step, loss=0.07194, avg_loss=0.07461]\n",
      "Step 515395  [5.532 sec/step, loss=0.07593, avg_loss=0.07461]\n",
      "Step 515396  [5.517 sec/step, loss=0.07134, avg_loss=0.07456]\n",
      "Step 515397  [5.505 sec/step, loss=0.07402, avg_loss=0.07453]\n",
      "Step 515398  [5.489 sec/step, loss=0.07493, avg_loss=0.07453]\n",
      "Step 515399  [5.501 sec/step, loss=0.07515, avg_loss=0.07454]\n",
      "Step 515400  [5.501 sec/step, loss=0.07354, avg_loss=0.07450]\n",
      "Writing summary at step: 515400\n",
      "Step 515401  [5.505 sec/step, loss=0.07456, avg_loss=0.07452]\n",
      "Step 515402  [5.522 sec/step, loss=0.07663, avg_loss=0.07455]\n",
      "Step 515403  [5.521 sec/step, loss=0.07456, avg_loss=0.07456]\n",
      "Step 515404  [5.524 sec/step, loss=0.07417, avg_loss=0.07454]\n",
      "Step 515405  [5.496 sec/step, loss=0.07514, avg_loss=0.07455]\n",
      "Step 515406  [5.511 sec/step, loss=0.07543, avg_loss=0.07456]\n",
      "Generated 32 batches of size 32 in 2.472 sec\n",
      "Step 515407  [5.499 sec/step, loss=0.07649, avg_loss=0.07459]\n",
      "Step 515408  [5.483 sec/step, loss=0.07463, avg_loss=0.07456]\n",
      "Step 515409  [5.491 sec/step, loss=0.07554, avg_loss=0.07457]\n",
      "Step 515410  [5.510 sec/step, loss=0.07691, avg_loss=0.07459]\n",
      "Step 515411  [5.496 sec/step, loss=0.07365, avg_loss=0.07457]\n",
      "Step 515412  [5.489 sec/step, loss=0.07520, avg_loss=0.07457]\n",
      "Step 515413  [5.516 sec/step, loss=0.07651, avg_loss=0.07467]\n",
      "Step 515414  [5.546 sec/step, loss=0.07362, avg_loss=0.07465]\n",
      "Step 515415  [5.522 sec/step, loss=0.06640, avg_loss=0.07454]\n",
      "Step 515416  [5.532 sec/step, loss=0.07444, avg_loss=0.07456]\n",
      "Step 515417  [5.539 sec/step, loss=0.07606, avg_loss=0.07456]\n",
      "Step 515418  [5.536 sec/step, loss=0.07644, avg_loss=0.07456]\n",
      "Step 515419  [5.499 sec/step, loss=0.07662, avg_loss=0.07464]\n",
      "Step 515420  [5.490 sec/step, loss=0.07428, avg_loss=0.07462]\n",
      "Step 515421  [5.480 sec/step, loss=0.07145, avg_loss=0.07459]\n",
      "Step 515422  [5.485 sec/step, loss=0.07568, avg_loss=0.07459]\n",
      "Step 515423  [5.473 sec/step, loss=0.07684, avg_loss=0.07461]\n",
      "Step 515424  [5.489 sec/step, loss=0.07428, avg_loss=0.07459]\n",
      "Step 515425  [5.500 sec/step, loss=0.07728, avg_loss=0.07462]\n",
      "Step 515426  [5.489 sec/step, loss=0.07574, avg_loss=0.07461]\n",
      "Step 515427  [5.502 sec/step, loss=0.07688, avg_loss=0.07464]\n",
      "Step 515428  [5.502 sec/step, loss=0.07519, avg_loss=0.07463]\n",
      "Step 515429  [5.495 sec/step, loss=0.07378, avg_loss=0.07461]\n",
      "Step 515430  [5.498 sec/step, loss=0.07319, avg_loss=0.07459]\n",
      "Step 515431  [5.548 sec/step, loss=0.06708, avg_loss=0.07454]\n",
      "Step 515432  [5.495 sec/step, loss=0.07427, avg_loss=0.07462]\n",
      "Step 515433  [5.479 sec/step, loss=0.07258, avg_loss=0.07459]\n",
      "Step 515434  [5.493 sec/step, loss=0.07467, avg_loss=0.07462]\n",
      "Step 515435  [5.483 sec/step, loss=0.07497, avg_loss=0.07463]\n",
      "Step 515436  [5.479 sec/step, loss=0.06607, avg_loss=0.07457]\n",
      "Step 515437  [5.473 sec/step, loss=0.07623, avg_loss=0.07457]\n",
      "Step 515438  [5.478 sec/step, loss=0.07697, avg_loss=0.07458]\n",
      "Generated 32 batches of size 32 in 2.423 sec\n",
      "Step 515439  [5.484 sec/step, loss=0.07739, avg_loss=0.07461]\n",
      "Step 515440  [5.487 sec/step, loss=0.07466, avg_loss=0.07460]\n",
      "Step 515441  [5.466 sec/step, loss=0.07578, avg_loss=0.07462]\n",
      "Step 515442  [5.483 sec/step, loss=0.07244, avg_loss=0.07467]\n",
      "Step 515443  [5.490 sec/step, loss=0.07664, avg_loss=0.07468]\n",
      "Step 515444  [5.470 sec/step, loss=0.07456, avg_loss=0.07465]\n",
      "Step 515445  [5.510 sec/step, loss=0.07426, avg_loss=0.07468]\n",
      "Step 515446  [5.496 sec/step, loss=0.07545, avg_loss=0.07468]\n",
      "Step 515447  [5.485 sec/step, loss=0.07377, avg_loss=0.07466]\n",
      "Step 515448  [5.489 sec/step, loss=0.07362, avg_loss=0.07464]\n",
      "Step 515449  [5.464 sec/step, loss=0.07253, avg_loss=0.07459]\n",
      "Step 515450  [5.461 sec/step, loss=0.07299, avg_loss=0.07456]\n",
      "Step 515451  [5.469 sec/step, loss=0.07626, avg_loss=0.07458]\n",
      "Step 515452  [5.445 sec/step, loss=0.07091, avg_loss=0.07452]\n",
      "Step 515453  [5.477 sec/step, loss=0.07564, avg_loss=0.07453]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 515454  [5.478 sec/step, loss=0.07634, avg_loss=0.07457]\n",
      "Step 515455  [5.524 sec/step, loss=0.06643, avg_loss=0.07447]\n",
      "Step 515456  [5.531 sec/step, loss=0.07563, avg_loss=0.07446]\n",
      "Step 515457  [5.492 sec/step, loss=0.07703, avg_loss=0.07455]\n",
      "Step 515458  [5.483 sec/step, loss=0.07311, avg_loss=0.07454]\n",
      "Step 515459  [5.492 sec/step, loss=0.07714, avg_loss=0.07455]\n",
      "Step 515460  [5.491 sec/step, loss=0.07637, avg_loss=0.07456]\n",
      "Step 515461  [5.517 sec/step, loss=0.07388, avg_loss=0.07457]\n",
      "Step 515462  [5.528 sec/step, loss=0.07751, avg_loss=0.07459]\n",
      "Step 515463  [5.522 sec/step, loss=0.07480, avg_loss=0.07457]\n",
      "Step 515464  [5.541 sec/step, loss=0.07601, avg_loss=0.07458]\n",
      "Step 515465  [5.544 sec/step, loss=0.07567, avg_loss=0.07458]\n",
      "Step 515466  [5.529 sec/step, loss=0.07071, avg_loss=0.07453]\n",
      "Step 515467  [5.538 sec/step, loss=0.07469, avg_loss=0.07462]\n",
      "Step 515468  [5.551 sec/step, loss=0.07510, avg_loss=0.07466]\n",
      "Step 515469  [5.570 sec/step, loss=0.07710, avg_loss=0.07469]\n",
      "Step 515470  [5.563 sec/step, loss=0.07613, avg_loss=0.07472]\n",
      "Generated 32 batches of size 32 in 2.543 sec\n",
      "Step 515471  [5.536 sec/step, loss=0.07474, avg_loss=0.07473]\n",
      "Step 515472  [5.527 sec/step, loss=0.07347, avg_loss=0.07469]\n",
      "Step 515473  [5.513 sec/step, loss=0.07203, avg_loss=0.07465]\n",
      "Step 515474  [5.516 sec/step, loss=0.07700, avg_loss=0.07466]\n",
      "Step 515475  [5.490 sec/step, loss=0.06656, avg_loss=0.07457]\n",
      "Step 515476  [5.481 sec/step, loss=0.07601, avg_loss=0.07456]\n",
      "Step 515477  [5.489 sec/step, loss=0.07580, avg_loss=0.07457]\n",
      "Step 515478  [5.487 sec/step, loss=0.07502, avg_loss=0.07456]\n",
      "Step 515479  [5.487 sec/step, loss=0.07511, avg_loss=0.07456]\n",
      "Step 515480  [5.476 sec/step, loss=0.07280, avg_loss=0.07456]\n",
      "Step 515481  [5.460 sec/step, loss=0.07544, avg_loss=0.07455]\n",
      "Step 515482  [5.447 sec/step, loss=0.07424, avg_loss=0.07453]\n",
      "Step 515483  [5.432 sec/step, loss=0.07254, avg_loss=0.07450]\n",
      "Step 515484  [5.434 sec/step, loss=0.07517, avg_loss=0.07449]\n",
      "Step 515485  [5.420 sec/step, loss=0.07233, avg_loss=0.07446]\n",
      "Step 515486  [5.406 sec/step, loss=0.06661, avg_loss=0.07436]\n",
      "Step 515487  [5.448 sec/step, loss=0.06806, avg_loss=0.07427]\n",
      "Step 515488  [5.438 sec/step, loss=0.07623, avg_loss=0.07427]\n",
      "Step 515489  [5.439 sec/step, loss=0.07456, avg_loss=0.07425]\n",
      "Step 515490  [5.398 sec/step, loss=0.07473, avg_loss=0.07434]\n",
      "Step 515491  [5.387 sec/step, loss=0.07086, avg_loss=0.07428]\n",
      "Step 515492  [5.371 sec/step, loss=0.07582, avg_loss=0.07428]\n",
      "Step 515493  [5.372 sec/step, loss=0.07583, avg_loss=0.07429]\n",
      "Step 515494  [5.386 sec/step, loss=0.07318, avg_loss=0.07430]\n",
      "Step 515495  [5.390 sec/step, loss=0.07350, avg_loss=0.07428]\n",
      "Step 515496  [5.400 sec/step, loss=0.07595, avg_loss=0.07432]\n",
      "Step 515497  [5.410 sec/step, loss=0.07657, avg_loss=0.07435]\n",
      "Step 515498  [5.431 sec/step, loss=0.07715, avg_loss=0.07437]\n",
      "Step 515499  [5.424 sec/step, loss=0.07274, avg_loss=0.07435]\n",
      "Step 515500  [5.440 sec/step, loss=0.07472, avg_loss=0.07436]\n",
      "Writing summary at step: 515500\n",
      "Step 515501  [5.463 sec/step, loss=0.07486, avg_loss=0.07436]\n",
      "Generated 32 batches of size 32 in 2.435 sec\n",
      "Step 515502  [5.459 sec/step, loss=0.07601, avg_loss=0.07435]\n",
      "Step 515503  [5.475 sec/step, loss=0.07676, avg_loss=0.07438]\n",
      "Step 515504  [5.500 sec/step, loss=0.07516, avg_loss=0.07439]\n",
      "Step 515505  [5.493 sec/step, loss=0.07514, avg_loss=0.07439]\n",
      "Step 515506  [5.492 sec/step, loss=0.07616, avg_loss=0.07439]\n",
      "Step 515507  [5.480 sec/step, loss=0.07616, avg_loss=0.07439]\n",
      "Step 515508  [5.497 sec/step, loss=0.07683, avg_loss=0.07441]\n",
      "Step 515509  [5.498 sec/step, loss=0.07681, avg_loss=0.07442]\n",
      "Step 515510  [5.483 sec/step, loss=0.07483, avg_loss=0.07440]\n",
      "Step 515511  [5.500 sec/step, loss=0.07590, avg_loss=0.07443]\n",
      "Step 515512  [5.549 sec/step, loss=0.06591, avg_loss=0.07433]\n",
      "Step 515513  [5.541 sec/step, loss=0.07305, avg_loss=0.07430]\n",
      "Step 515514  [5.507 sec/step, loss=0.07424, avg_loss=0.07431]\n",
      "Step 515515  [5.524 sec/step, loss=0.07519, avg_loss=0.07439]\n",
      "Step 515516  [5.510 sec/step, loss=0.07251, avg_loss=0.07437]\n",
      "Step 515517  [5.490 sec/step, loss=0.07229, avg_loss=0.07434]\n",
      "Step 515518  [5.472 sec/step, loss=0.06716, avg_loss=0.07424]\n",
      "Step 515519  [5.468 sec/step, loss=0.07708, avg_loss=0.07425]\n",
      "Step 515520  [5.455 sec/step, loss=0.07468, avg_loss=0.07425]\n",
      "Step 515521  [5.455 sec/step, loss=0.07305, avg_loss=0.07427]\n",
      "Step 515522  [5.448 sec/step, loss=0.07353, avg_loss=0.07425]\n",
      "Step 515523  [5.449 sec/step, loss=0.07717, avg_loss=0.07425]\n",
      "Step 515524  [5.421 sec/step, loss=0.07454, avg_loss=0.07425]\n",
      "Step 515525  [5.410 sec/step, loss=0.07338, avg_loss=0.07421]\n",
      "Step 515526  [5.430 sec/step, loss=0.07602, avg_loss=0.07422]\n",
      "Step 515527  [5.423 sec/step, loss=0.07581, avg_loss=0.07421]\n",
      "Step 515528  [5.432 sec/step, loss=0.07532, avg_loss=0.07421]\n",
      "Step 515529  [5.437 sec/step, loss=0.07610, avg_loss=0.07423]\n",
      "Step 515530  [5.436 sec/step, loss=0.07676, avg_loss=0.07427]\n",
      "Step 515531  [5.395 sec/step, loss=0.07464, avg_loss=0.07434]\n",
      "Step 515532  [5.402 sec/step, loss=0.07572, avg_loss=0.07436]\n",
      "Step 515533  [5.428 sec/step, loss=0.07682, avg_loss=0.07440]\n",
      "Generated 32 batches of size 32 in 2.562 sec\n",
      "Step 515534  [5.413 sec/step, loss=0.07517, avg_loss=0.07440]\n",
      "Step 515535  [5.425 sec/step, loss=0.07647, avg_loss=0.07442]\n",
      "Step 515536  [5.457 sec/step, loss=0.07686, avg_loss=0.07453]\n",
      "Step 515537  [5.466 sec/step, loss=0.07535, avg_loss=0.07452]\n",
      "Step 515538  [5.474 sec/step, loss=0.07590, avg_loss=0.07451]\n",
      "Step 515539  [5.465 sec/step, loss=0.07683, avg_loss=0.07450]\n",
      "Step 515540  [5.470 sec/step, loss=0.07563, avg_loss=0.07451]\n",
      "Step 515541  [5.480 sec/step, loss=0.07482, avg_loss=0.07450]\n",
      "Step 515542  [5.484 sec/step, loss=0.07571, avg_loss=0.07453]\n",
      "Step 515543  [5.532 sec/step, loss=0.06776, avg_loss=0.07444]\n",
      "Step 515544  [5.547 sec/step, loss=0.07489, avg_loss=0.07445]\n",
      "Step 515545  [5.504 sec/step, loss=0.07278, avg_loss=0.07443]\n",
      "Step 515546  [5.506 sec/step, loss=0.07646, avg_loss=0.07444]\n",
      "Step 515547  [5.523 sec/step, loss=0.07623, avg_loss=0.07447]\n",
      "Step 515548  [5.534 sec/step, loss=0.07648, avg_loss=0.07450]\n",
      "Step 515549  [5.566 sec/step, loss=0.07534, avg_loss=0.07452]\n",
      "Step 515550  [5.563 sec/step, loss=0.07376, avg_loss=0.07453]\n",
      "Step 515551  [5.545 sec/step, loss=0.06794, avg_loss=0.07445]\n",
      "Step 515552  [5.558 sec/step, loss=0.07496, avg_loss=0.07449]\n",
      "Step 515553  [5.531 sec/step, loss=0.07659, avg_loss=0.07450]\n",
      "Step 515554  [5.524 sec/step, loss=0.07549, avg_loss=0.07449]\n",
      "Step 515555  [5.465 sec/step, loss=0.07465, avg_loss=0.07457]\n",
      "Step 515556  [5.449 sec/step, loss=0.07507, avg_loss=0.07457]\n",
      "Step 515557  [5.451 sec/step, loss=0.07695, avg_loss=0.07457]\n",
      "Step 515558  [5.470 sec/step, loss=0.07670, avg_loss=0.07460]\n",
      "Step 515559  [5.465 sec/step, loss=0.07568, avg_loss=0.07459]\n",
      "Step 515560  [5.464 sec/step, loss=0.07651, avg_loss=0.07459]\n",
      "Step 515561  [5.444 sec/step, loss=0.07436, avg_loss=0.07459]\n",
      "Step 515562  [5.438 sec/step, loss=0.07648, avg_loss=0.07458]\n",
      "Step 515563  [5.457 sec/step, loss=0.07397, avg_loss=0.07458]\n",
      "Step 515564  [5.444 sec/step, loss=0.07747, avg_loss=0.07459]\n",
      "Step 515565  [5.437 sec/step, loss=0.07591, avg_loss=0.07459]\n",
      "Generated 32 batches of size 32 in 2.398 sec\n",
      "Step 515566  [5.452 sec/step, loss=0.07722, avg_loss=0.07466]\n",
      "Step 515567  [5.470 sec/step, loss=0.07401, avg_loss=0.07465]\n",
      "Step 515568  [5.466 sec/step, loss=0.07397, avg_loss=0.07464]\n",
      "Step 515569  [5.457 sec/step, loss=0.07515, avg_loss=0.07462]\n",
      "Step 515570  [5.458 sec/step, loss=0.07236, avg_loss=0.07458]\n",
      "Step 515571  [5.466 sec/step, loss=0.07557, avg_loss=0.07459]\n",
      "Step 515572  [5.465 sec/step, loss=0.07246, avg_loss=0.07458]\n",
      "Step 515573  [5.458 sec/step, loss=0.07188, avg_loss=0.07458]\n",
      "Step 515574  [5.457 sec/step, loss=0.07580, avg_loss=0.07457]\n",
      "Step 515575  [5.478 sec/step, loss=0.07428, avg_loss=0.07464]\n",
      "Step 515576  [5.467 sec/step, loss=0.07331, avg_loss=0.07462]\n",
      "Step 515577  [5.470 sec/step, loss=0.07503, avg_loss=0.07461]\n",
      "Step 515578  [5.488 sec/step, loss=0.07363, avg_loss=0.07460]\n",
      "Step 515579  [5.495 sec/step, loss=0.07499, avg_loss=0.07459]\n",
      "Step 515580  [5.491 sec/step, loss=0.06638, avg_loss=0.07453]\n",
      "Step 515581  [5.476 sec/step, loss=0.07233, avg_loss=0.07450]\n",
      "Step 515582  [5.501 sec/step, loss=0.07464, avg_loss=0.07450]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 515583  [5.501 sec/step, loss=0.07461, avg_loss=0.07452]\n",
      "Step 515584  [5.553 sec/step, loss=0.06682, avg_loss=0.07444]\n",
      "Step 515585  [5.556 sec/step, loss=0.07396, avg_loss=0.07446]\n",
      "Step 515586  [5.561 sec/step, loss=0.07199, avg_loss=0.07451]\n",
      "Step 515587  [5.524 sec/step, loss=0.07487, avg_loss=0.07458]\n",
      "Step 515588  [5.527 sec/step, loss=0.07337, avg_loss=0.07455]\n",
      "Step 515589  [5.509 sec/step, loss=0.07597, avg_loss=0.07456]\n",
      "Step 515590  [5.508 sec/step, loss=0.07548, avg_loss=0.07457]\n",
      "Step 515591  [5.518 sec/step, loss=0.07444, avg_loss=0.07461]\n",
      "Step 515592  [5.526 sec/step, loss=0.07425, avg_loss=0.07459]\n",
      "Step 515593  [5.522 sec/step, loss=0.07514, avg_loss=0.07458]\n",
      "Step 515594  [5.521 sec/step, loss=0.07627, avg_loss=0.07462]\n",
      "Step 515595  [5.520 sec/step, loss=0.07534, avg_loss=0.07463]\n",
      "Step 515596  [5.523 sec/step, loss=0.07709, avg_loss=0.07465]\n",
      "Step 515597  [5.509 sec/step, loss=0.07429, avg_loss=0.07462]\n",
      "Generated 32 batches of size 32 in 2.482 sec\n",
      "Step 515598  [5.504 sec/step, loss=0.07581, avg_loss=0.07461]\n",
      "Step 515599  [5.513 sec/step, loss=0.07699, avg_loss=0.07465]\n",
      "Step 515600  [5.497 sec/step, loss=0.07585, avg_loss=0.07466]\n",
      "Writing summary at step: 515600\n",
      "Step 515601  [5.499 sec/step, loss=0.07683, avg_loss=0.07468]\n",
      "Step 515602  [5.498 sec/step, loss=0.07668, avg_loss=0.07469]\n",
      "Step 515603  [5.484 sec/step, loss=0.07621, avg_loss=0.07468]\n",
      "Step 515604  [5.464 sec/step, loss=0.07594, avg_loss=0.07469]\n",
      "Step 515605  [5.464 sec/step, loss=0.07471, avg_loss=0.07469]\n",
      "Step 515606  [5.479 sec/step, loss=0.07669, avg_loss=0.07469]\n",
      "Step 515607  [5.481 sec/step, loss=0.07313, avg_loss=0.07466]\n",
      "Step 515608  [5.499 sec/step, loss=0.07361, avg_loss=0.07463]\n",
      "Step 515609  [5.494 sec/step, loss=0.07570, avg_loss=0.07462]\n",
      "Step 515610  [5.503 sec/step, loss=0.07613, avg_loss=0.07463]\n",
      "Step 515611  [5.496 sec/step, loss=0.07420, avg_loss=0.07462]\n",
      "Step 515612  [5.450 sec/step, loss=0.07492, avg_loss=0.07471]\n",
      "Step 515613  [5.457 sec/step, loss=0.07699, avg_loss=0.07474]\n",
      "Step 515614  [5.454 sec/step, loss=0.07242, avg_loss=0.07473]\n",
      "Step 515615  [5.463 sec/step, loss=0.07713, avg_loss=0.07475]\n",
      "Step 515616  [5.472 sec/step, loss=0.07674, avg_loss=0.07479]\n",
      "Step 515617  [5.487 sec/step, loss=0.07214, avg_loss=0.07479]\n",
      "Step 515618  [5.493 sec/step, loss=0.07454, avg_loss=0.07486]\n",
      "Step 515619  [5.484 sec/step, loss=0.07496, avg_loss=0.07484]\n",
      "Step 515620  [5.476 sec/step, loss=0.06696, avg_loss=0.07476]\n",
      "Step 515621  [5.540 sec/step, loss=0.06760, avg_loss=0.07471]\n",
      "Step 515622  [5.538 sec/step, loss=0.07419, avg_loss=0.07471]\n",
      "Step 515623  [5.519 sec/step, loss=0.07444, avg_loss=0.07469]\n",
      "Step 515624  [5.536 sec/step, loss=0.07720, avg_loss=0.07471]\n",
      "Step 515625  [5.538 sec/step, loss=0.07645, avg_loss=0.07474]\n",
      "Step 515626  [5.510 sec/step, loss=0.07619, avg_loss=0.07475]\n",
      "Step 515627  [5.514 sec/step, loss=0.07592, avg_loss=0.07475]\n",
      "Step 515628  [5.518 sec/step, loss=0.07589, avg_loss=0.07475]\n",
      "Generated 32 batches of size 32 in 2.420 sec\n",
      "Step 515629  [5.531 sec/step, loss=0.07436, avg_loss=0.07474]\n",
      "Step 515630  [5.533 sec/step, loss=0.07649, avg_loss=0.07473]\n",
      "Step 515631  [5.514 sec/step, loss=0.07518, avg_loss=0.07474]\n",
      "Step 515632  [5.520 sec/step, loss=0.07450, avg_loss=0.07473]\n",
      "Step 515633  [5.509 sec/step, loss=0.07606, avg_loss=0.07472]\n",
      "Step 515634  [5.513 sec/step, loss=0.07561, avg_loss=0.07472]\n",
      "Step 515635  [5.514 sec/step, loss=0.07309, avg_loss=0.07469]\n",
      "Step 515636  [5.485 sec/step, loss=0.07154, avg_loss=0.07464]\n",
      "Step 515637  [5.492 sec/step, loss=0.07688, avg_loss=0.07465]\n",
      "Step 515638  [5.476 sec/step, loss=0.07532, avg_loss=0.07465]\n",
      "Step 515639  [5.464 sec/step, loss=0.07196, avg_loss=0.07460]\n",
      "Step 515640  [5.459 sec/step, loss=0.07467, avg_loss=0.07459]\n",
      "Step 515641  [5.446 sec/step, loss=0.07512, avg_loss=0.07459]\n",
      "Step 515642  [5.451 sec/step, loss=0.07711, avg_loss=0.07460]\n",
      "Step 515643  [5.414 sec/step, loss=0.07681, avg_loss=0.07469]\n",
      "Step 515644  [5.417 sec/step, loss=0.07614, avg_loss=0.07471]\n",
      "Step 515645  [5.416 sec/step, loss=0.07404, avg_loss=0.07472]\n",
      "Step 515646  [5.428 sec/step, loss=0.07673, avg_loss=0.07472]\n",
      "Step 515647  [5.415 sec/step, loss=0.07617, avg_loss=0.07472]\n",
      "Step 515648  [5.396 sec/step, loss=0.07449, avg_loss=0.07470]\n",
      "Step 515649  [5.375 sec/step, loss=0.07588, avg_loss=0.07471]\n",
      "Step 515650  [5.374 sec/step, loss=0.07443, avg_loss=0.07471]\n",
      "Step 515651  [5.396 sec/step, loss=0.07609, avg_loss=0.07480]\n",
      "Step 515652  [5.399 sec/step, loss=0.07553, avg_loss=0.07480]\n",
      "Step 515653  [5.403 sec/step, loss=0.07486, avg_loss=0.07478]\n",
      "Step 515654  [5.402 sec/step, loss=0.07487, avg_loss=0.07478]\n",
      "Step 515655  [5.460 sec/step, loss=0.06551, avg_loss=0.07469]\n",
      "Step 515656  [5.473 sec/step, loss=0.07439, avg_loss=0.07468]\n",
      "Step 515657  [5.471 sec/step, loss=0.07454, avg_loss=0.07466]\n",
      "Step 515658  [5.471 sec/step, loss=0.07685, avg_loss=0.07466]\n",
      "Step 515659  [5.470 sec/step, loss=0.07618, avg_loss=0.07466]\n",
      "Step 515660  [5.458 sec/step, loss=0.07173, avg_loss=0.07461]\n",
      "Generated 32 batches of size 32 in 2.375 sec\n",
      "Step 515661  [5.477 sec/step, loss=0.07670, avg_loss=0.07464]\n",
      "Step 515662  [5.458 sec/step, loss=0.06615, avg_loss=0.07453]\n",
      "Step 515663  [5.427 sec/step, loss=0.07267, avg_loss=0.07452]\n",
      "Step 515664  [5.425 sec/step, loss=0.07555, avg_loss=0.07450]\n",
      "Step 515665  [5.427 sec/step, loss=0.07573, avg_loss=0.07450]\n",
      "Step 515666  [5.413 sec/step, loss=0.07464, avg_loss=0.07447]\n",
      "Step 515667  [5.420 sec/step, loss=0.07568, avg_loss=0.07449]\n",
      "Step 515668  [5.425 sec/step, loss=0.07616, avg_loss=0.07451]\n",
      "Step 515669  [5.445 sec/step, loss=0.07620, avg_loss=0.07452]\n",
      "Step 515670  [5.447 sec/step, loss=0.07530, avg_loss=0.07455]\n",
      "Step 515671  [5.432 sec/step, loss=0.07375, avg_loss=0.07453]\n",
      "Step 515672  [5.440 sec/step, loss=0.07688, avg_loss=0.07458]\n",
      "Step 515673  [5.501 sec/step, loss=0.06647, avg_loss=0.07452]\n",
      "Step 515674  [5.504 sec/step, loss=0.07698, avg_loss=0.07454]\n",
      "Step 515675  [5.491 sec/step, loss=0.07525, avg_loss=0.07455]\n",
      "Step 515676  [5.494 sec/step, loss=0.07602, avg_loss=0.07457]\n",
      "Step 515677  [5.482 sec/step, loss=0.07398, avg_loss=0.07456]\n",
      "Step 515678  [5.465 sec/step, loss=0.07388, avg_loss=0.07457]\n",
      "Step 515679  [5.495 sec/step, loss=0.07396, avg_loss=0.07455]\n",
      "Step 515680  [5.532 sec/step, loss=0.07706, avg_loss=0.07466]\n",
      "Step 515681  [5.550 sec/step, loss=0.07503, avg_loss=0.07469]\n",
      "Step 515682  [5.528 sec/step, loss=0.07336, avg_loss=0.07468]\n",
      "Step 515683  [5.534 sec/step, loss=0.07443, avg_loss=0.07467]\n",
      "Step 515684  [5.488 sec/step, loss=0.07495, avg_loss=0.07476]\n",
      "Step 515685  [5.498 sec/step, loss=0.07598, avg_loss=0.07478]\n",
      "Step 515686  [5.514 sec/step, loss=0.07474, avg_loss=0.07480]\n",
      "Step 515687  [5.504 sec/step, loss=0.07626, avg_loss=0.07482]\n",
      "Step 515688  [5.500 sec/step, loss=0.07302, avg_loss=0.07481]\n",
      "Step 515689  [5.499 sec/step, loss=0.07097, avg_loss=0.07476]\n",
      "Step 515690  [5.485 sec/step, loss=0.07593, avg_loss=0.07477]\n",
      "Step 515691  [5.490 sec/step, loss=0.07736, avg_loss=0.07480]\n",
      "Step 515692  [5.490 sec/step, loss=0.07485, avg_loss=0.07480]\n",
      "Generated 32 batches of size 32 in 2.450 sec\n",
      "Step 515693  [5.507 sec/step, loss=0.07603, avg_loss=0.07481]\n",
      "Step 515694  [5.509 sec/step, loss=0.07606, avg_loss=0.07481]\n",
      "Step 515695  [5.510 sec/step, loss=0.07518, avg_loss=0.07481]\n",
      "Step 515696  [5.483 sec/step, loss=0.06661, avg_loss=0.07470]\n",
      "Step 515697  [5.497 sec/step, loss=0.07482, avg_loss=0.07471]\n",
      "Step 515698  [5.503 sec/step, loss=0.07684, avg_loss=0.07472]\n",
      "Step 515699  [5.484 sec/step, loss=0.07116, avg_loss=0.07466]\n",
      "Step 515700  [5.485 sec/step, loss=0.07744, avg_loss=0.07468]\n",
      "Writing summary at step: 515700\n",
      "Step 515701  [5.473 sec/step, loss=0.07620, avg_loss=0.07467]\n",
      "Step 515702  [5.459 sec/step, loss=0.07408, avg_loss=0.07464]\n",
      "Step 515703  [5.512 sec/step, loss=0.06748, avg_loss=0.07456]\n",
      "Step 515704  [5.504 sec/step, loss=0.07450, avg_loss=0.07454]\n",
      "Step 515705  [5.512 sec/step, loss=0.07540, avg_loss=0.07455]\n",
      "Step 515706  [5.487 sec/step, loss=0.07506, avg_loss=0.07453]\n",
      "Step 515707  [5.478 sec/step, loss=0.07534, avg_loss=0.07456]\n",
      "Step 515708  [5.459 sec/step, loss=0.07716, avg_loss=0.07459]\n",
      "Step 515709  [5.456 sec/step, loss=0.07658, avg_loss=0.07460]\n",
      "Step 515710  [5.453 sec/step, loss=0.07434, avg_loss=0.07458]\n",
      "Step 515711  [5.462 sec/step, loss=0.07704, avg_loss=0.07461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 515712  [5.442 sec/step, loss=0.06701, avg_loss=0.07453]\n",
      "Step 515713  [5.428 sec/step, loss=0.07589, avg_loss=0.07452]\n",
      "Step 515714  [5.450 sec/step, loss=0.07687, avg_loss=0.07456]\n",
      "Step 515715  [5.430 sec/step, loss=0.07237, avg_loss=0.07452]\n",
      "Step 515716  [5.426 sec/step, loss=0.07608, avg_loss=0.07451]\n",
      "Step 515717  [5.448 sec/step, loss=0.07459, avg_loss=0.07453]\n",
      "Step 515718  [5.474 sec/step, loss=0.07719, avg_loss=0.07456]\n",
      "Step 515719  [5.483 sec/step, loss=0.07613, avg_loss=0.07457]\n",
      "Step 515720  [5.498 sec/step, loss=0.07284, avg_loss=0.07463]\n",
      "Step 515721  [5.469 sec/step, loss=0.07310, avg_loss=0.07469]\n",
      "Step 515722  [5.466 sec/step, loss=0.07428, avg_loss=0.07469]\n",
      "Step 515723  [5.473 sec/step, loss=0.07276, avg_loss=0.07467]\n",
      "Generated 32 batches of size 32 in 2.442 sec\n",
      "Step 515724  [5.477 sec/step, loss=0.07494, avg_loss=0.07465]\n",
      "Step 515725  [5.488 sec/step, loss=0.07694, avg_loss=0.07465]\n",
      "Step 515726  [5.498 sec/step, loss=0.07349, avg_loss=0.07463]\n",
      "Step 515727  [5.481 sec/step, loss=0.07296, avg_loss=0.07460]\n",
      "Step 515728  [5.472 sec/step, loss=0.07613, avg_loss=0.07460]\n",
      "Step 515729  [5.471 sec/step, loss=0.07665, avg_loss=0.07462]\n",
      "Step 515730  [5.462 sec/step, loss=0.07530, avg_loss=0.07461]\n",
      "Step 515731  [5.473 sec/step, loss=0.07232, avg_loss=0.07458]\n",
      "Step 515732  [5.474 sec/step, loss=0.07497, avg_loss=0.07459]\n",
      "Step 515733  [5.463 sec/step, loss=0.07448, avg_loss=0.07457]\n",
      "Step 515734  [5.460 sec/step, loss=0.07601, avg_loss=0.07457]\n",
      "Step 515735  [5.469 sec/step, loss=0.07387, avg_loss=0.07458]\n",
      "Step 515736  [5.488 sec/step, loss=0.07460, avg_loss=0.07461]\n",
      "Step 515737  [5.473 sec/step, loss=0.07588, avg_loss=0.07460]\n",
      "Step 515738  [5.483 sec/step, loss=0.07706, avg_loss=0.07462]\n",
      "Step 515739  [5.481 sec/step, loss=0.07443, avg_loss=0.07465]\n",
      "Step 515740  [5.516 sec/step, loss=0.07342, avg_loss=0.07463]\n",
      "Step 515741  [5.501 sec/step, loss=0.07267, avg_loss=0.07461]\n",
      "Step 515742  [5.493 sec/step, loss=0.07399, avg_loss=0.07458]\n",
      "Step 515743  [5.467 sec/step, loss=0.07324, avg_loss=0.07454]\n",
      "Step 515744  [5.458 sec/step, loss=0.07662, avg_loss=0.07455]\n",
      "Step 515745  [5.490 sec/step, loss=0.07632, avg_loss=0.07457]\n",
      "Step 515746  [5.476 sec/step, loss=0.07511, avg_loss=0.07455]\n",
      "Step 515747  [5.466 sec/step, loss=0.07143, avg_loss=0.07451]\n",
      "Step 515748  [5.489 sec/step, loss=0.07652, avg_loss=0.07453]\n",
      "Step 515749  [5.484 sec/step, loss=0.07485, avg_loss=0.07452]\n",
      "Step 515750  [5.491 sec/step, loss=0.07582, avg_loss=0.07453]\n",
      "Step 515751  [5.491 sec/step, loss=0.07490, avg_loss=0.07452]\n",
      "Step 515752  [5.499 sec/step, loss=0.07698, avg_loss=0.07453]\n",
      "Step 515753  [5.501 sec/step, loss=0.07591, avg_loss=0.07454]\n",
      "Step 515754  [5.519 sec/step, loss=0.07687, avg_loss=0.07456]\n",
      "Step 515755  [5.465 sec/step, loss=0.07325, avg_loss=0.07464]\n",
      "Generated 32 batches of size 32 in 2.385 sec\n",
      "Step 515756  [5.459 sec/step, loss=0.07559, avg_loss=0.07465]\n",
      "Step 515757  [5.453 sec/step, loss=0.07545, avg_loss=0.07466]\n",
      "Step 515758  [5.439 sec/step, loss=0.07254, avg_loss=0.07462]\n",
      "Step 515759  [5.443 sec/step, loss=0.07691, avg_loss=0.07462]\n",
      "Step 515760  [5.438 sec/step, loss=0.06678, avg_loss=0.07458]\n",
      "Step 515761  [5.438 sec/step, loss=0.07664, avg_loss=0.07457]\n",
      "Step 515762  [5.455 sec/step, loss=0.07605, avg_loss=0.07467]\n",
      "Step 515763  [5.509 sec/step, loss=0.06657, avg_loss=0.07461]\n",
      "Step 515764  [5.515 sec/step, loss=0.07542, avg_loss=0.07461]\n",
      "Step 515765  [5.533 sec/step, loss=0.07613, avg_loss=0.07462]\n",
      "Step 515766  [5.517 sec/step, loss=0.06645, avg_loss=0.07453]\n",
      "Step 515767  [5.501 sec/step, loss=0.07582, avg_loss=0.07454]\n",
      "Step 515768  [5.503 sec/step, loss=0.07503, avg_loss=0.07452]\n",
      "Step 515769  [5.471 sec/step, loss=0.07473, avg_loss=0.07451]\n",
      "Step 515770  [5.466 sec/step, loss=0.07400, avg_loss=0.07450]\n",
      "Step 515771  [5.492 sec/step, loss=0.07610, avg_loss=0.07452]\n",
      "Step 515772  [5.488 sec/step, loss=0.07677, avg_loss=0.07452]\n",
      "Step 515773  [5.438 sec/step, loss=0.07548, avg_loss=0.07461]\n",
      "Step 515774  [5.435 sec/step, loss=0.07506, avg_loss=0.07459]\n",
      "Step 515775  [5.447 sec/step, loss=0.07237, avg_loss=0.07456]\n",
      "Step 515776  [5.446 sec/step, loss=0.07608, avg_loss=0.07456]\n",
      "Step 515777  [5.450 sec/step, loss=0.07454, avg_loss=0.07457]\n",
      "Step 515778  [5.445 sec/step, loss=0.07323, avg_loss=0.07456]\n",
      "Step 515779  [5.426 sec/step, loss=0.07616, avg_loss=0.07458]\n",
      "Step 515780  [5.419 sec/step, loss=0.07579, avg_loss=0.07457]\n",
      "Step 515781  [5.418 sec/step, loss=0.07652, avg_loss=0.07458]\n",
      "Step 515782  [5.408 sec/step, loss=0.07177, avg_loss=0.07457]\n",
      "Step 515783  [5.406 sec/step, loss=0.07451, avg_loss=0.07457]\n",
      "Step 515784  [5.410 sec/step, loss=0.07694, avg_loss=0.07459]\n",
      "Step 515785  [5.390 sec/step, loss=0.07234, avg_loss=0.07455]\n",
      "Step 515786  [5.374 sec/step, loss=0.07468, avg_loss=0.07455]\n",
      "Step 515787  [5.370 sec/step, loss=0.07421, avg_loss=0.07453]\n",
      "Generated 32 batches of size 32 in 2.464 sec\n",
      "Step 515788  [5.403 sec/step, loss=0.07358, avg_loss=0.07454]\n",
      "Step 515789  [5.404 sec/step, loss=0.07205, avg_loss=0.07455]\n",
      "Step 515790  [5.403 sec/step, loss=0.07581, avg_loss=0.07455]\n",
      "Step 515791  [5.405 sec/step, loss=0.07674, avg_loss=0.07454]\n",
      "Step 515792  [5.406 sec/step, loss=0.07687, avg_loss=0.07456]\n",
      "Step 515793  [5.398 sec/step, loss=0.07389, avg_loss=0.07454]\n",
      "Step 515794  [5.408 sec/step, loss=0.07661, avg_loss=0.07455]\n",
      "Step 515795  [5.457 sec/step, loss=0.06695, avg_loss=0.07446]\n",
      "Step 515796  [5.480 sec/step, loss=0.07612, avg_loss=0.07456]\n",
      "Step 515797  [5.469 sec/step, loss=0.07420, avg_loss=0.07455]\n",
      "Step 515798  [5.454 sec/step, loss=0.07166, avg_loss=0.07450]\n",
      "Step 515799  [5.478 sec/step, loss=0.07663, avg_loss=0.07455]\n",
      "Step 515800  [5.477 sec/step, loss=0.07477, avg_loss=0.07453]\n",
      "Writing summary at step: 515800\n",
      "Step 515801  [5.486 sec/step, loss=0.07605, avg_loss=0.07453]\n",
      "Step 515802  [5.483 sec/step, loss=0.07365, avg_loss=0.07452]\n",
      "Step 515803  [5.421 sec/step, loss=0.07453, avg_loss=0.07459]\n",
      "Step 515804  [5.427 sec/step, loss=0.07538, avg_loss=0.07460]\n",
      "Step 515805  [5.452 sec/step, loss=0.07619, avg_loss=0.07461]\n",
      "Step 515806  [5.463 sec/step, loss=0.07650, avg_loss=0.07462]\n",
      "Step 515807  [5.468 sec/step, loss=0.07242, avg_loss=0.07459]\n",
      "Step 515808  [5.462 sec/step, loss=0.07675, avg_loss=0.07459]\n",
      "Step 515809  [5.478 sec/step, loss=0.07579, avg_loss=0.07458]\n",
      "Step 515810  [5.466 sec/step, loss=0.07125, avg_loss=0.07455]\n",
      "Step 515811  [5.469 sec/step, loss=0.07645, avg_loss=0.07455]\n",
      "Step 515812  [5.478 sec/step, loss=0.07479, avg_loss=0.07462]\n",
      "Step 515813  [5.488 sec/step, loss=0.07659, avg_loss=0.07463]\n",
      "Step 515814  [5.478 sec/step, loss=0.07446, avg_loss=0.07461]\n",
      "Step 515815  [5.487 sec/step, loss=0.07520, avg_loss=0.07463]\n",
      "Step 515816  [5.481 sec/step, loss=0.07587, avg_loss=0.07463]\n",
      "Step 515817  [5.455 sec/step, loss=0.07459, avg_loss=0.07463]\n",
      "Step 515818  [5.425 sec/step, loss=0.07176, avg_loss=0.07458]\n",
      "Generated 32 batches of size 32 in 2.416 sec\n",
      "Step 515819  [5.432 sec/step, loss=0.07728, avg_loss=0.07459]\n",
      "Step 515820  [5.449 sec/step, loss=0.07454, avg_loss=0.07461]\n",
      "Step 515821  [5.426 sec/step, loss=0.07608, avg_loss=0.07464]\n",
      "Step 515822  [5.415 sec/step, loss=0.06633, avg_loss=0.07456]\n",
      "Step 515823  [5.420 sec/step, loss=0.07571, avg_loss=0.07459]\n",
      "Step 515824  [5.455 sec/step, loss=0.06636, avg_loss=0.07450]\n",
      "Step 515825  [5.452 sec/step, loss=0.07643, avg_loss=0.07450]\n",
      "Step 515826  [5.462 sec/step, loss=0.07714, avg_loss=0.07453]\n",
      "Step 515827  [5.482 sec/step, loss=0.07500, avg_loss=0.07455]\n",
      "Step 515828  [5.471 sec/step, loss=0.07416, avg_loss=0.07453]\n",
      "Step 515829  [5.449 sec/step, loss=0.07192, avg_loss=0.07449]\n",
      "Step 515830  [5.449 sec/step, loss=0.07437, avg_loss=0.07448]\n",
      "Step 515831  [5.473 sec/step, loss=0.07272, avg_loss=0.07448]\n",
      "Step 515832  [5.452 sec/step, loss=0.07258, avg_loss=0.07446]\n",
      "Step 515833  [5.462 sec/step, loss=0.07657, avg_loss=0.07448]\n",
      "Step 515834  [5.475 sec/step, loss=0.07673, avg_loss=0.07448]\n",
      "Step 515835  [5.461 sec/step, loss=0.07574, avg_loss=0.07450]\n",
      "Step 515836  [5.462 sec/step, loss=0.07512, avg_loss=0.07451]\n",
      "Step 515837  [5.476 sec/step, loss=0.07699, avg_loss=0.07452]\n",
      "Step 515838  [5.467 sec/step, loss=0.07594, avg_loss=0.07451]\n",
      "Step 515839  [5.481 sec/step, loss=0.07743, avg_loss=0.07454]\n",
      "Step 515840  [5.466 sec/step, loss=0.07686, avg_loss=0.07457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 515841  [5.495 sec/step, loss=0.07576, avg_loss=0.07460]\n",
      "Step 515842  [5.490 sec/step, loss=0.07428, avg_loss=0.07461]\n",
      "Step 515843  [5.551 sec/step, loss=0.06694, avg_loss=0.07454]\n",
      "Step 515844  [5.555 sec/step, loss=0.07460, avg_loss=0.07452]\n",
      "Step 515845  [5.538 sec/step, loss=0.07229, avg_loss=0.07448]\n",
      "Step 515846  [5.543 sec/step, loss=0.07543, avg_loss=0.07449]\n",
      "Step 515847  [5.537 sec/step, loss=0.07288, avg_loss=0.07450]\n",
      "Step 515848  [5.537 sec/step, loss=0.07653, avg_loss=0.07450]\n",
      "Step 515849  [5.538 sec/step, loss=0.07449, avg_loss=0.07450]\n",
      "Step 515850  [5.519 sec/step, loss=0.06828, avg_loss=0.07442]\n",
      "Generated 32 batches of size 32 in 2.387 sec\n",
      "Step 515851  [5.521 sec/step, loss=0.07608, avg_loss=0.07443]\n",
      "Step 515852  [5.505 sec/step, loss=0.07559, avg_loss=0.07442]\n",
      "Step 515853  [5.497 sec/step, loss=0.07614, avg_loss=0.07442]\n",
      "Step 515854  [5.509 sec/step, loss=0.07584, avg_loss=0.07441]\n",
      "Step 515855  [5.509 sec/step, loss=0.07435, avg_loss=0.07442]\n",
      "Step 515856  [5.511 sec/step, loss=0.07709, avg_loss=0.07444]\n",
      "Step 515857  [5.516 sec/step, loss=0.07416, avg_loss=0.07442]\n",
      "Step 515858  [5.518 sec/step, loss=0.07334, avg_loss=0.07443]\n",
      "Step 515859  [5.517 sec/step, loss=0.07519, avg_loss=0.07442]\n",
      "Step 515860  [5.520 sec/step, loss=0.07240, avg_loss=0.07447]\n",
      "Step 515861  [5.516 sec/step, loss=0.07602, avg_loss=0.07447]\n",
      "Step 515862  [5.515 sec/step, loss=0.07135, avg_loss=0.07442]\n",
      "Step 515863  [5.476 sec/step, loss=0.07664, avg_loss=0.07452]\n",
      "Step 515864  [5.488 sec/step, loss=0.07466, avg_loss=0.07451]\n",
      "Step 515865  [5.482 sec/step, loss=0.07691, avg_loss=0.07452]\n",
      "Step 515866  [5.480 sec/step, loss=0.06497, avg_loss=0.07450]\n",
      "Step 515867  [5.479 sec/step, loss=0.07208, avg_loss=0.07447]\n",
      "Step 515868  [5.478 sec/step, loss=0.07586, avg_loss=0.07448]\n",
      "Step 515869  [5.500 sec/step, loss=0.07468, avg_loss=0.07447]\n",
      "Step 515870  [5.507 sec/step, loss=0.07635, avg_loss=0.07450]\n",
      "Step 515871  [5.495 sec/step, loss=0.07565, avg_loss=0.07449]\n",
      "Step 515872  [5.488 sec/step, loss=0.07553, avg_loss=0.07448]\n",
      "Step 515873  [5.483 sec/step, loss=0.07455, avg_loss=0.07447]\n",
      "Step 515874  [5.492 sec/step, loss=0.07730, avg_loss=0.07449]\n",
      "Step 515875  [5.479 sec/step, loss=0.07095, avg_loss=0.07448]\n",
      "Step 515876  [5.477 sec/step, loss=0.07429, avg_loss=0.07446]\n",
      "Step 515877  [5.470 sec/step, loss=0.07490, avg_loss=0.07447]\n",
      "Step 515878  [5.478 sec/step, loss=0.07663, avg_loss=0.07450]\n",
      "Step 515879  [5.475 sec/step, loss=0.07170, avg_loss=0.07446]\n",
      "Step 515880  [5.459 sec/step, loss=0.07615, avg_loss=0.07446]\n",
      "Step 515881  [5.484 sec/step, loss=0.07425, avg_loss=0.07444]\n",
      "Step 515882  [5.487 sec/step, loss=0.07463, avg_loss=0.07446]\n",
      "Generated 32 batches of size 32 in 2.511 sec\n",
      "Step 515883  [5.498 sec/step, loss=0.07430, avg_loss=0.07446]\n",
      "Step 515884  [5.488 sec/step, loss=0.07606, avg_loss=0.07445]\n",
      "Step 515885  [5.553 sec/step, loss=0.06692, avg_loss=0.07440]\n",
      "Step 515886  [5.566 sec/step, loss=0.07665, avg_loss=0.07442]\n",
      "Step 515887  [5.572 sec/step, loss=0.07605, avg_loss=0.07444]\n",
      "Step 515888  [5.553 sec/step, loss=0.07488, avg_loss=0.07445]\n",
      "Step 515889  [5.564 sec/step, loss=0.07551, avg_loss=0.07449]\n",
      "Step 515890  [5.560 sec/step, loss=0.07402, avg_loss=0.07447]\n",
      "Step 515891  [5.556 sec/step, loss=0.07614, avg_loss=0.07446]\n",
      "Step 515892  [5.549 sec/step, loss=0.07512, avg_loss=0.07444]\n",
      "Step 515893  [5.544 sec/step, loss=0.07528, avg_loss=0.07446]\n",
      "Step 515894  [5.530 sec/step, loss=0.07586, avg_loss=0.07445]\n",
      "Step 515895  [5.479 sec/step, loss=0.07508, avg_loss=0.07453]\n",
      "Step 515896  [5.482 sec/step, loss=0.07715, avg_loss=0.07454]\n",
      "Step 515897  [5.469 sec/step, loss=0.06705, avg_loss=0.07447]\n",
      "Step 515898  [5.466 sec/step, loss=0.07340, avg_loss=0.07449]\n",
      "Step 515899  [5.469 sec/step, loss=0.07689, avg_loss=0.07449]\n",
      "Step 515900  [5.470 sec/step, loss=0.07688, avg_loss=0.07451]\n",
      "Writing summary at step: 515900\n",
      "Step 515901  [5.483 sec/step, loss=0.07423, avg_loss=0.07449]\n",
      "Step 515902  [5.499 sec/step, loss=0.07547, avg_loss=0.07451]\n",
      "Step 515903  [5.502 sec/step, loss=0.07493, avg_loss=0.07452]\n",
      "Step 515904  [5.508 sec/step, loss=0.07674, avg_loss=0.07453]\n",
      "Step 515905  [5.486 sec/step, loss=0.07566, avg_loss=0.07452]\n",
      "Step 515906  [5.488 sec/step, loss=0.07547, avg_loss=0.07451]\n",
      "Step 515907  [5.491 sec/step, loss=0.07280, avg_loss=0.07452]\n",
      "Step 515908  [5.481 sec/step, loss=0.07407, avg_loss=0.07449]\n",
      "Step 515909  [5.449 sec/step, loss=0.07269, avg_loss=0.07446]\n",
      "Step 515910  [5.459 sec/step, loss=0.07373, avg_loss=0.07448]\n",
      "Step 515911  [5.500 sec/step, loss=0.06683, avg_loss=0.07439]\n",
      "Step 515912  [5.510 sec/step, loss=0.07678, avg_loss=0.07441]\n",
      "Step 515913  [5.504 sec/step, loss=0.07662, avg_loss=0.07441]\n",
      "Generated 32 batches of size 32 in 2.584 sec\n",
      "Step 515914  [5.504 sec/step, loss=0.07484, avg_loss=0.07441]\n",
      "Step 515915  [5.494 sec/step, loss=0.07509, avg_loss=0.07441]\n",
      "Step 515916  [5.493 sec/step, loss=0.07592, avg_loss=0.07441]\n",
      "Step 515917  [5.492 sec/step, loss=0.07590, avg_loss=0.07442]\n",
      "Step 515918  [5.518 sec/step, loss=0.07453, avg_loss=0.07445]\n",
      "Step 515919  [5.516 sec/step, loss=0.07688, avg_loss=0.07445]\n",
      "Step 515920  [5.512 sec/step, loss=0.07447, avg_loss=0.07445]\n",
      "Step 515921  [5.505 sec/step, loss=0.07212, avg_loss=0.07441]\n",
      "Step 515922  [5.549 sec/step, loss=0.07404, avg_loss=0.07449]\n",
      "Step 515923  [5.535 sec/step, loss=0.07401, avg_loss=0.07447]\n",
      "Step 515924  [5.491 sec/step, loss=0.07662, avg_loss=0.07457]\n",
      "Step 515925  [5.486 sec/step, loss=0.07638, avg_loss=0.07457]\n",
      "Step 515926  [5.512 sec/step, loss=0.06982, avg_loss=0.07450]\n",
      "Step 515927  [5.508 sec/step, loss=0.07604, avg_loss=0.07451]\n",
      "Step 515928  [5.518 sec/step, loss=0.07526, avg_loss=0.07452]\n",
      "Step 515929  [5.554 sec/step, loss=0.07306, avg_loss=0.07453]\n",
      "Step 515930  [5.565 sec/step, loss=0.07455, avg_loss=0.07453]\n",
      "Step 515931  [5.556 sec/step, loss=0.07533, avg_loss=0.07456]\n",
      "Step 515932  [5.580 sec/step, loss=0.07686, avg_loss=0.07460]\n",
      "Step 515933  [5.576 sec/step, loss=0.07169, avg_loss=0.07455]\n",
      "Step 515934  [5.562 sec/step, loss=0.07628, avg_loss=0.07455]\n",
      "Step 515935  [5.553 sec/step, loss=0.07462, avg_loss=0.07454]\n",
      "Step 515936  [5.548 sec/step, loss=0.07566, avg_loss=0.07454]\n",
      "Step 515937  [5.543 sec/step, loss=0.07709, avg_loss=0.07454]\n",
      "Step 515938  [5.551 sec/step, loss=0.07502, avg_loss=0.07453]\n",
      "Step 515939  [5.531 sec/step, loss=0.07160, avg_loss=0.07448]\n",
      "Step 515940  [5.515 sec/step, loss=0.07346, avg_loss=0.07444]\n",
      "Step 515941  [5.506 sec/step, loss=0.07620, avg_loss=0.07445]\n",
      "Step 515942  [5.520 sec/step, loss=0.07737, avg_loss=0.07448]\n",
      "Step 515943  [5.465 sec/step, loss=0.07597, avg_loss=0.07457]\n",
      "Step 515944  [5.460 sec/step, loss=0.07382, avg_loss=0.07456]\n",
      "Step 515945  [5.455 sec/step, loss=0.07466, avg_loss=0.07458]\n",
      "Generated 32 batches of size 32 in 2.530 sec\n",
      "Step 515946  [5.455 sec/step, loss=0.07584, avg_loss=0.07459]\n",
      "Step 515947  [5.451 sec/step, loss=0.06644, avg_loss=0.07452]\n",
      "Step 515948  [5.448 sec/step, loss=0.07619, avg_loss=0.07452]\n",
      "Step 515949  [5.458 sec/step, loss=0.07635, avg_loss=0.07454]\n",
      "Step 515950  [5.487 sec/step, loss=0.07698, avg_loss=0.07463]\n",
      "Step 515951  [5.485 sec/step, loss=0.07605, avg_loss=0.07462]\n",
      "Step 515952  [5.479 sec/step, loss=0.07459, avg_loss=0.07461]\n",
      "Step 515953  [5.468 sec/step, loss=0.07218, avg_loss=0.07458]\n",
      "Step 515954  [5.461 sec/step, loss=0.07692, avg_loss=0.07459]\n",
      "Step 515955  [5.475 sec/step, loss=0.07565, avg_loss=0.07460]\n",
      "Step 515956  [5.470 sec/step, loss=0.07534, avg_loss=0.07458]\n",
      "Step 515957  [5.456 sec/step, loss=0.07428, avg_loss=0.07458]\n",
      "Step 515958  [5.467 sec/step, loss=0.07668, avg_loss=0.07462]\n",
      "Step 515959  [5.464 sec/step, loss=0.07298, avg_loss=0.07459]\n",
      "Step 515960  [5.478 sec/step, loss=0.07387, avg_loss=0.07461]\n",
      "Step 515961  [5.484 sec/step, loss=0.07615, avg_loss=0.07461]\n",
      "Step 515962  [5.478 sec/step, loss=0.07195, avg_loss=0.07462]\n",
      "Step 515963  [5.471 sec/step, loss=0.07566, avg_loss=0.07461]\n",
      "Step 515964  [5.447 sec/step, loss=0.07179, avg_loss=0.07458]\n",
      "Step 515965  [5.449 sec/step, loss=0.07722, avg_loss=0.07458]\n",
      "Step 515966  [5.485 sec/step, loss=0.07660, avg_loss=0.07470]\n",
      "Step 515967  [5.488 sec/step, loss=0.07535, avg_loss=0.07473]\n",
      "Step 515968  [5.494 sec/step, loss=0.07314, avg_loss=0.07470]\n",
      "Step 515969  [5.531 sec/step, loss=0.06689, avg_loss=0.07462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 515970  [5.526 sec/step, loss=0.07597, avg_loss=0.07462]\n",
      "Step 515971  [5.504 sec/step, loss=0.06637, avg_loss=0.07453]\n",
      "Step 515972  [5.518 sec/step, loss=0.07676, avg_loss=0.07454]\n",
      "Step 515973  [5.516 sec/step, loss=0.07532, avg_loss=0.07455]\n",
      "Step 515974  [5.530 sec/step, loss=0.07388, avg_loss=0.07451]\n",
      "Step 515975  [5.543 sec/step, loss=0.07511, avg_loss=0.07456]\n",
      "Step 515976  [5.543 sec/step, loss=0.07438, avg_loss=0.07456]\n",
      "Step 515977  [5.551 sec/step, loss=0.07401, avg_loss=0.07455]\n",
      "Generated 32 batches of size 32 in 2.404 sec\n",
      "Step 515978  [5.547 sec/step, loss=0.07727, avg_loss=0.07455]\n",
      "Step 515979  [5.533 sec/step, loss=0.07321, avg_loss=0.07457]\n",
      "Step 515980  [5.545 sec/step, loss=0.07708, avg_loss=0.07458]\n",
      "Step 515981  [5.531 sec/step, loss=0.07661, avg_loss=0.07460]\n",
      "Step 515982  [5.524 sec/step, loss=0.07330, avg_loss=0.07459]\n",
      "Step 515983  [5.520 sec/step, loss=0.07525, avg_loss=0.07460]\n",
      "Step 515984  [5.526 sec/step, loss=0.07603, avg_loss=0.07460]\n",
      "Step 515985  [5.479 sec/step, loss=0.07595, avg_loss=0.07469]\n",
      "Step 515986  [5.469 sec/step, loss=0.07423, avg_loss=0.07466]\n",
      "Step 515987  [5.474 sec/step, loss=0.07677, avg_loss=0.07467]\n",
      "Step 515988  [5.464 sec/step, loss=0.07609, avg_loss=0.07468]\n",
      "Step 515989  [5.461 sec/step, loss=0.07589, avg_loss=0.07469]\n",
      "Step 515990  [5.457 sec/step, loss=0.07143, avg_loss=0.07466]\n",
      "Step 515991  [5.439 sec/step, loss=0.07442, avg_loss=0.07464]\n",
      "Step 515992  [5.445 sec/step, loss=0.07668, avg_loss=0.07466]\n",
      "Step 515993  [5.445 sec/step, loss=0.07528, avg_loss=0.07466]\n",
      "Step 515994  [5.448 sec/step, loss=0.07591, avg_loss=0.07466]\n",
      "Step 515995  [5.448 sec/step, loss=0.07441, avg_loss=0.07465]\n",
      "Step 515996  [5.451 sec/step, loss=0.07416, avg_loss=0.07462]\n",
      "Step 515997  [5.480 sec/step, loss=0.07442, avg_loss=0.07470]\n",
      "Step 515998  [5.490 sec/step, loss=0.07564, avg_loss=0.07472]\n",
      "Step 515999  [5.461 sec/step, loss=0.07106, avg_loss=0.07466]\n",
      "Step 516000  [5.446 sec/step, loss=0.07331, avg_loss=0.07463]\n",
      "Writing summary at step: 516000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-516000\n",
      "Saving audio and alignment...\n",
      "Input: juurijaa nay tshaauunii miin loong kay baaggoon koo batsaajaa~__________________________\n",
      "Step 516001  [5.430 sec/step, loss=0.07690, avg_loss=0.07465]\n",
      "Step 516002  [5.417 sec/step, loss=0.07461, avg_loss=0.07464]\n",
      "Step 516003  [5.439 sec/step, loss=0.07641, avg_loss=0.07466]\n",
      "Step 516004  [5.454 sec/step, loss=0.07572, avg_loss=0.07465]\n",
      "Step 516005  [5.442 sec/step, loss=0.07439, avg_loss=0.07464]\n",
      "Step 516006  [5.445 sec/step, loss=0.07526, avg_loss=0.07463]\n",
      "Step 516007  [5.445 sec/step, loss=0.07629, avg_loss=0.07467]\n",
      "Generated 32 batches of size 32 in 2.347 sec\n",
      "Step 516008  [5.457 sec/step, loss=0.07654, avg_loss=0.07469]\n",
      "Step 516009  [5.485 sec/step, loss=0.07669, avg_loss=0.07473]\n",
      "Step 516010  [5.484 sec/step, loss=0.07437, avg_loss=0.07474]\n",
      "Step 516011  [5.431 sec/step, loss=0.07552, avg_loss=0.07483]\n",
      "Step 516012  [5.421 sec/step, loss=0.07396, avg_loss=0.07480]\n",
      "Step 516013  [5.470 sec/step, loss=0.06642, avg_loss=0.07470]\n",
      "Step 516014  [5.470 sec/step, loss=0.07578, avg_loss=0.07471]\n",
      "Step 516015  [5.485 sec/step, loss=0.07527, avg_loss=0.07471]\n",
      "Step 516016  [5.487 sec/step, loss=0.07072, avg_loss=0.07465]\n",
      "Step 516017  [5.487 sec/step, loss=0.07528, avg_loss=0.07465]\n",
      "Step 516018  [5.479 sec/step, loss=0.07360, avg_loss=0.07464]\n",
      "Step 516019  [5.464 sec/step, loss=0.07541, avg_loss=0.07462]\n",
      "Step 516020  [5.460 sec/step, loss=0.07524, avg_loss=0.07463]\n",
      "Step 516021  [5.470 sec/step, loss=0.07502, avg_loss=0.07466]\n",
      "Step 516022  [5.443 sec/step, loss=0.07342, avg_loss=0.07466]\n",
      "Step 516023  [5.462 sec/step, loss=0.07422, avg_loss=0.07466]\n",
      "Step 516024  [5.456 sec/step, loss=0.07606, avg_loss=0.07465]\n",
      "Step 516025  [5.456 sec/step, loss=0.07654, avg_loss=0.07465]\n",
      "Step 516026  [5.409 sec/step, loss=0.07428, avg_loss=0.07470]\n",
      "Step 516027  [5.419 sec/step, loss=0.07516, avg_loss=0.07469]\n",
      "Step 516028  [5.403 sec/step, loss=0.07186, avg_loss=0.07466]\n",
      "Step 516029  [5.368 sec/step, loss=0.07443, avg_loss=0.07467]\n",
      "Step 516030  [5.372 sec/step, loss=0.07694, avg_loss=0.07469]\n",
      "Step 516031  [5.359 sec/step, loss=0.07682, avg_loss=0.07471]\n",
      "Step 516032  [5.344 sec/step, loss=0.07280, avg_loss=0.07467]\n",
      "Step 516033  [5.347 sec/step, loss=0.07552, avg_loss=0.07471]\n",
      "Step 516034  [5.347 sec/step, loss=0.07367, avg_loss=0.07468]\n",
      "Step 516035  [5.376 sec/step, loss=0.07592, avg_loss=0.07469]\n",
      "Step 516036  [5.365 sec/step, loss=0.07497, avg_loss=0.07469]\n",
      "Step 516037  [5.371 sec/step, loss=0.07638, avg_loss=0.07468]\n",
      "Step 516038  [5.347 sec/step, loss=0.07224, avg_loss=0.07465]\n",
      "Step 516039  [5.362 sec/step, loss=0.07544, avg_loss=0.07469]\n",
      "Generated 32 batches of size 32 in 2.419 sec\n",
      "Step 516040  [5.382 sec/step, loss=0.07695, avg_loss=0.07472]\n",
      "Step 516041  [5.360 sec/step, loss=0.06706, avg_loss=0.07463]\n",
      "Step 516042  [5.343 sec/step, loss=0.07426, avg_loss=0.07460]\n",
      "Step 516043  [5.350 sec/step, loss=0.07585, avg_loss=0.07460]\n",
      "Step 516044  [5.346 sec/step, loss=0.07593, avg_loss=0.07462]\n",
      "Step 516045  [5.361 sec/step, loss=0.07719, avg_loss=0.07465]\n",
      "Step 516046  [5.364 sec/step, loss=0.07660, avg_loss=0.07465]\n",
      "Step 516047  [5.408 sec/step, loss=0.07416, avg_loss=0.07473]\n",
      "Step 516048  [5.445 sec/step, loss=0.06663, avg_loss=0.07464]\n",
      "Step 516049  [5.492 sec/step, loss=0.06790, avg_loss=0.07455]\n",
      "Step 516050  [5.473 sec/step, loss=0.07459, avg_loss=0.07453]\n",
      "Step 516051  [5.467 sec/step, loss=0.07593, avg_loss=0.07453]\n",
      "Step 516052  [5.480 sec/step, loss=0.07552, avg_loss=0.07453]\n",
      "Step 516053  [5.506 sec/step, loss=0.07472, avg_loss=0.07456]\n",
      "Step 516054  [5.494 sec/step, loss=0.07301, avg_loss=0.07452]\n",
      "Step 516055  [5.480 sec/step, loss=0.07451, avg_loss=0.07451]\n",
      "Step 516056  [5.478 sec/step, loss=0.07586, avg_loss=0.07451]\n",
      "Step 516057  [5.489 sec/step, loss=0.07724, avg_loss=0.07454]\n",
      "Step 516058  [5.475 sec/step, loss=0.07238, avg_loss=0.07450]\n",
      "Step 516059  [5.484 sec/step, loss=0.07670, avg_loss=0.07454]\n",
      "Step 516060  [5.490 sec/step, loss=0.07716, avg_loss=0.07457]\n",
      "Step 516061  [5.479 sec/step, loss=0.07518, avg_loss=0.07456]\n",
      "Step 516062  [5.485 sec/step, loss=0.07589, avg_loss=0.07460]\n",
      "Step 516063  [5.485 sec/step, loss=0.07533, avg_loss=0.07460]\n",
      "Step 516064  [5.494 sec/step, loss=0.07564, avg_loss=0.07464]\n",
      "Step 516065  [5.488 sec/step, loss=0.07548, avg_loss=0.07462]\n",
      "Step 516066  [5.459 sec/step, loss=0.07154, avg_loss=0.07457]\n",
      "Step 516067  [5.455 sec/step, loss=0.07629, avg_loss=0.07458]\n",
      "Step 516068  [5.434 sec/step, loss=0.07536, avg_loss=0.07460]\n",
      "Step 516069  [5.381 sec/step, loss=0.07483, avg_loss=0.07468]\n",
      "Step 516070  [5.378 sec/step, loss=0.07426, avg_loss=0.07466]\n",
      "Step 516071  [5.418 sec/step, loss=0.07585, avg_loss=0.07476]\n",
      "Generated 32 batches of size 32 in 2.440 sec\n",
      "Step 516072  [5.418 sec/step, loss=0.07714, avg_loss=0.07476]\n",
      "Step 516073  [5.426 sec/step, loss=0.07567, avg_loss=0.07476]\n",
      "Step 516074  [5.414 sec/step, loss=0.07521, avg_loss=0.07478]\n",
      "Step 516075  [5.415 sec/step, loss=0.07583, avg_loss=0.07478]\n",
      "Step 516076  [5.404 sec/step, loss=0.07189, avg_loss=0.07476]\n",
      "Step 516077  [5.391 sec/step, loss=0.06683, avg_loss=0.07469]\n",
      "Step 516078  [5.407 sec/step, loss=0.07322, avg_loss=0.07465]\n",
      "Step 516079  [5.410 sec/step, loss=0.07333, avg_loss=0.07465]\n",
      "Step 516080  [5.412 sec/step, loss=0.07442, avg_loss=0.07462]\n",
      "Step 516081  [5.398 sec/step, loss=0.07521, avg_loss=0.07461]\n",
      "Step 516082  [5.408 sec/step, loss=0.07605, avg_loss=0.07464]\n",
      "Step 516083  [5.424 sec/step, loss=0.07572, avg_loss=0.07464]\n",
      "Step 516084  [5.429 sec/step, loss=0.07691, avg_loss=0.07465]\n",
      "Step 516085  [5.427 sec/step, loss=0.07609, avg_loss=0.07465]\n",
      "Step 516086  [5.433 sec/step, loss=0.07224, avg_loss=0.07463]\n",
      "Step 516087  [5.434 sec/step, loss=0.07710, avg_loss=0.07463]\n",
      "Step 516088  [5.436 sec/step, loss=0.07586, avg_loss=0.07463]\n",
      "Step 516089  [5.440 sec/step, loss=0.07664, avg_loss=0.07464]\n",
      "Step 516090  [5.455 sec/step, loss=0.07604, avg_loss=0.07469]\n",
      "Step 516091  [5.462 sec/step, loss=0.07487, avg_loss=0.07469]\n",
      "Step 516092  [5.443 sec/step, loss=0.07148, avg_loss=0.07464]\n",
      "Step 516093  [5.423 sec/step, loss=0.07268, avg_loss=0.07461]\n",
      "Step 516094  [5.428 sec/step, loss=0.07647, avg_loss=0.07462]\n",
      "Step 516095  [5.441 sec/step, loss=0.07526, avg_loss=0.07463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 516096  [5.423 sec/step, loss=0.07484, avg_loss=0.07463]\n",
      "Step 516097  [5.421 sec/step, loss=0.07661, avg_loss=0.07465]\n",
      "Step 516098  [5.424 sec/step, loss=0.07614, avg_loss=0.07466]\n",
      "Step 516099  [5.487 sec/step, loss=0.06577, avg_loss=0.07461]\n",
      "Step 516100  [5.487 sec/step, loss=0.07434, avg_loss=0.07462]\n",
      "Writing summary at step: 516100\n",
      "Step 516101  [5.479 sec/step, loss=0.07575, avg_loss=0.07461]\n",
      "Step 516102  [5.484 sec/step, loss=0.07629, avg_loss=0.07462]\n",
      "Generated 32 batches of size 32 in 2.297 sec\n",
      "Step 516103  [5.476 sec/step, loss=0.07288, avg_loss=0.07459]\n",
      "Step 516104  [5.477 sec/step, loss=0.07407, avg_loss=0.07457]\n",
      "Step 516105  [5.468 sec/step, loss=0.06620, avg_loss=0.07449]\n",
      "Step 516106  [5.470 sec/step, loss=0.07527, avg_loss=0.07449]\n",
      "Step 516107  [5.478 sec/step, loss=0.07496, avg_loss=0.07448]\n",
      "Step 516108  [5.472 sec/step, loss=0.07554, avg_loss=0.07447]\n",
      "Step 516109  [5.449 sec/step, loss=0.07496, avg_loss=0.07445]\n",
      "Step 516110  [5.445 sec/step, loss=0.07401, avg_loss=0.07444]\n",
      "Step 516111  [5.461 sec/step, loss=0.07606, avg_loss=0.07445]\n",
      "Step 516112  [5.458 sec/step, loss=0.07479, avg_loss=0.07446]\n",
      "Step 516113  [5.411 sec/step, loss=0.07566, avg_loss=0.07455]\n",
      "Step 516114  [5.417 sec/step, loss=0.07772, avg_loss=0.07457]\n",
      "Step 516115  [5.418 sec/step, loss=0.07585, avg_loss=0.07458]\n",
      "Step 516116  [5.422 sec/step, loss=0.07611, avg_loss=0.07463]\n",
      "Step 516117  [5.421 sec/step, loss=0.07528, avg_loss=0.07463]\n",
      "Step 516118  [5.432 sec/step, loss=0.07641, avg_loss=0.07466]\n",
      "Step 516119  [5.431 sec/step, loss=0.07409, avg_loss=0.07464]\n",
      "Step 516120  [5.423 sec/step, loss=0.07577, avg_loss=0.07465]\n",
      "Step 516121  [5.423 sec/step, loss=0.07520, avg_loss=0.07465]\n",
      "Step 516122  [5.409 sec/step, loss=0.07200, avg_loss=0.07464]\n",
      "Step 516123  [5.387 sec/step, loss=0.07185, avg_loss=0.07461]\n",
      "Step 516124  [5.381 sec/step, loss=0.07456, avg_loss=0.07460]\n",
      "Step 516125  [5.371 sec/step, loss=0.07475, avg_loss=0.07458]\n",
      "Step 516126  [5.388 sec/step, loss=0.07517, avg_loss=0.07459]\n",
      "Step 516127  [5.376 sec/step, loss=0.07568, avg_loss=0.07460]\n",
      "Step 516128  [5.391 sec/step, loss=0.07671, avg_loss=0.07464]\n",
      "Step 516129  [5.409 sec/step, loss=0.07489, avg_loss=0.07465]\n",
      "Step 516130  [5.386 sec/step, loss=0.07391, avg_loss=0.07462]\n",
      "Step 516131  [5.376 sec/step, loss=0.07191, avg_loss=0.07457]\n",
      "Step 516132  [5.386 sec/step, loss=0.07705, avg_loss=0.07461]\n",
      "Step 516133  [5.403 sec/step, loss=0.07403, avg_loss=0.07460]\n",
      "Step 516134  [5.407 sec/step, loss=0.07612, avg_loss=0.07462]\n",
      "Generated 32 batches of size 32 in 2.316 sec\n",
      "Step 516135  [5.393 sec/step, loss=0.07556, avg_loss=0.07462]\n",
      "Step 516136  [5.414 sec/step, loss=0.07704, avg_loss=0.07464]\n",
      "Step 516137  [5.429 sec/step, loss=0.07381, avg_loss=0.07461]\n",
      "Step 516138  [5.424 sec/step, loss=0.06701, avg_loss=0.07456]\n",
      "Step 516139  [5.416 sec/step, loss=0.07402, avg_loss=0.07455]\n",
      "Step 516140  [5.450 sec/step, loss=0.06761, avg_loss=0.07445]\n",
      "Step 516141  [5.474 sec/step, loss=0.07655, avg_loss=0.07455]\n",
      "Step 516142  [5.494 sec/step, loss=0.07665, avg_loss=0.07457]\n",
      "Step 516143  [5.494 sec/step, loss=0.07543, avg_loss=0.07457]\n",
      "Step 516144  [5.500 sec/step, loss=0.07620, avg_loss=0.07457]\n",
      "Step 516145  [5.485 sec/step, loss=0.07569, avg_loss=0.07455]\n",
      "Step 516146  [5.483 sec/step, loss=0.07691, avg_loss=0.07456]\n",
      "Step 516147  [5.449 sec/step, loss=0.07422, avg_loss=0.07456]\n",
      "Step 516148  [5.400 sec/step, loss=0.07596, avg_loss=0.07465]\n",
      "Step 516149  [5.376 sec/step, loss=0.07385, avg_loss=0.07471]\n",
      "Step 516150  [5.373 sec/step, loss=0.07508, avg_loss=0.07472]\n",
      "Step 516151  [5.388 sec/step, loss=0.07691, avg_loss=0.07473]\n",
      "Step 516152  [5.382 sec/step, loss=0.07355, avg_loss=0.07471]\n",
      "Step 516153  [5.354 sec/step, loss=0.06658, avg_loss=0.07463]\n",
      "Step 516154  [5.367 sec/step, loss=0.07703, avg_loss=0.07467]\n",
      "Step 516155  [5.378 sec/step, loss=0.07721, avg_loss=0.07469]\n",
      "Step 516156  [5.363 sec/step, loss=0.07290, avg_loss=0.07466]\n",
      "Step 516157  [5.359 sec/step, loss=0.07585, avg_loss=0.07465]\n",
      "Step 516158  [5.359 sec/step, loss=0.07328, avg_loss=0.07466]\n",
      "Step 516159  [5.356 sec/step, loss=0.07646, avg_loss=0.07466]\n",
      "Step 516160  [5.340 sec/step, loss=0.07182, avg_loss=0.07460]\n",
      "Step 516161  [5.345 sec/step, loss=0.07479, avg_loss=0.07460]\n",
      "Step 516162  [5.346 sec/step, loss=0.07270, avg_loss=0.07457]\n",
      "Step 516163  [5.348 sec/step, loss=0.07572, avg_loss=0.07457]\n",
      "Step 516164  [5.340 sec/step, loss=0.07636, avg_loss=0.07458]\n",
      "Step 516165  [5.331 sec/step, loss=0.07112, avg_loss=0.07453]\n",
      "Step 516166  [5.392 sec/step, loss=0.06623, avg_loss=0.07448]\n",
      "Generated 32 batches of size 32 in 2.525 sec\n",
      "Step 516167  [5.388 sec/step, loss=0.07455, avg_loss=0.07446]\n",
      "Step 516168  [5.409 sec/step, loss=0.07358, avg_loss=0.07445]\n",
      "Step 516169  [5.429 sec/step, loss=0.07622, avg_loss=0.07446]\n",
      "Step 516170  [5.437 sec/step, loss=0.07500, avg_loss=0.07447]\n",
      "Step 516171  [5.411 sec/step, loss=0.07584, avg_loss=0.07447]\n",
      "Step 516172  [5.409 sec/step, loss=0.07454, avg_loss=0.07444]\n",
      "Step 516173  [5.413 sec/step, loss=0.07539, avg_loss=0.07444]\n",
      "Step 516174  [5.392 sec/step, loss=0.07394, avg_loss=0.07443]\n",
      "Step 516175  [5.389 sec/step, loss=0.07550, avg_loss=0.07442]\n",
      "Step 516176  [5.420 sec/step, loss=0.07624, avg_loss=0.07447]\n",
      "Step 516177  [5.447 sec/step, loss=0.07467, avg_loss=0.07454]\n",
      "Step 516178  [5.413 sec/step, loss=0.07417, avg_loss=0.07455]\n",
      "Step 516179  [5.429 sec/step, loss=0.07591, avg_loss=0.07458]\n",
      "Step 516180  [5.420 sec/step, loss=0.07629, avg_loss=0.07460]\n",
      "Step 516181  [5.419 sec/step, loss=0.07225, avg_loss=0.07457]\n",
      "Step 516182  [5.437 sec/step, loss=0.07653, avg_loss=0.07457]\n",
      "Step 516183  [5.446 sec/step, loss=0.07303, avg_loss=0.07455]\n",
      "Step 516184  [5.425 sec/step, loss=0.07239, avg_loss=0.07450]\n",
      "Step 516185  [5.431 sec/step, loss=0.07637, avg_loss=0.07450]\n",
      "Step 516186  [5.433 sec/step, loss=0.07630, avg_loss=0.07454]\n",
      "Step 516187  [5.416 sec/step, loss=0.07345, avg_loss=0.07451]\n",
      "Step 516188  [5.417 sec/step, loss=0.07558, avg_loss=0.07450]\n",
      "Step 516189  [5.406 sec/step, loss=0.07508, avg_loss=0.07449]\n",
      "Step 516190  [5.399 sec/step, loss=0.07392, avg_loss=0.07447]\n",
      "Step 516191  [5.395 sec/step, loss=0.07466, avg_loss=0.07447]\n",
      "Step 516192  [5.408 sec/step, loss=0.07569, avg_loss=0.07451]\n",
      "Step 516193  [5.430 sec/step, loss=0.07682, avg_loss=0.07455]\n",
      "Step 516194  [5.424 sec/step, loss=0.07278, avg_loss=0.07451]\n",
      "Step 516195  [5.461 sec/step, loss=0.06643, avg_loss=0.07442]\n",
      "Step 516196  [5.466 sec/step, loss=0.07316, avg_loss=0.07441]\n",
      "Step 516197  [5.439 sec/step, loss=0.06554, avg_loss=0.07430]\n",
      "Step 516198  [5.439 sec/step, loss=0.07660, avg_loss=0.07430]\n",
      "Generated 32 batches of size 32 in 2.400 sec\n",
      "Step 516199  [5.393 sec/step, loss=0.07503, avg_loss=0.07439]\n",
      "Step 516200  [5.397 sec/step, loss=0.07654, avg_loss=0.07442]\n",
      "Writing summary at step: 516200\n",
      "Step 516201  [5.385 sec/step, loss=0.07183, avg_loss=0.07438]\n",
      "Step 516202  [5.398 sec/step, loss=0.07675, avg_loss=0.07438]\n",
      "Step 516203  [5.389 sec/step, loss=0.07571, avg_loss=0.07441]\n",
      "Step 516204  [5.353 sec/step, loss=0.07485, avg_loss=0.07442]\n",
      "Step 516205  [5.373 sec/step, loss=0.07496, avg_loss=0.07451]\n",
      "Step 516206  [5.374 sec/step, loss=0.07667, avg_loss=0.07452]\n",
      "Step 516207  [5.358 sec/step, loss=0.07465, avg_loss=0.07452]\n",
      "Step 516208  [5.365 sec/step, loss=0.07572, avg_loss=0.07452]\n",
      "Step 516209  [5.381 sec/step, loss=0.07487, avg_loss=0.07452]\n",
      "Step 516210  [5.386 sec/step, loss=0.07355, avg_loss=0.07451]\n",
      "Step 516211  [5.378 sec/step, loss=0.07541, avg_loss=0.07451]\n",
      "Step 516212  [5.399 sec/step, loss=0.07704, avg_loss=0.07453]\n",
      "Step 516213  [5.392 sec/step, loss=0.07349, avg_loss=0.07451]\n",
      "Step 516214  [5.388 sec/step, loss=0.07497, avg_loss=0.07448]\n",
      "Step 516215  [5.409 sec/step, loss=0.07363, avg_loss=0.07446]\n",
      "Step 516216  [5.396 sec/step, loss=0.07180, avg_loss=0.07441]\n",
      "Step 516217  [5.378 sec/step, loss=0.06778, avg_loss=0.07434]\n",
      "Step 516218  [5.373 sec/step, loss=0.07409, avg_loss=0.07432]\n",
      "Step 516219  [5.377 sec/step, loss=0.07632, avg_loss=0.07434]\n",
      "Step 516220  [5.371 sec/step, loss=0.07493, avg_loss=0.07433]\n",
      "Step 516221  [5.373 sec/step, loss=0.07619, avg_loss=0.07434]\n",
      "Step 516222  [5.385 sec/step, loss=0.07139, avg_loss=0.07433]\n",
      "Step 516223  [5.445 sec/step, loss=0.06673, avg_loss=0.07428]\n",
      "Step 516224  [5.459 sec/step, loss=0.07739, avg_loss=0.07431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 516225  [5.475 sec/step, loss=0.07669, avg_loss=0.07433]\n",
      "Step 516226  [5.473 sec/step, loss=0.07667, avg_loss=0.07434]\n",
      "Step 516227  [5.475 sec/step, loss=0.07627, avg_loss=0.07435]\n",
      "Step 516228  [5.486 sec/step, loss=0.07638, avg_loss=0.07435]\n",
      "Step 516229  [5.472 sec/step, loss=0.07629, avg_loss=0.07436]\n",
      "Generated 32 batches of size 32 in 2.508 sec\n",
      "Step 516230  [5.485 sec/step, loss=0.07552, avg_loss=0.07438]\n",
      "Step 516231  [5.481 sec/step, loss=0.07430, avg_loss=0.07440]\n",
      "Step 516232  [5.469 sec/step, loss=0.07435, avg_loss=0.07437]\n",
      "Step 516233  [5.454 sec/step, loss=0.07622, avg_loss=0.07440]\n",
      "Step 516234  [5.451 sec/step, loss=0.07623, avg_loss=0.07440]\n",
      "Step 516235  [5.446 sec/step, loss=0.07558, avg_loss=0.07440]\n",
      "Step 516236  [5.454 sec/step, loss=0.07410, avg_loss=0.07437]\n",
      "Step 516237  [5.442 sec/step, loss=0.07688, avg_loss=0.07440]\n",
      "Step 516238  [5.446 sec/step, loss=0.07251, avg_loss=0.07445]\n",
      "Step 516239  [5.451 sec/step, loss=0.07658, avg_loss=0.07448]\n",
      "Step 516240  [5.400 sec/step, loss=0.07514, avg_loss=0.07455]\n",
      "Step 516241  [5.440 sec/step, loss=0.06773, avg_loss=0.07447]\n",
      "Step 516242  [5.418 sec/step, loss=0.07481, avg_loss=0.07445]\n",
      "Step 516243  [5.421 sec/step, loss=0.07567, avg_loss=0.07445]\n",
      "Step 516244  [5.433 sec/step, loss=0.07607, avg_loss=0.07445]\n",
      "Step 516245  [5.423 sec/step, loss=0.07255, avg_loss=0.07442]\n",
      "Step 516246  [5.400 sec/step, loss=0.06664, avg_loss=0.07431]\n",
      "Step 516247  [5.396 sec/step, loss=0.07258, avg_loss=0.07430]\n",
      "Step 516248  [5.407 sec/step, loss=0.07402, avg_loss=0.07428]\n",
      "Step 516249  [5.376 sec/step, loss=0.07610, avg_loss=0.07430]\n",
      "Step 516250  [5.395 sec/step, loss=0.07496, avg_loss=0.07430]\n",
      "Step 516251  [5.380 sec/step, loss=0.07565, avg_loss=0.07429]\n",
      "Step 516252  [5.407 sec/step, loss=0.07435, avg_loss=0.07430]\n",
      "Step 516253  [5.422 sec/step, loss=0.07213, avg_loss=0.07435]\n",
      "Step 516254  [5.417 sec/step, loss=0.07558, avg_loss=0.07434]\n",
      "Step 516255  [5.407 sec/step, loss=0.07453, avg_loss=0.07431]\n",
      "Step 516256  [5.430 sec/step, loss=0.07685, avg_loss=0.07435]\n",
      "Step 516257  [5.430 sec/step, loss=0.07533, avg_loss=0.07434]\n",
      "Step 516258  [5.454 sec/step, loss=0.07416, avg_loss=0.07435]\n",
      "Step 516259  [5.448 sec/step, loss=0.07622, avg_loss=0.07435]\n",
      "Step 516260  [5.472 sec/step, loss=0.07657, avg_loss=0.07440]\n",
      "Step 516261  [5.470 sec/step, loss=0.07660, avg_loss=0.07442]\n",
      "Generated 32 batches of size 32 in 2.560 sec\n",
      "Step 516262  [5.471 sec/step, loss=0.07247, avg_loss=0.07441]\n",
      "Step 516263  [5.470 sec/step, loss=0.07519, avg_loss=0.07441]\n",
      "Step 516264  [5.471 sec/step, loss=0.07559, avg_loss=0.07440]\n",
      "Step 516265  [5.466 sec/step, loss=0.07452, avg_loss=0.07443]\n",
      "Step 516266  [5.409 sec/step, loss=0.07420, avg_loss=0.07451]\n",
      "Step 516267  [5.408 sec/step, loss=0.07487, avg_loss=0.07452]\n",
      "Step 516268  [5.410 sec/step, loss=0.07668, avg_loss=0.07455]\n",
      "Step 516269  [5.389 sec/step, loss=0.07239, avg_loss=0.07451]\n",
      "Step 516270  [5.383 sec/step, loss=0.07557, avg_loss=0.07452]\n",
      "Step 516271  [5.394 sec/step, loss=0.07747, avg_loss=0.07453]\n",
      "Step 516272  [5.379 sec/step, loss=0.07554, avg_loss=0.07454]\n",
      "Step 516273  [5.376 sec/step, loss=0.07661, avg_loss=0.07455]\n",
      "Step 516274  [5.380 sec/step, loss=0.07553, avg_loss=0.07457]\n",
      "Step 516275  [5.394 sec/step, loss=0.07741, avg_loss=0.07459]\n",
      "Step 516276  [5.376 sec/step, loss=0.07481, avg_loss=0.07457]\n",
      "Step 516277  [5.370 sec/step, loss=0.07497, avg_loss=0.07458]\n",
      "Step 516278  [5.370 sec/step, loss=0.07453, avg_loss=0.07458]\n",
      "Step 516279  [5.358 sec/step, loss=0.07610, avg_loss=0.07458]\n",
      "Step 516280  [5.347 sec/step, loss=0.07151, avg_loss=0.07454]\n",
      "Step 516281  [5.356 sec/step, loss=0.07732, avg_loss=0.07459]\n",
      "Step 516282  [5.341 sec/step, loss=0.07414, avg_loss=0.07456]\n",
      "Step 516283  [5.310 sec/step, loss=0.07289, avg_loss=0.07456]\n",
      "Step 516284  [5.372 sec/step, loss=0.06702, avg_loss=0.07451]\n",
      "Step 516285  [5.352 sec/step, loss=0.07259, avg_loss=0.07447]\n",
      "Step 516286  [5.362 sec/step, loss=0.07713, avg_loss=0.07448]\n",
      "Step 516287  [5.354 sec/step, loss=0.07264, avg_loss=0.07447]\n",
      "Step 516288  [5.344 sec/step, loss=0.07412, avg_loss=0.07446]\n",
      "Step 516289  [5.355 sec/step, loss=0.07781, avg_loss=0.07448]\n",
      "Step 516290  [5.374 sec/step, loss=0.07639, avg_loss=0.07451]\n",
      "Step 516291  [5.397 sec/step, loss=0.07715, avg_loss=0.07453]\n",
      "Step 516292  [5.402 sec/step, loss=0.07425, avg_loss=0.07452]\n",
      "Step 516293  [5.386 sec/step, loss=0.07457, avg_loss=0.07450]\n",
      "Generated 32 batches of size 32 in 2.420 sec\n",
      "Step 516294  [5.391 sec/step, loss=0.07667, avg_loss=0.07453]\n",
      "Step 516295  [5.367 sec/step, loss=0.07364, avg_loss=0.07461]\n",
      "Step 516296  [5.370 sec/step, loss=0.07616, avg_loss=0.07464]\n",
      "Step 516297  [5.391 sec/step, loss=0.07632, avg_loss=0.07474]\n",
      "Step 516298  [5.389 sec/step, loss=0.07540, avg_loss=0.07473]\n",
      "Step 516299  [5.382 sec/step, loss=0.07396, avg_loss=0.07472]\n",
      "Step 516300  [5.390 sec/step, loss=0.07653, avg_loss=0.07472]\n",
      "Writing summary at step: 516300\n",
      "Step 516301  [5.409 sec/step, loss=0.07546, avg_loss=0.07476]\n",
      "Step 516302  [5.393 sec/step, loss=0.07587, avg_loss=0.07475]\n",
      "Step 516303  [5.395 sec/step, loss=0.07516, avg_loss=0.07474]\n",
      "Step 516304  [5.405 sec/step, loss=0.07521, avg_loss=0.07475]\n",
      "Step 516305  [5.391 sec/step, loss=0.07470, avg_loss=0.07474]\n",
      "Step 516306  [5.364 sec/step, loss=0.06663, avg_loss=0.07464]\n",
      "Step 516307  [5.386 sec/step, loss=0.07681, avg_loss=0.07467]\n",
      "Step 516308  [5.375 sec/step, loss=0.07430, avg_loss=0.07465]\n",
      "Step 516309  [5.382 sec/step, loss=0.07511, avg_loss=0.07465]\n",
      "Step 516310  [5.388 sec/step, loss=0.07499, avg_loss=0.07467]\n",
      "Step 516311  [5.392 sec/step, loss=0.07674, avg_loss=0.07468]\n",
      "Step 516312  [5.382 sec/step, loss=0.07524, avg_loss=0.07466]\n",
      "Step 516313  [5.379 sec/step, loss=0.07382, avg_loss=0.07467]\n",
      "Step 516314  [5.369 sec/step, loss=0.07467, avg_loss=0.07466]\n",
      "Step 516315  [5.344 sec/step, loss=0.07710, avg_loss=0.07470]\n",
      "Step 516316  [5.381 sec/step, loss=0.07325, avg_loss=0.07471]\n",
      "Step 516317  [5.395 sec/step, loss=0.07288, avg_loss=0.07476]\n",
      "Step 516318  [5.392 sec/step, loss=0.07437, avg_loss=0.07477]\n",
      "Step 516319  [5.399 sec/step, loss=0.07744, avg_loss=0.07478]\n",
      "Step 516320  [5.395 sec/step, loss=0.07126, avg_loss=0.07474]\n",
      "Step 516321  [5.405 sec/step, loss=0.07681, avg_loss=0.07475]\n",
      "Step 516322  [5.402 sec/step, loss=0.07347, avg_loss=0.07477]\n",
      "Step 516323  [5.356 sec/step, loss=0.07461, avg_loss=0.07485]\n",
      "Step 516324  [5.346 sec/step, loss=0.07456, avg_loss=0.07482]\n",
      "Generated 32 batches of size 32 in 2.379 sec\n",
      "Step 516325  [5.346 sec/step, loss=0.07659, avg_loss=0.07482]\n",
      "Step 516326  [5.387 sec/step, loss=0.06643, avg_loss=0.07472]\n",
      "Step 516327  [5.386 sec/step, loss=0.07603, avg_loss=0.07471]\n",
      "Step 516328  [5.389 sec/step, loss=0.07577, avg_loss=0.07471]\n",
      "Step 516329  [5.407 sec/step, loss=0.07425, avg_loss=0.07469]\n",
      "Step 516330  [5.388 sec/step, loss=0.07206, avg_loss=0.07465]\n",
      "Step 516331  [5.399 sec/step, loss=0.07518, avg_loss=0.07466]\n",
      "Step 516332  [5.415 sec/step, loss=0.07445, avg_loss=0.07466]\n",
      "Step 516333  [5.409 sec/step, loss=0.07589, avg_loss=0.07466]\n",
      "Step 516334  [5.462 sec/step, loss=0.06624, avg_loss=0.07456]\n",
      "Step 516335  [5.475 sec/step, loss=0.07668, avg_loss=0.07457]\n",
      "Step 516336  [5.465 sec/step, loss=0.07708, avg_loss=0.07460]\n",
      "Step 516337  [5.456 sec/step, loss=0.07506, avg_loss=0.07458]\n",
      "Step 516338  [5.480 sec/step, loss=0.07666, avg_loss=0.07462]\n",
      "Step 516339  [5.471 sec/step, loss=0.07449, avg_loss=0.07460]\n",
      "Step 516340  [5.473 sec/step, loss=0.07383, avg_loss=0.07459]\n",
      "Step 516341  [5.428 sec/step, loss=0.07560, avg_loss=0.07467]\n",
      "Step 516342  [5.450 sec/step, loss=0.07680, avg_loss=0.07469]\n",
      "Step 516343  [5.457 sec/step, loss=0.07490, avg_loss=0.07468]\n",
      "Step 516344  [5.439 sec/step, loss=0.07365, avg_loss=0.07466]\n",
      "Step 516345  [5.478 sec/step, loss=0.07619, avg_loss=0.07469]\n",
      "Step 516346  [5.491 sec/step, loss=0.07406, avg_loss=0.07477]\n",
      "Step 516347  [5.501 sec/step, loss=0.07507, avg_loss=0.07479]\n",
      "Step 516348  [5.498 sec/step, loss=0.07547, avg_loss=0.07481]\n",
      "Step 516349  [5.508 sec/step, loss=0.07369, avg_loss=0.07478]\n",
      "Step 516350  [5.504 sec/step, loss=0.07560, avg_loss=0.07479]\n",
      "Step 516351  [5.514 sec/step, loss=0.07465, avg_loss=0.07478]\n",
      "Step 516352  [5.497 sec/step, loss=0.07723, avg_loss=0.07481]\n",
      "Step 516353  [5.500 sec/step, loss=0.07653, avg_loss=0.07485]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 516354  [5.493 sec/step, loss=0.07290, avg_loss=0.07482]\n",
      "Step 516355  [5.491 sec/step, loss=0.07607, avg_loss=0.07484]\n",
      "Step 516356  [5.468 sec/step, loss=0.07255, avg_loss=0.07480]\n",
      "Generated 32 batches of size 32 in 2.499 sec\n",
      "Step 516357  [5.459 sec/step, loss=0.07465, avg_loss=0.07479]\n",
      "Step 516358  [5.422 sec/step, loss=0.06779, avg_loss=0.07473]\n",
      "Step 516359  [5.420 sec/step, loss=0.07570, avg_loss=0.07472]\n",
      "Step 516360  [5.426 sec/step, loss=0.07425, avg_loss=0.07470]\n",
      "Step 516361  [5.418 sec/step, loss=0.07509, avg_loss=0.07468]\n",
      "Step 516362  [5.433 sec/step, loss=0.07738, avg_loss=0.07473]\n",
      "Step 516363  [5.418 sec/step, loss=0.07351, avg_loss=0.07471]\n",
      "Step 516364  [5.410 sec/step, loss=0.07445, avg_loss=0.07470]\n",
      "Step 516365  [5.416 sec/step, loss=0.07335, avg_loss=0.07469]\n",
      "Step 516366  [5.441 sec/step, loss=0.07593, avg_loss=0.07471]\n",
      "Step 516367  [5.450 sec/step, loss=0.07563, avg_loss=0.07472]\n",
      "Step 516368  [5.451 sec/step, loss=0.07686, avg_loss=0.07472]\n",
      "Step 516369  [5.455 sec/step, loss=0.07272, avg_loss=0.07472]\n",
      "Step 516370  [5.453 sec/step, loss=0.07456, avg_loss=0.07471]\n",
      "Step 516371  [5.435 sec/step, loss=0.07097, avg_loss=0.07465]\n",
      "Step 516372  [5.440 sec/step, loss=0.07667, avg_loss=0.07466]\n",
      "Step 516373  [5.488 sec/step, loss=0.06678, avg_loss=0.07456]\n",
      "Step 516374  [5.481 sec/step, loss=0.07498, avg_loss=0.07455]\n",
      "Step 516375  [5.474 sec/step, loss=0.07446, avg_loss=0.07452]\n",
      "Step 516376  [5.484 sec/step, loss=0.07680, avg_loss=0.07454]\n",
      "Step 516377  [5.473 sec/step, loss=0.07342, avg_loss=0.07453]\n",
      "Step 516378  [5.490 sec/step, loss=0.07612, avg_loss=0.07454]\n",
      "Step 516379  [5.483 sec/step, loss=0.07460, avg_loss=0.07453]\n",
      "Step 516380  [5.492 sec/step, loss=0.07562, avg_loss=0.07457]\n",
      "Step 516381  [5.466 sec/step, loss=0.06564, avg_loss=0.07445]\n",
      "Step 516382  [5.464 sec/step, loss=0.07603, avg_loss=0.07447]\n",
      "Step 516383  [5.480 sec/step, loss=0.07693, avg_loss=0.07451]\n",
      "Step 516384  [5.417 sec/step, loss=0.07227, avg_loss=0.07457]\n",
      "Step 516385  [5.432 sec/step, loss=0.07547, avg_loss=0.07459]\n",
      "Step 516386  [5.425 sec/step, loss=0.07341, avg_loss=0.07456]\n",
      "Step 516387  [5.451 sec/step, loss=0.07621, avg_loss=0.07459]\n",
      "Step 516388  [5.454 sec/step, loss=0.07542, avg_loss=0.07461]\n",
      "Generated 32 batches of size 32 in 2.401 sec\n",
      "Step 516389  [5.465 sec/step, loss=0.07544, avg_loss=0.07458]\n",
      "Step 516390  [5.457 sec/step, loss=0.07379, avg_loss=0.07456]\n",
      "Step 516391  [5.449 sec/step, loss=0.07730, avg_loss=0.07456]\n",
      "Step 516392  [5.440 sec/step, loss=0.07297, avg_loss=0.07455]\n",
      "Step 516393  [5.443 sec/step, loss=0.07262, avg_loss=0.07453]\n",
      "Step 516394  [5.465 sec/step, loss=0.07355, avg_loss=0.07449]\n",
      "Step 516395  [5.440 sec/step, loss=0.07512, avg_loss=0.07451]\n",
      "Step 516396  [5.435 sec/step, loss=0.07427, avg_loss=0.07449]\n",
      "Step 516397  [5.433 sec/step, loss=0.07571, avg_loss=0.07448]\n",
      "Step 516398  [5.438 sec/step, loss=0.07639, avg_loss=0.07449]\n",
      "Step 516399  [5.492 sec/step, loss=0.06702, avg_loss=0.07442]\n",
      "Step 516400  [5.493 sec/step, loss=0.07359, avg_loss=0.07440]\n",
      "Writing summary at step: 516400\n",
      "Step 516401  [5.488 sec/step, loss=0.07589, avg_loss=0.07440]\n",
      "Step 516402  [5.481 sec/step, loss=0.07472, avg_loss=0.07439]\n",
      "Step 516403  [5.476 sec/step, loss=0.07486, avg_loss=0.07439]\n",
      "Step 516404  [5.461 sec/step, loss=0.06650, avg_loss=0.07430]\n",
      "Step 516405  [5.486 sec/step, loss=0.07615, avg_loss=0.07431]\n",
      "Step 516406  [5.506 sec/step, loss=0.07617, avg_loss=0.07441]\n",
      "Step 516407  [5.499 sec/step, loss=0.07517, avg_loss=0.07439]\n",
      "Step 516408  [5.502 sec/step, loss=0.07599, avg_loss=0.07441]\n",
      "Step 516409  [5.486 sec/step, loss=0.07292, avg_loss=0.07439]\n",
      "Step 516410  [5.491 sec/step, loss=0.07503, avg_loss=0.07439]\n",
      "Step 516411  [5.502 sec/step, loss=0.07354, avg_loss=0.07436]\n",
      "Step 516412  [5.496 sec/step, loss=0.07466, avg_loss=0.07435]\n",
      "Step 516413  [5.505 sec/step, loss=0.07194, avg_loss=0.07433]\n",
      "Step 516414  [5.500 sec/step, loss=0.07177, avg_loss=0.07430]\n",
      "Step 516415  [5.495 sec/step, loss=0.07592, avg_loss=0.07429]\n",
      "Step 516416  [5.464 sec/step, loss=0.07428, avg_loss=0.07430]\n",
      "Step 516417  [5.480 sec/step, loss=0.07697, avg_loss=0.07434]\n",
      "Step 516418  [5.482 sec/step, loss=0.07551, avg_loss=0.07435]\n",
      "Step 516419  [5.502 sec/step, loss=0.07395, avg_loss=0.07432]\n",
      "Generated 32 batches of size 32 in 2.402 sec\n",
      "Step 516420  [5.526 sec/step, loss=0.07699, avg_loss=0.07437]\n",
      "Step 516421  [5.520 sec/step, loss=0.07563, avg_loss=0.07436]\n",
      "Step 516422  [5.527 sec/step, loss=0.07490, avg_loss=0.07438]\n",
      "Step 516423  [5.516 sec/step, loss=0.07376, avg_loss=0.07437]\n",
      "Step 516424  [5.532 sec/step, loss=0.07629, avg_loss=0.07439]\n",
      "Step 516425  [5.529 sec/step, loss=0.07528, avg_loss=0.07437]\n",
      "Step 516426  [5.465 sec/step, loss=0.07228, avg_loss=0.07443]\n",
      "Step 516427  [5.463 sec/step, loss=0.07556, avg_loss=0.07443]\n",
      "Step 516428  [5.452 sec/step, loss=0.07713, avg_loss=0.07444]\n",
      "Step 516429  [5.435 sec/step, loss=0.07601, avg_loss=0.07446]\n",
      "Step 516430  [5.459 sec/step, loss=0.07564, avg_loss=0.07449]\n",
      "Step 516431  [5.452 sec/step, loss=0.07171, avg_loss=0.07446]\n",
      "Step 516432  [5.452 sec/step, loss=0.07687, avg_loss=0.07448]\n",
      "Step 516433  [5.450 sec/step, loss=0.07589, avg_loss=0.07448]\n",
      "Step 516434  [5.422 sec/step, loss=0.07448, avg_loss=0.07457]\n",
      "Step 516435  [5.411 sec/step, loss=0.07244, avg_loss=0.07452]\n",
      "Step 516436  [5.402 sec/step, loss=0.07523, avg_loss=0.07450]\n",
      "Step 516437  [5.409 sec/step, loss=0.07677, avg_loss=0.07452]\n",
      "Step 516438  [5.413 sec/step, loss=0.07522, avg_loss=0.07451]\n",
      "Step 516439  [5.425 sec/step, loss=0.07310, avg_loss=0.07449]\n",
      "Step 516440  [5.425 sec/step, loss=0.07606, avg_loss=0.07452]\n",
      "Step 516441  [5.411 sec/step, loss=0.07465, avg_loss=0.07451]\n",
      "Step 516442  [5.404 sec/step, loss=0.07527, avg_loss=0.07449]\n",
      "Step 516443  [5.400 sec/step, loss=0.07748, avg_loss=0.07452]\n",
      "Step 516444  [5.396 sec/step, loss=0.07423, avg_loss=0.07452]\n",
      "Step 516445  [5.376 sec/step, loss=0.07530, avg_loss=0.07451]\n",
      "Step 516446  [5.373 sec/step, loss=0.07450, avg_loss=0.07452]\n",
      "Step 516447  [5.372 sec/step, loss=0.07533, avg_loss=0.07452]\n",
      "Step 516448  [5.353 sec/step, loss=0.07189, avg_loss=0.07448]\n",
      "Step 516449  [5.398 sec/step, loss=0.06677, avg_loss=0.07442]\n",
      "Step 516450  [5.388 sec/step, loss=0.07262, avg_loss=0.07439]\n",
      "Step 516451  [5.360 sec/step, loss=0.06634, avg_loss=0.07430]\n",
      "Generated 32 batches of size 32 in 2.356 sec\n",
      "Step 516452  [5.371 sec/step, loss=0.07783, avg_loss=0.07431]\n",
      "Step 516453  [5.364 sec/step, loss=0.07430, avg_loss=0.07429]\n",
      "Step 516454  [5.371 sec/step, loss=0.07305, avg_loss=0.07429]\n",
      "Step 516455  [5.401 sec/step, loss=0.07434, avg_loss=0.07427]\n",
      "Step 516456  [5.401 sec/step, loss=0.07233, avg_loss=0.07427]\n",
      "Step 516457  [5.416 sec/step, loss=0.07604, avg_loss=0.07428]\n",
      "Step 516458  [5.435 sec/step, loss=0.07551, avg_loss=0.07436]\n",
      "Step 516459  [5.441 sec/step, loss=0.07718, avg_loss=0.07437]\n",
      "Step 516460  [5.431 sec/step, loss=0.07649, avg_loss=0.07440]\n",
      "Step 516461  [5.429 sec/step, loss=0.07195, avg_loss=0.07437]\n",
      "Step 516462  [5.418 sec/step, loss=0.07557, avg_loss=0.07435]\n",
      "Step 516463  [5.427 sec/step, loss=0.07657, avg_loss=0.07438]\n",
      "Step 516464  [5.437 sec/step, loss=0.07637, avg_loss=0.07440]\n",
      "Step 516465  [5.435 sec/step, loss=0.07397, avg_loss=0.07440]\n",
      "Step 516466  [5.418 sec/step, loss=0.07526, avg_loss=0.07440]\n",
      "Step 516467  [5.423 sec/step, loss=0.07677, avg_loss=0.07441]\n",
      "Step 516468  [5.398 sec/step, loss=0.07217, avg_loss=0.07436]\n",
      "Step 516469  [5.400 sec/step, loss=0.07499, avg_loss=0.07438]\n",
      "Step 516470  [5.388 sec/step, loss=0.06646, avg_loss=0.07430]\n",
      "Step 516471  [5.401 sec/step, loss=0.07579, avg_loss=0.07435]\n",
      "Step 516472  [5.389 sec/step, loss=0.07455, avg_loss=0.07433]\n",
      "Step 516473  [5.325 sec/step, loss=0.07204, avg_loss=0.07438]\n",
      "Step 516474  [5.343 sec/step, loss=0.07602, avg_loss=0.07439]\n",
      "Step 516475  [5.331 sec/step, loss=0.07311, avg_loss=0.07438]\n",
      "Step 516476  [5.324 sec/step, loss=0.07613, avg_loss=0.07437]\n",
      "Step 516477  [5.342 sec/step, loss=0.07642, avg_loss=0.07440]\n",
      "Step 516478  [5.326 sec/step, loss=0.07408, avg_loss=0.07438]\n",
      "Step 516479  [5.347 sec/step, loss=0.07653, avg_loss=0.07440]\n",
      "Step 516480  [5.363 sec/step, loss=0.07579, avg_loss=0.07440]\n",
      "Step 516481  [5.384 sec/step, loss=0.07480, avg_loss=0.07450]\n",
      "Step 516482  [5.414 sec/step, loss=0.07269, avg_loss=0.07446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 516483  [5.408 sec/step, loss=0.07371, avg_loss=0.07443]\n",
      "Generated 32 batches of size 32 in 2.397 sec\n",
      "Step 516484  [5.434 sec/step, loss=0.07778, avg_loss=0.07448]\n",
      "Step 516485  [5.436 sec/step, loss=0.07628, avg_loss=0.07449]\n",
      "Step 516486  [5.446 sec/step, loss=0.07599, avg_loss=0.07452]\n",
      "Step 516487  [5.435 sec/step, loss=0.07363, avg_loss=0.07449]\n",
      "Step 516488  [5.489 sec/step, loss=0.06612, avg_loss=0.07440]\n",
      "Step 516489  [5.479 sec/step, loss=0.07671, avg_loss=0.07441]\n",
      "Step 516490  [5.468 sec/step, loss=0.07556, avg_loss=0.07443]\n",
      "Step 516491  [5.454 sec/step, loss=0.07497, avg_loss=0.07441]\n",
      "Step 516492  [5.462 sec/step, loss=0.07671, avg_loss=0.07444]\n",
      "Step 516493  [5.487 sec/step, loss=0.07379, avg_loss=0.07446]\n",
      "Step 516494  [5.466 sec/step, loss=0.07236, avg_loss=0.07444]\n",
      "Step 516495  [5.465 sec/step, loss=0.07456, avg_loss=0.07444]\n",
      "Step 516496  [5.457 sec/step, loss=0.07235, avg_loss=0.07442]\n",
      "Step 516497  [5.453 sec/step, loss=0.07401, avg_loss=0.07440]\n",
      "Step 516498  [5.455 sec/step, loss=0.07630, avg_loss=0.07440]\n",
      "Step 516499  [5.408 sec/step, loss=0.07621, avg_loss=0.07449]\n",
      "Step 516500  [5.403 sec/step, loss=0.07580, avg_loss=0.07452]\n",
      "Writing summary at step: 516500\n",
      "Step 516501  [5.406 sec/step, loss=0.07595, avg_loss=0.07452]\n",
      "Step 516502  [5.416 sec/step, loss=0.07500, avg_loss=0.07452]\n",
      "Step 516503  [5.421 sec/step, loss=0.07543, avg_loss=0.07452]\n",
      "Step 516504  [5.419 sec/step, loss=0.06604, avg_loss=0.07452]\n",
      "Step 516505  [5.396 sec/step, loss=0.07445, avg_loss=0.07450]\n",
      "Step 516506  [5.394 sec/step, loss=0.07585, avg_loss=0.07450]\n",
      "Step 516507  [5.384 sec/step, loss=0.07335, avg_loss=0.07448]\n",
      "Step 516508  [5.375 sec/step, loss=0.07208, avg_loss=0.07444]\n",
      "Step 516509  [5.382 sec/step, loss=0.07725, avg_loss=0.07449]\n",
      "Step 516510  [5.384 sec/step, loss=0.07513, avg_loss=0.07449]\n",
      "Step 516511  [5.361 sec/step, loss=0.07599, avg_loss=0.07451]\n",
      "Step 516512  [5.361 sec/step, loss=0.07331, avg_loss=0.07450]\n",
      "Step 516513  [5.386 sec/step, loss=0.07405, avg_loss=0.07452]\n",
      "Step 516514  [5.408 sec/step, loss=0.07615, avg_loss=0.07456]\n",
      "Generated 32 batches of size 32 in 2.392 sec\n",
      "Step 516515  [5.429 sec/step, loss=0.07672, avg_loss=0.07457]\n",
      "Step 516516  [5.484 sec/step, loss=0.06713, avg_loss=0.07450]\n",
      "Step 516517  [5.484 sec/step, loss=0.07555, avg_loss=0.07448]\n",
      "Step 516518  [5.477 sec/step, loss=0.07584, avg_loss=0.07449]\n",
      "Step 516519  [5.443 sec/step, loss=0.07418, avg_loss=0.07449]\n",
      "Step 516520  [5.420 sec/step, loss=0.07474, avg_loss=0.07447]\n",
      "Step 516521  [5.423 sec/step, loss=0.07701, avg_loss=0.07448]\n",
      "Step 516522  [5.424 sec/step, loss=0.07257, avg_loss=0.07446]\n",
      "Step 516523  [5.440 sec/step, loss=0.07651, avg_loss=0.07449]\n",
      "Step 516524  [5.430 sec/step, loss=0.07521, avg_loss=0.07448]\n",
      "Step 516525  [5.450 sec/step, loss=0.07342, avg_loss=0.07446]\n",
      "Step 516526  [5.478 sec/step, loss=0.07606, avg_loss=0.07449]\n",
      "Step 516527  [5.463 sec/step, loss=0.07252, avg_loss=0.07446]\n",
      "Step 516528  [5.454 sec/step, loss=0.07480, avg_loss=0.07444]\n",
      "Step 516529  [5.467 sec/step, loss=0.07455, avg_loss=0.07443]\n",
      "Step 516530  [5.459 sec/step, loss=0.07613, avg_loss=0.07443]\n",
      "Step 516531  [5.512 sec/step, loss=0.06662, avg_loss=0.07438]\n",
      "Step 516532  [5.511 sec/step, loss=0.07687, avg_loss=0.07438]\n",
      "Step 516533  [5.520 sec/step, loss=0.07588, avg_loss=0.07438]\n",
      "Step 516534  [5.498 sec/step, loss=0.07520, avg_loss=0.07439]\n",
      "Step 516535  [5.496 sec/step, loss=0.07186, avg_loss=0.07438]\n",
      "Step 516536  [5.490 sec/step, loss=0.07413, avg_loss=0.07437]\n",
      "Step 516537  [5.470 sec/step, loss=0.07495, avg_loss=0.07435]\n",
      "Step 516538  [5.439 sec/step, loss=0.06611, avg_loss=0.07426]\n",
      "Step 516539  [5.447 sec/step, loss=0.07652, avg_loss=0.07430]\n",
      "Step 516540  [5.448 sec/step, loss=0.07653, avg_loss=0.07430]\n",
      "Step 516541  [5.448 sec/step, loss=0.07186, avg_loss=0.07427]\n",
      "Step 516542  [5.452 sec/step, loss=0.07691, avg_loss=0.07429]\n",
      "Step 516543  [5.462 sec/step, loss=0.07669, avg_loss=0.07428]\n",
      "Step 516544  [5.481 sec/step, loss=0.07684, avg_loss=0.07431]\n",
      "Step 516545  [5.489 sec/step, loss=0.07658, avg_loss=0.07432]\n",
      "Step 516546  [5.498 sec/step, loss=0.07273, avg_loss=0.07430]\n",
      "Generated 32 batches of size 32 in 2.382 sec\n",
      "Step 516547  [5.509 sec/step, loss=0.07526, avg_loss=0.07430]\n",
      "Step 516548  [5.522 sec/step, loss=0.07606, avg_loss=0.07434]\n",
      "Step 516549  [5.462 sec/step, loss=0.07510, avg_loss=0.07443]\n",
      "Step 516550  [5.466 sec/step, loss=0.07349, avg_loss=0.07443]\n",
      "Step 516551  [5.479 sec/step, loss=0.07385, avg_loss=0.07451]\n",
      "Step 516552  [5.466 sec/step, loss=0.07527, avg_loss=0.07448]\n",
      "Step 516553  [5.466 sec/step, loss=0.07602, avg_loss=0.07450]\n",
      "Step 516554  [5.458 sec/step, loss=0.07520, avg_loss=0.07452]\n",
      "Step 516555  [5.439 sec/step, loss=0.07676, avg_loss=0.07455]\n",
      "Step 516556  [5.454 sec/step, loss=0.07505, avg_loss=0.07457]\n",
      "Step 516557  [5.469 sec/step, loss=0.07604, avg_loss=0.07457]\n",
      "Step 516558  [5.479 sec/step, loss=0.07708, avg_loss=0.07459]\n",
      "Step 516559  [5.478 sec/step, loss=0.07720, avg_loss=0.07459]\n",
      "Step 516560  [5.518 sec/step, loss=0.06619, avg_loss=0.07449]\n",
      "Step 516561  [5.532 sec/step, loss=0.07716, avg_loss=0.07454]\n",
      "Step 516562  [5.546 sec/step, loss=0.07557, avg_loss=0.07454]\n",
      "Step 516563  [5.552 sec/step, loss=0.07500, avg_loss=0.07452]\n",
      "Step 516564  [5.556 sec/step, loss=0.07368, avg_loss=0.07450]\n",
      "Step 516565  [5.569 sec/step, loss=0.07571, avg_loss=0.07451]\n",
      "Step 516566  [5.569 sec/step, loss=0.07582, avg_loss=0.07452]\n",
      "Step 516567  [5.553 sec/step, loss=0.07377, avg_loss=0.07449]\n",
      "Step 516568  [5.561 sec/step, loss=0.07555, avg_loss=0.07452]\n",
      "Step 516569  [5.550 sec/step, loss=0.07450, avg_loss=0.07452]\n",
      "Step 516570  [5.550 sec/step, loss=0.06646, avg_loss=0.07452]\n",
      "Step 516571  [5.562 sec/step, loss=0.07625, avg_loss=0.07452]\n",
      "Step 516572  [5.561 sec/step, loss=0.07074, avg_loss=0.07449]\n",
      "Step 516573  [5.589 sec/step, loss=0.07415, avg_loss=0.07451]\n",
      "Step 516574  [5.574 sec/step, loss=0.07263, avg_loss=0.07447]\n",
      "Step 516575  [5.589 sec/step, loss=0.07688, avg_loss=0.07451]\n",
      "Step 516576  [5.596 sec/step, loss=0.07719, avg_loss=0.07452]\n",
      "Step 516577  [5.588 sec/step, loss=0.07655, avg_loss=0.07452]\n",
      "Step 516578  [5.592 sec/step, loss=0.07436, avg_loss=0.07452]\n",
      "Generated 32 batches of size 32 in 2.798 sec\n",
      "Step 516579  [5.572 sec/step, loss=0.07166, avg_loss=0.07448]\n",
      "Step 516580  [5.545 sec/step, loss=0.07523, avg_loss=0.07447]\n",
      "Step 516581  [5.542 sec/step, loss=0.07359, avg_loss=0.07446]\n",
      "Step 516582  [5.515 sec/step, loss=0.07477, avg_loss=0.07448]\n",
      "Step 516583  [5.515 sec/step, loss=0.07502, avg_loss=0.07449]\n",
      "Step 516584  [5.501 sec/step, loss=0.07599, avg_loss=0.07447]\n",
      "Step 516585  [5.507 sec/step, loss=0.07485, avg_loss=0.07446]\n",
      "Step 516586  [5.492 sec/step, loss=0.07545, avg_loss=0.07445]\n",
      "Step 516587  [5.489 sec/step, loss=0.07137, avg_loss=0.07443]\n",
      "Step 516588  [5.439 sec/step, loss=0.07290, avg_loss=0.07450]\n",
      "Step 516589  [5.429 sec/step, loss=0.07431, avg_loss=0.07448]\n",
      "Step 516590  [5.443 sec/step, loss=0.07490, avg_loss=0.07447]\n",
      "Step 516591  [5.455 sec/step, loss=0.07514, avg_loss=0.07447]\n",
      "Step 516592  [5.459 sec/step, loss=0.07739, avg_loss=0.07448]\n",
      "Step 516593  [5.425 sec/step, loss=0.07291, avg_loss=0.07447]\n",
      "Step 516594  [5.416 sec/step, loss=0.07563, avg_loss=0.07450]\n",
      "Step 516595  [5.413 sec/step, loss=0.07611, avg_loss=0.07452]\n",
      "Step 516596  [5.437 sec/step, loss=0.07428, avg_loss=0.07454]\n",
      "Step 516597  [5.441 sec/step, loss=0.07639, avg_loss=0.07456]\n",
      "Step 516598  [5.429 sec/step, loss=0.07655, avg_loss=0.07456]\n",
      "Step 516599  [5.441 sec/step, loss=0.07681, avg_loss=0.07457]\n",
      "Step 516600  [5.444 sec/step, loss=0.07714, avg_loss=0.07458]\n",
      "Writing summary at step: 516600\n",
      "Step 516601  [5.434 sec/step, loss=0.07404, avg_loss=0.07456]\n",
      "Step 516602  [5.427 sec/step, loss=0.07440, avg_loss=0.07456]\n",
      "Step 516603  [5.440 sec/step, loss=0.07668, avg_loss=0.07457]\n",
      "Step 516604  [5.458 sec/step, loss=0.07559, avg_loss=0.07466]\n",
      "Step 516605  [5.458 sec/step, loss=0.07511, avg_loss=0.07467]\n",
      "Step 516606  [5.463 sec/step, loss=0.07626, avg_loss=0.07468]\n",
      "Step 516607  [5.495 sec/step, loss=0.07343, avg_loss=0.07468]\n",
      "Step 516608  [5.510 sec/step, loss=0.07608, avg_loss=0.07472]\n",
      "Step 516609  [5.493 sec/step, loss=0.07467, avg_loss=0.07469]\n",
      "Generated 32 batches of size 32 in 2.416 sec\n",
      "Step 516610  [5.502 sec/step, loss=0.07679, avg_loss=0.07471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 516611  [5.502 sec/step, loss=0.07263, avg_loss=0.07467]\n",
      "Step 516612  [5.556 sec/step, loss=0.06808, avg_loss=0.07462]\n",
      "Step 516613  [5.539 sec/step, loss=0.07703, avg_loss=0.07465]\n",
      "Step 516614  [5.535 sec/step, loss=0.07566, avg_loss=0.07465]\n",
      "Step 516615  [5.507 sec/step, loss=0.07204, avg_loss=0.07460]\n",
      "Step 516616  [5.438 sec/step, loss=0.06623, avg_loss=0.07459]\n",
      "Step 516617  [5.425 sec/step, loss=0.07271, avg_loss=0.07456]\n",
      "Step 516618  [5.424 sec/step, loss=0.07627, avg_loss=0.07457]\n",
      "Step 516619  [5.441 sec/step, loss=0.07655, avg_loss=0.07459]\n",
      "Step 516620  [5.453 sec/step, loss=0.07599, avg_loss=0.07460]\n",
      "Step 516621  [5.454 sec/step, loss=0.07670, avg_loss=0.07460]\n",
      "Step 516622  [5.450 sec/step, loss=0.07365, avg_loss=0.07461]\n",
      "Step 516623  [5.431 sec/step, loss=0.07483, avg_loss=0.07459]\n",
      "Step 516624  [5.434 sec/step, loss=0.07670, avg_loss=0.07461]\n",
      "Step 516625  [5.419 sec/step, loss=0.07684, avg_loss=0.07464]\n",
      "Step 516626  [5.455 sec/step, loss=0.06639, avg_loss=0.07455]\n",
      "Step 516627  [5.483 sec/step, loss=0.07554, avg_loss=0.07458]\n",
      "Step 516628  [5.481 sec/step, loss=0.07041, avg_loss=0.07453]\n",
      "Step 516629  [5.499 sec/step, loss=0.07369, avg_loss=0.07452]\n",
      "Step 516630  [5.493 sec/step, loss=0.07465, avg_loss=0.07451]\n",
      "Step 516631  [5.442 sec/step, loss=0.07504, avg_loss=0.07459]\n",
      "Step 516632  [5.440 sec/step, loss=0.07472, avg_loss=0.07457]\n",
      "Step 516633  [5.418 sec/step, loss=0.06840, avg_loss=0.07450]\n",
      "Step 516634  [5.423 sec/step, loss=0.07528, avg_loss=0.07450]\n",
      "Step 516635  [5.425 sec/step, loss=0.07596, avg_loss=0.07454]\n",
      "Step 516636  [5.428 sec/step, loss=0.07607, avg_loss=0.07456]\n",
      "Step 516637  [5.436 sec/step, loss=0.07647, avg_loss=0.07457]\n",
      "Step 516638  [5.458 sec/step, loss=0.07506, avg_loss=0.07466]\n",
      "Step 516639  [5.454 sec/step, loss=0.07644, avg_loss=0.07466]\n",
      "Step 516640  [5.454 sec/step, loss=0.07295, avg_loss=0.07463]\n",
      "Step 516641  [5.480 sec/step, loss=0.07589, avg_loss=0.07467]\n",
      "Generated 32 batches of size 32 in 2.523 sec\n",
      "Step 516642  [5.472 sec/step, loss=0.07391, avg_loss=0.07464]\n",
      "Step 516643  [5.472 sec/step, loss=0.07624, avg_loss=0.07463]\n",
      "Step 516644  [5.449 sec/step, loss=0.07233, avg_loss=0.07459]\n",
      "Step 516645  [5.431 sec/step, loss=0.07487, avg_loss=0.07457]\n",
      "Step 516646  [5.434 sec/step, loss=0.07468, avg_loss=0.07459]\n",
      "Step 516647  [5.424 sec/step, loss=0.07568, avg_loss=0.07459]\n",
      "Step 516648  [5.426 sec/step, loss=0.07555, avg_loss=0.07459]\n",
      "Step 516649  [5.422 sec/step, loss=0.07174, avg_loss=0.07455]\n",
      "Step 516650  [5.423 sec/step, loss=0.07190, avg_loss=0.07454]\n",
      "Step 516651  [5.427 sec/step, loss=0.07178, avg_loss=0.07452]\n",
      "Step 516652  [5.414 sec/step, loss=0.07389, avg_loss=0.07450]\n",
      "Step 516653  [5.413 sec/step, loss=0.07404, avg_loss=0.07448]\n",
      "Step 516654  [5.427 sec/step, loss=0.07481, avg_loss=0.07448]\n",
      "Step 516655  [5.422 sec/step, loss=0.07578, avg_loss=0.07447]\n",
      "Step 516656  [5.430 sec/step, loss=0.07642, avg_loss=0.07448]\n",
      "Step 516657  [5.417 sec/step, loss=0.07567, avg_loss=0.07448]\n",
      "Step 516658  [5.411 sec/step, loss=0.07632, avg_loss=0.07447]\n",
      "Step 516659  [5.417 sec/step, loss=0.07647, avg_loss=0.07447]\n",
      "Step 516660  [5.376 sec/step, loss=0.07620, avg_loss=0.07457]\n",
      "Step 516661  [5.366 sec/step, loss=0.07588, avg_loss=0.07455]\n",
      "Step 516662  [5.346 sec/step, loss=0.07585, avg_loss=0.07456]\n",
      "Step 516663  [5.346 sec/step, loss=0.07519, avg_loss=0.07456]\n",
      "Step 516664  [5.332 sec/step, loss=0.07510, avg_loss=0.07457]\n",
      "Step 516665  [5.324 sec/step, loss=0.07343, avg_loss=0.07455]\n",
      "Step 516666  [5.324 sec/step, loss=0.07588, avg_loss=0.07455]\n",
      "Step 516667  [5.349 sec/step, loss=0.07662, avg_loss=0.07458]\n",
      "Step 516668  [5.349 sec/step, loss=0.07345, avg_loss=0.07456]\n",
      "Step 516669  [5.370 sec/step, loss=0.07678, avg_loss=0.07458]\n",
      "Step 516670  [5.368 sec/step, loss=0.06637, avg_loss=0.07458]\n",
      "Step 516671  [5.361 sec/step, loss=0.07720, avg_loss=0.07459]\n",
      "Step 516672  [5.372 sec/step, loss=0.07555, avg_loss=0.07464]\n",
      "Step 516673  [5.364 sec/step, loss=0.07334, avg_loss=0.07463]\n",
      "Generated 32 batches of size 32 in 2.698 sec\n",
      "Step 516674  [5.361 sec/step, loss=0.07454, avg_loss=0.07465]\n",
      "Step 516675  [5.348 sec/step, loss=0.07279, avg_loss=0.07461]\n",
      "Step 516676  [5.353 sec/step, loss=0.07704, avg_loss=0.07460]\n",
      "Step 516677  [5.354 sec/step, loss=0.07301, avg_loss=0.07457]\n",
      "Step 516678  [5.346 sec/step, loss=0.07163, avg_loss=0.07454]\n",
      "Step 516679  [5.341 sec/step, loss=0.07287, avg_loss=0.07455]\n",
      "Step 516680  [5.378 sec/step, loss=0.07368, avg_loss=0.07454]\n",
      "Step 516681  [5.375 sec/step, loss=0.07606, avg_loss=0.07456]\n",
      "Step 516682  [5.425 sec/step, loss=0.06717, avg_loss=0.07449]\n",
      "Step 516683  [5.430 sec/step, loss=0.07532, avg_loss=0.07449]\n",
      "Step 516684  [5.435 sec/step, loss=0.07621, avg_loss=0.07449]\n",
      "Step 516685  [5.450 sec/step, loss=0.07366, avg_loss=0.07448]\n",
      "Step 516686  [5.447 sec/step, loss=0.07545, avg_loss=0.07448]\n",
      "Step 516687  [5.459 sec/step, loss=0.07586, avg_loss=0.07453]\n",
      "Step 516688  [5.465 sec/step, loss=0.07706, avg_loss=0.07457]\n",
      "Step 516689  [5.462 sec/step, loss=0.07560, avg_loss=0.07458]\n",
      "Step 516690  [5.502 sec/step, loss=0.06659, avg_loss=0.07450]\n",
      "Step 516691  [5.494 sec/step, loss=0.07580, avg_loss=0.07450]\n",
      "Step 516692  [5.495 sec/step, loss=0.07663, avg_loss=0.07450]\n",
      "Step 516693  [5.495 sec/step, loss=0.07191, avg_loss=0.07449]\n",
      "Step 516694  [5.513 sec/step, loss=0.07636, avg_loss=0.07449]\n",
      "Step 516695  [5.530 sec/step, loss=0.07469, avg_loss=0.07448]\n",
      "Step 516696  [5.516 sec/step, loss=0.07511, avg_loss=0.07449]\n",
      "Step 516697  [5.507 sec/step, loss=0.07454, avg_loss=0.07447]\n",
      "Step 516698  [5.507 sec/step, loss=0.07549, avg_loss=0.07446]\n",
      "Step 516699  [5.498 sec/step, loss=0.07496, avg_loss=0.07444]\n",
      "Step 516700  [5.493 sec/step, loss=0.07561, avg_loss=0.07442]\n",
      "Writing summary at step: 516700\n",
      "Step 516701  [5.495 sec/step, loss=0.07192, avg_loss=0.07440]\n",
      "Step 516702  [5.504 sec/step, loss=0.07625, avg_loss=0.07442]\n",
      "Step 516703  [5.511 sec/step, loss=0.07598, avg_loss=0.07441]\n",
      "Step 516704  [5.501 sec/step, loss=0.07209, avg_loss=0.07438]\n",
      "Generated 32 batches of size 32 in 2.909 sec\n",
      "Step 516705  [5.494 sec/step, loss=0.06539, avg_loss=0.07428]\n",
      "Step 516706  [5.499 sec/step, loss=0.07500, avg_loss=0.07427]\n",
      "Step 516707  [5.468 sec/step, loss=0.07298, avg_loss=0.07427]\n",
      "Step 516708  [5.470 sec/step, loss=0.07706, avg_loss=0.07428]\n",
      "Step 516709  [5.472 sec/step, loss=0.07173, avg_loss=0.07425]\n",
      "Step 516710  [5.461 sec/step, loss=0.07665, avg_loss=0.07424]\n",
      "Step 516711  [5.458 sec/step, loss=0.07506, avg_loss=0.07427]\n",
      "Step 516712  [5.409 sec/step, loss=0.07651, avg_loss=0.07435]\n",
      "Step 516713  [5.399 sec/step, loss=0.07502, avg_loss=0.07433]\n",
      "Step 516714  [5.389 sec/step, loss=0.07621, avg_loss=0.07434]\n",
      "Step 516715  [5.406 sec/step, loss=0.07452, avg_loss=0.07436]\n",
      "Step 516716  [5.436 sec/step, loss=0.07640, avg_loss=0.07447]\n",
      "Step 516717  [5.447 sec/step, loss=0.07692, avg_loss=0.07451]\n",
      "Step 516718  [5.447 sec/step, loss=0.07552, avg_loss=0.07450]\n",
      "Step 516719  [5.446 sec/step, loss=0.07661, avg_loss=0.07450]\n",
      "Step 516720  [5.455 sec/step, loss=0.07692, avg_loss=0.07451]\n",
      "Step 516721  [5.434 sec/step, loss=0.07220, avg_loss=0.07446]\n",
      "Step 516722  [5.456 sec/step, loss=0.07372, avg_loss=0.07447]\n",
      "Step 516723  [5.466 sec/step, loss=0.07296, avg_loss=0.07445]\n",
      "Step 516724  [5.465 sec/step, loss=0.07463, avg_loss=0.07443]\n",
      "Step 516725  [5.479 sec/step, loss=0.07388, avg_loss=0.07440]\n",
      "Step 516726  [5.429 sec/step, loss=0.07491, avg_loss=0.07448]\n",
      "Step 516727  [5.421 sec/step, loss=0.07477, avg_loss=0.07447]\n",
      "Step 516728  [5.422 sec/step, loss=0.07313, avg_loss=0.07450]\n",
      "Step 516729  [5.396 sec/step, loss=0.07667, avg_loss=0.07453]\n",
      "Step 516730  [5.416 sec/step, loss=0.07729, avg_loss=0.07456]\n",
      "Step 516731  [5.466 sec/step, loss=0.06723, avg_loss=0.07448]\n",
      "Step 516732  [5.451 sec/step, loss=0.07355, avg_loss=0.07447]\n",
      "Step 516733  [5.467 sec/step, loss=0.07207, avg_loss=0.07450]\n",
      "Step 516734  [5.464 sec/step, loss=0.07542, avg_loss=0.07451]\n",
      "Step 516735  [5.464 sec/step, loss=0.07556, avg_loss=0.07450]\n",
      "Step 516736  [5.460 sec/step, loss=0.07439, avg_loss=0.07448]\n",
      "Generated 32 batches of size 32 in 2.367 sec\n",
      "Step 516737  [5.469 sec/step, loss=0.07550, avg_loss=0.07447]\n",
      "Step 516738  [5.476 sec/step, loss=0.07370, avg_loss=0.07446]\n",
      "Step 516739  [5.450 sec/step, loss=0.06793, avg_loss=0.07438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 516740  [5.434 sec/step, loss=0.07323, avg_loss=0.07438]\n",
      "Step 516741  [5.414 sec/step, loss=0.07391, avg_loss=0.07436]\n",
      "Step 516742  [5.418 sec/step, loss=0.07586, avg_loss=0.07438]\n",
      "Step 516743  [5.389 sec/step, loss=0.07491, avg_loss=0.07437]\n",
      "Step 516744  [5.398 sec/step, loss=0.07598, avg_loss=0.07440]\n",
      "Step 516745  [5.412 sec/step, loss=0.07659, avg_loss=0.07442]\n",
      "Step 516746  [5.410 sec/step, loss=0.07515, avg_loss=0.07442]\n",
      "Step 516747  [5.410 sec/step, loss=0.07418, avg_loss=0.07441]\n",
      "Step 516748  [5.405 sec/step, loss=0.07229, avg_loss=0.07438]\n",
      "Step 516749  [5.418 sec/step, loss=0.07498, avg_loss=0.07441]\n",
      "Step 516750  [5.420 sec/step, loss=0.07622, avg_loss=0.07445]\n",
      "Step 516751  [5.447 sec/step, loss=0.07331, avg_loss=0.07447]\n",
      "Step 516752  [5.459 sec/step, loss=0.07630, avg_loss=0.07449]\n",
      "Step 516753  [5.454 sec/step, loss=0.07520, avg_loss=0.07450]\n",
      "Step 516754  [5.446 sec/step, loss=0.07588, avg_loss=0.07451]\n",
      "Step 516755  [5.456 sec/step, loss=0.07648, avg_loss=0.07452]\n",
      "Step 516756  [5.444 sec/step, loss=0.07594, avg_loss=0.07452]\n",
      "Step 516757  [5.485 sec/step, loss=0.06706, avg_loss=0.07443]\n",
      "Step 516758  [5.474 sec/step, loss=0.07504, avg_loss=0.07442]\n",
      "Step 516759  [5.455 sec/step, loss=0.07455, avg_loss=0.07440]\n",
      "Step 516760  [5.455 sec/step, loss=0.07770, avg_loss=0.07441]\n",
      "Step 516761  [5.472 sec/step, loss=0.07359, avg_loss=0.07439]\n",
      "Step 516762  [5.479 sec/step, loss=0.07535, avg_loss=0.07438]\n",
      "Step 516763  [5.476 sec/step, loss=0.07656, avg_loss=0.07440]\n",
      "Step 516764  [5.469 sec/step, loss=0.07250, avg_loss=0.07437]\n",
      "Step 516765  [5.479 sec/step, loss=0.07724, avg_loss=0.07441]\n",
      "Step 516766  [5.492 sec/step, loss=0.07632, avg_loss=0.07442]\n",
      "Step 516767  [5.485 sec/step, loss=0.07679, avg_loss=0.07442]\n",
      "Step 516768  [5.487 sec/step, loss=0.07245, avg_loss=0.07441]\n",
      "Generated 32 batches of size 32 in 2.437 sec\n",
      "Step 516769  [5.482 sec/step, loss=0.07554, avg_loss=0.07439]\n",
      "Step 516770  [5.484 sec/step, loss=0.06599, avg_loss=0.07439]\n",
      "Step 516771  [5.488 sec/step, loss=0.07478, avg_loss=0.07437]\n",
      "Step 516772  [5.477 sec/step, loss=0.07157, avg_loss=0.07433]\n",
      "Step 516773  [5.481 sec/step, loss=0.07572, avg_loss=0.07435]\n",
      "Step 516774  [5.481 sec/step, loss=0.07466, avg_loss=0.07435]\n",
      "Step 516775  [5.479 sec/step, loss=0.07440, avg_loss=0.07437]\n",
      "Step 516776  [5.476 sec/step, loss=0.07709, avg_loss=0.07437]\n",
      "Step 516777  [5.469 sec/step, loss=0.07615, avg_loss=0.07440]\n",
      "Step 516778  [5.486 sec/step, loss=0.07569, avg_loss=0.07444]\n",
      "Step 516779  [5.497 sec/step, loss=0.07271, avg_loss=0.07444]\n",
      "Step 516780  [5.467 sec/step, loss=0.07597, avg_loss=0.07446]\n",
      "Step 516781  [5.497 sec/step, loss=0.07412, avg_loss=0.07444]\n",
      "Step 516782  [5.431 sec/step, loss=0.06704, avg_loss=0.07444]\n",
      "Step 516783  [5.471 sec/step, loss=0.06696, avg_loss=0.07436]\n",
      "Step 516784  [5.478 sec/step, loss=0.07533, avg_loss=0.07435]\n",
      "Step 516785  [5.449 sec/step, loss=0.07593, avg_loss=0.07437]\n",
      "Step 516786  [5.448 sec/step, loss=0.07285, avg_loss=0.07435]\n",
      "Step 516787  [5.442 sec/step, loss=0.07583, avg_loss=0.07434]\n",
      "Step 516788  [5.420 sec/step, loss=0.07173, avg_loss=0.07429]\n",
      "Step 516789  [5.411 sec/step, loss=0.07201, avg_loss=0.07426]\n",
      "Step 516790  [5.375 sec/step, loss=0.07558, avg_loss=0.07435]\n",
      "Step 516791  [5.369 sec/step, loss=0.07490, avg_loss=0.07434]\n",
      "Step 516792  [5.369 sec/step, loss=0.07710, avg_loss=0.07434]\n",
      "Step 516793  [5.394 sec/step, loss=0.07648, avg_loss=0.07439]\n",
      "Step 516794  [5.379 sec/step, loss=0.07505, avg_loss=0.07437]\n",
      "Step 516795  [5.355 sec/step, loss=0.07481, avg_loss=0.07438]\n",
      "Step 516796  [5.362 sec/step, loss=0.07683, avg_loss=0.07439]\n",
      "Step 516797  [5.380 sec/step, loss=0.07430, avg_loss=0.07439]\n",
      "Step 516798  [5.383 sec/step, loss=0.07555, avg_loss=0.07439]\n",
      "Step 516799  [5.371 sec/step, loss=0.07482, avg_loss=0.07439]\n",
      "Step 516800  [5.368 sec/step, loss=0.07655, avg_loss=0.07440]\n",
      "Writing summary at step: 516800\n",
      "Generated 32 batches of size 32 in 2.382 sec\n",
      "Step 516801  [5.373 sec/step, loss=0.07631, avg_loss=0.07444]\n",
      "Step 516802  [5.373 sec/step, loss=0.07216, avg_loss=0.07440]\n",
      "Step 516803  [5.364 sec/step, loss=0.07686, avg_loss=0.07441]\n",
      "Step 516804  [5.376 sec/step, loss=0.07575, avg_loss=0.07445]\n",
      "Step 516805  [5.386 sec/step, loss=0.07419, avg_loss=0.07453]\n",
      "Step 516806  [5.389 sec/step, loss=0.07653, avg_loss=0.07455]\n",
      "Step 516807  [5.413 sec/step, loss=0.07662, avg_loss=0.07459]\n",
      "Step 516808  [5.412 sec/step, loss=0.07653, avg_loss=0.07458]\n",
      "Step 516809  [5.430 sec/step, loss=0.07436, avg_loss=0.07461]\n",
      "Step 516810  [5.412 sec/step, loss=0.07497, avg_loss=0.07459]\n",
      "Step 516811  [5.424 sec/step, loss=0.07572, avg_loss=0.07460]\n",
      "Step 516812  [5.418 sec/step, loss=0.07413, avg_loss=0.07457]\n",
      "Step 516813  [5.436 sec/step, loss=0.07587, avg_loss=0.07458]\n",
      "Step 516814  [5.424 sec/step, loss=0.06745, avg_loss=0.07449]\n",
      "Step 516815  [5.414 sec/step, loss=0.07261, avg_loss=0.07448]\n",
      "Step 516816  [5.414 sec/step, loss=0.07445, avg_loss=0.07446]\n",
      "Step 516817  [5.411 sec/step, loss=0.07709, avg_loss=0.07446]\n",
      "Step 516818  [5.420 sec/step, loss=0.07673, avg_loss=0.07447]\n",
      "Step 516819  [5.410 sec/step, loss=0.07558, avg_loss=0.07446]\n",
      "Step 516820  [5.403 sec/step, loss=0.07568, avg_loss=0.07445]\n",
      "Step 516821  [5.422 sec/step, loss=0.07675, avg_loss=0.07449]\n",
      "Step 516822  [5.427 sec/step, loss=0.07551, avg_loss=0.07451]\n",
      "Step 516823  [5.428 sec/step, loss=0.07550, avg_loss=0.07454]\n",
      "Step 516824  [5.423 sec/step, loss=0.07600, avg_loss=0.07455]\n",
      "Step 516825  [5.402 sec/step, loss=0.07511, avg_loss=0.07456]\n",
      "Step 516826  [5.410 sec/step, loss=0.07543, avg_loss=0.07457]\n",
      "Step 516827  [5.419 sec/step, loss=0.07666, avg_loss=0.07459]\n",
      "Step 516828  [5.437 sec/step, loss=0.07679, avg_loss=0.07462]\n",
      "Step 516829  [5.433 sec/step, loss=0.07594, avg_loss=0.07461]\n",
      "Step 516830  [5.407 sec/step, loss=0.07232, avg_loss=0.07457]\n",
      "Step 516831  [5.348 sec/step, loss=0.07422, avg_loss=0.07464]\n",
      "Generated 32 batches of size 32 in 2.493 sec\n",
      "Step 516832  [5.351 sec/step, loss=0.07165, avg_loss=0.07462]\n",
      "Step 516833  [5.338 sec/step, loss=0.07273, avg_loss=0.07462]\n",
      "Step 516834  [5.337 sec/step, loss=0.07609, avg_loss=0.07463]\n",
      "Step 516835  [5.387 sec/step, loss=0.06672, avg_loss=0.07454]\n",
      "Step 516836  [5.389 sec/step, loss=0.07383, avg_loss=0.07454]\n",
      "Step 516837  [5.389 sec/step, loss=0.07649, avg_loss=0.07455]\n",
      "Step 516838  [5.375 sec/step, loss=0.07623, avg_loss=0.07457]\n",
      "Step 516839  [5.393 sec/step, loss=0.07545, avg_loss=0.07465]\n",
      "Step 516840  [5.407 sec/step, loss=0.07438, avg_loss=0.07466]\n",
      "Step 516841  [5.401 sec/step, loss=0.07484, avg_loss=0.07467]\n",
      "Step 516842  [5.409 sec/step, loss=0.07716, avg_loss=0.07468]\n",
      "Step 516843  [5.418 sec/step, loss=0.07563, avg_loss=0.07469]\n",
      "Step 516844  [5.415 sec/step, loss=0.07409, avg_loss=0.07467]\n",
      "Step 516845  [5.404 sec/step, loss=0.07546, avg_loss=0.07466]\n",
      "Step 516846  [5.392 sec/step, loss=0.07303, avg_loss=0.07464]\n",
      "Step 516847  [5.405 sec/step, loss=0.07623, avg_loss=0.07466]\n",
      "Step 516848  [5.410 sec/step, loss=0.07588, avg_loss=0.07469]\n",
      "Step 516849  [5.423 sec/step, loss=0.07671, avg_loss=0.07471]\n",
      "Step 516850  [5.417 sec/step, loss=0.07291, avg_loss=0.07468]\n",
      "Step 516851  [5.388 sec/step, loss=0.07470, avg_loss=0.07469]\n",
      "Step 516852  [5.392 sec/step, loss=0.07522, avg_loss=0.07468]\n",
      "Step 516853  [5.411 sec/step, loss=0.07675, avg_loss=0.07469]\n",
      "Step 516854  [5.408 sec/step, loss=0.07664, avg_loss=0.07470]\n",
      "Step 516855  [5.387 sec/step, loss=0.07482, avg_loss=0.07469]\n",
      "Step 516856  [5.400 sec/step, loss=0.07527, avg_loss=0.07468]\n",
      "Step 516857  [5.345 sec/step, loss=0.07463, avg_loss=0.07475]\n",
      "Step 516858  [5.349 sec/step, loss=0.07597, avg_loss=0.07476]\n",
      "Step 516859  [5.406 sec/step, loss=0.06663, avg_loss=0.07468]\n",
      "Step 516860  [5.403 sec/step, loss=0.07549, avg_loss=0.07466]\n",
      "Step 516861  [5.390 sec/step, loss=0.07658, avg_loss=0.07469]\n",
      "Step 516862  [5.372 sec/step, loss=0.07213, avg_loss=0.07466]\n",
      "Step 516863  [5.370 sec/step, loss=0.07518, avg_loss=0.07465]\n",
      "Generated 32 batches of size 32 in 2.405 sec\n",
      "Step 516864  [5.416 sec/step, loss=0.07400, avg_loss=0.07466]\n",
      "Step 516865  [5.408 sec/step, loss=0.07610, avg_loss=0.07465]\n",
      "Step 516866  [5.395 sec/step, loss=0.07267, avg_loss=0.07461]\n",
      "Step 516867  [5.373 sec/step, loss=0.07098, avg_loss=0.07456]\n",
      "Step 516868  [5.390 sec/step, loss=0.07622, avg_loss=0.07459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 516869  [5.366 sec/step, loss=0.06555, avg_loss=0.07449]\n",
      "Step 516870  [5.393 sec/step, loss=0.07376, avg_loss=0.07457]\n",
      "Step 516871  [5.386 sec/step, loss=0.07482, avg_loss=0.07457]\n",
      "Step 516872  [5.421 sec/step, loss=0.07433, avg_loss=0.07460]\n",
      "Step 516873  [5.438 sec/step, loss=0.07425, avg_loss=0.07458]\n",
      "Step 516874  [5.430 sec/step, loss=0.07276, avg_loss=0.07457]\n",
      "Step 516875  [5.430 sec/step, loss=0.07313, avg_loss=0.07455]\n",
      "Step 516876  [5.425 sec/step, loss=0.07492, avg_loss=0.07453]\n",
      "Step 516877  [5.443 sec/step, loss=0.07640, avg_loss=0.07453]\n",
      "Step 516878  [5.437 sec/step, loss=0.07520, avg_loss=0.07453]\n",
      "Step 516879  [5.433 sec/step, loss=0.07434, avg_loss=0.07454]\n",
      "Step 516880  [5.438 sec/step, loss=0.07597, avg_loss=0.07455]\n",
      "Step 516881  [5.414 sec/step, loss=0.07612, avg_loss=0.07456]\n",
      "Step 516882  [5.438 sec/step, loss=0.07763, avg_loss=0.07467]\n",
      "Step 516883  [5.384 sec/step, loss=0.07619, avg_loss=0.07476]\n",
      "Step 516884  [5.373 sec/step, loss=0.07494, avg_loss=0.07476]\n",
      "Step 516885  [5.372 sec/step, loss=0.07588, avg_loss=0.07476]\n",
      "Step 516886  [5.367 sec/step, loss=0.07509, avg_loss=0.07478]\n",
      "Step 516887  [5.372 sec/step, loss=0.07545, avg_loss=0.07478]\n",
      "Step 516888  [5.436 sec/step, loss=0.06750, avg_loss=0.07474]\n",
      "Step 516889  [5.459 sec/step, loss=0.07671, avg_loss=0.07478]\n",
      "Step 516890  [5.444 sec/step, loss=0.07512, avg_loss=0.07478]\n",
      "Step 516891  [5.458 sec/step, loss=0.07658, avg_loss=0.07479]\n",
      "Step 516892  [5.462 sec/step, loss=0.07627, avg_loss=0.07479]\n",
      "Step 516893  [5.452 sec/step, loss=0.07574, avg_loss=0.07478]\n",
      "Step 516894  [5.450 sec/step, loss=0.07299, avg_loss=0.07476]\n",
      "Step 516895  [5.457 sec/step, loss=0.07444, avg_loss=0.07475]\n",
      "Generated 32 batches of size 32 in 2.415 sec\n",
      "Step 516896  [5.468 sec/step, loss=0.07483, avg_loss=0.07473]\n",
      "Step 516897  [5.466 sec/step, loss=0.07516, avg_loss=0.07474]\n",
      "Step 516898  [5.476 sec/step, loss=0.07725, avg_loss=0.07476]\n",
      "Step 516899  [5.475 sec/step, loss=0.07442, avg_loss=0.07476]\n",
      "Step 516900  [5.479 sec/step, loss=0.07637, avg_loss=0.07475]\n",
      "Writing summary at step: 516900\n",
      "Step 516901  [5.460 sec/step, loss=0.06711, avg_loss=0.07466]\n",
      "Step 516902  [5.447 sec/step, loss=0.07158, avg_loss=0.07466]\n",
      "Step 516903  [5.447 sec/step, loss=0.07651, avg_loss=0.07465]\n",
      "Step 516904  [5.457 sec/step, loss=0.07710, avg_loss=0.07467]\n",
      "Step 516905  [5.466 sec/step, loss=0.07561, avg_loss=0.07468]\n",
      "Step 516906  [5.454 sec/step, loss=0.07595, avg_loss=0.07467]\n",
      "Step 516907  [5.428 sec/step, loss=0.07107, avg_loss=0.07462]\n",
      "Step 516908  [5.429 sec/step, loss=0.07649, avg_loss=0.07462]\n",
      "Step 516909  [5.410 sec/step, loss=0.07246, avg_loss=0.07460]\n",
      "Step 516910  [5.412 sec/step, loss=0.07412, avg_loss=0.07459]\n",
      "Step 516911  [5.422 sec/step, loss=0.07588, avg_loss=0.07459]\n",
      "Step 516912  [5.423 sec/step, loss=0.07598, avg_loss=0.07461]\n",
      "Step 516913  [5.420 sec/step, loss=0.07673, avg_loss=0.07462]\n",
      "Step 516914  [5.419 sec/step, loss=0.06724, avg_loss=0.07462]\n",
      "Step 516915  [5.414 sec/step, loss=0.07479, avg_loss=0.07464]\n",
      "Step 516916  [5.405 sec/step, loss=0.07470, avg_loss=0.07464]\n",
      "Step 516917  [5.395 sec/step, loss=0.07405, avg_loss=0.07461]\n",
      "Step 516918  [5.403 sec/step, loss=0.07406, avg_loss=0.07459]\n",
      "Step 516919  [5.404 sec/step, loss=0.07583, avg_loss=0.07459]\n",
      "Step 516920  [5.450 sec/step, loss=0.06639, avg_loss=0.07449]\n",
      "Step 516921  [5.450 sec/step, loss=0.07717, avg_loss=0.07450]\n",
      "Step 516922  [5.419 sec/step, loss=0.07485, avg_loss=0.07449]\n",
      "Step 516923  [5.446 sec/step, loss=0.07387, avg_loss=0.07448]\n",
      "Step 516924  [5.443 sec/step, loss=0.07557, avg_loss=0.07447]\n",
      "Step 516925  [5.436 sec/step, loss=0.07172, avg_loss=0.07444]\n",
      "Step 516926  [5.439 sec/step, loss=0.07484, avg_loss=0.07443]\n",
      "Generated 32 batches of size 32 in 2.446 sec\n",
      "Step 516927  [5.438 sec/step, loss=0.07537, avg_loss=0.07442]\n",
      "Step 516928  [5.427 sec/step, loss=0.07544, avg_loss=0.07441]\n",
      "Step 516929  [5.429 sec/step, loss=0.07459, avg_loss=0.07439]\n",
      "Step 516930  [5.426 sec/step, loss=0.07292, avg_loss=0.07440]\n",
      "Step 516931  [5.447 sec/step, loss=0.07443, avg_loss=0.07440]\n",
      "Step 516932  [5.453 sec/step, loss=0.07501, avg_loss=0.07443]\n",
      "Step 516933  [5.472 sec/step, loss=0.07545, avg_loss=0.07446]\n",
      "Step 516934  [5.473 sec/step, loss=0.07684, avg_loss=0.07447]\n",
      "Step 516935  [5.432 sec/step, loss=0.07647, avg_loss=0.07457]\n",
      "Step 516936  [5.445 sec/step, loss=0.07672, avg_loss=0.07459]\n",
      "Step 516937  [5.450 sec/step, loss=0.07608, avg_loss=0.07459]\n",
      "Step 516938  [5.462 sec/step, loss=0.07690, avg_loss=0.07460]\n",
      "Step 516939  [5.458 sec/step, loss=0.07587, avg_loss=0.07460]\n",
      "Step 516940  [5.442 sec/step, loss=0.06615, avg_loss=0.07452]\n",
      "Step 516941  [5.453 sec/step, loss=0.07439, avg_loss=0.07451]\n",
      "Step 516942  [5.449 sec/step, loss=0.07623, avg_loss=0.07451]\n",
      "Step 516943  [5.448 sec/step, loss=0.07340, avg_loss=0.07448]\n",
      "Step 516944  [5.471 sec/step, loss=0.07619, avg_loss=0.07450]\n",
      "Step 516945  [5.460 sec/step, loss=0.07115, avg_loss=0.07446]\n",
      "Step 516946  [5.458 sec/step, loss=0.07458, avg_loss=0.07448]\n",
      "Step 516947  [5.447 sec/step, loss=0.07616, avg_loss=0.07448]\n",
      "Step 516948  [5.467 sec/step, loss=0.07475, avg_loss=0.07446]\n",
      "Step 516949  [5.449 sec/step, loss=0.07508, avg_loss=0.07445]\n",
      "Step 516950  [5.450 sec/step, loss=0.07451, avg_loss=0.07446]\n",
      "Step 516951  [5.460 sec/step, loss=0.07667, avg_loss=0.07448]\n",
      "Step 516952  [5.456 sec/step, loss=0.07547, avg_loss=0.07449]\n",
      "Step 516953  [5.461 sec/step, loss=0.07668, avg_loss=0.07449]\n",
      "Step 516954  [5.471 sec/step, loss=0.07638, avg_loss=0.07448]\n",
      "Step 516955  [5.487 sec/step, loss=0.07585, avg_loss=0.07449]\n",
      "Step 516956  [5.482 sec/step, loss=0.07590, avg_loss=0.07450]\n",
      "Step 516957  [5.475 sec/step, loss=0.07229, avg_loss=0.07448]\n",
      "Step 516958  [5.471 sec/step, loss=0.07348, avg_loss=0.07445]\n",
      "Generated 32 batches of size 32 in 2.498 sec\n",
      "Step 516959  [5.426 sec/step, loss=0.07550, avg_loss=0.07454]\n",
      "Step 516960  [5.421 sec/step, loss=0.07526, avg_loss=0.07454]\n",
      "Step 516961  [5.407 sec/step, loss=0.07379, avg_loss=0.07451]\n",
      "Step 516962  [5.472 sec/step, loss=0.06648, avg_loss=0.07445]\n",
      "Step 516963  [5.474 sec/step, loss=0.07615, avg_loss=0.07446]\n",
      "Step 516964  [5.449 sec/step, loss=0.07526, avg_loss=0.07448]\n",
      "Step 516965  [5.458 sec/step, loss=0.07714, avg_loss=0.07449]\n",
      "Step 516966  [5.455 sec/step, loss=0.07596, avg_loss=0.07452]\n",
      "Step 516967  [5.468 sec/step, loss=0.07584, avg_loss=0.07457]\n",
      "Step 516968  [5.454 sec/step, loss=0.07678, avg_loss=0.07457]\n",
      "Step 516969  [5.467 sec/step, loss=0.07419, avg_loss=0.07466]\n",
      "Step 516970  [5.454 sec/step, loss=0.07367, avg_loss=0.07466]\n",
      "Step 516971  [5.432 sec/step, loss=0.06626, avg_loss=0.07457]\n",
      "Step 516972  [5.407 sec/step, loss=0.07502, avg_loss=0.07458]\n",
      "Step 516973  [5.366 sec/step, loss=0.07251, avg_loss=0.07456]\n",
      "Step 516974  [5.429 sec/step, loss=0.06671, avg_loss=0.07450]\n",
      "Step 516975  [5.447 sec/step, loss=0.07473, avg_loss=0.07452]\n",
      "Step 516976  [5.457 sec/step, loss=0.07619, avg_loss=0.07453]\n",
      "Step 516977  [5.451 sec/step, loss=0.07578, avg_loss=0.07452]\n",
      "Step 516978  [5.465 sec/step, loss=0.07709, avg_loss=0.07454]\n",
      "Step 516979  [5.475 sec/step, loss=0.07597, avg_loss=0.07456]\n",
      "Step 516980  [5.492 sec/step, loss=0.07447, avg_loss=0.07454]\n",
      "Step 516981  [5.502 sec/step, loss=0.07681, avg_loss=0.07455]\n",
      "Step 516982  [5.500 sec/step, loss=0.07542, avg_loss=0.07453]\n",
      "Step 516983  [5.509 sec/step, loss=0.07555, avg_loss=0.07452]\n",
      "Step 516984  [5.507 sec/step, loss=0.07141, avg_loss=0.07449]\n",
      "Step 516985  [5.500 sec/step, loss=0.07498, avg_loss=0.07448]\n",
      "Step 516986  [5.537 sec/step, loss=0.07382, avg_loss=0.07447]\n",
      "Step 516987  [5.534 sec/step, loss=0.07587, avg_loss=0.07447]\n",
      "Step 516988  [5.477 sec/step, loss=0.07482, avg_loss=0.07454]\n",
      "Step 516989  [5.469 sec/step, loss=0.07220, avg_loss=0.07450]\n",
      "Step 516990  [5.479 sec/step, loss=0.07637, avg_loss=0.07451]\n",
      "Generated 32 batches of size 32 in 2.554 sec\n",
      "Step 516991  [5.480 sec/step, loss=0.07620, avg_loss=0.07451]\n",
      "Step 516992  [5.469 sec/step, loss=0.07706, avg_loss=0.07452]\n",
      "Step 516993  [5.477 sec/step, loss=0.07691, avg_loss=0.07453]\n",
      "Step 516994  [5.475 sec/step, loss=0.07287, avg_loss=0.07453]\n",
      "Step 516995  [5.470 sec/step, loss=0.07436, avg_loss=0.07452]\n",
      "Step 516996  [5.442 sec/step, loss=0.07179, avg_loss=0.07449]\n",
      "Step 516997  [5.444 sec/step, loss=0.07478, avg_loss=0.07449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 516998  [5.428 sec/step, loss=0.07618, avg_loss=0.07448]\n",
      "Step 516999  [5.434 sec/step, loss=0.07570, avg_loss=0.07449]\n",
      "Step 517000  [5.430 sec/step, loss=0.07581, avg_loss=0.07449]\n",
      "Writing summary at step: 517000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-517000\n",
      "Saving audio and alignment...\n",
      "Input: sangiin dzhagrroon kay kaesoon sae batsnay vaaloo kharray hoo kur aankh dzhukaaoo~_________________\n",
      "Step 517001  [5.451 sec/step, loss=0.07558, avg_loss=0.07457]\n",
      "Step 517002  [5.471 sec/step, loss=0.07668, avg_loss=0.07462]\n",
      "Step 517003  [5.462 sec/step, loss=0.07600, avg_loss=0.07462]\n",
      "Step 517004  [5.456 sec/step, loss=0.07469, avg_loss=0.07459]\n",
      "Step 517005  [5.445 sec/step, loss=0.07409, avg_loss=0.07458]\n",
      "Step 517006  [5.434 sec/step, loss=0.07108, avg_loss=0.07453]\n",
      "Step 517007  [5.455 sec/step, loss=0.07710, avg_loss=0.07459]\n",
      "Step 517008  [5.462 sec/step, loss=0.07389, avg_loss=0.07456]\n",
      "Step 517009  [5.465 sec/step, loss=0.07494, avg_loss=0.07459]\n",
      "Step 517010  [5.471 sec/step, loss=0.07662, avg_loss=0.07461]\n",
      "Step 517011  [5.454 sec/step, loss=0.07662, avg_loss=0.07462]\n",
      "Step 517012  [5.442 sec/step, loss=0.06678, avg_loss=0.07453]\n",
      "Step 517013  [5.426 sec/step, loss=0.07532, avg_loss=0.07452]\n",
      "Step 517014  [5.455 sec/step, loss=0.07427, avg_loss=0.07459]\n",
      "Step 517015  [5.462 sec/step, loss=0.07212, avg_loss=0.07456]\n",
      "Step 517016  [5.467 sec/step, loss=0.07558, avg_loss=0.07457]\n",
      "Step 517017  [5.471 sec/step, loss=0.07507, avg_loss=0.07458]\n",
      "Step 517018  [5.438 sec/step, loss=0.07180, avg_loss=0.07456]\n",
      "Step 517019  [5.429 sec/step, loss=0.07437, avg_loss=0.07454]\n",
      "Step 517020  [5.387 sec/step, loss=0.07680, avg_loss=0.07464]\n",
      "Generated 32 batches of size 32 in 2.427 sec\n",
      "Step 517021  [5.408 sec/step, loss=0.07387, avg_loss=0.07461]\n",
      "Step 517022  [5.412 sec/step, loss=0.07306, avg_loss=0.07459]\n",
      "Step 517023  [5.389 sec/step, loss=0.07614, avg_loss=0.07462]\n",
      "Step 517024  [5.404 sec/step, loss=0.07637, avg_loss=0.07462]\n",
      "Step 517025  [5.415 sec/step, loss=0.07519, avg_loss=0.07466]\n",
      "Step 517026  [5.400 sec/step, loss=0.07546, avg_loss=0.07467]\n",
      "Step 517027  [5.396 sec/step, loss=0.07751, avg_loss=0.07469]\n",
      "Step 517028  [5.445 sec/step, loss=0.06697, avg_loss=0.07460]\n",
      "Step 517029  [5.463 sec/step, loss=0.07358, avg_loss=0.07459]\n",
      "Step 517030  [5.480 sec/step, loss=0.07503, avg_loss=0.07461]\n",
      "Step 517031  [5.462 sec/step, loss=0.07408, avg_loss=0.07461]\n",
      "Step 517032  [5.456 sec/step, loss=0.07303, avg_loss=0.07459]\n",
      "Step 517033  [5.474 sec/step, loss=0.07590, avg_loss=0.07459]\n",
      "Step 517034  [5.521 sec/step, loss=0.06670, avg_loss=0.07449]\n",
      "Step 517035  [5.513 sec/step, loss=0.07578, avg_loss=0.07449]\n",
      "Step 517036  [5.519 sec/step, loss=0.07660, avg_loss=0.07448]\n",
      "Step 517037  [5.513 sec/step, loss=0.07618, avg_loss=0.07449]\n",
      "Step 517038  [5.518 sec/step, loss=0.07397, avg_loss=0.07446]\n",
      "Step 517039  [5.510 sec/step, loss=0.07539, avg_loss=0.07445]\n",
      "Step 517040  [5.524 sec/step, loss=0.07269, avg_loss=0.07452]\n",
      "Step 517041  [5.532 sec/step, loss=0.07664, avg_loss=0.07454]\n",
      "Step 517042  [5.525 sec/step, loss=0.07671, avg_loss=0.07454]\n",
      "Step 517043  [5.519 sec/step, loss=0.07114, avg_loss=0.07452]\n",
      "Step 517044  [5.500 sec/step, loss=0.07560, avg_loss=0.07452]\n",
      "Step 517045  [5.522 sec/step, loss=0.07702, avg_loss=0.07457]\n",
      "Step 517046  [5.542 sec/step, loss=0.07498, avg_loss=0.07458]\n",
      "Step 517047  [5.526 sec/step, loss=0.07247, avg_loss=0.07454]\n",
      "Step 517048  [5.493 sec/step, loss=0.07345, avg_loss=0.07453]\n",
      "Step 517049  [5.493 sec/step, loss=0.07495, avg_loss=0.07453]\n",
      "Step 517050  [5.514 sec/step, loss=0.07629, avg_loss=0.07454]\n",
      "Step 517051  [5.510 sec/step, loss=0.07586, avg_loss=0.07454]\n",
      "Step 517052  [5.503 sec/step, loss=0.07605, avg_loss=0.07454]\n",
      "Generated 32 batches of size 32 in 2.896 sec\n",
      "Step 517053  [5.476 sec/step, loss=0.06755, avg_loss=0.07445]\n",
      "Step 517054  [5.463 sec/step, loss=0.07488, avg_loss=0.07444]\n",
      "Step 517055  [5.456 sec/step, loss=0.07532, avg_loss=0.07443]\n",
      "Step 517056  [5.456 sec/step, loss=0.07576, avg_loss=0.07443]\n",
      "Step 517057  [5.467 sec/step, loss=0.07514, avg_loss=0.07446]\n",
      "Step 517058  [5.480 sec/step, loss=0.07667, avg_loss=0.07449]\n",
      "Step 517059  [5.488 sec/step, loss=0.07745, avg_loss=0.07451]\n",
      "Step 517060  [5.497 sec/step, loss=0.07501, avg_loss=0.07451]\n",
      "Step 517061  [5.500 sec/step, loss=0.07607, avg_loss=0.07453]\n",
      "Step 517062  [5.439 sec/step, loss=0.07478, avg_loss=0.07461]\n",
      "Step 517063  [5.446 sec/step, loss=0.07696, avg_loss=0.07462]\n",
      "Step 517064  [5.465 sec/step, loss=0.07439, avg_loss=0.07461]\n",
      "Step 517065  [5.448 sec/step, loss=0.07397, avg_loss=0.07458]\n",
      "Step 517066  [5.452 sec/step, loss=0.07237, avg_loss=0.07454]\n",
      "Step 517067  [5.460 sec/step, loss=0.07521, avg_loss=0.07454]\n",
      "Step 517068  [5.450 sec/step, loss=0.07430, avg_loss=0.07451]\n",
      "Step 517069  [5.464 sec/step, loss=0.07670, avg_loss=0.07454]\n",
      "Step 517070  [5.484 sec/step, loss=0.07396, avg_loss=0.07454]\n",
      "Step 517071  [5.510 sec/step, loss=0.07331, avg_loss=0.07461]\n",
      "Step 517072  [5.516 sec/step, loss=0.07520, avg_loss=0.07461]\n",
      "Step 517073  [5.534 sec/step, loss=0.07600, avg_loss=0.07465]\n",
      "Step 517074  [5.488 sec/step, loss=0.07592, avg_loss=0.07474]\n",
      "Step 517075  [5.471 sec/step, loss=0.07588, avg_loss=0.07475]\n",
      "Step 517076  [5.452 sec/step, loss=0.07357, avg_loss=0.07473]\n",
      "Step 517077  [5.493 sec/step, loss=0.06734, avg_loss=0.07464]\n",
      "Step 517078  [5.481 sec/step, loss=0.07633, avg_loss=0.07463]\n",
      "Step 517079  [5.487 sec/step, loss=0.07689, avg_loss=0.07464]\n",
      "Step 517080  [5.469 sec/step, loss=0.07264, avg_loss=0.07462]\n",
      "Step 517081  [5.454 sec/step, loss=0.07578, avg_loss=0.07461]\n",
      "Step 517082  [5.448 sec/step, loss=0.07423, avg_loss=0.07460]\n",
      "Step 517083  [5.457 sec/step, loss=0.07642, avg_loss=0.07461]\n",
      "Step 517084  [5.460 sec/step, loss=0.07506, avg_loss=0.07465]\n",
      "Generated 32 batches of size 32 in 2.551 sec\n",
      "Step 517085  [5.472 sec/step, loss=0.07259, avg_loss=0.07462]\n",
      "Step 517086  [5.437 sec/step, loss=0.07476, avg_loss=0.07463]\n",
      "Step 517087  [5.420 sec/step, loss=0.07214, avg_loss=0.07460]\n",
      "Step 517088  [5.433 sec/step, loss=0.07625, avg_loss=0.07461]\n",
      "Step 517089  [5.431 sec/step, loss=0.07657, avg_loss=0.07465]\n",
      "Step 517090  [5.405 sec/step, loss=0.06612, avg_loss=0.07455]\n",
      "Step 517091  [5.384 sec/step, loss=0.07285, avg_loss=0.07452]\n",
      "Step 517092  [5.391 sec/step, loss=0.07654, avg_loss=0.07451]\n",
      "Step 517093  [5.390 sec/step, loss=0.07767, avg_loss=0.07452]\n",
      "Step 517094  [5.383 sec/step, loss=0.07226, avg_loss=0.07451]\n",
      "Step 517095  [5.400 sec/step, loss=0.07671, avg_loss=0.07454]\n",
      "Step 517096  [5.414 sec/step, loss=0.07537, avg_loss=0.07457]\n",
      "Step 517097  [5.403 sec/step, loss=0.07475, avg_loss=0.07457]\n",
      "Step 517098  [5.399 sec/step, loss=0.07490, avg_loss=0.07456]\n",
      "Step 517099  [5.402 sec/step, loss=0.07656, avg_loss=0.07457]\n",
      "Step 517100  [5.398 sec/step, loss=0.07501, avg_loss=0.07456]\n",
      "Writing summary at step: 517100\n",
      "Step 517101  [5.391 sec/step, loss=0.07586, avg_loss=0.07456]\n",
      "Step 517102  [5.395 sec/step, loss=0.07510, avg_loss=0.07455]\n",
      "Step 517103  [5.399 sec/step, loss=0.07557, avg_loss=0.07454]\n",
      "Step 517104  [5.401 sec/step, loss=0.07612, avg_loss=0.07456]\n",
      "Step 517105  [5.407 sec/step, loss=0.07555, avg_loss=0.07457]\n",
      "Step 517106  [5.420 sec/step, loss=0.07589, avg_loss=0.07462]\n",
      "Step 517107  [5.404 sec/step, loss=0.07356, avg_loss=0.07459]\n",
      "Step 517108  [5.402 sec/step, loss=0.07714, avg_loss=0.07462]\n",
      "Step 517109  [5.411 sec/step, loss=0.07239, avg_loss=0.07459]\n",
      "Step 517110  [5.405 sec/step, loss=0.07437, avg_loss=0.07457]\n",
      "Step 517111  [5.412 sec/step, loss=0.07729, avg_loss=0.07458]\n",
      "Step 517112  [5.437 sec/step, loss=0.07379, avg_loss=0.07465]\n",
      "Step 517113  [5.424 sec/step, loss=0.07344, avg_loss=0.07463]\n",
      "Step 517114  [5.397 sec/step, loss=0.06767, avg_loss=0.07456]\n",
      "Step 517115  [5.388 sec/step, loss=0.07160, avg_loss=0.07456]\n",
      "Generated 32 batches of size 32 in 2.385 sec\n",
      "Step 517116  [5.394 sec/step, loss=0.07666, avg_loss=0.07457]\n",
      "Step 517117  [5.387 sec/step, loss=0.07112, avg_loss=0.07453]\n",
      "Step 517118  [5.415 sec/step, loss=0.07621, avg_loss=0.07457]\n",
      "Step 517119  [5.439 sec/step, loss=0.07612, avg_loss=0.07459]\n",
      "Step 517120  [5.482 sec/step, loss=0.06669, avg_loss=0.07449]\n",
      "Step 517121  [5.459 sec/step, loss=0.07586, avg_loss=0.07451]\n",
      "Step 517122  [5.459 sec/step, loss=0.07598, avg_loss=0.07454]\n",
      "Step 517123  [5.457 sec/step, loss=0.07690, avg_loss=0.07455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 517124  [5.471 sec/step, loss=0.07312, avg_loss=0.07451]\n",
      "Step 517125  [5.452 sec/step, loss=0.07122, avg_loss=0.07447]\n",
      "Step 517126  [5.472 sec/step, loss=0.07729, avg_loss=0.07449]\n",
      "Step 517127  [5.465 sec/step, loss=0.07630, avg_loss=0.07448]\n",
      "Step 517128  [5.405 sec/step, loss=0.07502, avg_loss=0.07456]\n",
      "Step 517129  [5.392 sec/step, loss=0.07620, avg_loss=0.07459]\n",
      "Step 517130  [5.397 sec/step, loss=0.07477, avg_loss=0.07458]\n",
      "Step 517131  [5.397 sec/step, loss=0.07650, avg_loss=0.07461]\n",
      "Step 517132  [5.398 sec/step, loss=0.07449, avg_loss=0.07462]\n",
      "Step 517133  [5.375 sec/step, loss=0.07666, avg_loss=0.07463]\n",
      "Step 517134  [5.311 sec/step, loss=0.07157, avg_loss=0.07468]\n",
      "Step 517135  [5.293 sec/step, loss=0.06635, avg_loss=0.07458]\n",
      "Step 517136  [5.271 sec/step, loss=0.07482, avg_loss=0.07457]\n",
      "Step 517137  [5.269 sec/step, loss=0.07728, avg_loss=0.07458]\n",
      "Step 517138  [5.247 sec/step, loss=0.07387, avg_loss=0.07458]\n",
      "Step 517139  [5.309 sec/step, loss=0.06713, avg_loss=0.07449]\n",
      "Step 517140  [5.337 sec/step, loss=0.07438, avg_loss=0.07451]\n",
      "Step 517141  [5.334 sec/step, loss=0.07435, avg_loss=0.07449]\n",
      "Step 517142  [5.350 sec/step, loss=0.07654, avg_loss=0.07449]\n",
      "Step 517143  [5.367 sec/step, loss=0.07420, avg_loss=0.07452]\n",
      "Step 517144  [5.370 sec/step, loss=0.07490, avg_loss=0.07451]\n",
      "Step 517145  [5.365 sec/step, loss=0.07669, avg_loss=0.07451]\n",
      "Step 517146  [5.366 sec/step, loss=0.07383, avg_loss=0.07450]\n",
      "Step 517147  [5.391 sec/step, loss=0.07707, avg_loss=0.07454]\n",
      "Generated 32 batches of size 32 in 2.514 sec\n",
      "Step 517148  [5.405 sec/step, loss=0.07539, avg_loss=0.07456]\n",
      "Step 517149  [5.426 sec/step, loss=0.07686, avg_loss=0.07458]\n",
      "Step 517150  [5.420 sec/step, loss=0.07719, avg_loss=0.07459]\n",
      "Step 517151  [5.412 sec/step, loss=0.07268, avg_loss=0.07456]\n",
      "Step 517152  [5.410 sec/step, loss=0.07484, avg_loss=0.07455]\n",
      "Step 517153  [5.437 sec/step, loss=0.07652, avg_loss=0.07463]\n",
      "Step 517154  [5.436 sec/step, loss=0.07556, avg_loss=0.07464]\n",
      "Step 517155  [5.436 sec/step, loss=0.07574, avg_loss=0.07465]\n",
      "Step 517156  [5.432 sec/step, loss=0.07295, avg_loss=0.07462]\n",
      "Step 517157  [5.425 sec/step, loss=0.07428, avg_loss=0.07461]\n",
      "Step 517158  [5.426 sec/step, loss=0.07664, avg_loss=0.07461]\n",
      "Step 517159  [5.419 sec/step, loss=0.07601, avg_loss=0.07459]\n",
      "Step 517160  [5.394 sec/step, loss=0.07212, avg_loss=0.07457]\n",
      "Step 517161  [5.416 sec/step, loss=0.07602, avg_loss=0.07457]\n",
      "Step 517162  [5.453 sec/step, loss=0.07433, avg_loss=0.07456]\n",
      "Step 517163  [5.446 sec/step, loss=0.07602, avg_loss=0.07455]\n",
      "Step 517164  [5.417 sec/step, loss=0.07400, avg_loss=0.07455]\n",
      "Step 517165  [5.414 sec/step, loss=0.07383, avg_loss=0.07455]\n",
      "Step 517166  [5.424 sec/step, loss=0.07703, avg_loss=0.07459]\n",
      "Step 517167  [5.425 sec/step, loss=0.07535, avg_loss=0.07459]\n",
      "Step 517168  [5.485 sec/step, loss=0.06724, avg_loss=0.07452]\n",
      "Step 517169  [5.475 sec/step, loss=0.07517, avg_loss=0.07451]\n",
      "Step 517170  [5.439 sec/step, loss=0.06571, avg_loss=0.07443]\n",
      "Step 517171  [5.445 sec/step, loss=0.07692, avg_loss=0.07446]\n",
      "Step 517172  [5.450 sec/step, loss=0.07730, avg_loss=0.07448]\n",
      "Step 517173  [5.448 sec/step, loss=0.07522, avg_loss=0.07447]\n",
      "Step 517174  [5.453 sec/step, loss=0.07659, avg_loss=0.07448]\n",
      "Step 517175  [5.459 sec/step, loss=0.07628, avg_loss=0.07449]\n",
      "Step 517176  [5.469 sec/step, loss=0.07249, avg_loss=0.07447]\n",
      "Step 517177  [5.417 sec/step, loss=0.07567, avg_loss=0.07456]\n",
      "Step 517178  [5.422 sec/step, loss=0.07647, avg_loss=0.07456]\n",
      "Step 517179  [5.426 sec/step, loss=0.07659, avg_loss=0.07456]\n",
      "Generated 32 batches of size 32 in 2.431 sec\n",
      "Step 517180  [5.437 sec/step, loss=0.07150, avg_loss=0.07454]\n",
      "Step 517181  [5.460 sec/step, loss=0.07375, avg_loss=0.07452]\n",
      "Step 517182  [5.451 sec/step, loss=0.07496, avg_loss=0.07453]\n",
      "Step 517183  [5.446 sec/step, loss=0.07627, avg_loss=0.07453]\n",
      "Step 517184  [5.449 sec/step, loss=0.07226, avg_loss=0.07450]\n",
      "Step 517185  [5.437 sec/step, loss=0.07127, avg_loss=0.07449]\n",
      "Step 517186  [5.443 sec/step, loss=0.07309, avg_loss=0.07447]\n",
      "Step 517187  [5.453 sec/step, loss=0.07587, avg_loss=0.07451]\n",
      "Step 517188  [5.445 sec/step, loss=0.07336, avg_loss=0.07448]\n",
      "Step 517189  [5.449 sec/step, loss=0.07596, avg_loss=0.07447]\n",
      "Step 517190  [5.475 sec/step, loss=0.07660, avg_loss=0.07458]\n",
      "Step 517191  [5.518 sec/step, loss=0.07337, avg_loss=0.07458]\n",
      "Step 517192  [5.513 sec/step, loss=0.07653, avg_loss=0.07458]\n",
      "Step 517193  [5.499 sec/step, loss=0.07465, avg_loss=0.07455]\n",
      "Step 517194  [5.495 sec/step, loss=0.07253, avg_loss=0.07456]\n",
      "Step 517195  [5.490 sec/step, loss=0.07615, avg_loss=0.07455]\n",
      "Step 517196  [5.499 sec/step, loss=0.07659, avg_loss=0.07456]\n",
      "Step 517197  [5.502 sec/step, loss=0.07587, avg_loss=0.07458]\n",
      "Step 517198  [5.517 sec/step, loss=0.07570, avg_loss=0.07458]\n",
      "Step 517199  [5.522 sec/step, loss=0.07584, avg_loss=0.07458]\n",
      "Step 517200  [5.520 sec/step, loss=0.07574, avg_loss=0.07458]\n",
      "Writing summary at step: 517200\n",
      "Step 517201  [5.525 sec/step, loss=0.07485, avg_loss=0.07457]\n",
      "Step 517202  [5.514 sec/step, loss=0.07549, avg_loss=0.07458]\n",
      "Step 517203  [5.513 sec/step, loss=0.07265, avg_loss=0.07455]\n",
      "Step 517204  [5.523 sec/step, loss=0.07675, avg_loss=0.07455]\n",
      "Step 517205  [5.517 sec/step, loss=0.07296, avg_loss=0.07453]\n",
      "Step 517206  [5.508 sec/step, loss=0.07395, avg_loss=0.07451]\n",
      "Step 517207  [5.508 sec/step, loss=0.07212, avg_loss=0.07449]\n",
      "Step 517208  [5.485 sec/step, loss=0.07496, avg_loss=0.07447]\n",
      "Step 517209  [5.467 sec/step, loss=0.06663, avg_loss=0.07441]\n",
      "Step 517210  [5.522 sec/step, loss=0.06691, avg_loss=0.07434]\n",
      "Generated 32 batches of size 32 in 2.390 sec\n",
      "Step 517211  [5.529 sec/step, loss=0.07526, avg_loss=0.07432]\n",
      "Step 517212  [5.539 sec/step, loss=0.07626, avg_loss=0.07434]\n",
      "Step 517213  [5.566 sec/step, loss=0.07458, avg_loss=0.07436]\n",
      "Step 517214  [5.595 sec/step, loss=0.07414, avg_loss=0.07442]\n",
      "Step 517215  [5.617 sec/step, loss=0.07681, avg_loss=0.07447]\n",
      "Step 517216  [5.596 sec/step, loss=0.07464, avg_loss=0.07445]\n",
      "Step 517217  [5.601 sec/step, loss=0.07597, avg_loss=0.07450]\n",
      "Step 517218  [5.594 sec/step, loss=0.07556, avg_loss=0.07449]\n",
      "Step 517219  [5.581 sec/step, loss=0.07376, avg_loss=0.07447]\n",
      "Step 517220  [5.540 sec/step, loss=0.07687, avg_loss=0.07457]\n",
      "Step 517221  [5.562 sec/step, loss=0.07400, avg_loss=0.07455]\n",
      "Step 517222  [5.549 sec/step, loss=0.06662, avg_loss=0.07446]\n",
      "Step 517223  [5.543 sec/step, loss=0.07474, avg_loss=0.07444]\n",
      "Step 517224  [5.520 sec/step, loss=0.07604, avg_loss=0.07447]\n",
      "Step 517225  [5.550 sec/step, loss=0.07336, avg_loss=0.07449]\n",
      "Step 517226  [5.544 sec/step, loss=0.07614, avg_loss=0.07448]\n",
      "Step 517227  [5.537 sec/step, loss=0.07427, avg_loss=0.07446]\n",
      "Step 517228  [5.550 sec/step, loss=0.07512, avg_loss=0.07446]\n",
      "Step 517229  [5.535 sec/step, loss=0.07234, avg_loss=0.07442]\n",
      "Step 517230  [5.520 sec/step, loss=0.07522, avg_loss=0.07442]\n",
      "Step 517231  [5.535 sec/step, loss=0.07679, avg_loss=0.07443]\n",
      "Step 517232  [5.591 sec/step, loss=0.06690, avg_loss=0.07435]\n",
      "Step 517233  [5.589 sec/step, loss=0.07306, avg_loss=0.07432]\n",
      "Step 517234  [5.589 sec/step, loss=0.07262, avg_loss=0.07433]\n",
      "Step 517235  [5.613 sec/step, loss=0.07686, avg_loss=0.07443]\n",
      "Step 517236  [5.618 sec/step, loss=0.07549, avg_loss=0.07444]\n",
      "Step 517237  [5.603 sec/step, loss=0.07194, avg_loss=0.07438]\n",
      "Step 517238  [5.604 sec/step, loss=0.07466, avg_loss=0.07439]\n",
      "Step 517239  [5.564 sec/step, loss=0.07475, avg_loss=0.07447]\n",
      "Step 517240  [5.541 sec/step, loss=0.07649, avg_loss=0.07449]\n",
      "Step 517241  [5.550 sec/step, loss=0.07684, avg_loss=0.07451]\n",
      "Step 517242  [5.544 sec/step, loss=0.07444, avg_loss=0.07449]\n",
      "Generated 32 batches of size 32 in 2.422 sec\n",
      "Step 517243  [5.550 sec/step, loss=0.07580, avg_loss=0.07451]\n",
      "Step 517244  [5.552 sec/step, loss=0.07454, avg_loss=0.07451]\n",
      "Step 517245  [5.551 sec/step, loss=0.07492, avg_loss=0.07449]\n",
      "Step 517246  [5.555 sec/step, loss=0.07681, avg_loss=0.07452]\n",
      "Step 517247  [5.542 sec/step, loss=0.07617, avg_loss=0.07451]\n",
      "Step 517248  [5.536 sec/step, loss=0.07513, avg_loss=0.07451]\n",
      "Step 517249  [5.530 sec/step, loss=0.07595, avg_loss=0.07450]\n",
      "Step 517250  [5.529 sec/step, loss=0.07528, avg_loss=0.07448]\n",
      "Step 517251  [5.534 sec/step, loss=0.07514, avg_loss=0.07450]\n",
      "Step 517252  [5.549 sec/step, loss=0.07500, avg_loss=0.07450]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 517253  [5.542 sec/step, loss=0.07482, avg_loss=0.07449]\n",
      "Step 517254  [5.563 sec/step, loss=0.07610, avg_loss=0.07449]\n",
      "Step 517255  [5.573 sec/step, loss=0.07625, avg_loss=0.07450]\n",
      "Step 517256  [5.569 sec/step, loss=0.07574, avg_loss=0.07453]\n",
      "Step 517257  [5.566 sec/step, loss=0.07446, avg_loss=0.07453]\n",
      "Step 517258  [5.541 sec/step, loss=0.07255, avg_loss=0.07449]\n",
      "Step 517259  [5.542 sec/step, loss=0.07548, avg_loss=0.07448]\n",
      "Step 517260  [5.540 sec/step, loss=0.06615, avg_loss=0.07442]\n",
      "Step 517261  [5.537 sec/step, loss=0.07683, avg_loss=0.07443]\n",
      "Step 517262  [5.515 sec/step, loss=0.07595, avg_loss=0.07445]\n",
      "Step 517263  [5.527 sec/step, loss=0.07585, avg_loss=0.07444]\n",
      "Step 517264  [5.535 sec/step, loss=0.07682, avg_loss=0.07447]\n",
      "Step 517265  [5.549 sec/step, loss=0.07553, avg_loss=0.07449]\n",
      "Step 517266  [5.548 sec/step, loss=0.07419, avg_loss=0.07446]\n",
      "Step 517267  [5.549 sec/step, loss=0.07702, avg_loss=0.07448]\n",
      "Step 517268  [5.489 sec/step, loss=0.07425, avg_loss=0.07455]\n",
      "Step 517269  [5.489 sec/step, loss=0.07542, avg_loss=0.07455]\n",
      "Step 517270  [5.514 sec/step, loss=0.07582, avg_loss=0.07465]\n",
      "Step 517271  [5.492 sec/step, loss=0.07430, avg_loss=0.07463]\n",
      "Step 517272  [5.490 sec/step, loss=0.07603, avg_loss=0.07461]\n",
      "Step 517273  [5.478 sec/step, loss=0.07155, avg_loss=0.07458]\n",
      "Step 517274  [5.498 sec/step, loss=0.07325, avg_loss=0.07454]\n",
      "Generated 32 batches of size 32 in 2.431 sec\n",
      "Step 517275  [5.509 sec/step, loss=0.07705, avg_loss=0.07455]\n",
      "Step 517276  [5.509 sec/step, loss=0.07622, avg_loss=0.07459]\n",
      "Step 517277  [5.511 sec/step, loss=0.07390, avg_loss=0.07457]\n",
      "Step 517278  [5.503 sec/step, loss=0.07604, avg_loss=0.07457]\n",
      "Step 517279  [5.486 sec/step, loss=0.07412, avg_loss=0.07454]\n",
      "Step 517280  [5.473 sec/step, loss=0.07241, avg_loss=0.07455]\n",
      "Step 517281  [5.505 sec/step, loss=0.06658, avg_loss=0.07448]\n",
      "Step 517282  [5.516 sec/step, loss=0.07466, avg_loss=0.07448]\n",
      "Step 517283  [5.503 sec/step, loss=0.07289, avg_loss=0.07444]\n",
      "Step 517284  [5.481 sec/step, loss=0.06741, avg_loss=0.07439]\n",
      "Step 517285  [5.511 sec/step, loss=0.07646, avg_loss=0.07445]\n",
      "Step 517286  [5.529 sec/step, loss=0.07697, avg_loss=0.07448]\n",
      "Step 517287  [5.528 sec/step, loss=0.07564, avg_loss=0.07448]\n",
      "Step 517288  [5.520 sec/step, loss=0.07245, avg_loss=0.07447]\n",
      "Step 517289  [5.506 sec/step, loss=0.07106, avg_loss=0.07442]\n",
      "Step 517290  [5.501 sec/step, loss=0.07589, avg_loss=0.07442]\n",
      "Step 517291  [5.458 sec/step, loss=0.07265, avg_loss=0.07441]\n",
      "Step 517292  [5.446 sec/step, loss=0.07577, avg_loss=0.07440]\n",
      "Step 517293  [5.461 sec/step, loss=0.07589, avg_loss=0.07441]\n",
      "Step 517294  [5.469 sec/step, loss=0.07465, avg_loss=0.07444]\n",
      "Step 517295  [5.478 sec/step, loss=0.07712, avg_loss=0.07444]\n",
      "Step 517296  [5.471 sec/step, loss=0.07445, avg_loss=0.07442]\n",
      "Step 517297  [5.469 sec/step, loss=0.07558, avg_loss=0.07442]\n",
      "Step 517298  [5.473 sec/step, loss=0.07638, avg_loss=0.07443]\n",
      "Step 517299  [5.469 sec/step, loss=0.07295, avg_loss=0.07440]\n",
      "Step 517300  [5.464 sec/step, loss=0.07522, avg_loss=0.07439]\n",
      "Writing summary at step: 517300\n",
      "Step 517301  [5.465 sec/step, loss=0.07581, avg_loss=0.07440]\n",
      "Step 517302  [5.463 sec/step, loss=0.07240, avg_loss=0.07437]\n",
      "Step 517303  [5.467 sec/step, loss=0.07667, avg_loss=0.07441]\n",
      "Step 517304  [5.452 sec/step, loss=0.07577, avg_loss=0.07440]\n",
      "Step 517305  [5.456 sec/step, loss=0.07479, avg_loss=0.07442]\n",
      "Generated 32 batches of size 32 in 2.439 sec\n",
      "Step 517306  [5.493 sec/step, loss=0.07359, avg_loss=0.07442]\n",
      "Step 517307  [5.504 sec/step, loss=0.07566, avg_loss=0.07445]\n",
      "Step 517308  [5.525 sec/step, loss=0.07460, avg_loss=0.07445]\n",
      "Step 517309  [5.593 sec/step, loss=0.06714, avg_loss=0.07445]\n",
      "Step 517310  [5.552 sec/step, loss=0.07666, avg_loss=0.07455]\n",
      "Step 517311  [5.537 sec/step, loss=0.07428, avg_loss=0.07454]\n",
      "Step 517312  [5.530 sec/step, loss=0.07620, avg_loss=0.07454]\n",
      "Step 517313  [5.512 sec/step, loss=0.07401, avg_loss=0.07454]\n",
      "Step 517314  [5.506 sec/step, loss=0.07537, avg_loss=0.07455]\n",
      "Step 517315  [5.506 sec/step, loss=0.07651, avg_loss=0.07454]\n",
      "Step 517316  [5.509 sec/step, loss=0.07326, avg_loss=0.07453]\n",
      "Step 517317  [5.509 sec/step, loss=0.07231, avg_loss=0.07449]\n",
      "Step 517318  [5.517 sec/step, loss=0.07614, avg_loss=0.07450]\n",
      "Step 517319  [5.523 sec/step, loss=0.07756, avg_loss=0.07454]\n",
      "Step 517320  [5.532 sec/step, loss=0.07636, avg_loss=0.07453]\n",
      "Step 517321  [5.497 sec/step, loss=0.07526, avg_loss=0.07455]\n",
      "Step 517322  [5.523 sec/step, loss=0.07643, avg_loss=0.07464]\n",
      "Step 517323  [5.510 sec/step, loss=0.06585, avg_loss=0.07455]\n",
      "Step 517324  [5.516 sec/step, loss=0.07479, avg_loss=0.07454]\n",
      "Step 517325  [5.502 sec/step, loss=0.07535, avg_loss=0.07456]\n",
      "Step 517326  [5.503 sec/step, loss=0.07687, avg_loss=0.07457]\n",
      "Step 517327  [5.495 sec/step, loss=0.07435, avg_loss=0.07457]\n",
      "Step 517328  [5.521 sec/step, loss=0.07330, avg_loss=0.07455]\n",
      "Step 517329  [5.535 sec/step, loss=0.07604, avg_loss=0.07459]\n",
      "Step 517330  [5.532 sec/step, loss=0.07145, avg_loss=0.07455]\n",
      "Step 517331  [5.513 sec/step, loss=0.07366, avg_loss=0.07452]\n",
      "Step 517332  [5.463 sec/step, loss=0.07439, avg_loss=0.07459]\n",
      "Step 517333  [5.468 sec/step, loss=0.07544, avg_loss=0.07462]\n",
      "Step 517334  [5.484 sec/step, loss=0.07339, avg_loss=0.07463]\n",
      "Step 517335  [5.477 sec/step, loss=0.07486, avg_loss=0.07461]\n",
      "Step 517336  [5.485 sec/step, loss=0.07580, avg_loss=0.07461]\n",
      "Step 517337  [5.504 sec/step, loss=0.07715, avg_loss=0.07466]\n",
      "Generated 32 batches of size 32 in 2.531 sec\n",
      "Step 517338  [5.519 sec/step, loss=0.07656, avg_loss=0.07468]\n",
      "Step 517339  [5.552 sec/step, loss=0.07075, avg_loss=0.07464]\n",
      "Step 517340  [5.548 sec/step, loss=0.07599, avg_loss=0.07464]\n",
      "Step 517341  [5.532 sec/step, loss=0.07637, avg_loss=0.07463]\n",
      "Step 517342  [5.516 sec/step, loss=0.07409, avg_loss=0.07463]\n",
      "Step 517343  [5.515 sec/step, loss=0.07720, avg_loss=0.07464]\n",
      "Step 517344  [5.513 sec/step, loss=0.07676, avg_loss=0.07466]\n",
      "Step 517345  [5.523 sec/step, loss=0.07329, avg_loss=0.07465]\n",
      "Step 517346  [5.494 sec/step, loss=0.07255, avg_loss=0.07460]\n",
      "Step 517347  [5.511 sec/step, loss=0.07713, avg_loss=0.07461]\n",
      "Step 517348  [5.518 sec/step, loss=0.07494, avg_loss=0.07461]\n",
      "Step 517349  [5.508 sec/step, loss=0.07599, avg_loss=0.07461]\n",
      "Step 517350  [5.501 sec/step, loss=0.07560, avg_loss=0.07462]\n",
      "Step 517351  [5.506 sec/step, loss=0.07644, avg_loss=0.07463]\n",
      "Step 517352  [5.492 sec/step, loss=0.07208, avg_loss=0.07460]\n",
      "Step 517353  [5.474 sec/step, loss=0.07261, avg_loss=0.07458]\n",
      "Step 517354  [5.438 sec/step, loss=0.06690, avg_loss=0.07449]\n",
      "Step 517355  [5.419 sec/step, loss=0.07477, avg_loss=0.07447]\n",
      "Step 517356  [5.411 sec/step, loss=0.07483, avg_loss=0.07446]\n",
      "Step 517357  [5.435 sec/step, loss=0.07695, avg_loss=0.07449]\n",
      "Step 517358  [5.439 sec/step, loss=0.07294, avg_loss=0.07449]\n",
      "Step 517359  [5.485 sec/step, loss=0.06645, avg_loss=0.07440]\n",
      "Step 517360  [5.506 sec/step, loss=0.07577, avg_loss=0.07450]\n",
      "Step 517361  [5.520 sec/step, loss=0.07420, avg_loss=0.07447]\n",
      "Step 517362  [5.536 sec/step, loss=0.07367, avg_loss=0.07445]\n",
      "Step 517363  [5.520 sec/step, loss=0.07495, avg_loss=0.07444]\n",
      "Step 517364  [5.514 sec/step, loss=0.07463, avg_loss=0.07442]\n",
      "Step 517365  [5.516 sec/step, loss=0.07672, avg_loss=0.07443]\n",
      "Step 517366  [5.516 sec/step, loss=0.07690, avg_loss=0.07446]\n",
      "Step 517367  [5.515 sec/step, loss=0.07489, avg_loss=0.07443]\n",
      "Step 517368  [5.525 sec/step, loss=0.07390, avg_loss=0.07443]\n",
      "Step 517369  [5.531 sec/step, loss=0.07479, avg_loss=0.07442]\n",
      "Generated 32 batches of size 32 in 2.398 sec\n",
      "Step 517370  [5.533 sec/step, loss=0.07673, avg_loss=0.07443]\n",
      "Step 517371  [5.549 sec/step, loss=0.07644, avg_loss=0.07445]\n",
      "Step 517372  [5.544 sec/step, loss=0.07574, avg_loss=0.07445]\n",
      "Step 517373  [5.551 sec/step, loss=0.07345, avg_loss=0.07447]\n",
      "Step 517374  [5.526 sec/step, loss=0.07462, avg_loss=0.07448]\n",
      "Step 517375  [5.526 sec/step, loss=0.07460, avg_loss=0.07446]\n",
      "Step 517376  [5.533 sec/step, loss=0.07635, avg_loss=0.07446]\n",
      "Step 517377  [5.525 sec/step, loss=0.07387, avg_loss=0.07446]\n",
      "Step 517378  [5.526 sec/step, loss=0.07574, avg_loss=0.07446]\n",
      "Step 517379  [5.516 sec/step, loss=0.07157, avg_loss=0.07443]\n",
      "Step 517380  [5.515 sec/step, loss=0.07584, avg_loss=0.07447]\n",
      "Step 517381  [5.472 sec/step, loss=0.07673, avg_loss=0.07457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 517382  [5.483 sec/step, loss=0.07606, avg_loss=0.07458]\n",
      "Step 517383  [5.485 sec/step, loss=0.07295, avg_loss=0.07458]\n",
      "Step 517384  [5.512 sec/step, loss=0.07693, avg_loss=0.07468]\n",
      "Step 517385  [5.506 sec/step, loss=0.07600, avg_loss=0.07467]\n",
      "Step 517386  [5.492 sec/step, loss=0.07603, avg_loss=0.07466]\n",
      "Step 517387  [5.500 sec/step, loss=0.07570, avg_loss=0.07466]\n",
      "Step 517388  [5.516 sec/step, loss=0.07319, avg_loss=0.07467]\n",
      "Step 517389  [5.520 sec/step, loss=0.07451, avg_loss=0.07471]\n",
      "Step 517390  [5.519 sec/step, loss=0.07589, avg_loss=0.07471]\n",
      "Step 517391  [5.526 sec/step, loss=0.07497, avg_loss=0.07473]\n",
      "Step 517392  [5.533 sec/step, loss=0.07574, avg_loss=0.07473]\n",
      "Step 517393  [5.522 sec/step, loss=0.07583, avg_loss=0.07473]\n",
      "Step 517394  [5.541 sec/step, loss=0.07661, avg_loss=0.07475]\n",
      "Step 517395  [5.528 sec/step, loss=0.07538, avg_loss=0.07473]\n",
      "Step 517396  [5.529 sec/step, loss=0.07342, avg_loss=0.07472]\n",
      "Step 517397  [5.580 sec/step, loss=0.06682, avg_loss=0.07463]\n",
      "Step 517398  [5.556 sec/step, loss=0.07116, avg_loss=0.07458]\n",
      "Step 517399  [5.555 sec/step, loss=0.07476, avg_loss=0.07460]\n",
      "Step 517400  [5.551 sec/step, loss=0.07465, avg_loss=0.07459]\n",
      "Writing summary at step: 517400\n",
      "Generated 32 batches of size 32 in 2.436 sec\n",
      "Step 517401  [5.583 sec/step, loss=0.07417, avg_loss=0.07458]\n",
      "Step 517402  [5.600 sec/step, loss=0.07528, avg_loss=0.07461]\n",
      "Step 517403  [5.602 sec/step, loss=0.07696, avg_loss=0.07461]\n",
      "Step 517404  [5.609 sec/step, loss=0.07656, avg_loss=0.07462]\n",
      "Step 517405  [5.613 sec/step, loss=0.07487, avg_loss=0.07462]\n",
      "Step 517406  [5.578 sec/step, loss=0.07188, avg_loss=0.07460]\n",
      "Step 517407  [5.588 sec/step, loss=0.07538, avg_loss=0.07460]\n",
      "Step 517408  [5.569 sec/step, loss=0.07367, avg_loss=0.07459]\n",
      "Step 517409  [5.520 sec/step, loss=0.07342, avg_loss=0.07465]\n",
      "Step 517410  [5.518 sec/step, loss=0.07681, avg_loss=0.07465]\n",
      "Step 517411  [5.512 sec/step, loss=0.07469, avg_loss=0.07466]\n",
      "Step 517412  [5.512 sec/step, loss=0.07526, avg_loss=0.07465]\n",
      "Step 517413  [5.509 sec/step, loss=0.07356, avg_loss=0.07464]\n",
      "Step 517414  [5.511 sec/step, loss=0.07636, avg_loss=0.07465]\n",
      "Step 517415  [5.484 sec/step, loss=0.07236, avg_loss=0.07461]\n",
      "Step 517416  [5.489 sec/step, loss=0.07543, avg_loss=0.07463]\n",
      "Step 517417  [5.502 sec/step, loss=0.07575, avg_loss=0.07467]\n",
      "Step 517418  [5.487 sec/step, loss=0.07228, avg_loss=0.07463]\n",
      "Step 517419  [5.494 sec/step, loss=0.07761, avg_loss=0.07463]\n",
      "Step 517420  [5.488 sec/step, loss=0.07519, avg_loss=0.07462]\n",
      "Step 517421  [5.495 sec/step, loss=0.07445, avg_loss=0.07461]\n",
      "Step 517422  [5.475 sec/step, loss=0.07208, avg_loss=0.07457]\n",
      "Step 517423  [5.496 sec/step, loss=0.07606, avg_loss=0.07467]\n",
      "Step 517424  [5.494 sec/step, loss=0.07696, avg_loss=0.07469]\n",
      "Step 517425  [5.539 sec/step, loss=0.06637, avg_loss=0.07460]\n",
      "Step 517426  [5.524 sec/step, loss=0.07434, avg_loss=0.07458]\n",
      "Step 517427  [5.563 sec/step, loss=0.07333, avg_loss=0.07456]\n",
      "Step 517428  [5.540 sec/step, loss=0.07492, avg_loss=0.07458]\n",
      "Step 517429  [5.536 sec/step, loss=0.07564, avg_loss=0.07458]\n",
      "Step 517430  [5.567 sec/step, loss=0.07432, avg_loss=0.07461]\n",
      "Step 517431  [5.577 sec/step, loss=0.07634, avg_loss=0.07463]\n",
      "Step 517432  [5.573 sec/step, loss=0.07351, avg_loss=0.07462]\n",
      "Generated 32 batches of size 32 in 2.428 sec\n",
      "Step 517433  [5.585 sec/step, loss=0.07662, avg_loss=0.07464]\n",
      "Step 517434  [5.582 sec/step, loss=0.07252, avg_loss=0.07463]\n",
      "Step 517435  [5.566 sec/step, loss=0.06753, avg_loss=0.07455]\n",
      "Step 517436  [5.568 sec/step, loss=0.07364, avg_loss=0.07453]\n",
      "Step 517437  [5.561 sec/step, loss=0.07629, avg_loss=0.07452]\n",
      "Step 517438  [5.549 sec/step, loss=0.07631, avg_loss=0.07452]\n",
      "Step 517439  [5.498 sec/step, loss=0.07444, avg_loss=0.07456]\n",
      "Step 517440  [5.505 sec/step, loss=0.07326, avg_loss=0.07453]\n",
      "Step 517441  [5.523 sec/step, loss=0.07362, avg_loss=0.07450]\n",
      "Step 517442  [5.539 sec/step, loss=0.07692, avg_loss=0.07453]\n",
      "Step 517443  [5.538 sec/step, loss=0.07593, avg_loss=0.07452]\n",
      "Step 517444  [5.533 sec/step, loss=0.07340, avg_loss=0.07448]\n",
      "Step 517445  [5.530 sec/step, loss=0.07705, avg_loss=0.07452]\n",
      "Step 517446  [5.554 sec/step, loss=0.07534, avg_loss=0.07455]\n",
      "Step 517447  [5.522 sec/step, loss=0.06601, avg_loss=0.07444]\n",
      "Step 517448  [5.514 sec/step, loss=0.07513, avg_loss=0.07444]\n",
      "Step 517449  [5.517 sec/step, loss=0.07434, avg_loss=0.07442]\n",
      "Step 517450  [5.530 sec/step, loss=0.07575, avg_loss=0.07443]\n",
      "Step 517451  [5.517 sec/step, loss=0.07473, avg_loss=0.07441]\n",
      "Step 517452  [5.537 sec/step, loss=0.07629, avg_loss=0.07445]\n",
      "Step 517453  [5.552 sec/step, loss=0.07661, avg_loss=0.07449]\n",
      "Step 517454  [5.570 sec/step, loss=0.07567, avg_loss=0.07458]\n",
      "Step 517455  [5.572 sec/step, loss=0.07485, avg_loss=0.07458]\n",
      "Step 517456  [5.595 sec/step, loss=0.07677, avg_loss=0.07460]\n",
      "Step 517457  [5.633 sec/step, loss=0.06716, avg_loss=0.07450]\n",
      "Step 517458  [5.629 sec/step, loss=0.07225, avg_loss=0.07449]\n",
      "Step 517459  [5.605 sec/step, loss=0.07322, avg_loss=0.07456]\n",
      "Step 517460  [5.608 sec/step, loss=0.07663, avg_loss=0.07457]\n",
      "Step 517461  [5.570 sec/step, loss=0.07204, avg_loss=0.07455]\n",
      "Step 517462  [5.553 sec/step, loss=0.07568, avg_loss=0.07457]\n",
      "Step 517463  [5.548 sec/step, loss=0.07373, avg_loss=0.07456]\n",
      "Step 517464  [5.565 sec/step, loss=0.07652, avg_loss=0.07458]\n",
      "Generated 32 batches of size 32 in 2.413 sec\n",
      "Step 517465  [5.572 sec/step, loss=0.07414, avg_loss=0.07455]\n",
      "Step 517466  [5.568 sec/step, loss=0.07550, avg_loss=0.07454]\n",
      "Step 517467  [5.564 sec/step, loss=0.07589, avg_loss=0.07455]\n",
      "Step 517468  [5.569 sec/step, loss=0.07467, avg_loss=0.07455]\n",
      "Step 517469  [5.560 sec/step, loss=0.07550, avg_loss=0.07456]\n",
      "Step 517470  [5.542 sec/step, loss=0.07469, avg_loss=0.07454]\n",
      "Step 517471  [5.530 sec/step, loss=0.07416, avg_loss=0.07452]\n",
      "Step 517472  [5.532 sec/step, loss=0.07548, avg_loss=0.07451]\n",
      "Step 517473  [5.532 sec/step, loss=0.07616, avg_loss=0.07454]\n",
      "Step 517474  [5.549 sec/step, loss=0.07403, avg_loss=0.07454]\n",
      "Step 517475  [5.535 sec/step, loss=0.07210, avg_loss=0.07451]\n",
      "Step 517476  [5.521 sec/step, loss=0.07495, avg_loss=0.07450]\n",
      "Step 517477  [5.532 sec/step, loss=0.07545, avg_loss=0.07451]\n",
      "Step 517478  [5.518 sec/step, loss=0.07300, avg_loss=0.07449]\n",
      "Step 517479  [5.561 sec/step, loss=0.07400, avg_loss=0.07451]\n",
      "Step 517480  [5.562 sec/step, loss=0.07231, avg_loss=0.07447]\n",
      "Step 517481  [5.545 sec/step, loss=0.07421, avg_loss=0.07445]\n",
      "Step 517482  [5.543 sec/step, loss=0.07667, avg_loss=0.07446]\n",
      "Step 517483  [5.527 sec/step, loss=0.06718, avg_loss=0.07440]\n",
      "Step 517484  [5.533 sec/step, loss=0.07705, avg_loss=0.07440]\n",
      "Step 517485  [5.525 sec/step, loss=0.07423, avg_loss=0.07438]\n",
      "Step 517486  [5.525 sec/step, loss=0.07561, avg_loss=0.07438]\n",
      "Step 517487  [5.517 sec/step, loss=0.07324, avg_loss=0.07435]\n",
      "Step 517488  [5.517 sec/step, loss=0.07523, avg_loss=0.07437]\n",
      "Step 517489  [5.519 sec/step, loss=0.07627, avg_loss=0.07439]\n",
      "Step 517490  [5.518 sec/step, loss=0.07591, avg_loss=0.07439]\n",
      "Step 517491  [5.534 sec/step, loss=0.07630, avg_loss=0.07440]\n",
      "Step 517492  [5.531 sec/step, loss=0.07417, avg_loss=0.07439]\n",
      "Step 517493  [5.543 sec/step, loss=0.07710, avg_loss=0.07440]\n",
      "Step 517494  [5.531 sec/step, loss=0.07509, avg_loss=0.07439]\n",
      "Step 517495  [5.520 sec/step, loss=0.07440, avg_loss=0.07438]\n",
      "Step 517496  [5.527 sec/step, loss=0.07497, avg_loss=0.07439]\n",
      "Generated 32 batches of size 32 in 2.394 sec\n",
      "Step 517497  [5.485 sec/step, loss=0.07494, avg_loss=0.07447]\n",
      "Step 517498  [5.507 sec/step, loss=0.07442, avg_loss=0.07451]\n",
      "Step 517499  [5.521 sec/step, loss=0.07693, avg_loss=0.07453]\n",
      "Step 517500  [5.525 sec/step, loss=0.07469, avg_loss=0.07453]\n",
      "Writing summary at step: 517500\n",
      "Step 517501  [5.496 sec/step, loss=0.07577, avg_loss=0.07454]\n",
      "Step 517502  [5.532 sec/step, loss=0.06716, avg_loss=0.07446]\n",
      "Step 517503  [5.532 sec/step, loss=0.07661, avg_loss=0.07446]\n",
      "Step 517504  [5.512 sec/step, loss=0.07216, avg_loss=0.07441]\n",
      "Step 517505  [5.518 sec/step, loss=0.07651, avg_loss=0.07443]\n",
      "Step 517506  [5.526 sec/step, loss=0.07448, avg_loss=0.07446]\n",
      "Step 517507  [5.509 sec/step, loss=0.07559, avg_loss=0.07446]\n",
      "Step 517508  [5.528 sec/step, loss=0.07620, avg_loss=0.07448]\n",
      "Step 517509  [5.519 sec/step, loss=0.07540, avg_loss=0.07450]\n",
      "Step 517510  [5.562 sec/step, loss=0.06551, avg_loss=0.07439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 517511  [5.578 sec/step, loss=0.07507, avg_loss=0.07439]\n",
      "Step 517512  [5.592 sec/step, loss=0.07571, avg_loss=0.07440]\n",
      "Step 517513  [5.597 sec/step, loss=0.07483, avg_loss=0.07441]\n",
      "Step 517514  [5.605 sec/step, loss=0.07604, avg_loss=0.07441]\n",
      "Step 517515  [5.628 sec/step, loss=0.07689, avg_loss=0.07445]\n",
      "Step 517516  [5.631 sec/step, loss=0.07499, avg_loss=0.07445]\n",
      "Step 517517  [5.630 sec/step, loss=0.07656, avg_loss=0.07446]\n",
      "Step 517518  [5.631 sec/step, loss=0.07589, avg_loss=0.07449]\n",
      "Step 517519  [5.624 sec/step, loss=0.07393, avg_loss=0.07446]\n",
      "Step 517520  [5.610 sec/step, loss=0.07599, avg_loss=0.07446]\n",
      "Step 517521  [5.596 sec/step, loss=0.07196, avg_loss=0.07444]\n",
      "Step 517522  [5.599 sec/step, loss=0.07391, avg_loss=0.07446]\n",
      "Step 517523  [5.584 sec/step, loss=0.07181, avg_loss=0.07442]\n",
      "Step 517524  [5.577 sec/step, loss=0.07279, avg_loss=0.07437]\n",
      "Step 517525  [5.539 sec/step, loss=0.07408, avg_loss=0.07445]\n",
      "Step 517526  [5.526 sec/step, loss=0.06593, avg_loss=0.07437]\n",
      "Step 517527  [5.500 sec/step, loss=0.07620, avg_loss=0.07440]\n",
      "Generated 32 batches of size 32 in 2.416 sec\n",
      "Step 517528  [5.510 sec/step, loss=0.07702, avg_loss=0.07442]\n",
      "Step 517529  [5.530 sec/step, loss=0.07591, avg_loss=0.07442]\n",
      "Step 517530  [5.508 sec/step, loss=0.07570, avg_loss=0.07443]\n",
      "Step 517531  [5.505 sec/step, loss=0.07458, avg_loss=0.07442]\n",
      "Step 517532  [5.498 sec/step, loss=0.07468, avg_loss=0.07443]\n",
      "Step 517533  [5.486 sec/step, loss=0.07571, avg_loss=0.07442]\n",
      "Step 517534  [5.502 sec/step, loss=0.07637, avg_loss=0.07446]\n",
      "Step 517535  [5.513 sec/step, loss=0.07277, avg_loss=0.07451]\n",
      "Step 517536  [5.502 sec/step, loss=0.07081, avg_loss=0.07448]\n",
      "Step 517537  [5.504 sec/step, loss=0.07552, avg_loss=0.07447]\n",
      "Step 517538  [5.508 sec/step, loss=0.07510, avg_loss=0.07446]\n",
      "Step 517539  [5.515 sec/step, loss=0.07218, avg_loss=0.07444]\n",
      "Step 517540  [5.560 sec/step, loss=0.06767, avg_loss=0.07438]\n",
      "Step 517541  [5.541 sec/step, loss=0.07599, avg_loss=0.07441]\n",
      "Step 517542  [5.536 sec/step, loss=0.07656, avg_loss=0.07440]\n",
      "Step 517543  [5.539 sec/step, loss=0.07460, avg_loss=0.07439]\n",
      "Step 517544  [5.529 sec/step, loss=0.07449, avg_loss=0.07440]\n",
      "Step 517545  [5.526 sec/step, loss=0.07586, avg_loss=0.07439]\n",
      "Step 517546  [5.513 sec/step, loss=0.07438, avg_loss=0.07438]\n",
      "Step 517547  [5.512 sec/step, loss=0.06705, avg_loss=0.07439]\n",
      "Step 517548  [5.537 sec/step, loss=0.07415, avg_loss=0.07438]\n",
      "Step 517549  [5.532 sec/step, loss=0.07053, avg_loss=0.07434]\n",
      "Step 517550  [5.526 sec/step, loss=0.07632, avg_loss=0.07435]\n",
      "Step 517551  [5.535 sec/step, loss=0.07626, avg_loss=0.07436]\n",
      "Step 517552  [5.530 sec/step, loss=0.07428, avg_loss=0.07434]\n",
      "Step 517553  [5.521 sec/step, loss=0.07406, avg_loss=0.07432]\n",
      "Step 517554  [5.520 sec/step, loss=0.07565, avg_loss=0.07432]\n",
      "Step 517555  [5.523 sec/step, loss=0.07442, avg_loss=0.07431]\n",
      "Step 517556  [5.513 sec/step, loss=0.07359, avg_loss=0.07428]\n",
      "Step 517557  [5.480 sec/step, loss=0.07511, avg_loss=0.07436]\n",
      "Step 517558  [5.521 sec/step, loss=0.07452, avg_loss=0.07438]\n",
      "Step 517559  [5.479 sec/step, loss=0.07171, avg_loss=0.07437]\n",
      "Generated 32 batches of size 32 in 2.440 sec\n",
      "Step 517560  [5.490 sec/step, loss=0.07629, avg_loss=0.07436]\n",
      "Step 517561  [5.498 sec/step, loss=0.07397, avg_loss=0.07438]\n",
      "Step 517562  [5.503 sec/step, loss=0.07671, avg_loss=0.07439]\n",
      "Step 517563  [5.513 sec/step, loss=0.07317, avg_loss=0.07439]\n",
      "Step 517564  [5.513 sec/step, loss=0.07657, avg_loss=0.07439]\n",
      "Step 517565  [5.487 sec/step, loss=0.07187, avg_loss=0.07437]\n",
      "Step 517566  [5.486 sec/step, loss=0.07684, avg_loss=0.07438]\n",
      "Step 517567  [5.489 sec/step, loss=0.07720, avg_loss=0.07439]\n",
      "Step 517568  [5.486 sec/step, loss=0.07642, avg_loss=0.07441]\n",
      "Step 517569  [5.485 sec/step, loss=0.07648, avg_loss=0.07442]\n",
      "Step 517570  [5.511 sec/step, loss=0.07518, avg_loss=0.07442]\n",
      "Step 517571  [5.498 sec/step, loss=0.06766, avg_loss=0.07436]\n",
      "Step 517572  [5.494 sec/step, loss=0.07534, avg_loss=0.07436]\n",
      "Step 517573  [5.501 sec/step, loss=0.07615, avg_loss=0.07436]\n",
      "Step 517574  [5.486 sec/step, loss=0.07600, avg_loss=0.07438]\n",
      "Step 517575  [5.477 sec/step, loss=0.07348, avg_loss=0.07439]\n",
      "Step 517576  [5.479 sec/step, loss=0.07164, avg_loss=0.07436]\n",
      "Step 517577  [5.471 sec/step, loss=0.07298, avg_loss=0.07433]\n",
      "Step 517578  [5.490 sec/step, loss=0.07421, avg_loss=0.07435]\n",
      "Step 517579  [5.459 sec/step, loss=0.07629, avg_loss=0.07437]\n",
      "Step 517580  [5.472 sec/step, loss=0.07691, avg_loss=0.07441]\n",
      "Step 517581  [5.482 sec/step, loss=0.07448, avg_loss=0.07442]\n",
      "Step 517582  [5.483 sec/step, loss=0.07735, avg_loss=0.07442]\n",
      "Step 517583  [5.515 sec/step, loss=0.07620, avg_loss=0.07451]\n",
      "Step 517584  [5.553 sec/step, loss=0.06646, avg_loss=0.07441]\n",
      "Step 517585  [5.557 sec/step, loss=0.07687, avg_loss=0.07443]\n",
      "Step 517586  [5.564 sec/step, loss=0.07572, avg_loss=0.07444]\n",
      "Step 517587  [5.572 sec/step, loss=0.07560, avg_loss=0.07446]\n",
      "Step 517588  [5.553 sec/step, loss=0.07374, avg_loss=0.07444]\n",
      "Step 517589  [5.566 sec/step, loss=0.07722, avg_loss=0.07445]\n",
      "Step 517590  [5.568 sec/step, loss=0.07645, avg_loss=0.07446]\n",
      "Step 517591  [5.560 sec/step, loss=0.07292, avg_loss=0.07443]\n",
      "Generated 32 batches of size 32 in 2.549 sec\n",
      "Step 517592  [5.555 sec/step, loss=0.07433, avg_loss=0.07443]\n",
      "Step 517593  [5.537 sec/step, loss=0.07496, avg_loss=0.07441]\n",
      "Step 517594  [5.556 sec/step, loss=0.07626, avg_loss=0.07442]\n",
      "Step 517595  [5.568 sec/step, loss=0.07566, avg_loss=0.07443]\n",
      "Step 517596  [5.566 sec/step, loss=0.07481, avg_loss=0.07443]\n",
      "Step 517597  [5.552 sec/step, loss=0.07446, avg_loss=0.07442]\n",
      "Step 517598  [5.568 sec/step, loss=0.07352, avg_loss=0.07441]\n",
      "Step 517599  [5.562 sec/step, loss=0.07461, avg_loss=0.07439]\n",
      "Step 517600  [5.562 sec/step, loss=0.07466, avg_loss=0.07439]\n",
      "Writing summary at step: 517600\n",
      "Step 517601  [5.568 sec/step, loss=0.07557, avg_loss=0.07439]\n",
      "Step 517602  [5.529 sec/step, loss=0.07696, avg_loss=0.07449]\n",
      "Step 517603  [5.522 sec/step, loss=0.07593, avg_loss=0.07448]\n",
      "Step 517604  [5.530 sec/step, loss=0.07451, avg_loss=0.07450]\n",
      "Step 517605  [5.530 sec/step, loss=0.07676, avg_loss=0.07451]\n",
      "Step 517606  [5.518 sec/step, loss=0.07415, avg_loss=0.07450]\n",
      "Step 517607  [5.526 sec/step, loss=0.07683, avg_loss=0.07452]\n",
      "Step 517608  [5.519 sec/step, loss=0.07414, avg_loss=0.07450]\n",
      "Step 517609  [5.519 sec/step, loss=0.07483, avg_loss=0.07449]\n",
      "Step 517610  [5.496 sec/step, loss=0.07422, avg_loss=0.07458]\n",
      "Step 517611  [5.501 sec/step, loss=0.07691, avg_loss=0.07460]\n",
      "Step 517612  [5.479 sec/step, loss=0.07607, avg_loss=0.07460]\n",
      "Step 517613  [5.488 sec/step, loss=0.07672, avg_loss=0.07462]\n",
      "Step 517614  [5.470 sec/step, loss=0.07587, avg_loss=0.07462]\n",
      "Step 517615  [5.451 sec/step, loss=0.07504, avg_loss=0.07460]\n",
      "Step 517616  [5.452 sec/step, loss=0.07499, avg_loss=0.07460]\n",
      "Step 517617  [5.450 sec/step, loss=0.07646, avg_loss=0.07460]\n",
      "Step 517618  [5.450 sec/step, loss=0.07522, avg_loss=0.07459]\n",
      "Step 517619  [5.444 sec/step, loss=0.07544, avg_loss=0.07460]\n",
      "Step 517620  [5.441 sec/step, loss=0.07410, avg_loss=0.07459]\n",
      "Step 517621  [5.442 sec/step, loss=0.07266, avg_loss=0.07459]\n",
      "Step 517622  [5.501 sec/step, loss=0.06761, avg_loss=0.07453]\n",
      "Generated 32 batches of size 32 in 2.444 sec\n",
      "Step 517623  [5.537 sec/step, loss=0.07442, avg_loss=0.07456]\n",
      "Step 517624  [5.537 sec/step, loss=0.07284, avg_loss=0.07456]\n",
      "Step 517625  [5.528 sec/step, loss=0.07426, avg_loss=0.07456]\n",
      "Step 517626  [5.529 sec/step, loss=0.06691, avg_loss=0.07457]\n",
      "Step 517627  [5.542 sec/step, loss=0.07506, avg_loss=0.07456]\n",
      "Step 517628  [5.516 sec/step, loss=0.07214, avg_loss=0.07451]\n",
      "Step 517629  [5.494 sec/step, loss=0.07357, avg_loss=0.07448]\n",
      "Step 517630  [5.502 sec/step, loss=0.07548, avg_loss=0.07448]\n",
      "Step 517631  [5.521 sec/step, loss=0.07577, avg_loss=0.07449]\n",
      "Step 517632  [5.540 sec/step, loss=0.07653, avg_loss=0.07451]\n",
      "Step 517633  [5.549 sec/step, loss=0.07656, avg_loss=0.07452]\n",
      "Step 517634  [5.523 sec/step, loss=0.07468, avg_loss=0.07450]\n",
      "Step 517635  [5.531 sec/step, loss=0.07291, avg_loss=0.07451]\n",
      "Step 517636  [5.550 sec/step, loss=0.07479, avg_loss=0.07455]\n",
      "Step 517637  [5.542 sec/step, loss=0.07571, avg_loss=0.07455]\n",
      "Step 517638  [5.533 sec/step, loss=0.07459, avg_loss=0.07454]\n",
      "Step 517639  [5.544 sec/step, loss=0.07690, avg_loss=0.07459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 517640  [5.499 sec/step, loss=0.07498, avg_loss=0.07466]\n",
      "Step 517641  [5.503 sec/step, loss=0.07513, avg_loss=0.07465]\n",
      "Step 517642  [5.491 sec/step, loss=0.07414, avg_loss=0.07463]\n",
      "Step 517643  [5.474 sec/step, loss=0.07326, avg_loss=0.07462]\n",
      "Step 517644  [5.484 sec/step, loss=0.07602, avg_loss=0.07463]\n",
      "Step 517645  [5.462 sec/step, loss=0.06640, avg_loss=0.07454]\n",
      "Step 517646  [5.493 sec/step, loss=0.07335, avg_loss=0.07453]\n",
      "Step 517647  [5.515 sec/step, loss=0.07555, avg_loss=0.07461]\n",
      "Step 517648  [5.477 sec/step, loss=0.07227, avg_loss=0.07459]\n",
      "Step 517649  [5.491 sec/step, loss=0.07503, avg_loss=0.07464]\n",
      "Step 517650  [5.475 sec/step, loss=0.07364, avg_loss=0.07461]\n",
      "Step 517651  [5.472 sec/step, loss=0.07187, avg_loss=0.07457]\n",
      "Step 517652  [5.462 sec/step, loss=0.07606, avg_loss=0.07459]\n",
      "Step 517653  [5.473 sec/step, loss=0.07617, avg_loss=0.07461]\n",
      "Step 517654  [5.460 sec/step, loss=0.07082, avg_loss=0.07456]\n",
      "Generated 32 batches of size 32 in 2.404 sec\n",
      "Step 517655  [5.480 sec/step, loss=0.07647, avg_loss=0.07458]\n",
      "Step 517656  [5.529 sec/step, loss=0.06635, avg_loss=0.07451]\n",
      "Step 517657  [5.512 sec/step, loss=0.07548, avg_loss=0.07451]\n",
      "Step 517658  [5.496 sec/step, loss=0.07696, avg_loss=0.07453]\n",
      "Step 517659  [5.521 sec/step, loss=0.07656, avg_loss=0.07458]\n",
      "Step 517660  [5.506 sec/step, loss=0.07618, avg_loss=0.07458]\n",
      "Step 517661  [5.514 sec/step, loss=0.07539, avg_loss=0.07460]\n",
      "Step 517662  [5.526 sec/step, loss=0.07664, avg_loss=0.07460]\n",
      "Step 517663  [5.522 sec/step, loss=0.07544, avg_loss=0.07462]\n",
      "Step 517664  [5.506 sec/step, loss=0.07225, avg_loss=0.07458]\n",
      "Step 517665  [5.526 sec/step, loss=0.07666, avg_loss=0.07462]\n",
      "Step 517666  [5.518 sec/step, loss=0.07249, avg_loss=0.07458]\n",
      "Step 517667  [5.506 sec/step, loss=0.07542, avg_loss=0.07456]\n",
      "Step 517668  [5.503 sec/step, loss=0.07512, avg_loss=0.07455]\n",
      "Step 517669  [5.511 sec/step, loss=0.07542, avg_loss=0.07454]\n",
      "Step 517670  [5.504 sec/step, loss=0.07722, avg_loss=0.07456]\n",
      "Step 517671  [5.525 sec/step, loss=0.07581, avg_loss=0.07464]\n",
      "Step 517672  [5.519 sec/step, loss=0.07497, avg_loss=0.07464]\n",
      "Step 517673  [5.567 sec/step, loss=0.06590, avg_loss=0.07453]\n",
      "Step 517674  [5.558 sec/step, loss=0.07423, avg_loss=0.07452]\n",
      "Step 517675  [5.578 sec/step, loss=0.07341, avg_loss=0.07452]\n",
      "Step 517676  [5.582 sec/step, loss=0.07654, avg_loss=0.07456]\n",
      "Step 517677  [5.603 sec/step, loss=0.07638, avg_loss=0.07460]\n",
      "Step 517678  [5.618 sec/step, loss=0.07653, avg_loss=0.07462]\n",
      "Step 517679  [5.650 sec/step, loss=0.07381, avg_loss=0.07460]\n",
      "Step 517680  [5.631 sec/step, loss=0.07485, avg_loss=0.07458]\n",
      "Step 517681  [5.635 sec/step, loss=0.07462, avg_loss=0.07458]\n",
      "Step 517682  [5.615 sec/step, loss=0.07328, avg_loss=0.07454]\n",
      "Step 517683  [5.615 sec/step, loss=0.07670, avg_loss=0.07454]\n",
      "Step 517684  [5.562 sec/step, loss=0.07592, avg_loss=0.07464]\n",
      "Step 517685  [5.542 sec/step, loss=0.07480, avg_loss=0.07462]\n",
      "Step 517686  [5.518 sec/step, loss=0.07135, avg_loss=0.07457]\n",
      "Generated 32 batches of size 32 in 2.572 sec\n",
      "Step 517687  [5.518 sec/step, loss=0.07541, avg_loss=0.07457]\n",
      "Step 517688  [5.544 sec/step, loss=0.07695, avg_loss=0.07460]\n",
      "Step 517689  [5.542 sec/step, loss=0.07447, avg_loss=0.07457]\n",
      "Step 517690  [5.520 sec/step, loss=0.06754, avg_loss=0.07449]\n",
      "Step 517691  [5.529 sec/step, loss=0.07480, avg_loss=0.07450]\n",
      "Step 517692  [5.536 sec/step, loss=0.07623, avg_loss=0.07452]\n",
      "Step 517693  [5.560 sec/step, loss=0.07447, avg_loss=0.07452]\n",
      "Step 517694  [5.544 sec/step, loss=0.07486, avg_loss=0.07450]\n",
      "Step 517695  [5.532 sec/step, loss=0.07140, avg_loss=0.07446]\n",
      "Step 517696  [5.524 sec/step, loss=0.07372, avg_loss=0.07445]\n",
      "Step 517697  [5.539 sec/step, loss=0.07623, avg_loss=0.07447]\n",
      "Step 517698  [5.512 sec/step, loss=0.07465, avg_loss=0.07448]\n",
      "Step 517699  [5.505 sec/step, loss=0.07476, avg_loss=0.07448]\n",
      "Step 517700  [5.502 sec/step, loss=0.07262, avg_loss=0.07446]\n",
      "Writing summary at step: 517700\n",
      "Step 517701  [5.500 sec/step, loss=0.07653, avg_loss=0.07447]\n",
      "Step 517702  [5.481 sec/step, loss=0.07411, avg_loss=0.07444]\n",
      "Step 517703  [5.476 sec/step, loss=0.07341, avg_loss=0.07442]\n",
      "Step 517704  [5.472 sec/step, loss=0.07382, avg_loss=0.07441]\n",
      "Step 517705  [5.459 sec/step, loss=0.07414, avg_loss=0.07438]\n",
      "Step 517706  [5.477 sec/step, loss=0.07686, avg_loss=0.07441]\n",
      "Step 517707  [5.469 sec/step, loss=0.07571, avg_loss=0.07440]\n",
      "Step 517708  [5.463 sec/step, loss=0.07549, avg_loss=0.07441]\n",
      "Step 517709  [5.461 sec/step, loss=0.07464, avg_loss=0.07441]\n",
      "Step 517710  [5.482 sec/step, loss=0.06745, avg_loss=0.07434]\n",
      "Step 517711  [5.477 sec/step, loss=0.07292, avg_loss=0.07430]\n",
      "Step 517712  [5.484 sec/step, loss=0.07479, avg_loss=0.07429]\n",
      "Step 517713  [5.493 sec/step, loss=0.07701, avg_loss=0.07429]\n",
      "Step 517714  [5.500 sec/step, loss=0.07569, avg_loss=0.07429]\n",
      "Step 517715  [5.507 sec/step, loss=0.07601, avg_loss=0.07430]\n",
      "Step 517716  [5.511 sec/step, loss=0.07686, avg_loss=0.07432]\n",
      "Step 517717  [5.489 sec/step, loss=0.07268, avg_loss=0.07428]\n",
      "Generated 32 batches of size 32 in 2.443 sec\n",
      "Step 517718  [5.521 sec/step, loss=0.07426, avg_loss=0.07427]\n",
      "Step 517719  [5.525 sec/step, loss=0.07273, avg_loss=0.07425]\n",
      "Step 517720  [5.525 sec/step, loss=0.07463, avg_loss=0.07425]\n",
      "Step 517721  [5.523 sec/step, loss=0.06569, avg_loss=0.07418]\n",
      "Step 517722  [5.496 sec/step, loss=0.07350, avg_loss=0.07424]\n",
      "Step 517723  [5.476 sec/step, loss=0.07695, avg_loss=0.07427]\n",
      "Step 517724  [5.489 sec/step, loss=0.07641, avg_loss=0.07430]\n",
      "Step 517725  [5.489 sec/step, loss=0.07551, avg_loss=0.07431]\n",
      "Step 517726  [5.520 sec/step, loss=0.07454, avg_loss=0.07439]\n",
      "Step 517727  [5.506 sec/step, loss=0.07635, avg_loss=0.07440]\n",
      "Step 517728  [5.544 sec/step, loss=0.07371, avg_loss=0.07442]\n",
      "Step 517729  [5.556 sec/step, loss=0.07674, avg_loss=0.07445]\n",
      "Step 517730  [5.542 sec/step, loss=0.07128, avg_loss=0.07441]\n",
      "Step 517731  [5.524 sec/step, loss=0.07469, avg_loss=0.07440]\n",
      "Step 517732  [5.520 sec/step, loss=0.07602, avg_loss=0.07439]\n",
      "Step 517733  [5.559 sec/step, loss=0.06656, avg_loss=0.07429]\n",
      "Step 517734  [5.552 sec/step, loss=0.06728, avg_loss=0.07422]\n",
      "Step 517735  [5.556 sec/step, loss=0.07509, avg_loss=0.07424]\n",
      "Step 517736  [5.561 sec/step, loss=0.07399, avg_loss=0.07423]\n",
      "Step 517737  [5.562 sec/step, loss=0.07445, avg_loss=0.07422]\n",
      "Step 517738  [5.566 sec/step, loss=0.07460, avg_loss=0.07422]\n",
      "Step 517739  [5.559 sec/step, loss=0.07303, avg_loss=0.07418]\n",
      "Step 517740  [5.548 sec/step, loss=0.07141, avg_loss=0.07415]\n",
      "Step 517741  [5.560 sec/step, loss=0.07624, avg_loss=0.07416]\n",
      "Step 517742  [5.550 sec/step, loss=0.07243, avg_loss=0.07414]\n",
      "Step 517743  [5.553 sec/step, loss=0.07554, avg_loss=0.07416]\n",
      "Step 517744  [5.561 sec/step, loss=0.07565, avg_loss=0.07416]\n",
      "Step 517745  [5.578 sec/step, loss=0.07587, avg_loss=0.07425]\n",
      "Step 517746  [5.558 sec/step, loss=0.07622, avg_loss=0.07428]\n",
      "Step 517747  [5.549 sec/step, loss=0.07417, avg_loss=0.07427]\n",
      "Step 517748  [5.570 sec/step, loss=0.07610, avg_loss=0.07431]\n",
      "Step 517749  [5.554 sec/step, loss=0.07431, avg_loss=0.07430]\n",
      "Generated 32 batches of size 32 in 2.392 sec\n",
      "Step 517750  [5.574 sec/step, loss=0.07449, avg_loss=0.07431]\n",
      "Step 517751  [5.579 sec/step, loss=0.07562, avg_loss=0.07434]\n",
      "Step 517752  [5.589 sec/step, loss=0.07645, avg_loss=0.07435]\n",
      "Step 517753  [5.585 sec/step, loss=0.07288, avg_loss=0.07432]\n",
      "Step 517754  [5.586 sec/step, loss=0.07236, avg_loss=0.07433]\n",
      "Step 517755  [5.586 sec/step, loss=0.07659, avg_loss=0.07433]\n",
      "Step 517756  [5.543 sec/step, loss=0.07730, avg_loss=0.07444]\n",
      "Step 517757  [5.546 sec/step, loss=0.07626, avg_loss=0.07445]\n",
      "Step 517758  [5.546 sec/step, loss=0.07698, avg_loss=0.07445]\n",
      "Step 517759  [5.543 sec/step, loss=0.07711, avg_loss=0.07446]\n",
      "Step 517760  [5.544 sec/step, loss=0.07589, avg_loss=0.07445]\n",
      "Step 517761  [5.527 sec/step, loss=0.07469, avg_loss=0.07445]\n",
      "Step 517762  [5.516 sec/step, loss=0.07602, avg_loss=0.07444]\n",
      "Step 517763  [5.516 sec/step, loss=0.07603, avg_loss=0.07445]\n",
      "Step 517764  [5.513 sec/step, loss=0.07444, avg_loss=0.07447]\n",
      "Step 517765  [5.500 sec/step, loss=0.07617, avg_loss=0.07446]\n",
      "Step 517766  [5.525 sec/step, loss=0.07527, avg_loss=0.07449]\n",
      "Step 517767  [5.528 sec/step, loss=0.07252, avg_loss=0.07446]\n",
      "Step 517768  [5.536 sec/step, loss=0.07595, avg_loss=0.07447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 517769  [5.543 sec/step, loss=0.07442, avg_loss=0.07446]\n",
      "Step 517770  [5.519 sec/step, loss=0.07288, avg_loss=0.07442]\n",
      "Step 517771  [5.519 sec/step, loss=0.07557, avg_loss=0.07441]\n",
      "Step 517772  [5.528 sec/step, loss=0.07552, avg_loss=0.07442]\n",
      "Step 517773  [5.485 sec/step, loss=0.07674, avg_loss=0.07453]\n",
      "Step 517774  [5.540 sec/step, loss=0.06696, avg_loss=0.07445]\n",
      "Step 517775  [5.525 sec/step, loss=0.07410, avg_loss=0.07446]\n",
      "Step 517776  [5.536 sec/step, loss=0.07676, avg_loss=0.07446]\n",
      "Step 517777  [5.517 sec/step, loss=0.07460, avg_loss=0.07445]\n",
      "Step 517778  [5.509 sec/step, loss=0.07705, avg_loss=0.07445]\n",
      "Step 517779  [5.493 sec/step, loss=0.07427, avg_loss=0.07446]\n",
      "Step 517780  [5.519 sec/step, loss=0.07613, avg_loss=0.07447]\n",
      "Step 517781  [5.514 sec/step, loss=0.07575, avg_loss=0.07448]\n",
      "Generated 32 batches of size 32 in 2.416 sec\n",
      "Step 517782  [5.543 sec/step, loss=0.07675, avg_loss=0.07451]\n",
      "Step 517783  [5.513 sec/step, loss=0.06646, avg_loss=0.07441]\n",
      "Step 517784  [5.511 sec/step, loss=0.07616, avg_loss=0.07441]\n",
      "Step 517785  [5.527 sec/step, loss=0.07578, avg_loss=0.07442]\n",
      "Step 517786  [5.539 sec/step, loss=0.07313, avg_loss=0.07444]\n",
      "Step 517787  [5.534 sec/step, loss=0.07453, avg_loss=0.07443]\n",
      "Step 517788  [5.515 sec/step, loss=0.07428, avg_loss=0.07441]\n",
      "Step 517789  [5.512 sec/step, loss=0.07621, avg_loss=0.07442]\n",
      "Step 517790  [5.521 sec/step, loss=0.07132, avg_loss=0.07446]\n",
      "Step 517791  [5.514 sec/step, loss=0.07483, avg_loss=0.07446]\n",
      "Step 517792  [5.534 sec/step, loss=0.07462, avg_loss=0.07445]\n",
      "Step 517793  [5.522 sec/step, loss=0.07562, avg_loss=0.07446]\n",
      "Step 517794  [5.505 sec/step, loss=0.07217, avg_loss=0.07443]\n",
      "Step 517795  [5.517 sec/step, loss=0.07505, avg_loss=0.07447]\n",
      "Step 517796  [5.530 sec/step, loss=0.07392, avg_loss=0.07447]\n",
      "Step 517797  [5.519 sec/step, loss=0.07470, avg_loss=0.07445]\n",
      "Step 517798  [5.519 sec/step, loss=0.07560, avg_loss=0.07446]\n",
      "Step 517799  [5.568 sec/step, loss=0.06739, avg_loss=0.07439]\n",
      "Step 517800  [5.591 sec/step, loss=0.07706, avg_loss=0.07443]\n",
      "Writing summary at step: 517800\n",
      "Step 517801  [5.592 sec/step, loss=0.07744, avg_loss=0.07444]\n",
      "Step 517802  [5.590 sec/step, loss=0.07138, avg_loss=0.07442]\n",
      "Step 517803  [5.601 sec/step, loss=0.07504, avg_loss=0.07443]\n",
      "Step 517804  [5.601 sec/step, loss=0.07460, avg_loss=0.07444]\n",
      "Step 517805  [5.609 sec/step, loss=0.07585, avg_loss=0.07446]\n",
      "Step 517806  [5.598 sec/step, loss=0.07524, avg_loss=0.07444]\n",
      "Step 517807  [5.605 sec/step, loss=0.07466, avg_loss=0.07443]\n",
      "Step 517808  [5.596 sec/step, loss=0.07227, avg_loss=0.07440]\n",
      "Step 517809  [5.604 sec/step, loss=0.07437, avg_loss=0.07440]\n",
      "Step 517810  [5.569 sec/step, loss=0.07668, avg_loss=0.07449]\n",
      "Step 517811  [5.559 sec/step, loss=0.07596, avg_loss=0.07452]\n",
      "Step 517812  [5.547 sec/step, loss=0.07591, avg_loss=0.07453]\n",
      "Generated 32 batches of size 32 in 2.544 sec\n",
      "Step 517813  [5.529 sec/step, loss=0.07430, avg_loss=0.07450]\n",
      "Step 517814  [5.519 sec/step, loss=0.07341, avg_loss=0.07448]\n",
      "Step 517815  [5.530 sec/step, loss=0.07710, avg_loss=0.07449]\n",
      "Step 517816  [5.533 sec/step, loss=0.07712, avg_loss=0.07449]\n",
      "Step 517817  [5.545 sec/step, loss=0.07248, avg_loss=0.07449]\n",
      "Step 517818  [5.537 sec/step, loss=0.07247, avg_loss=0.07447]\n",
      "Step 517819  [5.517 sec/step, loss=0.06717, avg_loss=0.07442]\n",
      "Step 517820  [5.531 sec/step, loss=0.07426, avg_loss=0.07441]\n",
      "Step 517821  [5.551 sec/step, loss=0.07574, avg_loss=0.07451]\n",
      "Step 517822  [5.524 sec/step, loss=0.07427, avg_loss=0.07452]\n",
      "Step 517823  [5.567 sec/step, loss=0.06585, avg_loss=0.07441]\n",
      "Step 517824  [5.568 sec/step, loss=0.07557, avg_loss=0.07440]\n",
      "Step 517825  [5.566 sec/step, loss=0.07461, avg_loss=0.07439]\n",
      "Step 517826  [5.556 sec/step, loss=0.07612, avg_loss=0.07441]\n",
      "Step 517827  [5.563 sec/step, loss=0.07703, avg_loss=0.07442]\n",
      "Step 517828  [5.554 sec/step, loss=0.07479, avg_loss=0.07443]\n",
      "Step 517829  [5.549 sec/step, loss=0.07237, avg_loss=0.07438]\n",
      "Step 517830  [5.550 sec/step, loss=0.07506, avg_loss=0.07442]\n",
      "Step 517831  [5.560 sec/step, loss=0.07644, avg_loss=0.07444]\n",
      "Step 517832  [5.565 sec/step, loss=0.07434, avg_loss=0.07442]\n",
      "Step 517833  [5.520 sec/step, loss=0.07532, avg_loss=0.07451]\n",
      "Step 517834  [5.535 sec/step, loss=0.07186, avg_loss=0.07456]\n",
      "Step 517835  [5.533 sec/step, loss=0.07626, avg_loss=0.07457]\n",
      "Step 517836  [5.502 sec/step, loss=0.07444, avg_loss=0.07457]\n",
      "Step 517837  [5.505 sec/step, loss=0.07332, avg_loss=0.07456]\n",
      "Step 517838  [5.514 sec/step, loss=0.07508, avg_loss=0.07456]\n",
      "Step 517839  [5.509 sec/step, loss=0.07551, avg_loss=0.07459]\n",
      "Step 517840  [5.504 sec/step, loss=0.07161, avg_loss=0.07459]\n",
      "Step 517841  [5.491 sec/step, loss=0.07534, avg_loss=0.07458]\n",
      "Step 517842  [5.514 sec/step, loss=0.07576, avg_loss=0.07462]\n",
      "Step 517843  [5.511 sec/step, loss=0.07294, avg_loss=0.07459]\n",
      "Step 517844  [5.498 sec/step, loss=0.07416, avg_loss=0.07457]\n",
      "Generated 32 batches of size 32 in 2.747 sec\n",
      "Step 517845  [5.487 sec/step, loss=0.07157, avg_loss=0.07453]\n",
      "Step 517846  [5.486 sec/step, loss=0.07667, avg_loss=0.07454]\n",
      "Step 517847  [5.474 sec/step, loss=0.06550, avg_loss=0.07445]\n",
      "Step 517848  [5.469 sec/step, loss=0.07639, avg_loss=0.07445]\n",
      "Step 517849  [5.489 sec/step, loss=0.07731, avg_loss=0.07448]\n",
      "Step 517850  [5.484 sec/step, loss=0.07669, avg_loss=0.07450]\n",
      "Step 517851  [5.491 sec/step, loss=0.07669, avg_loss=0.07452]\n",
      "Step 517852  [5.476 sec/step, loss=0.07583, avg_loss=0.07451]\n",
      "Step 517853  [5.501 sec/step, loss=0.07561, avg_loss=0.07454]\n",
      "Step 517854  [5.507 sec/step, loss=0.07402, avg_loss=0.07455]\n",
      "Step 517855  [5.495 sec/step, loss=0.07623, avg_loss=0.07455]\n",
      "Step 517856  [5.537 sec/step, loss=0.06827, avg_loss=0.07446]\n",
      "Step 517857  [5.544 sec/step, loss=0.07679, avg_loss=0.07446]\n",
      "Step 517858  [5.543 sec/step, loss=0.07389, avg_loss=0.07443]\n",
      "Step 517859  [5.535 sec/step, loss=0.07388, avg_loss=0.07440]\n",
      "Step 517860  [5.530 sec/step, loss=0.07592, avg_loss=0.07440]\n",
      "Step 517861  [5.540 sec/step, loss=0.07312, avg_loss=0.07439]\n",
      "Step 517862  [5.520 sec/step, loss=0.07482, avg_loss=0.07437]\n",
      "Step 517863  [5.515 sec/step, loss=0.07555, avg_loss=0.07437]\n",
      "Step 517864  [5.523 sec/step, loss=0.07244, avg_loss=0.07435]\n",
      "Step 517865  [5.530 sec/step, loss=0.07458, avg_loss=0.07433]\n",
      "Step 517866  [5.498 sec/step, loss=0.07431, avg_loss=0.07432]\n",
      "Step 517867  [5.523 sec/step, loss=0.07405, avg_loss=0.07434]\n",
      "Step 517868  [5.507 sec/step, loss=0.07451, avg_loss=0.07432]\n",
      "Step 517869  [5.507 sec/step, loss=0.07709, avg_loss=0.07435]\n",
      "Step 517870  [5.530 sec/step, loss=0.07533, avg_loss=0.07438]\n",
      "Step 517871  [5.528 sec/step, loss=0.07627, avg_loss=0.07438]\n",
      "Step 517872  [5.532 sec/step, loss=0.07667, avg_loss=0.07439]\n",
      "Step 517873  [5.545 sec/step, loss=0.07367, avg_loss=0.07436]\n",
      "Step 517874  [5.505 sec/step, loss=0.07687, avg_loss=0.07446]\n",
      "Step 517875  [5.515 sec/step, loss=0.07598, avg_loss=0.07448]\n",
      "Step 517876  [5.517 sec/step, loss=0.07693, avg_loss=0.07448]\n",
      "Generated 32 batches of size 32 in 2.469 sec\n",
      "Step 517877  [5.525 sec/step, loss=0.07505, avg_loss=0.07449]\n",
      "Step 517878  [5.510 sec/step, loss=0.07349, avg_loss=0.07445]\n",
      "Step 517879  [5.480 sec/step, loss=0.06637, avg_loss=0.07437]\n",
      "Step 517880  [5.451 sec/step, loss=0.07220, avg_loss=0.07433]\n",
      "Step 517881  [5.463 sec/step, loss=0.07684, avg_loss=0.07434]\n",
      "Step 517882  [5.448 sec/step, loss=0.07535, avg_loss=0.07433]\n",
      "Step 517883  [5.449 sec/step, loss=0.07214, avg_loss=0.07439]\n",
      "Step 517884  [5.464 sec/step, loss=0.07510, avg_loss=0.07438]\n",
      "Step 517885  [5.460 sec/step, loss=0.07573, avg_loss=0.07438]\n",
      "Step 517886  [5.481 sec/step, loss=0.07613, avg_loss=0.07441]\n",
      "Step 517887  [5.491 sec/step, loss=0.07757, avg_loss=0.07444]\n",
      "Step 517888  [5.493 sec/step, loss=0.07437, avg_loss=0.07444]\n",
      "Step 517889  [5.491 sec/step, loss=0.07603, avg_loss=0.07444]\n",
      "Step 517890  [5.484 sec/step, loss=0.06833, avg_loss=0.07441]\n",
      "Step 517891  [5.465 sec/step, loss=0.07288, avg_loss=0.07439]\n",
      "Step 517892  [5.433 sec/step, loss=0.07467, avg_loss=0.07439]\n",
      "Step 517893  [5.447 sec/step, loss=0.07668, avg_loss=0.07440]\n",
      "Step 517894  [5.459 sec/step, loss=0.07247, avg_loss=0.07440]\n",
      "Step 517895  [5.486 sec/step, loss=0.07368, avg_loss=0.07439]\n",
      "Step 517896  [5.475 sec/step, loss=0.07602, avg_loss=0.07441]\n",
      "Step 517897  [5.479 sec/step, loss=0.07553, avg_loss=0.07442]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 517898  [5.482 sec/step, loss=0.07624, avg_loss=0.07442]\n",
      "Step 517899  [5.423 sec/step, loss=0.07137, avg_loss=0.07446]\n",
      "Step 517900  [5.403 sec/step, loss=0.07514, avg_loss=0.07444]\n",
      "Writing summary at step: 517900\n",
      "Step 517901  [5.394 sec/step, loss=0.07565, avg_loss=0.07443]\n",
      "Step 517902  [5.411 sec/step, loss=0.07620, avg_loss=0.07447]\n",
      "Step 517903  [5.409 sec/step, loss=0.07658, avg_loss=0.07449]\n",
      "Step 517904  [5.420 sec/step, loss=0.07561, avg_loss=0.07450]\n",
      "Step 517905  [5.428 sec/step, loss=0.07459, avg_loss=0.07449]\n",
      "Step 517906  [5.477 sec/step, loss=0.06713, avg_loss=0.07441]\n",
      "Step 517907  [5.480 sec/step, loss=0.07712, avg_loss=0.07443]\n",
      "Generated 32 batches of size 32 in 2.410 sec\n",
      "Step 517908  [5.509 sec/step, loss=0.07656, avg_loss=0.07447]\n",
      "Step 517909  [5.510 sec/step, loss=0.07624, avg_loss=0.07449]\n",
      "Step 517910  [5.493 sec/step, loss=0.07409, avg_loss=0.07447]\n",
      "Step 517911  [5.505 sec/step, loss=0.07691, avg_loss=0.07448]\n",
      "Step 517912  [5.506 sec/step, loss=0.07590, avg_loss=0.07448]\n",
      "Step 517913  [5.518 sec/step, loss=0.07676, avg_loss=0.07450]\n",
      "Step 517914  [5.522 sec/step, loss=0.07573, avg_loss=0.07452]\n",
      "Step 517915  [5.513 sec/step, loss=0.07388, avg_loss=0.07449]\n",
      "Step 517916  [5.492 sec/step, loss=0.07310, avg_loss=0.07445]\n",
      "Step 517917  [5.544 sec/step, loss=0.06659, avg_loss=0.07439]\n",
      "Step 517918  [5.545 sec/step, loss=0.07390, avg_loss=0.07441]\n",
      "Step 517919  [5.567 sec/step, loss=0.07600, avg_loss=0.07449]\n",
      "Step 517920  [5.567 sec/step, loss=0.07514, avg_loss=0.07450]\n",
      "Step 517921  [5.549 sec/step, loss=0.07235, avg_loss=0.07447]\n",
      "Step 517922  [5.557 sec/step, loss=0.07585, avg_loss=0.07448]\n",
      "Step 517923  [5.515 sec/step, loss=0.07644, avg_loss=0.07459]\n",
      "Step 517924  [5.516 sec/step, loss=0.07643, avg_loss=0.07460]\n",
      "Step 517925  [5.518 sec/step, loss=0.07537, avg_loss=0.07461]\n",
      "Step 517926  [5.511 sec/step, loss=0.07480, avg_loss=0.07459]\n",
      "Step 517927  [5.485 sec/step, loss=0.06672, avg_loss=0.07449]\n",
      "Step 517928  [5.466 sec/step, loss=0.07273, avg_loss=0.07447]\n",
      "Step 517929  [5.463 sec/step, loss=0.07606, avg_loss=0.07451]\n",
      "Step 517930  [5.489 sec/step, loss=0.07635, avg_loss=0.07452]\n",
      "Step 517931  [5.481 sec/step, loss=0.07421, avg_loss=0.07450]\n",
      "Step 517932  [5.473 sec/step, loss=0.07628, avg_loss=0.07452]\n",
      "Step 517933  [5.479 sec/step, loss=0.07687, avg_loss=0.07453]\n",
      "Step 517934  [5.475 sec/step, loss=0.07466, avg_loss=0.07456]\n",
      "Step 517935  [5.479 sec/step, loss=0.07683, avg_loss=0.07457]\n",
      "Step 517936  [5.487 sec/step, loss=0.07396, avg_loss=0.07456]\n",
      "Step 517937  [5.479 sec/step, loss=0.07474, avg_loss=0.07458]\n",
      "Step 517938  [5.486 sec/step, loss=0.07485, avg_loss=0.07457]\n",
      "Step 517939  [5.497 sec/step, loss=0.07657, avg_loss=0.07458]\n",
      "Generated 32 batches of size 32 in 2.478 sec\n",
      "Step 517940  [5.501 sec/step, loss=0.07471, avg_loss=0.07461]\n",
      "Step 517941  [5.505 sec/step, loss=0.07262, avg_loss=0.07459]\n",
      "Step 517942  [5.497 sec/step, loss=0.07457, avg_loss=0.07458]\n",
      "Step 517943  [5.509 sec/step, loss=0.07652, avg_loss=0.07461]\n",
      "Step 517944  [5.529 sec/step, loss=0.07699, avg_loss=0.07464]\n",
      "Step 517945  [5.534 sec/step, loss=0.07372, avg_loss=0.07466]\n",
      "Step 517946  [5.532 sec/step, loss=0.07620, avg_loss=0.07466]\n",
      "Step 517947  [5.538 sec/step, loss=0.07102, avg_loss=0.07471]\n",
      "Step 517948  [5.533 sec/step, loss=0.07596, avg_loss=0.07471]\n",
      "Step 517949  [5.510 sec/step, loss=0.07450, avg_loss=0.07468]\n",
      "Step 517950  [5.498 sec/step, loss=0.07410, avg_loss=0.07465]\n",
      "Step 517951  [5.488 sec/step, loss=0.07394, avg_loss=0.07463]\n",
      "Step 517952  [5.503 sec/step, loss=0.07444, avg_loss=0.07461]\n",
      "Step 517953  [5.483 sec/step, loss=0.07526, avg_loss=0.07461]\n",
      "Step 517954  [5.487 sec/step, loss=0.07515, avg_loss=0.07462]\n",
      "Step 517955  [5.535 sec/step, loss=0.06707, avg_loss=0.07453]\n",
      "Step 517956  [5.494 sec/step, loss=0.07348, avg_loss=0.07458]\n",
      "Step 517957  [5.470 sec/step, loss=0.07220, avg_loss=0.07453]\n",
      "Step 517958  [5.442 sec/step, loss=0.06732, avg_loss=0.07447]\n",
      "Step 517959  [5.435 sec/step, loss=0.07444, avg_loss=0.07447]\n",
      "Step 517960  [5.443 sec/step, loss=0.07539, avg_loss=0.07447]\n",
      "Step 517961  [5.457 sec/step, loss=0.07702, avg_loss=0.07451]\n",
      "Step 517962  [5.483 sec/step, loss=0.07663, avg_loss=0.07453]\n",
      "Step 517963  [5.507 sec/step, loss=0.07382, avg_loss=0.07451]\n",
      "Step 517964  [5.504 sec/step, loss=0.07590, avg_loss=0.07454]\n",
      "Step 517965  [5.496 sec/step, loss=0.07358, avg_loss=0.07453]\n",
      "Step 517966  [5.496 sec/step, loss=0.07423, avg_loss=0.07453]\n",
      "Step 517967  [5.468 sec/step, loss=0.07210, avg_loss=0.07451]\n",
      "Step 517968  [5.478 sec/step, loss=0.07539, avg_loss=0.07452]\n",
      "Step 517969  [5.469 sec/step, loss=0.07654, avg_loss=0.07452]\n",
      "Step 517970  [5.457 sec/step, loss=0.07617, avg_loss=0.07452]\n",
      "Step 517971  [5.463 sec/step, loss=0.07662, avg_loss=0.07453]\n",
      "Generated 32 batches of size 32 in 2.353 sec\n",
      "Step 517972  [5.465 sec/step, loss=0.07483, avg_loss=0.07451]\n",
      "Step 517973  [5.459 sec/step, loss=0.07672, avg_loss=0.07454]\n",
      "Step 517974  [5.450 sec/step, loss=0.07646, avg_loss=0.07454]\n",
      "Step 517975  [5.454 sec/step, loss=0.07740, avg_loss=0.07455]\n",
      "Step 517976  [5.443 sec/step, loss=0.07532, avg_loss=0.07453]\n",
      "Step 517977  [5.427 sec/step, loss=0.07226, avg_loss=0.07451]\n",
      "Step 517978  [5.432 sec/step, loss=0.07513, avg_loss=0.07452]\n",
      "Step 517979  [5.460 sec/step, loss=0.07495, avg_loss=0.07461]\n",
      "Step 517980  [5.499 sec/step, loss=0.07379, avg_loss=0.07462]\n",
      "Step 517981  [5.485 sec/step, loss=0.07585, avg_loss=0.07461]\n",
      "Step 517982  [5.484 sec/step, loss=0.07491, avg_loss=0.07461]\n",
      "Step 517983  [5.502 sec/step, loss=0.07668, avg_loss=0.07466]\n",
      "Step 517984  [5.490 sec/step, loss=0.07279, avg_loss=0.07463]\n",
      "Step 517985  [5.495 sec/step, loss=0.07658, avg_loss=0.07464]\n",
      "Step 517986  [5.481 sec/step, loss=0.07590, avg_loss=0.07464]\n",
      "Step 517987  [5.462 sec/step, loss=0.07423, avg_loss=0.07461]\n",
      "Step 517988  [5.456 sec/step, loss=0.07448, avg_loss=0.07461]\n",
      "Step 517989  [5.454 sec/step, loss=0.07532, avg_loss=0.07460]\n",
      "Step 517990  [5.472 sec/step, loss=0.07542, avg_loss=0.07467]\n",
      "Step 517991  [5.496 sec/step, loss=0.07447, avg_loss=0.07469]\n",
      "Step 517992  [5.494 sec/step, loss=0.07170, avg_loss=0.07466]\n",
      "Step 517993  [5.488 sec/step, loss=0.07556, avg_loss=0.07465]\n",
      "Step 517994  [5.540 sec/step, loss=0.06670, avg_loss=0.07459]\n",
      "Step 517995  [5.523 sec/step, loss=0.07666, avg_loss=0.07462]\n",
      "Step 517996  [5.536 sec/step, loss=0.07650, avg_loss=0.07462]\n",
      "Step 517997  [5.541 sec/step, loss=0.07663, avg_loss=0.07463]\n",
      "Step 517998  [5.530 sec/step, loss=0.07470, avg_loss=0.07462]\n",
      "Step 517999  [5.553 sec/step, loss=0.07638, avg_loss=0.07467]\n",
      "Step 518000  [5.564 sec/step, loss=0.07448, avg_loss=0.07466]\n",
      "Writing summary at step: 518000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-518000\n",
      "Saving audio and alignment...\n",
      "Input: vaedijuu or vaelding kay baqaajaadzaatd pir gadriiay nay apnay baehhnooii gganii baloots koo madduu kajjaa~___________________________________\n",
      "Step 518001  [5.570 sec/step, loss=0.07618, avg_loss=0.07467]\n",
      "Generated 32 batches of size 32 in 2.452 sec\n",
      "Step 518002  [5.572 sec/step, loss=0.07554, avg_loss=0.07466]\n",
      "Step 518003  [5.552 sec/step, loss=0.07279, avg_loss=0.07462]\n",
      "Step 518004  [5.575 sec/step, loss=0.07354, avg_loss=0.07460]\n",
      "Step 518005  [5.548 sec/step, loss=0.06560, avg_loss=0.07451]\n",
      "Step 518006  [5.499 sec/step, loss=0.07251, avg_loss=0.07457]\n",
      "Step 518007  [5.483 sec/step, loss=0.07344, avg_loss=0.07453]\n",
      "Step 518008  [5.460 sec/step, loss=0.07420, avg_loss=0.07450]\n",
      "Step 518009  [5.464 sec/step, loss=0.07589, avg_loss=0.07450]\n",
      "Step 518010  [5.479 sec/step, loss=0.07694, avg_loss=0.07453]\n",
      "Step 518011  [5.496 sec/step, loss=0.07298, avg_loss=0.07449]\n",
      "Step 518012  [5.512 sec/step, loss=0.07631, avg_loss=0.07449]\n",
      "Step 518013  [5.556 sec/step, loss=0.06609, avg_loss=0.07439]\n",
      "Step 518014  [5.554 sec/step, loss=0.07638, avg_loss=0.07439]\n",
      "Step 518015  [5.574 sec/step, loss=0.07566, avg_loss=0.07441]\n",
      "Step 518016  [5.577 sec/step, loss=0.07412, avg_loss=0.07442]\n",
      "Step 518017  [5.533 sec/step, loss=0.07585, avg_loss=0.07452]\n",
      "Step 518018  [5.502 sec/step, loss=0.07391, avg_loss=0.07452]\n",
      "Step 518019  [5.510 sec/step, loss=0.07506, avg_loss=0.07451]\n",
      "Step 518020  [5.500 sec/step, loss=0.07653, avg_loss=0.07452]\n",
      "Step 518021  [5.529 sec/step, loss=0.07473, avg_loss=0.07454]\n",
      "Step 518022  [5.523 sec/step, loss=0.07400, avg_loss=0.07453]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 518023  [5.522 sec/step, loss=0.07467, avg_loss=0.07451]\n",
      "Step 518024  [5.502 sec/step, loss=0.07250, avg_loss=0.07447]\n",
      "Step 518025  [5.497 sec/step, loss=0.07401, avg_loss=0.07445]\n",
      "Step 518026  [5.505 sec/step, loss=0.07297, avg_loss=0.07444]\n",
      "Step 518027  [5.536 sec/step, loss=0.07614, avg_loss=0.07453]\n",
      "Step 518028  [5.526 sec/step, loss=0.07139, avg_loss=0.07452]\n",
      "Step 518029  [5.542 sec/step, loss=0.07710, avg_loss=0.07453]\n",
      "Step 518030  [5.510 sec/step, loss=0.07182, avg_loss=0.07448]\n",
      "Step 518031  [5.519 sec/step, loss=0.07647, avg_loss=0.07450]\n",
      "Step 518032  [5.517 sec/step, loss=0.07467, avg_loss=0.07449]\n",
      "Step 518033  [5.499 sec/step, loss=0.07385, avg_loss=0.07446]\n",
      "Generated 32 batches of size 32 in 2.454 sec\n",
      "Step 518034  [5.520 sec/step, loss=0.07695, avg_loss=0.07448]\n",
      "Step 518035  [5.494 sec/step, loss=0.06638, avg_loss=0.07438]\n",
      "Step 518036  [5.500 sec/step, loss=0.07651, avg_loss=0.07440]\n",
      "Step 518037  [5.514 sec/step, loss=0.07669, avg_loss=0.07442]\n",
      "Step 518038  [5.500 sec/step, loss=0.07481, avg_loss=0.07442]\n",
      "Step 518039  [5.482 sec/step, loss=0.07465, avg_loss=0.07440]\n",
      "Step 518040  [5.488 sec/step, loss=0.07522, avg_loss=0.07441]\n",
      "Step 518041  [5.496 sec/step, loss=0.07654, avg_loss=0.07445]\n",
      "Step 518042  [5.496 sec/step, loss=0.07464, avg_loss=0.07445]\n",
      "Step 518043  [5.500 sec/step, loss=0.07646, avg_loss=0.07445]\n",
      "Step 518044  [5.482 sec/step, loss=0.07424, avg_loss=0.07442]\n",
      "Step 518045  [5.486 sec/step, loss=0.07498, avg_loss=0.07443]\n",
      "Step 518046  [5.486 sec/step, loss=0.07594, avg_loss=0.07443]\n",
      "Step 518047  [5.525 sec/step, loss=0.07404, avg_loss=0.07446]\n",
      "Step 518048  [5.580 sec/step, loss=0.06763, avg_loss=0.07438]\n",
      "Step 518049  [5.599 sec/step, loss=0.07505, avg_loss=0.07438]\n",
      "Step 518050  [5.585 sec/step, loss=0.06628, avg_loss=0.07430]\n",
      "Step 518051  [5.590 sec/step, loss=0.07233, avg_loss=0.07429]\n",
      "Step 518052  [5.578 sec/step, loss=0.07495, avg_loss=0.07429]\n",
      "Step 518053  [5.580 sec/step, loss=0.07506, avg_loss=0.07429]\n",
      "Step 518054  [5.580 sec/step, loss=0.07209, avg_loss=0.07426]\n",
      "Step 518055  [5.540 sec/step, loss=0.07660, avg_loss=0.07436]\n",
      "Step 518056  [5.524 sec/step, loss=0.07388, avg_loss=0.07436]\n",
      "Step 518057  [5.539 sec/step, loss=0.07555, avg_loss=0.07439]\n",
      "Step 518058  [5.558 sec/step, loss=0.07606, avg_loss=0.07448]\n",
      "Step 518059  [5.581 sec/step, loss=0.07653, avg_loss=0.07450]\n",
      "Step 518060  [5.600 sec/step, loss=0.07459, avg_loss=0.07449]\n",
      "Step 518061  [5.578 sec/step, loss=0.07234, avg_loss=0.07445]\n",
      "Step 518062  [5.576 sec/step, loss=0.07630, avg_loss=0.07444]\n",
      "Step 518063  [5.553 sec/step, loss=0.07569, avg_loss=0.07446]\n",
      "Step 518064  [5.566 sec/step, loss=0.07696, avg_loss=0.07447]\n",
      "Step 518065  [5.573 sec/step, loss=0.07304, avg_loss=0.07447]\n",
      "Generated 32 batches of size 32 in 2.394 sec\n",
      "Step 518066  [5.590 sec/step, loss=0.07521, avg_loss=0.07448]\n",
      "Step 518067  [5.580 sec/step, loss=0.07258, avg_loss=0.07448]\n",
      "Step 518068  [5.587 sec/step, loss=0.07539, avg_loss=0.07448]\n",
      "Step 518069  [5.604 sec/step, loss=0.07106, avg_loss=0.07443]\n",
      "Step 518070  [5.614 sec/step, loss=0.07529, avg_loss=0.07442]\n",
      "Step 518071  [5.597 sec/step, loss=0.07503, avg_loss=0.07440]\n",
      "Step 518072  [5.581 sec/step, loss=0.07403, avg_loss=0.07439]\n",
      "Step 518073  [5.579 sec/step, loss=0.07680, avg_loss=0.07439]\n",
      "Step 518074  [5.586 sec/step, loss=0.07379, avg_loss=0.07437]\n",
      "Step 518075  [5.578 sec/step, loss=0.07665, avg_loss=0.07436]\n",
      "Step 518076  [5.580 sec/step, loss=0.07643, avg_loss=0.07437]\n",
      "Step 518077  [5.614 sec/step, loss=0.07434, avg_loss=0.07439]\n",
      "Step 518078  [5.622 sec/step, loss=0.07729, avg_loss=0.07441]\n",
      "Step 518079  [5.604 sec/step, loss=0.07445, avg_loss=0.07441]\n",
      "Step 518080  [5.625 sec/step, loss=0.06564, avg_loss=0.07433]\n",
      "Step 518081  [5.621 sec/step, loss=0.07562, avg_loss=0.07433]\n",
      "Step 518082  [5.628 sec/step, loss=0.07642, avg_loss=0.07434]\n",
      "Step 518083  [5.625 sec/step, loss=0.07197, avg_loss=0.07429]\n",
      "Step 518084  [5.623 sec/step, loss=0.07118, avg_loss=0.07428]\n",
      "Step 518085  [5.605 sec/step, loss=0.07100, avg_loss=0.07422]\n",
      "Step 518086  [5.612 sec/step, loss=0.07675, avg_loss=0.07423]\n",
      "Step 518087  [5.626 sec/step, loss=0.07468, avg_loss=0.07423]\n",
      "Step 518088  [5.636 sec/step, loss=0.07548, avg_loss=0.07424]\n",
      "Step 518089  [5.652 sec/step, loss=0.07565, avg_loss=0.07425]\n",
      "Step 518090  [5.656 sec/step, loss=0.07615, avg_loss=0.07425]\n",
      "Step 518091  [5.655 sec/step, loss=0.07652, avg_loss=0.07428]\n",
      "Step 518092  [5.652 sec/step, loss=0.07220, avg_loss=0.07428]\n",
      "Step 518093  [5.639 sec/step, loss=0.07560, avg_loss=0.07428]\n",
      "Step 518094  [5.590 sec/step, loss=0.07535, avg_loss=0.07437]\n",
      "Step 518095  [5.588 sec/step, loss=0.07442, avg_loss=0.07434]\n",
      "Step 518096  [5.574 sec/step, loss=0.07442, avg_loss=0.07432]\n",
      "Step 518097  [5.566 sec/step, loss=0.07345, avg_loss=0.07429]\n",
      "Generated 32 batches of size 32 in 2.798 sec\n",
      "Step 518098  [5.561 sec/step, loss=0.06747, avg_loss=0.07422]\n",
      "Step 518099  [5.539 sec/step, loss=0.07414, avg_loss=0.07420]\n",
      "Step 518100  [5.538 sec/step, loss=0.07606, avg_loss=0.07421]\n",
      "Writing summary at step: 518100\n",
      "Step 518101  [5.549 sec/step, loss=0.07557, avg_loss=0.07421]\n",
      "Step 518102  [5.535 sec/step, loss=0.07388, avg_loss=0.07419]\n",
      "Step 518103  [5.540 sec/step, loss=0.07497, avg_loss=0.07421]\n",
      "Step 518104  [5.516 sec/step, loss=0.07510, avg_loss=0.07423]\n",
      "Step 518105  [5.522 sec/step, loss=0.07480, avg_loss=0.07432]\n",
      "Step 518106  [5.517 sec/step, loss=0.07357, avg_loss=0.07433]\n",
      "Step 518107  [5.529 sec/step, loss=0.07678, avg_loss=0.07436]\n",
      "Step 518108  [5.544 sec/step, loss=0.07690, avg_loss=0.07439]\n",
      "Step 518109  [5.545 sec/step, loss=0.07300, avg_loss=0.07436]\n",
      "Step 518110  [5.526 sec/step, loss=0.07483, avg_loss=0.07434]\n",
      "Step 518111  [5.497 sec/step, loss=0.07191, avg_loss=0.07433]\n",
      "Step 518112  [5.485 sec/step, loss=0.07586, avg_loss=0.07433]\n",
      "Step 518113  [5.436 sec/step, loss=0.07585, avg_loss=0.07442]\n",
      "Step 518114  [5.437 sec/step, loss=0.07503, avg_loss=0.07441]\n",
      "Step 518115  [5.441 sec/step, loss=0.07462, avg_loss=0.07440]\n",
      "Step 518116  [5.473 sec/step, loss=0.07445, avg_loss=0.07440]\n",
      "Step 518117  [5.456 sec/step, loss=0.07203, avg_loss=0.07436]\n",
      "Step 518118  [5.452 sec/step, loss=0.07415, avg_loss=0.07437]\n",
      "Step 518119  [5.429 sec/step, loss=0.07470, avg_loss=0.07436]\n",
      "Step 518120  [5.439 sec/step, loss=0.07442, avg_loss=0.07434]\n",
      "Step 518121  [5.430 sec/step, loss=0.07570, avg_loss=0.07435]\n",
      "Step 518122  [5.441 sec/step, loss=0.07417, avg_loss=0.07435]\n",
      "Step 518123  [5.435 sec/step, loss=0.07246, avg_loss=0.07433]\n",
      "Step 518124  [5.438 sec/step, loss=0.07491, avg_loss=0.07436]\n",
      "Step 518125  [5.422 sec/step, loss=0.06605, avg_loss=0.07428]\n",
      "Step 518126  [5.421 sec/step, loss=0.07574, avg_loss=0.07430]\n",
      "Step 518127  [5.417 sec/step, loss=0.07701, avg_loss=0.07431]\n",
      "Step 518128  [5.427 sec/step, loss=0.07643, avg_loss=0.07436]\n",
      "Generated 32 batches of size 32 in 2.519 sec\n",
      "Step 518129  [5.411 sec/step, loss=0.07188, avg_loss=0.07431]\n",
      "Step 518130  [5.431 sec/step, loss=0.07586, avg_loss=0.07435]\n",
      "Step 518131  [5.436 sec/step, loss=0.07547, avg_loss=0.07434]\n",
      "Step 518132  [5.440 sec/step, loss=0.07622, avg_loss=0.07436]\n",
      "Step 518133  [5.441 sec/step, loss=0.07617, avg_loss=0.07438]\n",
      "Step 518134  [5.414 sec/step, loss=0.07216, avg_loss=0.07433]\n",
      "Step 518135  [5.444 sec/step, loss=0.07696, avg_loss=0.07444]\n",
      "Step 518136  [5.436 sec/step, loss=0.07456, avg_loss=0.07442]\n",
      "Step 518137  [5.480 sec/step, loss=0.06783, avg_loss=0.07433]\n",
      "Step 518138  [5.484 sec/step, loss=0.07551, avg_loss=0.07434]\n",
      "Step 518139  [5.493 sec/step, loss=0.07477, avg_loss=0.07434]\n",
      "Step 518140  [5.493 sec/step, loss=0.07483, avg_loss=0.07433]\n",
      "Step 518141  [5.492 sec/step, loss=0.07639, avg_loss=0.07433]\n",
      "Step 518142  [5.496 sec/step, loss=0.07532, avg_loss=0.07434]\n",
      "Step 518143  [5.482 sec/step, loss=0.07606, avg_loss=0.07434]\n",
      "Step 518144  [5.494 sec/step, loss=0.07628, avg_loss=0.07436]\n",
      "Step 518145  [5.505 sec/step, loss=0.07629, avg_loss=0.07437]\n",
      "Step 518146  [5.494 sec/step, loss=0.07506, avg_loss=0.07436]\n",
      "Step 518147  [5.463 sec/step, loss=0.07120, avg_loss=0.07433]\n",
      "Step 518148  [5.400 sec/step, loss=0.07140, avg_loss=0.07437]\n",
      "Step 518149  [5.400 sec/step, loss=0.07671, avg_loss=0.07439]\n",
      "Step 518150  [5.409 sec/step, loss=0.07390, avg_loss=0.07446]\n",
      "Step 518151  [5.411 sec/step, loss=0.07530, avg_loss=0.07449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 518152  [5.404 sec/step, loss=0.07412, avg_loss=0.07448]\n",
      "Step 518153  [5.411 sec/step, loss=0.07606, avg_loss=0.07449]\n",
      "Step 518154  [5.410 sec/step, loss=0.07268, avg_loss=0.07450]\n",
      "Step 518155  [5.401 sec/step, loss=0.07425, avg_loss=0.07448]\n",
      "Step 518156  [5.417 sec/step, loss=0.07689, avg_loss=0.07451]\n",
      "Step 518157  [5.409 sec/step, loss=0.07348, avg_loss=0.07449]\n",
      "Step 518158  [5.403 sec/step, loss=0.07609, avg_loss=0.07449]\n",
      "Step 518159  [5.437 sec/step, loss=0.06649, avg_loss=0.07439]\n",
      "Step 518160  [5.398 sec/step, loss=0.07150, avg_loss=0.07435]\n",
      "Generated 32 batches of size 32 in 2.481 sec\n",
      "Step 518161  [5.420 sec/step, loss=0.07693, avg_loss=0.07440]\n",
      "Step 518162  [5.410 sec/step, loss=0.07315, avg_loss=0.07437]\n",
      "Step 518163  [5.439 sec/step, loss=0.07358, avg_loss=0.07435]\n",
      "Step 518164  [5.437 sec/step, loss=0.07545, avg_loss=0.07433]\n",
      "Step 518165  [5.436 sec/step, loss=0.07548, avg_loss=0.07436]\n",
      "Step 518166  [5.427 sec/step, loss=0.07648, avg_loss=0.07437]\n",
      "Step 518167  [5.429 sec/step, loss=0.07463, avg_loss=0.07439]\n",
      "Step 518168  [5.438 sec/step, loss=0.07596, avg_loss=0.07440]\n",
      "Step 518169  [5.402 sec/step, loss=0.06618, avg_loss=0.07435]\n",
      "Step 518170  [5.406 sec/step, loss=0.07688, avg_loss=0.07436]\n",
      "Step 518171  [5.440 sec/step, loss=0.07601, avg_loss=0.07437]\n",
      "Step 518172  [5.440 sec/step, loss=0.07626, avg_loss=0.07440]\n",
      "Step 518173  [5.439 sec/step, loss=0.07436, avg_loss=0.07437]\n",
      "Step 518174  [5.430 sec/step, loss=0.07605, avg_loss=0.07439]\n",
      "Step 518175  [5.428 sec/step, loss=0.07376, avg_loss=0.07436]\n",
      "Step 518176  [5.435 sec/step, loss=0.07701, avg_loss=0.07437]\n",
      "Step 518177  [5.411 sec/step, loss=0.07528, avg_loss=0.07438]\n",
      "Step 518178  [5.392 sec/step, loss=0.07196, avg_loss=0.07433]\n",
      "Step 518179  [5.448 sec/step, loss=0.06706, avg_loss=0.07425]\n",
      "Step 518180  [5.393 sec/step, loss=0.07460, avg_loss=0.07434]\n",
      "Step 518181  [5.391 sec/step, loss=0.07476, avg_loss=0.07433]\n",
      "Step 518182  [5.379 sec/step, loss=0.07589, avg_loss=0.07433]\n",
      "Step 518183  [5.384 sec/step, loss=0.07506, avg_loss=0.07436]\n",
      "Step 518184  [5.391 sec/step, loss=0.07617, avg_loss=0.07441]\n",
      "Step 518185  [5.395 sec/step, loss=0.07241, avg_loss=0.07442]\n",
      "Step 518186  [5.395 sec/step, loss=0.07483, avg_loss=0.07440]\n",
      "Step 518187  [5.385 sec/step, loss=0.07228, avg_loss=0.07438]\n",
      "Step 518188  [5.397 sec/step, loss=0.07731, avg_loss=0.07440]\n",
      "Step 518189  [5.389 sec/step, loss=0.07718, avg_loss=0.07441]\n",
      "Step 518190  [5.388 sec/step, loss=0.07377, avg_loss=0.07439]\n",
      "Step 518191  [5.384 sec/step, loss=0.07329, avg_loss=0.07436]\n",
      "Step 518192  [5.407 sec/step, loss=0.07675, avg_loss=0.07440]\n",
      "Generated 32 batches of size 32 in 2.386 sec\n",
      "Step 518193  [5.424 sec/step, loss=0.07650, avg_loss=0.07441]\n",
      "Step 518194  [5.433 sec/step, loss=0.07633, avg_loss=0.07442]\n",
      "Step 518195  [5.422 sec/step, loss=0.07596, avg_loss=0.07444]\n",
      "Step 518196  [5.418 sec/step, loss=0.07201, avg_loss=0.07441]\n",
      "Step 518197  [5.409 sec/step, loss=0.07484, avg_loss=0.07443]\n",
      "Step 518198  [5.402 sec/step, loss=0.06743, avg_loss=0.07443]\n",
      "Step 518199  [5.396 sec/step, loss=0.07286, avg_loss=0.07441]\n",
      "Step 518200  [5.387 sec/step, loss=0.07344, avg_loss=0.07439]\n",
      "Writing summary at step: 518200\n",
      "Step 518201  [5.420 sec/step, loss=0.06691, avg_loss=0.07430]\n",
      "Step 518202  [5.410 sec/step, loss=0.07228, avg_loss=0.07428]\n",
      "Step 518203  [5.429 sec/step, loss=0.07649, avg_loss=0.07430]\n",
      "Step 518204  [5.424 sec/step, loss=0.07361, avg_loss=0.07429]\n",
      "Step 518205  [5.450 sec/step, loss=0.07523, avg_loss=0.07429]\n",
      "Step 518206  [5.456 sec/step, loss=0.07379, avg_loss=0.07429]\n",
      "Step 518207  [5.453 sec/step, loss=0.07468, avg_loss=0.07427]\n",
      "Step 518208  [5.445 sec/step, loss=0.07561, avg_loss=0.07426]\n",
      "Step 518209  [5.442 sec/step, loss=0.07450, avg_loss=0.07427]\n",
      "Step 518210  [5.462 sec/step, loss=0.07689, avg_loss=0.07429]\n",
      "Step 518211  [5.473 sec/step, loss=0.07664, avg_loss=0.07434]\n",
      "Step 518212  [5.474 sec/step, loss=0.07599, avg_loss=0.07434]\n",
      "Step 518213  [5.477 sec/step, loss=0.07573, avg_loss=0.07434]\n",
      "Step 518214  [5.484 sec/step, loss=0.07589, avg_loss=0.07435]\n",
      "Step 518215  [5.457 sec/step, loss=0.07552, avg_loss=0.07436]\n",
      "Step 518216  [5.418 sec/step, loss=0.06561, avg_loss=0.07427]\n",
      "Step 518217  [5.425 sec/step, loss=0.07422, avg_loss=0.07429]\n",
      "Step 518218  [5.422 sec/step, loss=0.07449, avg_loss=0.07430]\n",
      "Step 518219  [5.442 sec/step, loss=0.07716, avg_loss=0.07432]\n",
      "Step 518220  [5.440 sec/step, loss=0.07411, avg_loss=0.07432]\n",
      "Step 518221  [5.425 sec/step, loss=0.07183, avg_loss=0.07428]\n",
      "Step 518222  [5.423 sec/step, loss=0.07480, avg_loss=0.07428]\n",
      "Step 518223  [5.415 sec/step, loss=0.07415, avg_loss=0.07430]\n",
      "Generated 32 batches of size 32 in 2.525 sec\n",
      "Step 518224  [5.421 sec/step, loss=0.07552, avg_loss=0.07431]\n",
      "Step 518225  [5.458 sec/step, loss=0.07558, avg_loss=0.07440]\n",
      "Step 518226  [5.483 sec/step, loss=0.07379, avg_loss=0.07438]\n",
      "Step 518227  [5.472 sec/step, loss=0.07387, avg_loss=0.07435]\n",
      "Step 518228  [5.484 sec/step, loss=0.07749, avg_loss=0.07436]\n",
      "Step 518229  [5.482 sec/step, loss=0.07579, avg_loss=0.07440]\n",
      "Step 518230  [5.467 sec/step, loss=0.07440, avg_loss=0.07439]\n",
      "Step 518231  [5.457 sec/step, loss=0.07494, avg_loss=0.07438]\n",
      "Step 518232  [5.454 sec/step, loss=0.07605, avg_loss=0.07438]\n",
      "Step 518233  [5.445 sec/step, loss=0.07245, avg_loss=0.07434]\n",
      "Step 518234  [5.472 sec/step, loss=0.07426, avg_loss=0.07436]\n",
      "Step 518235  [5.466 sec/step, loss=0.07617, avg_loss=0.07436]\n",
      "Step 518236  [5.479 sec/step, loss=0.07610, avg_loss=0.07437]\n",
      "Step 518237  [5.442 sec/step, loss=0.07714, avg_loss=0.07446]\n",
      "Step 518238  [5.443 sec/step, loss=0.07508, avg_loss=0.07446]\n",
      "Step 518239  [5.456 sec/step, loss=0.07644, avg_loss=0.07448]\n",
      "Step 518240  [5.455 sec/step, loss=0.07417, avg_loss=0.07447]\n",
      "Step 518241  [5.448 sec/step, loss=0.07500, avg_loss=0.07446]\n",
      "Step 518242  [5.444 sec/step, loss=0.07301, avg_loss=0.07443]\n",
      "Step 518243  [5.449 sec/step, loss=0.07475, avg_loss=0.07442]\n",
      "Step 518244  [5.465 sec/step, loss=0.07339, avg_loss=0.07439]\n",
      "Step 518245  [5.444 sec/step, loss=0.07506, avg_loss=0.07438]\n",
      "Step 518246  [5.461 sec/step, loss=0.07614, avg_loss=0.07439]\n",
      "Step 518247  [5.465 sec/step, loss=0.07508, avg_loss=0.07443]\n",
      "Step 518248  [5.504 sec/step, loss=0.07381, avg_loss=0.07445]\n",
      "Step 518249  [5.508 sec/step, loss=0.07512, avg_loss=0.07444]\n",
      "Step 518250  [5.500 sec/step, loss=0.06578, avg_loss=0.07435]\n",
      "Step 518251  [5.496 sec/step, loss=0.07627, avg_loss=0.07436]\n",
      "Step 518252  [5.492 sec/step, loss=0.07103, avg_loss=0.07433]\n",
      "Step 518253  [5.475 sec/step, loss=0.07594, avg_loss=0.07433]\n",
      "Step 518254  [5.476 sec/step, loss=0.07489, avg_loss=0.07435]\n",
      "Step 518255  [5.528 sec/step, loss=0.06558, avg_loss=0.07427]\n",
      "Generated 32 batches of size 32 in 2.542 sec\n",
      "Step 518256  [5.533 sec/step, loss=0.07662, avg_loss=0.07426]\n",
      "Step 518257  [5.543 sec/step, loss=0.07614, avg_loss=0.07429]\n",
      "Step 518258  [5.540 sec/step, loss=0.07402, avg_loss=0.07427]\n",
      "Step 518259  [5.504 sec/step, loss=0.07743, avg_loss=0.07438]\n",
      "Step 518260  [5.523 sec/step, loss=0.07613, avg_loss=0.07443]\n",
      "Step 518261  [5.507 sec/step, loss=0.07236, avg_loss=0.07438]\n",
      "Step 518262  [5.510 sec/step, loss=0.07603, avg_loss=0.07441]\n",
      "Step 518263  [5.478 sec/step, loss=0.07514, avg_loss=0.07443]\n",
      "Step 518264  [5.468 sec/step, loss=0.07435, avg_loss=0.07441]\n",
      "Step 518265  [5.461 sec/step, loss=0.07471, avg_loss=0.07441]\n",
      "Step 518266  [5.478 sec/step, loss=0.07592, avg_loss=0.07440]\n",
      "Step 518267  [5.487 sec/step, loss=0.07371, avg_loss=0.07439]\n",
      "Step 518268  [5.476 sec/step, loss=0.07459, avg_loss=0.07438]\n",
      "Step 518269  [5.483 sec/step, loss=0.07474, avg_loss=0.07446]\n",
      "Step 518270  [5.479 sec/step, loss=0.07585, avg_loss=0.07445]\n",
      "Step 518271  [5.460 sec/step, loss=0.07479, avg_loss=0.07444]\n",
      "Step 518272  [5.475 sec/step, loss=0.07545, avg_loss=0.07443]\n",
      "Step 518273  [5.463 sec/step, loss=0.07567, avg_loss=0.07445]\n",
      "Step 518274  [5.473 sec/step, loss=0.07482, avg_loss=0.07443]\n",
      "Step 518275  [5.458 sec/step, loss=0.06716, avg_loss=0.07437]\n",
      "Step 518276  [5.446 sec/step, loss=0.07638, avg_loss=0.07436]\n",
      "Step 518277  [5.450 sec/step, loss=0.07581, avg_loss=0.07437]\n",
      "Step 518278  [5.467 sec/step, loss=0.07694, avg_loss=0.07442]\n",
      "Step 518279  [5.417 sec/step, loss=0.07571, avg_loss=0.07450]\n",
      "Step 518280  [5.437 sec/step, loss=0.07653, avg_loss=0.07452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 518281  [5.456 sec/step, loss=0.07694, avg_loss=0.07454]\n",
      "Step 518282  [5.465 sec/step, loss=0.07607, avg_loss=0.07455]\n",
      "Step 518283  [5.513 sec/step, loss=0.06707, avg_loss=0.07447]\n",
      "Step 518284  [5.536 sec/step, loss=0.07377, avg_loss=0.07444]\n",
      "Step 518285  [5.541 sec/step, loss=0.07422, avg_loss=0.07446]\n",
      "Step 518286  [5.523 sec/step, loss=0.07463, avg_loss=0.07446]\n",
      "Step 518287  [5.518 sec/step, loss=0.07251, avg_loss=0.07446]\n",
      "Generated 32 batches of size 32 in 2.524 sec\n",
      "Step 518288  [5.523 sec/step, loss=0.07715, avg_loss=0.07446]\n",
      "Step 518289  [5.528 sec/step, loss=0.07668, avg_loss=0.07445]\n",
      "Step 518290  [5.521 sec/step, loss=0.07462, avg_loss=0.07446]\n",
      "Step 518291  [5.514 sec/step, loss=0.07605, avg_loss=0.07449]\n",
      "Step 518292  [5.509 sec/step, loss=0.07560, avg_loss=0.07448]\n",
      "Step 518293  [5.498 sec/step, loss=0.07151, avg_loss=0.07443]\n",
      "Step 518294  [5.551 sec/step, loss=0.07542, avg_loss=0.07442]\n",
      "Step 518295  [5.555 sec/step, loss=0.07615, avg_loss=0.07442]\n",
      "Step 518296  [5.540 sec/step, loss=0.07248, avg_loss=0.07443]\n",
      "Step 518297  [5.547 sec/step, loss=0.07178, avg_loss=0.07440]\n",
      "Step 518298  [5.589 sec/step, loss=0.07419, avg_loss=0.07446]\n",
      "Step 518299  [5.593 sec/step, loss=0.07207, avg_loss=0.07446]\n",
      "Step 518300  [5.598 sec/step, loss=0.07555, avg_loss=0.07448]\n",
      "Writing summary at step: 518300\n",
      "Step 518301  [5.561 sec/step, loss=0.07674, avg_loss=0.07457]\n",
      "Step 518302  [5.568 sec/step, loss=0.07431, avg_loss=0.07459]\n",
      "Step 518303  [5.567 sec/step, loss=0.07693, avg_loss=0.07460]\n",
      "Step 518304  [5.570 sec/step, loss=0.07268, avg_loss=0.07459]\n",
      "Step 518305  [5.564 sec/step, loss=0.07413, avg_loss=0.07458]\n",
      "Step 518306  [5.555 sec/step, loss=0.07467, avg_loss=0.07459]\n",
      "Step 518307  [5.555 sec/step, loss=0.07584, avg_loss=0.07460]\n",
      "Step 518308  [5.549 sec/step, loss=0.07472, avg_loss=0.07459]\n",
      "Step 518309  [5.575 sec/step, loss=0.07385, avg_loss=0.07458]\n",
      "Step 518310  [5.572 sec/step, loss=0.07564, avg_loss=0.07457]\n",
      "Step 518311  [5.547 sec/step, loss=0.06677, avg_loss=0.07447]\n",
      "Step 518312  [5.550 sec/step, loss=0.07604, avg_loss=0.07447]\n",
      "Step 518313  [5.596 sec/step, loss=0.06591, avg_loss=0.07438]\n",
      "Step 518314  [5.587 sec/step, loss=0.07284, avg_loss=0.07434]\n",
      "Step 518315  [5.601 sec/step, loss=0.07707, avg_loss=0.07436]\n",
      "Step 518316  [5.603 sec/step, loss=0.07258, avg_loss=0.07443]\n",
      "Step 518317  [5.615 sec/step, loss=0.07665, avg_loss=0.07445]\n",
      "Step 518318  [5.628 sec/step, loss=0.07417, avg_loss=0.07445]\n",
      "Generated 32 batches of size 32 in 2.554 sec\n",
      "Step 518319  [5.625 sec/step, loss=0.07521, avg_loss=0.07443]\n",
      "Step 518320  [5.630 sec/step, loss=0.07598, avg_loss=0.07445]\n",
      "Step 518321  [5.643 sec/step, loss=0.07611, avg_loss=0.07449]\n",
      "Step 518322  [5.645 sec/step, loss=0.07663, avg_loss=0.07451]\n",
      "Step 518323  [5.656 sec/step, loss=0.07658, avg_loss=0.07454]\n",
      "Step 518324  [5.641 sec/step, loss=0.07458, avg_loss=0.07453]\n",
      "Step 518325  [5.629 sec/step, loss=0.07566, avg_loss=0.07453]\n",
      "Step 518326  [5.606 sec/step, loss=0.07526, avg_loss=0.07454]\n",
      "Step 518327  [5.608 sec/step, loss=0.07596, avg_loss=0.07456]\n",
      "Step 518328  [5.611 sec/step, loss=0.07709, avg_loss=0.07456]\n",
      "Step 518329  [5.609 sec/step, loss=0.07359, avg_loss=0.07454]\n",
      "Step 518330  [5.618 sec/step, loss=0.07500, avg_loss=0.07454]\n",
      "Step 518331  [5.616 sec/step, loss=0.07238, avg_loss=0.07452]\n",
      "Step 518332  [5.604 sec/step, loss=0.07144, avg_loss=0.07447]\n",
      "Step 518333  [5.632 sec/step, loss=0.07362, avg_loss=0.07448]\n",
      "Step 518334  [5.609 sec/step, loss=0.07438, avg_loss=0.07448]\n",
      "Step 518335  [5.604 sec/step, loss=0.07664, avg_loss=0.07449]\n",
      "Step 518336  [5.598 sec/step, loss=0.07543, avg_loss=0.07448]\n",
      "Step 518337  [5.583 sec/step, loss=0.07337, avg_loss=0.07444]\n",
      "Step 518338  [5.590 sec/step, loss=0.07353, avg_loss=0.07443]\n",
      "Step 518339  [5.578 sec/step, loss=0.07490, avg_loss=0.07441]\n",
      "Step 518340  [5.574 sec/step, loss=0.07351, avg_loss=0.07441]\n",
      "Step 518341  [5.580 sec/step, loss=0.07695, avg_loss=0.07443]\n",
      "Step 518342  [5.590 sec/step, loss=0.07549, avg_loss=0.07445]\n",
      "Step 518343  [5.597 sec/step, loss=0.07717, avg_loss=0.07447]\n",
      "Step 518344  [5.588 sec/step, loss=0.07665, avg_loss=0.07451]\n",
      "Step 518345  [5.603 sec/step, loss=0.07428, avg_loss=0.07450]\n",
      "Step 518346  [5.580 sec/step, loss=0.07188, avg_loss=0.07446]\n",
      "Step 518347  [5.631 sec/step, loss=0.06759, avg_loss=0.07438]\n",
      "Step 518348  [5.607 sec/step, loss=0.07517, avg_loss=0.07440]\n",
      "Step 518349  [5.587 sec/step, loss=0.07506, avg_loss=0.07440]\n",
      "Step 518350  [5.633 sec/step, loss=0.07383, avg_loss=0.07448]\n",
      "Generated 32 batches of size 32 in 2.536 sec\n",
      "Step 518351  [5.654 sec/step, loss=0.07590, avg_loss=0.07447]\n",
      "Step 518352  [5.661 sec/step, loss=0.07585, avg_loss=0.07452]\n",
      "Step 518353  [5.661 sec/step, loss=0.07329, avg_loss=0.07449]\n",
      "Step 518354  [5.646 sec/step, loss=0.06638, avg_loss=0.07441]\n",
      "Step 518355  [5.598 sec/step, loss=0.07516, avg_loss=0.07450]\n",
      "Step 518356  [5.592 sec/step, loss=0.07685, avg_loss=0.07451]\n",
      "Step 518357  [5.592 sec/step, loss=0.07559, avg_loss=0.07450]\n",
      "Step 518358  [5.597 sec/step, loss=0.07584, avg_loss=0.07452]\n",
      "Step 518359  [5.593 sec/step, loss=0.07464, avg_loss=0.07449]\n",
      "Step 518360  [5.616 sec/step, loss=0.07323, avg_loss=0.07446]\n",
      "Step 518361  [5.621 sec/step, loss=0.07525, avg_loss=0.07449]\n",
      "Step 518362  [5.611 sec/step, loss=0.07366, avg_loss=0.07447]\n",
      "Step 518363  [5.618 sec/step, loss=0.07552, avg_loss=0.07447]\n",
      "Step 518364  [5.617 sec/step, loss=0.07075, avg_loss=0.07444]\n",
      "Step 518365  [5.631 sec/step, loss=0.07561, avg_loss=0.07444]\n",
      "Step 518366  [5.605 sec/step, loss=0.07439, avg_loss=0.07443]\n",
      "Step 518367  [5.680 sec/step, loss=0.06570, avg_loss=0.07435]\n",
      "Step 518368  [5.686 sec/step, loss=0.07507, avg_loss=0.07435]\n",
      "Step 518369  [5.701 sec/step, loss=0.07582, avg_loss=0.07436]\n",
      "Step 518370  [5.707 sec/step, loss=0.07697, avg_loss=0.07438]\n",
      "Step 518371  [5.711 sec/step, loss=0.07626, avg_loss=0.07439]\n",
      "Step 518372  [5.697 sec/step, loss=0.07403, avg_loss=0.07438]\n",
      "Step 518373  [5.713 sec/step, loss=0.07432, avg_loss=0.07436]\n",
      "Step 518374  [5.702 sec/step, loss=0.07416, avg_loss=0.07436]\n",
      "Step 518375  [5.703 sec/step, loss=0.07226, avg_loss=0.07441]\n",
      "Step 518376  [5.698 sec/step, loss=0.07552, avg_loss=0.07440]\n",
      "Step 518377  [5.706 sec/step, loss=0.07710, avg_loss=0.07441]\n",
      "Step 518378  [5.692 sec/step, loss=0.07462, avg_loss=0.07439]\n",
      "Step 518379  [5.690 sec/step, loss=0.07602, avg_loss=0.07439]\n",
      "Step 518380  [5.665 sec/step, loss=0.07485, avg_loss=0.07437]\n",
      "Step 518381  [5.670 sec/step, loss=0.07643, avg_loss=0.07437]\n",
      "Step 518382  [5.660 sec/step, loss=0.07511, avg_loss=0.07436]\n",
      "Generated 32 batches of size 32 in 2.802 sec\n",
      "Step 518383  [5.593 sec/step, loss=0.06679, avg_loss=0.07436]\n",
      "Step 518384  [5.556 sec/step, loss=0.07165, avg_loss=0.07434]\n",
      "Step 518385  [5.558 sec/step, loss=0.07482, avg_loss=0.07434]\n",
      "Step 518386  [5.581 sec/step, loss=0.07563, avg_loss=0.07435]\n",
      "Step 518387  [5.593 sec/step, loss=0.07657, avg_loss=0.07439]\n",
      "Step 518388  [5.585 sec/step, loss=0.07642, avg_loss=0.07439]\n",
      "Step 518389  [5.583 sec/step, loss=0.07399, avg_loss=0.07436]\n",
      "Step 518390  [5.594 sec/step, loss=0.07710, avg_loss=0.07438]\n",
      "Step 518391  [5.599 sec/step, loss=0.07585, avg_loss=0.07438]\n",
      "Step 518392  [5.581 sec/step, loss=0.07173, avg_loss=0.07434]\n",
      "Step 518393  [5.581 sec/step, loss=0.07545, avg_loss=0.07438]\n",
      "Step 518394  [5.515 sec/step, loss=0.07594, avg_loss=0.07439]\n",
      "Step 518395  [5.526 sec/step, loss=0.07672, avg_loss=0.07439]\n",
      "Step 518396  [5.536 sec/step, loss=0.07362, avg_loss=0.07440]\n",
      "Step 518397  [5.546 sec/step, loss=0.07652, avg_loss=0.07445]\n",
      "Step 518398  [5.535 sec/step, loss=0.07657, avg_loss=0.07448]\n",
      "Step 518399  [5.554 sec/step, loss=0.07537, avg_loss=0.07451]\n",
      "Step 518400  [5.567 sec/step, loss=0.07489, avg_loss=0.07450]\n",
      "Writing summary at step: 518400\n",
      "Step 518401  [5.564 sec/step, loss=0.07573, avg_loss=0.07449]\n",
      "Step 518402  [5.586 sec/step, loss=0.07694, avg_loss=0.07452]\n",
      "Step 518403  [5.568 sec/step, loss=0.07484, avg_loss=0.07450]\n",
      "Step 518404  [5.564 sec/step, loss=0.07495, avg_loss=0.07452]\n",
      "Step 518405  [5.565 sec/step, loss=0.07670, avg_loss=0.07455]\n",
      "Step 518406  [5.562 sec/step, loss=0.07466, avg_loss=0.07455]\n",
      "Step 518407  [5.539 sec/step, loss=0.06598, avg_loss=0.07445]\n",
      "Step 518408  [5.544 sec/step, loss=0.07706, avg_loss=0.07447]\n",
      "Step 518409  [5.545 sec/step, loss=0.07413, avg_loss=0.07447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 518410  [5.540 sec/step, loss=0.07612, avg_loss=0.07448]\n",
      "Step 518411  [5.556 sec/step, loss=0.07478, avg_loss=0.07456]\n",
      "Step 518412  [5.551 sec/step, loss=0.07519, avg_loss=0.07455]\n",
      "Step 518413  [5.495 sec/step, loss=0.07612, avg_loss=0.07465]\n",
      "Generated 32 batches of size 32 in 2.394 sec\n",
      "Step 518414  [5.510 sec/step, loss=0.07671, avg_loss=0.07469]\n",
      "Step 518415  [5.503 sec/step, loss=0.07535, avg_loss=0.07467]\n",
      "Step 518416  [5.505 sec/step, loss=0.07112, avg_loss=0.07466]\n",
      "Step 518417  [5.547 sec/step, loss=0.06824, avg_loss=0.07457]\n",
      "Step 518418  [5.545 sec/step, loss=0.07495, avg_loss=0.07458]\n",
      "Step 518419  [5.561 sec/step, loss=0.07438, avg_loss=0.07457]\n",
      "Step 518420  [5.543 sec/step, loss=0.07282, avg_loss=0.07454]\n",
      "Step 518421  [5.542 sec/step, loss=0.07627, avg_loss=0.07454]\n",
      "Step 518422  [5.524 sec/step, loss=0.07350, avg_loss=0.07451]\n",
      "Step 518423  [5.510 sec/step, loss=0.07461, avg_loss=0.07449]\n",
      "Step 518424  [5.519 sec/step, loss=0.07545, avg_loss=0.07450]\n",
      "Step 518425  [5.522 sec/step, loss=0.07702, avg_loss=0.07452]\n",
      "Step 518426  [5.509 sec/step, loss=0.07516, avg_loss=0.07451]\n",
      "Step 518427  [5.521 sec/step, loss=0.07677, avg_loss=0.07452]\n",
      "Step 518428  [5.555 sec/step, loss=0.06571, avg_loss=0.07441]\n",
      "Step 518429  [5.570 sec/step, loss=0.07665, avg_loss=0.07444]\n",
      "Step 518430  [5.576 sec/step, loss=0.07503, avg_loss=0.07444]\n",
      "Step 518431  [5.575 sec/step, loss=0.07527, avg_loss=0.07447]\n",
      "Step 518432  [5.581 sec/step, loss=0.07442, avg_loss=0.07450]\n",
      "Step 518433  [5.569 sec/step, loss=0.07638, avg_loss=0.07453]\n",
      "Step 518434  [5.564 sec/step, loss=0.07343, avg_loss=0.07452]\n",
      "Step 518435  [5.559 sec/step, loss=0.07431, avg_loss=0.07449]\n",
      "Step 518436  [5.565 sec/step, loss=0.07687, avg_loss=0.07451]\n",
      "Step 518437  [5.576 sec/step, loss=0.07331, avg_loss=0.07451]\n",
      "Step 518438  [5.562 sec/step, loss=0.07499, avg_loss=0.07452]\n",
      "Step 518439  [5.561 sec/step, loss=0.07597, avg_loss=0.07453]\n",
      "Step 518440  [5.572 sec/step, loss=0.07555, avg_loss=0.07455]\n",
      "Step 518441  [5.553 sec/step, loss=0.07241, avg_loss=0.07451]\n",
      "Step 518442  [5.549 sec/step, loss=0.07572, avg_loss=0.07451]\n",
      "Step 518443  [5.532 sec/step, loss=0.07296, avg_loss=0.07447]\n",
      "Step 518444  [5.523 sec/step, loss=0.07504, avg_loss=0.07445]\n",
      "Step 518445  [5.502 sec/step, loss=0.06739, avg_loss=0.07438]\n",
      "Generated 32 batches of size 32 in 2.406 sec\n",
      "Step 518446  [5.537 sec/step, loss=0.07684, avg_loss=0.07443]\n",
      "Step 518447  [5.490 sec/step, loss=0.07480, avg_loss=0.07450]\n",
      "Step 518448  [5.474 sec/step, loss=0.07231, avg_loss=0.07448]\n",
      "Step 518449  [5.491 sec/step, loss=0.07707, avg_loss=0.07450]\n",
      "Step 518450  [5.463 sec/step, loss=0.07334, avg_loss=0.07449]\n",
      "Step 518451  [5.447 sec/step, loss=0.07510, avg_loss=0.07448]\n",
      "Step 518452  [5.449 sec/step, loss=0.07494, avg_loss=0.07447]\n",
      "Step 518453  [5.467 sec/step, loss=0.07621, avg_loss=0.07450]\n",
      "Step 518454  [5.509 sec/step, loss=0.07528, avg_loss=0.07459]\n",
      "Step 518455  [5.515 sec/step, loss=0.07602, avg_loss=0.07460]\n",
      "Step 518456  [5.525 sec/step, loss=0.07350, avg_loss=0.07457]\n",
      "Step 518457  [5.507 sec/step, loss=0.07228, avg_loss=0.07453]\n",
      "Step 518458  [5.498 sec/step, loss=0.07508, avg_loss=0.07453]\n",
      "Step 518459  [5.515 sec/step, loss=0.07430, avg_loss=0.07452]\n",
      "Step 518460  [5.489 sec/step, loss=0.07641, avg_loss=0.07455]\n",
      "Step 518461  [5.501 sec/step, loss=0.07642, avg_loss=0.07457]\n",
      "Step 518462  [5.512 sec/step, loss=0.07622, avg_loss=0.07459]\n",
      "Step 518463  [5.503 sec/step, loss=0.07393, avg_loss=0.07458]\n",
      "Step 518464  [5.504 sec/step, loss=0.07565, avg_loss=0.07462]\n",
      "Step 518465  [5.496 sec/step, loss=0.07519, avg_loss=0.07462]\n",
      "Step 518466  [5.508 sec/step, loss=0.07578, avg_loss=0.07463]\n",
      "Step 518467  [5.423 sec/step, loss=0.07129, avg_loss=0.07469]\n",
      "Step 518468  [5.410 sec/step, loss=0.07480, avg_loss=0.07469]\n",
      "Step 518469  [5.415 sec/step, loss=0.07526, avg_loss=0.07468]\n",
      "Step 518470  [5.398 sec/step, loss=0.07410, avg_loss=0.07465]\n",
      "Step 518471  [5.397 sec/step, loss=0.07532, avg_loss=0.07464]\n",
      "Step 518472  [5.408 sec/step, loss=0.07729, avg_loss=0.07468]\n",
      "Step 518473  [5.396 sec/step, loss=0.07581, avg_loss=0.07469]\n",
      "Step 518474  [5.386 sec/step, loss=0.07455, avg_loss=0.07470]\n",
      "Step 518475  [5.385 sec/step, loss=0.06761, avg_loss=0.07465]\n",
      "Step 518476  [5.394 sec/step, loss=0.07653, avg_loss=0.07466]\n",
      "Step 518477  [5.388 sec/step, loss=0.07588, avg_loss=0.07465]\n",
      "Generated 32 batches of size 32 in 2.383 sec\n",
      "Step 518478  [5.404 sec/step, loss=0.07508, avg_loss=0.07465]\n",
      "Step 518479  [5.409 sec/step, loss=0.07612, avg_loss=0.07465]\n",
      "Step 518480  [5.470 sec/step, loss=0.06764, avg_loss=0.07458]\n",
      "Step 518481  [5.450 sec/step, loss=0.07473, avg_loss=0.07456]\n",
      "Step 518482  [5.460 sec/step, loss=0.07674, avg_loss=0.07458]\n",
      "Step 518483  [5.471 sec/step, loss=0.07371, avg_loss=0.07465]\n",
      "Step 518484  [5.483 sec/step, loss=0.07243, avg_loss=0.07466]\n",
      "Step 518485  [5.480 sec/step, loss=0.07528, avg_loss=0.07466]\n",
      "Step 518486  [5.478 sec/step, loss=0.07663, avg_loss=0.07467]\n",
      "Step 518487  [5.481 sec/step, loss=0.07490, avg_loss=0.07465]\n",
      "Step 518488  [5.489 sec/step, loss=0.07595, avg_loss=0.07465]\n",
      "Step 518489  [5.495 sec/step, loss=0.07526, avg_loss=0.07466]\n",
      "Step 518490  [5.499 sec/step, loss=0.07683, avg_loss=0.07466]\n",
      "Step 518491  [5.505 sec/step, loss=0.07446, avg_loss=0.07465]\n",
      "Step 518492  [5.518 sec/step, loss=0.07495, avg_loss=0.07468]\n",
      "Step 518493  [5.524 sec/step, loss=0.07529, avg_loss=0.07468]\n",
      "Step 518494  [5.525 sec/step, loss=0.07596, avg_loss=0.07468]\n",
      "Step 518495  [5.506 sec/step, loss=0.07440, avg_loss=0.07465]\n",
      "Step 518496  [5.502 sec/step, loss=0.07405, avg_loss=0.07466]\n",
      "Step 518497  [5.544 sec/step, loss=0.06566, avg_loss=0.07455]\n",
      "Step 518498  [5.533 sec/step, loss=0.07609, avg_loss=0.07454]\n",
      "Step 518499  [5.535 sec/step, loss=0.07671, avg_loss=0.07456]\n",
      "Step 518500  [5.523 sec/step, loss=0.07437, avg_loss=0.07455]\n",
      "Writing summary at step: 518500\n",
      "Step 518501  [5.513 sec/step, loss=0.07509, avg_loss=0.07455]\n",
      "Step 518502  [5.495 sec/step, loss=0.07406, avg_loss=0.07452]\n",
      "Step 518503  [5.527 sec/step, loss=0.07516, avg_loss=0.07452]\n",
      "Step 518504  [5.545 sec/step, loss=0.07732, avg_loss=0.07454]\n",
      "Step 518505  [5.539 sec/step, loss=0.07506, avg_loss=0.07453]\n",
      "Step 518506  [5.546 sec/step, loss=0.07574, avg_loss=0.07454]\n",
      "Step 518507  [5.550 sec/step, loss=0.07152, avg_loss=0.07459]\n",
      "Step 518508  [5.551 sec/step, loss=0.07608, avg_loss=0.07458]\n",
      "Generated 32 batches of size 32 in 2.613 sec\n",
      "Step 518509  [5.525 sec/step, loss=0.07209, avg_loss=0.07456]\n",
      "Step 518510  [5.529 sec/step, loss=0.07665, avg_loss=0.07457]\n",
      "Step 518511  [5.519 sec/step, loss=0.07198, avg_loss=0.07454]\n",
      "Step 518512  [5.515 sec/step, loss=0.07334, avg_loss=0.07452]\n",
      "Step 518513  [5.523 sec/step, loss=0.07597, avg_loss=0.07452]\n",
      "Step 518514  [5.501 sec/step, loss=0.07478, avg_loss=0.07450]\n",
      "Step 518515  [5.503 sec/step, loss=0.07657, avg_loss=0.07451]\n",
      "Step 518516  [5.517 sec/step, loss=0.07614, avg_loss=0.07456]\n",
      "Step 518517  [5.447 sec/step, loss=0.06572, avg_loss=0.07454]\n",
      "Step 518518  [5.444 sec/step, loss=0.07384, avg_loss=0.07453]\n",
      "Step 518519  [5.431 sec/step, loss=0.07684, avg_loss=0.07455]\n",
      "Step 518520  [5.424 sec/step, loss=0.07084, avg_loss=0.07453]\n",
      "Step 518521  [5.427 sec/step, loss=0.07556, avg_loss=0.07453]\n",
      "Step 518522  [5.432 sec/step, loss=0.07155, avg_loss=0.07451]\n",
      "Step 518523  [5.436 sec/step, loss=0.07484, avg_loss=0.07451]\n",
      "Step 518524  [5.437 sec/step, loss=0.07615, avg_loss=0.07452]\n",
      "Step 518525  [5.430 sec/step, loss=0.07562, avg_loss=0.07450]\n",
      "Step 518526  [5.443 sec/step, loss=0.07658, avg_loss=0.07452]\n",
      "Step 518527  [5.444 sec/step, loss=0.07477, avg_loss=0.07450]\n",
      "Step 518528  [5.408 sec/step, loss=0.07662, avg_loss=0.07461]\n",
      "Step 518529  [5.394 sec/step, loss=0.07590, avg_loss=0.07460]\n",
      "Step 518530  [5.391 sec/step, loss=0.07279, avg_loss=0.07458]\n",
      "Step 518531  [5.388 sec/step, loss=0.07538, avg_loss=0.07458]\n",
      "Step 518532  [5.397 sec/step, loss=0.07667, avg_loss=0.07460]\n",
      "Step 518533  [5.391 sec/step, loss=0.07563, avg_loss=0.07459]\n",
      "Step 518534  [5.407 sec/step, loss=0.07369, avg_loss=0.07459]\n",
      "Step 518535  [5.418 sec/step, loss=0.07669, avg_loss=0.07462]\n",
      "Step 518536  [5.402 sec/step, loss=0.07408, avg_loss=0.07459]\n",
      "Step 518537  [5.397 sec/step, loss=0.07209, avg_loss=0.07458]\n",
      "Step 518538  [5.399 sec/step, loss=0.07405, avg_loss=0.07457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 518539  [5.419 sec/step, loss=0.07407, avg_loss=0.07455]\n",
      "Step 518540  [5.442 sec/step, loss=0.07376, avg_loss=0.07453]\n",
      "Generated 32 batches of size 32 in 2.607 sec\n",
      "Step 518541  [5.442 sec/step, loss=0.07468, avg_loss=0.07455]\n",
      "Step 518542  [5.423 sec/step, loss=0.07142, avg_loss=0.07451]\n",
      "Step 518543  [5.426 sec/step, loss=0.07374, avg_loss=0.07452]\n",
      "Step 518544  [5.432 sec/step, loss=0.07551, avg_loss=0.07452]\n",
      "Step 518545  [5.458 sec/step, loss=0.07633, avg_loss=0.07461]\n",
      "Step 518546  [5.449 sec/step, loss=0.07683, avg_loss=0.07461]\n",
      "Step 518547  [5.428 sec/step, loss=0.06770, avg_loss=0.07454]\n",
      "Step 518548  [5.444 sec/step, loss=0.07570, avg_loss=0.07458]\n",
      "Step 518549  [5.485 sec/step, loss=0.06767, avg_loss=0.07448]\n",
      "Step 518550  [5.482 sec/step, loss=0.07172, avg_loss=0.07447]\n",
      "Step 518551  [5.476 sec/step, loss=0.07613, avg_loss=0.07448]\n",
      "Step 518552  [5.457 sec/step, loss=0.06647, avg_loss=0.07439]\n",
      "Step 518553  [5.468 sec/step, loss=0.07408, avg_loss=0.07437]\n",
      "Step 518554  [5.457 sec/step, loss=0.07643, avg_loss=0.07438]\n",
      "Step 518555  [5.438 sec/step, loss=0.07398, avg_loss=0.07436]\n",
      "Step 518556  [5.418 sec/step, loss=0.07555, avg_loss=0.07438]\n",
      "Step 518557  [5.444 sec/step, loss=0.07682, avg_loss=0.07443]\n",
      "Step 518558  [5.464 sec/step, loss=0.07470, avg_loss=0.07442]\n",
      "Step 518559  [5.448 sec/step, loss=0.07328, avg_loss=0.07441]\n",
      "Step 518560  [5.454 sec/step, loss=0.07468, avg_loss=0.07440]\n",
      "Step 518561  [5.445 sec/step, loss=0.07540, avg_loss=0.07439]\n",
      "Step 518562  [5.432 sec/step, loss=0.07485, avg_loss=0.07437]\n",
      "Step 518563  [5.426 sec/step, loss=0.07257, avg_loss=0.07436]\n",
      "Step 518564  [5.434 sec/step, loss=0.07655, avg_loss=0.07437]\n",
      "Step 518565  [5.436 sec/step, loss=0.07618, avg_loss=0.07438]\n",
      "Step 518566  [5.436 sec/step, loss=0.07611, avg_loss=0.07438]\n",
      "Step 518567  [5.436 sec/step, loss=0.07276, avg_loss=0.07440]\n",
      "Step 518568  [5.435 sec/step, loss=0.07344, avg_loss=0.07438]\n",
      "Step 518569  [5.418 sec/step, loss=0.07409, avg_loss=0.07437]\n",
      "Step 518570  [5.421 sec/step, loss=0.07522, avg_loss=0.07438]\n",
      "Step 518571  [5.421 sec/step, loss=0.07708, avg_loss=0.07440]\n",
      "Step 518572  [5.410 sec/step, loss=0.07577, avg_loss=0.07438]\n",
      "Generated 32 batches of size 32 in 2.443 sec\n",
      "Step 518573  [5.431 sec/step, loss=0.07595, avg_loss=0.07439]\n",
      "Step 518574  [5.454 sec/step, loss=0.07656, avg_loss=0.07441]\n",
      "Step 518575  [5.469 sec/step, loss=0.07555, avg_loss=0.07448]\n",
      "Step 518576  [5.462 sec/step, loss=0.07379, avg_loss=0.07446]\n",
      "Step 518577  [5.470 sec/step, loss=0.07679, avg_loss=0.07447]\n",
      "Step 518578  [5.511 sec/step, loss=0.06615, avg_loss=0.07438]\n",
      "Step 518579  [5.514 sec/step, loss=0.07439, avg_loss=0.07436]\n",
      "Step 518580  [5.458 sec/step, loss=0.07317, avg_loss=0.07442]\n",
      "Step 518581  [5.452 sec/step, loss=0.07448, avg_loss=0.07441]\n",
      "Step 518582  [5.442 sec/step, loss=0.07637, avg_loss=0.07441]\n",
      "Step 518583  [5.434 sec/step, loss=0.07166, avg_loss=0.07439]\n",
      "Step 518584  [5.439 sec/step, loss=0.07587, avg_loss=0.07442]\n",
      "Step 518585  [5.465 sec/step, loss=0.07396, avg_loss=0.07441]\n",
      "Step 518586  [5.432 sec/step, loss=0.06718, avg_loss=0.07432]\n",
      "Step 518587  [5.431 sec/step, loss=0.07559, avg_loss=0.07432]\n",
      "Step 518588  [5.429 sec/step, loss=0.07625, avg_loss=0.07433]\n",
      "Step 518589  [5.423 sec/step, loss=0.07536, avg_loss=0.07433]\n",
      "Step 518590  [5.416 sec/step, loss=0.07485, avg_loss=0.07431]\n",
      "Step 518591  [5.414 sec/step, loss=0.07741, avg_loss=0.07434]\n",
      "Step 518592  [5.419 sec/step, loss=0.07611, avg_loss=0.07435]\n",
      "Step 518593  [5.461 sec/step, loss=0.06702, avg_loss=0.07426]\n",
      "Step 518594  [5.465 sec/step, loss=0.07599, avg_loss=0.07427]\n",
      "Step 518595  [5.484 sec/step, loss=0.07681, avg_loss=0.07429]\n",
      "Step 518596  [5.500 sec/step, loss=0.07657, avg_loss=0.07431]\n",
      "Step 518597  [5.443 sec/step, loss=0.07126, avg_loss=0.07437]\n",
      "Step 518598  [5.433 sec/step, loss=0.07443, avg_loss=0.07435]\n",
      "Step 518599  [5.417 sec/step, loss=0.07382, avg_loss=0.07432]\n",
      "Step 518600  [5.415 sec/step, loss=0.07613, avg_loss=0.07434]\n",
      "Writing summary at step: 518600\n",
      "Step 518601  [5.429 sec/step, loss=0.07505, avg_loss=0.07434]\n",
      "Step 518602  [5.446 sec/step, loss=0.07758, avg_loss=0.07438]\n",
      "Step 518603  [5.419 sec/step, loss=0.07550, avg_loss=0.07438]\n",
      "Generated 32 batches of size 32 in 2.541 sec\n",
      "Step 518604  [5.411 sec/step, loss=0.07536, avg_loss=0.07436]\n",
      "Step 518605  [5.433 sec/step, loss=0.07402, avg_loss=0.07435]\n",
      "Step 518606  [5.421 sec/step, loss=0.07225, avg_loss=0.07432]\n",
      "Step 518607  [5.439 sec/step, loss=0.07610, avg_loss=0.07436]\n",
      "Step 518608  [5.429 sec/step, loss=0.07423, avg_loss=0.07434]\n",
      "Step 518609  [5.429 sec/step, loss=0.07212, avg_loss=0.07434]\n",
      "Step 518610  [5.417 sec/step, loss=0.07357, avg_loss=0.07431]\n",
      "Step 518611  [5.428 sec/step, loss=0.07454, avg_loss=0.07434]\n",
      "Step 518612  [5.440 sec/step, loss=0.07544, avg_loss=0.07436]\n",
      "Step 518613  [5.440 sec/step, loss=0.07477, avg_loss=0.07435]\n",
      "Step 518614  [5.454 sec/step, loss=0.07575, avg_loss=0.07436]\n",
      "Step 518615  [5.428 sec/step, loss=0.06486, avg_loss=0.07424]\n",
      "Step 518616  [5.441 sec/step, loss=0.07529, avg_loss=0.07423]\n",
      "Step 518617  [5.459 sec/step, loss=0.07363, avg_loss=0.07431]\n",
      "Step 518618  [5.459 sec/step, loss=0.07553, avg_loss=0.07433]\n",
      "Step 518619  [5.456 sec/step, loss=0.07651, avg_loss=0.07432]\n",
      "Step 518620  [5.466 sec/step, loss=0.07448, avg_loss=0.07436]\n",
      "Step 518621  [5.472 sec/step, loss=0.07708, avg_loss=0.07438]\n",
      "Step 518622  [5.465 sec/step, loss=0.07299, avg_loss=0.07439]\n",
      "Step 518623  [5.462 sec/step, loss=0.07523, avg_loss=0.07439]\n",
      "Step 518624  [5.511 sec/step, loss=0.06639, avg_loss=0.07430]\n",
      "Step 518625  [5.495 sec/step, loss=0.07320, avg_loss=0.07427]\n",
      "Step 518626  [5.500 sec/step, loss=0.07595, avg_loss=0.07427]\n",
      "Step 518627  [5.505 sec/step, loss=0.07579, avg_loss=0.07428]\n",
      "Step 518628  [5.494 sec/step, loss=0.07677, avg_loss=0.07428]\n",
      "Step 518629  [5.501 sec/step, loss=0.07565, avg_loss=0.07427]\n",
      "Step 518630  [5.495 sec/step, loss=0.07553, avg_loss=0.07430]\n",
      "Step 518631  [5.492 sec/step, loss=0.07424, avg_loss=0.07429]\n",
      "Step 518632  [5.501 sec/step, loss=0.07622, avg_loss=0.07429]\n",
      "Step 518633  [5.498 sec/step, loss=0.07442, avg_loss=0.07427]\n",
      "Step 518634  [5.496 sec/step, loss=0.07502, avg_loss=0.07429]\n",
      "Step 518635  [5.478 sec/step, loss=0.07144, avg_loss=0.07423]\n",
      "Generated 32 batches of size 32 in 2.438 sec\n",
      "Step 518636  [5.500 sec/step, loss=0.07470, avg_loss=0.07424]\n",
      "Step 518637  [5.491 sec/step, loss=0.07465, avg_loss=0.07427]\n",
      "Step 518638  [5.487 sec/step, loss=0.07291, avg_loss=0.07425]\n",
      "Step 518639  [5.475 sec/step, loss=0.07679, avg_loss=0.07428]\n",
      "Step 518640  [5.454 sec/step, loss=0.07511, avg_loss=0.07430]\n",
      "Step 518641  [5.462 sec/step, loss=0.07501, avg_loss=0.07430]\n",
      "Step 518642  [5.480 sec/step, loss=0.07453, avg_loss=0.07433]\n",
      "Step 518643  [5.511 sec/step, loss=0.07536, avg_loss=0.07435]\n",
      "Step 518644  [5.506 sec/step, loss=0.07718, avg_loss=0.07436]\n",
      "Step 518645  [5.509 sec/step, loss=0.07677, avg_loss=0.07437]\n",
      "Step 518646  [5.490 sec/step, loss=0.07296, avg_loss=0.07433]\n",
      "Step 518647  [5.503 sec/step, loss=0.07573, avg_loss=0.07441]\n",
      "Step 518648  [5.499 sec/step, loss=0.07525, avg_loss=0.07440]\n",
      "Step 518649  [5.450 sec/step, loss=0.07608, avg_loss=0.07449]\n",
      "Step 518650  [5.451 sec/step, loss=0.07276, avg_loss=0.07450]\n",
      "Step 518651  [5.443 sec/step, loss=0.07402, avg_loss=0.07448]\n",
      "Step 518652  [5.488 sec/step, loss=0.07346, avg_loss=0.07455]\n",
      "Step 518653  [5.442 sec/step, loss=0.06575, avg_loss=0.07446]\n",
      "Step 518654  [5.424 sec/step, loss=0.07405, avg_loss=0.07444]\n",
      "Step 518655  [5.441 sec/step, loss=0.07628, avg_loss=0.07446]\n",
      "Step 518656  [5.450 sec/step, loss=0.07672, avg_loss=0.07448]\n",
      "Step 518657  [5.445 sec/step, loss=0.07354, avg_loss=0.07444]\n",
      "Step 518658  [5.447 sec/step, loss=0.07641, avg_loss=0.07446]\n",
      "Step 518659  [5.434 sec/step, loss=0.07260, avg_loss=0.07445]\n",
      "Step 518660  [5.431 sec/step, loss=0.07317, avg_loss=0.07444]\n",
      "Step 518661  [5.414 sec/step, loss=0.07128, avg_loss=0.07440]\n",
      "Step 518662  [5.429 sec/step, loss=0.07514, avg_loss=0.07440]\n",
      "Step 518663  [5.455 sec/step, loss=0.07471, avg_loss=0.07442]\n",
      "Step 518664  [5.468 sec/step, loss=0.07386, avg_loss=0.07439]\n",
      "Step 518665  [5.466 sec/step, loss=0.07644, avg_loss=0.07440]\n",
      "Step 518666  [5.466 sec/step, loss=0.07534, avg_loss=0.07439]\n",
      "Step 518667  [5.465 sec/step, loss=0.07441, avg_loss=0.07441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 32 batches of size 32 in 2.825 sec\n",
      "Step 518668  [5.456 sec/step, loss=0.07187, avg_loss=0.07439]\n",
      "Step 518669  [5.471 sec/step, loss=0.07668, avg_loss=0.07442]\n",
      "Step 518670  [5.486 sec/step, loss=0.07707, avg_loss=0.07443]\n",
      "Step 518671  [5.530 sec/step, loss=0.06649, avg_loss=0.07433]\n",
      "Step 518672  [5.534 sec/step, loss=0.07194, avg_loss=0.07429]\n",
      "Step 518673  [5.520 sec/step, loss=0.07687, avg_loss=0.07430]\n",
      "Step 518674  [5.507 sec/step, loss=0.07541, avg_loss=0.07429]\n",
      "Step 518675  [5.502 sec/step, loss=0.07446, avg_loss=0.07428]\n",
      "Step 518676  [5.505 sec/step, loss=0.07572, avg_loss=0.07430]\n",
      "Step 518677  [5.494 sec/step, loss=0.07497, avg_loss=0.07428]\n",
      "Step 518678  [5.449 sec/step, loss=0.07470, avg_loss=0.07436]\n",
      "Step 518679  [5.428 sec/step, loss=0.06773, avg_loss=0.07430]\n",
      "Step 518680  [5.435 sec/step, loss=0.07597, avg_loss=0.07432]\n",
      "Step 518681  [5.450 sec/step, loss=0.07564, avg_loss=0.07434]\n",
      "Step 518682  [5.453 sec/step, loss=0.07527, avg_loss=0.07433]\n",
      "Step 518683  [5.465 sec/step, loss=0.07590, avg_loss=0.07437]\n",
      "Step 518684  [5.459 sec/step, loss=0.07218, avg_loss=0.07433]\n",
      "Step 518685  [5.432 sec/step, loss=0.07447, avg_loss=0.07434]\n",
      "Step 518686  [5.443 sec/step, loss=0.07127, avg_loss=0.07438]\n",
      "Step 518687  [5.452 sec/step, loss=0.07668, avg_loss=0.07439]\n",
      "Step 518688  [5.430 sec/step, loss=0.07127, avg_loss=0.07434]\n",
      "Step 518689  [5.433 sec/step, loss=0.07692, avg_loss=0.07435]\n",
      "Step 518690  [5.433 sec/step, loss=0.07530, avg_loss=0.07436]\n",
      "Step 518691  [5.452 sec/step, loss=0.07369, avg_loss=0.07432]\n",
      "Step 518692  [5.498 sec/step, loss=0.06767, avg_loss=0.07424]\n",
      "Step 518693  [5.458 sec/step, loss=0.07459, avg_loss=0.07431]\n",
      "Step 518694  [5.465 sec/step, loss=0.07646, avg_loss=0.07432]\n",
      "Step 518695  [5.443 sec/step, loss=0.07293, avg_loss=0.07428]\n",
      "Step 518696  [5.446 sec/step, loss=0.07674, avg_loss=0.07428]\n",
      "Step 518697  [5.451 sec/step, loss=0.07567, avg_loss=0.07432]\n",
      "Step 518698  [5.468 sec/step, loss=0.07660, avg_loss=0.07435]\n",
      "Step 518699  [5.470 sec/step, loss=0.07580, avg_loss=0.07436]\n",
      "Generated 32 batches of size 32 in 2.748 sec\n",
      "Step 518700  [5.463 sec/step, loss=0.07273, avg_loss=0.07433]\n",
      "Writing summary at step: 518700\n",
      "Step 518701  [5.452 sec/step, loss=0.07694, avg_loss=0.07435]\n",
      "Step 518702  [5.460 sec/step, loss=0.07460, avg_loss=0.07432]\n",
      "Step 518703  [5.462 sec/step, loss=0.07570, avg_loss=0.07432]\n",
      "Step 518704  [5.451 sec/step, loss=0.07439, avg_loss=0.07431]\n",
      "Step 518705  [5.432 sec/step, loss=0.07577, avg_loss=0.07433]\n",
      "Step 518706  [5.461 sec/step, loss=0.07504, avg_loss=0.07436]\n",
      "Step 518707  [5.451 sec/step, loss=0.07468, avg_loss=0.07434]\n",
      "Step 518708  [5.463 sec/step, loss=0.07504, avg_loss=0.07435]\n",
      "Step 518709  [5.463 sec/step, loss=0.07538, avg_loss=0.07438]\n",
      "Step 518710  [5.467 sec/step, loss=0.07507, avg_loss=0.07440]\n",
      "Step 518711  [5.485 sec/step, loss=0.07514, avg_loss=0.07441]\n",
      "Step 518712  [5.483 sec/step, loss=0.07673, avg_loss=0.07442]\n",
      "Step 518713  [5.475 sec/step, loss=0.07621, avg_loss=0.07443]\n",
      "Step 518714  [5.474 sec/step, loss=0.07652, avg_loss=0.07444]\n",
      "Step 518715  [5.501 sec/step, loss=0.07634, avg_loss=0.07456]\n",
      "Step 518716  [5.512 sec/step, loss=0.07504, avg_loss=0.07455]\n",
      "Step 518717  [5.497 sec/step, loss=0.07215, avg_loss=0.07454]\n",
      "Step 518718  [5.501 sec/step, loss=0.07554, avg_loss=0.07454]\n",
      "Step 518719  [5.483 sec/step, loss=0.07223, avg_loss=0.07450]\n",
      "Step 518720  [5.494 sec/step, loss=0.07514, avg_loss=0.07450]\n",
      "Step 518721  [5.479 sec/step, loss=0.07312, avg_loss=0.07446]\n",
      "Step 518722  [5.491 sec/step, loss=0.07552, avg_loss=0.07449]\n",
      "Step 518723  [5.513 sec/step, loss=0.07692, avg_loss=0.07450]\n",
      "Step 518724  [5.458 sec/step, loss=0.07482, avg_loss=0.07459]\n",
      "Step 518725  [5.483 sec/step, loss=0.07646, avg_loss=0.07462]\n",
      "Step 518726  [5.472 sec/step, loss=0.07450, avg_loss=0.07461]\n",
      "Step 518727  [5.436 sec/step, loss=0.06687, avg_loss=0.07452]\n",
      "Step 518728  [5.438 sec/step, loss=0.07521, avg_loss=0.07450]\n",
      "Step 518729  [5.440 sec/step, loss=0.07675, avg_loss=0.07451]\n",
      "Step 518730  [5.446 sec/step, loss=0.07530, avg_loss=0.07451]\n",
      "Generated 32 batches of size 32 in 2.580 sec\n",
      "Step 518731  [5.447 sec/step, loss=0.07422, avg_loss=0.07451]\n",
      "Step 518732  [5.448 sec/step, loss=0.07553, avg_loss=0.07450]\n",
      "Step 518733  [5.505 sec/step, loss=0.06584, avg_loss=0.07442]\n",
      "Step 518734  [5.518 sec/step, loss=0.07517, avg_loss=0.07442]\n",
      "Step 518735  [5.526 sec/step, loss=0.07559, avg_loss=0.07446]\n",
      "Step 518736  [5.510 sec/step, loss=0.07049, avg_loss=0.07442]\n",
      "Step 518737  [5.512 sec/step, loss=0.07358, avg_loss=0.07441]\n",
      "Step 518738  [5.510 sec/step, loss=0.07467, avg_loss=0.07443]\n",
      "Step 518739  [5.491 sec/step, loss=0.07492, avg_loss=0.07441]\n",
      "Step 518740  [5.496 sec/step, loss=0.07673, avg_loss=0.07442]\n",
      "Step 518741  [5.476 sec/step, loss=0.06650, avg_loss=0.07434]\n",
      "Step 518742  [5.489 sec/step, loss=0.07577, avg_loss=0.07435]\n",
      "Step 518743  [5.475 sec/step, loss=0.07626, avg_loss=0.07436]\n",
      "Step 518744  [5.483 sec/step, loss=0.07637, avg_loss=0.07435]\n",
      "Step 518745  [5.472 sec/step, loss=0.07418, avg_loss=0.07433]\n",
      "Step 518746  [5.478 sec/step, loss=0.07552, avg_loss=0.07435]\n",
      "Step 518747  [5.478 sec/step, loss=0.07423, avg_loss=0.07434]\n",
      "Step 518748  [5.464 sec/step, loss=0.07280, avg_loss=0.07431]\n",
      "Step 518749  [5.461 sec/step, loss=0.07564, avg_loss=0.07431]\n",
      "Step 518750  [5.459 sec/step, loss=0.07535, avg_loss=0.07433]\n",
      "Step 518751  [5.465 sec/step, loss=0.07289, avg_loss=0.07432]\n",
      "Step 518752  [5.443 sec/step, loss=0.07661, avg_loss=0.07435]\n",
      "Step 518753  [5.457 sec/step, loss=0.07437, avg_loss=0.07444]\n",
      "Step 518754  [5.465 sec/step, loss=0.07543, avg_loss=0.07445]\n",
      "Step 518755  [5.465 sec/step, loss=0.07670, avg_loss=0.07446]\n",
      "Step 518756  [5.453 sec/step, loss=0.07508, avg_loss=0.07444]\n",
      "Step 518757  [5.497 sec/step, loss=0.06646, avg_loss=0.07437]\n",
      "Step 518758  [5.496 sec/step, loss=0.07668, avg_loss=0.07437]\n",
      "Step 518759  [5.491 sec/step, loss=0.07481, avg_loss=0.07439]\n",
      "Step 518760  [5.492 sec/step, loss=0.07582, avg_loss=0.07442]\n",
      "Step 518761  [5.505 sec/step, loss=0.07621, avg_loss=0.07447]\n",
      "Step 518762  [5.508 sec/step, loss=0.07651, avg_loss=0.07448]\n",
      "Generated 32 batches of size 32 in 2.499 sec\n",
      "Step 518763  [5.490 sec/step, loss=0.07440, avg_loss=0.07448]\n",
      "Step 518764  [5.474 sec/step, loss=0.07543, avg_loss=0.07450]\n",
      "Step 518765  [5.462 sec/step, loss=0.07190, avg_loss=0.07445]\n",
      "Step 518766  [5.469 sec/step, loss=0.07475, avg_loss=0.07445]\n",
      "Step 518767  [5.472 sec/step, loss=0.07407, avg_loss=0.07444]\n",
      "Step 518768  [5.510 sec/step, loss=0.07343, avg_loss=0.07446]\n",
      "Step 518769  [5.510 sec/step, loss=0.07723, avg_loss=0.07446]\n",
      "Step 518770  [5.500 sec/step, loss=0.07371, avg_loss=0.07443]\n",
      "Step 518771  [5.450 sec/step, loss=0.07623, avg_loss=0.07453]\n",
      "Step 518772  [5.456 sec/step, loss=0.07540, avg_loss=0.07456]\n",
      "Step 518773  [5.458 sec/step, loss=0.07441, avg_loss=0.07454]\n",
      "Step 518774  [5.470 sec/step, loss=0.07646, avg_loss=0.07455]\n",
      "Step 518775  [5.482 sec/step, loss=0.07602, avg_loss=0.07456]\n",
      "Step 518776  [5.475 sec/step, loss=0.07327, avg_loss=0.07454]\n",
      "Step 518777  [5.487 sec/step, loss=0.07438, avg_loss=0.07453]\n",
      "Step 518778  [5.493 sec/step, loss=0.07701, avg_loss=0.07456]\n",
      "Step 518779  [5.505 sec/step, loss=0.07315, avg_loss=0.07461]\n",
      "Step 518780  [5.502 sec/step, loss=0.07476, avg_loss=0.07460]\n",
      "Step 518781  [5.547 sec/step, loss=0.06685, avg_loss=0.07451]\n",
      "Step 518782  [5.533 sec/step, loss=0.07192, avg_loss=0.07448]\n",
      "Step 518783  [5.559 sec/step, loss=0.07389, avg_loss=0.07446]\n",
      "Step 518784  [5.542 sec/step, loss=0.06728, avg_loss=0.07441]\n",
      "Step 518785  [5.554 sec/step, loss=0.07456, avg_loss=0.07441]\n",
      "Step 518786  [5.561 sec/step, loss=0.07579, avg_loss=0.07445]\n",
      "Step 518787  [5.537 sec/step, loss=0.07455, avg_loss=0.07443]\n",
      "Step 518788  [5.546 sec/step, loss=0.07548, avg_loss=0.07447]\n",
      "Step 518789  [5.556 sec/step, loss=0.07435, avg_loss=0.07445]\n",
      "Step 518790  [5.560 sec/step, loss=0.07647, avg_loss=0.07446]\n",
      "Step 518791  [5.529 sec/step, loss=0.07383, avg_loss=0.07446]\n",
      "Step 518792  [5.483 sec/step, loss=0.07482, avg_loss=0.07453]\n",
      "Step 518793  [5.477 sec/step, loss=0.07486, avg_loss=0.07454]\n",
      "Step 518794  [5.461 sec/step, loss=0.07420, avg_loss=0.07451]\n",
      "Generated 32 batches of size 32 in 2.526 sec\n",
      "Step 518795  [5.467 sec/step, loss=0.07367, avg_loss=0.07452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 518796  [5.458 sec/step, loss=0.07556, avg_loss=0.07451]\n",
      "Step 518797  [5.476 sec/step, loss=0.07624, avg_loss=0.07451]\n",
      "Step 518798  [5.463 sec/step, loss=0.07480, avg_loss=0.07450]\n",
      "Step 518799  [5.463 sec/step, loss=0.07546, avg_loss=0.07449]\n",
      "Step 518800  [5.458 sec/step, loss=0.07173, avg_loss=0.07448]\n",
      "Writing summary at step: 518800\n",
      "Step 518801  [5.454 sec/step, loss=0.07569, avg_loss=0.07447]\n",
      "Step 518802  [5.431 sec/step, loss=0.07551, avg_loss=0.07448]\n",
      "Step 518803  [5.436 sec/step, loss=0.07613, avg_loss=0.07448]\n",
      "Step 518804  [5.456 sec/step, loss=0.07706, avg_loss=0.07451]\n",
      "Step 518805  [5.440 sec/step, loss=0.07377, avg_loss=0.07449]\n",
      "Step 518806  [5.435 sec/step, loss=0.07629, avg_loss=0.07450]\n",
      "Step 518807  [5.444 sec/step, loss=0.07580, avg_loss=0.07451]\n",
      "Step 518808  [5.439 sec/step, loss=0.07540, avg_loss=0.07452]\n",
      "Step 518809  [5.438 sec/step, loss=0.07478, avg_loss=0.07451]\n",
      "Step 518810  [5.425 sec/step, loss=0.07445, avg_loss=0.07451]\n",
      "Step 518811  [5.434 sec/step, loss=0.07397, avg_loss=0.07449]\n",
      "Step 518812  [5.442 sec/step, loss=0.07663, avg_loss=0.07449]\n",
      "Step 518813  [5.440 sec/step, loss=0.07236, avg_loss=0.07445]\n",
      "Step 518814  [5.433 sec/step, loss=0.07430, avg_loss=0.07443]\n",
      "Step 518815  [5.426 sec/step, loss=0.07592, avg_loss=0.07443]\n",
      "Step 518816  [5.412 sec/step, loss=0.07661, avg_loss=0.07444]\n",
      "Step 518817  [5.478 sec/step, loss=0.06731, avg_loss=0.07440]\n",
      "Step 518818  [5.470 sec/step, loss=0.07267, avg_loss=0.07437]\n",
      "Step 518819  [5.493 sec/step, loss=0.07507, avg_loss=0.07440]\n",
      "Step 518820  [5.483 sec/step, loss=0.07364, avg_loss=0.07438]\n",
      "Step 518821  [5.499 sec/step, loss=0.07395, avg_loss=0.07439]\n",
      "Step 518822  [5.494 sec/step, loss=0.07533, avg_loss=0.07439]\n",
      "Step 518823  [5.480 sec/step, loss=0.07594, avg_loss=0.07438]\n",
      "Step 518824  [5.474 sec/step, loss=0.07040, avg_loss=0.07433]\n",
      "Step 518825  [5.474 sec/step, loss=0.07530, avg_loss=0.07432]\n",
      "Generated 32 batches of size 32 in 2.422 sec\n",
      "Step 518826  [5.484 sec/step, loss=0.07569, avg_loss=0.07433]\n",
      "Step 518827  [5.523 sec/step, loss=0.07418, avg_loss=0.07441]\n",
      "Step 518828  [5.523 sec/step, loss=0.07507, avg_loss=0.07441]\n",
      "Step 518829  [5.513 sec/step, loss=0.07405, avg_loss=0.07438]\n",
      "Step 518830  [5.490 sec/step, loss=0.06566, avg_loss=0.07428]\n",
      "Step 518831  [5.492 sec/step, loss=0.07519, avg_loss=0.07429]\n",
      "Step 518832  [5.484 sec/step, loss=0.07634, avg_loss=0.07430]\n",
      "Step 518833  [5.420 sec/step, loss=0.07060, avg_loss=0.07435]\n",
      "Step 518834  [5.412 sec/step, loss=0.07496, avg_loss=0.07435]\n",
      "Step 518835  [5.410 sec/step, loss=0.07414, avg_loss=0.07433]\n",
      "Step 518836  [5.398 sec/step, loss=0.07324, avg_loss=0.07436]\n",
      "Step 518837  [5.413 sec/step, loss=0.07599, avg_loss=0.07438]\n",
      "Step 518838  [5.419 sec/step, loss=0.07299, avg_loss=0.07437]\n",
      "Step 518839  [5.447 sec/step, loss=0.07592, avg_loss=0.07438]\n",
      "Step 518840  [5.435 sec/step, loss=0.07530, avg_loss=0.07436]\n",
      "Step 518841  [5.504 sec/step, loss=0.06634, avg_loss=0.07436]\n",
      "Step 518842  [5.488 sec/step, loss=0.07601, avg_loss=0.07436]\n",
      "Step 518843  [5.469 sec/step, loss=0.07424, avg_loss=0.07434]\n",
      "Step 518844  [5.457 sec/step, loss=0.07570, avg_loss=0.07433]\n",
      "Step 518845  [5.469 sec/step, loss=0.07586, avg_loss=0.07435]\n",
      "Step 518846  [5.457 sec/step, loss=0.07268, avg_loss=0.07432]\n",
      "Step 518847  [5.460 sec/step, loss=0.07460, avg_loss=0.07433]\n",
      "Step 518848  [5.483 sec/step, loss=0.07679, avg_loss=0.07437]\n",
      "Step 518849  [5.495 sec/step, loss=0.07673, avg_loss=0.07438]\n",
      "Step 518850  [5.523 sec/step, loss=0.07582, avg_loss=0.07438]\n",
      "Step 518851  [5.521 sec/step, loss=0.07642, avg_loss=0.07442]\n",
      "Step 518852  [5.505 sec/step, loss=0.07452, avg_loss=0.07440]\n",
      "Step 518853  [5.493 sec/step, loss=0.06694, avg_loss=0.07432]\n",
      "Step 518854  [5.495 sec/step, loss=0.07445, avg_loss=0.07431]\n",
      "Step 518855  [5.487 sec/step, loss=0.07565, avg_loss=0.07430]\n",
      "Step 518856  [5.491 sec/step, loss=0.07587, avg_loss=0.07431]\n",
      "Step 518857  [5.451 sec/step, loss=0.07671, avg_loss=0.07441]\n",
      "Generated 32 batches of size 32 in 2.532 sec\n",
      "Step 518858  [5.437 sec/step, loss=0.07483, avg_loss=0.07439]\n",
      "Step 518859  [5.444 sec/step, loss=0.07380, avg_loss=0.07438]\n",
      "Step 518860  [5.436 sec/step, loss=0.07380, avg_loss=0.07436]\n",
      "Step 518861  [5.442 sec/step, loss=0.07687, avg_loss=0.07437]\n",
      "Step 518862  [5.439 sec/step, loss=0.07468, avg_loss=0.07435]\n",
      "Step 518863  [5.450 sec/step, loss=0.07611, avg_loss=0.07437]\n",
      "Step 518864  [5.450 sec/step, loss=0.07446, avg_loss=0.07436]\n",
      "Step 518865  [5.466 sec/step, loss=0.07700, avg_loss=0.07441]\n",
      "Step 518866  [5.461 sec/step, loss=0.07492, avg_loss=0.07441]\n",
      "Step 518867  [5.474 sec/step, loss=0.07686, avg_loss=0.07444]\n",
      "Step 518868  [5.444 sec/step, loss=0.07384, avg_loss=0.07444]\n",
      "Step 518869  [5.447 sec/step, loss=0.07651, avg_loss=0.07444]\n",
      "Step 518870  [5.455 sec/step, loss=0.07400, avg_loss=0.07444]\n",
      "Step 518871  [5.474 sec/step, loss=0.07384, avg_loss=0.07442]\n",
      "Step 518872  [5.464 sec/step, loss=0.07494, avg_loss=0.07441]\n",
      "Step 518873  [5.441 sec/step, loss=0.07098, avg_loss=0.07438]\n",
      "Step 518874  [5.478 sec/step, loss=0.06799, avg_loss=0.07429]\n",
      "Step 518875  [5.487 sec/step, loss=0.07654, avg_loss=0.07430]\n",
      "Step 518876  [5.482 sec/step, loss=0.07362, avg_loss=0.07430]\n",
      "Step 518877  [5.471 sec/step, loss=0.07638, avg_loss=0.07432]\n",
      "Step 518878  [5.460 sec/step, loss=0.07525, avg_loss=0.07430]\n",
      "Step 518879  [5.473 sec/step, loss=0.07719, avg_loss=0.07434]\n",
      "Step 518880  [5.476 sec/step, loss=0.07589, avg_loss=0.07436]\n",
      "Step 518881  [5.453 sec/step, loss=0.07423, avg_loss=0.07443]\n",
      "Step 518882  [5.469 sec/step, loss=0.07446, avg_loss=0.07445]\n",
      "Step 518883  [5.439 sec/step, loss=0.07430, avg_loss=0.07446]\n",
      "Step 518884  [5.460 sec/step, loss=0.07565, avg_loss=0.07454]\n",
      "Step 518885  [5.438 sec/step, loss=0.07143, avg_loss=0.07451]\n",
      "Step 518886  [5.420 sec/step, loss=0.06520, avg_loss=0.07441]\n",
      "Step 518887  [5.444 sec/step, loss=0.07749, avg_loss=0.07443]\n",
      "Step 518888  [5.439 sec/step, loss=0.07191, avg_loss=0.07440]\n",
      "Step 518889  [5.408 sec/step, loss=0.07468, avg_loss=0.07440]\n",
      "Generated 32 batches of size 32 in 2.680 sec\n",
      "Step 518890  [5.392 sec/step, loss=0.07496, avg_loss=0.07439]\n",
      "Step 518891  [5.394 sec/step, loss=0.07571, avg_loss=0.07441]\n",
      "Step 518892  [5.386 sec/step, loss=0.07613, avg_loss=0.07442]\n",
      "Step 518893  [5.390 sec/step, loss=0.07569, avg_loss=0.07443]\n",
      "Step 518894  [5.404 sec/step, loss=0.07517, avg_loss=0.07444]\n",
      "Step 518895  [5.419 sec/step, loss=0.07486, avg_loss=0.07445]\n",
      "Step 518896  [5.416 sec/step, loss=0.07558, avg_loss=0.07445]\n",
      "Step 518897  [5.406 sec/step, loss=0.07568, avg_loss=0.07444]\n",
      "Step 518898  [5.406 sec/step, loss=0.07465, avg_loss=0.07444]\n",
      "Step 518899  [5.390 sec/step, loss=0.06837, avg_loss=0.07437]\n",
      "Step 518900  [5.402 sec/step, loss=0.07232, avg_loss=0.07438]\n",
      "Writing summary at step: 518900\n",
      "Step 518901  [5.419 sec/step, loss=0.07660, avg_loss=0.07439]\n",
      "Step 518902  [5.414 sec/step, loss=0.07481, avg_loss=0.07438]\n",
      "Step 518903  [5.411 sec/step, loss=0.07432, avg_loss=0.07436]\n",
      "Step 518904  [5.422 sec/step, loss=0.07373, avg_loss=0.07433]\n",
      "Step 518905  [5.419 sec/step, loss=0.07325, avg_loss=0.07432]\n",
      "Step 518906  [5.413 sec/step, loss=0.07330, avg_loss=0.07429]\n",
      "Step 518907  [5.415 sec/step, loss=0.07571, avg_loss=0.07429]\n",
      "Step 518908  [5.417 sec/step, loss=0.07544, avg_loss=0.07429]\n",
      "Step 518909  [5.423 sec/step, loss=0.07644, avg_loss=0.07431]\n",
      "Step 518910  [5.426 sec/step, loss=0.07506, avg_loss=0.07431]\n",
      "Step 518911  [5.401 sec/step, loss=0.07578, avg_loss=0.07433]\n",
      "Step 518912  [5.387 sec/step, loss=0.07627, avg_loss=0.07433]\n",
      "Step 518913  [5.401 sec/step, loss=0.07679, avg_loss=0.07437]\n",
      "Step 518914  [5.456 sec/step, loss=0.06776, avg_loss=0.07431]\n",
      "Step 518915  [5.465 sec/step, loss=0.07659, avg_loss=0.07431]\n",
      "Step 518916  [5.450 sec/step, loss=0.07449, avg_loss=0.07429]\n",
      "Step 518917  [5.414 sec/step, loss=0.07678, avg_loss=0.07439]\n",
      "Step 518918  [5.432 sec/step, loss=0.07691, avg_loss=0.07443]\n",
      "Step 518919  [5.427 sec/step, loss=0.07733, avg_loss=0.07445]\n",
      "Step 518920  [5.433 sec/step, loss=0.07294, avg_loss=0.07445]\n",
      "Generated 32 batches of size 32 in 2.483 sec\n",
      "Step 518921  [5.428 sec/step, loss=0.07455, avg_loss=0.07445]\n",
      "Step 518922  [5.427 sec/step, loss=0.07609, avg_loss=0.07446]\n",
      "Step 518923  [5.425 sec/step, loss=0.07102, avg_loss=0.07441]\n",
      "Step 518924  [5.427 sec/step, loss=0.07107, avg_loss=0.07442]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 518925  [5.402 sec/step, loss=0.07345, avg_loss=0.07440]\n",
      "Step 518926  [5.410 sec/step, loss=0.07584, avg_loss=0.07440]\n",
      "Step 518927  [5.387 sec/step, loss=0.07400, avg_loss=0.07440]\n",
      "Step 518928  [5.393 sec/step, loss=0.07581, avg_loss=0.07441]\n",
      "Step 518929  [5.395 sec/step, loss=0.07548, avg_loss=0.07442]\n",
      "Step 518930  [5.403 sec/step, loss=0.07219, avg_loss=0.07449]\n",
      "Step 518931  [5.412 sec/step, loss=0.07692, avg_loss=0.07450]\n",
      "Step 518932  [5.398 sec/step, loss=0.07473, avg_loss=0.07449]\n",
      "Step 518933  [5.418 sec/step, loss=0.07553, avg_loss=0.07454]\n",
      "Step 518934  [5.407 sec/step, loss=0.07268, avg_loss=0.07451]\n",
      "Step 518935  [5.424 sec/step, loss=0.07607, avg_loss=0.07453]\n",
      "Step 518936  [5.453 sec/step, loss=0.07601, avg_loss=0.07456]\n",
      "Step 518937  [5.444 sec/step, loss=0.07691, avg_loss=0.07457]\n",
      "Step 518938  [5.457 sec/step, loss=0.07672, avg_loss=0.07461]\n",
      "Step 518939  [5.435 sec/step, loss=0.07588, avg_loss=0.07461]\n",
      "Step 518940  [5.442 sec/step, loss=0.07512, avg_loss=0.07460]\n",
      "Step 518941  [5.409 sec/step, loss=0.07616, avg_loss=0.07470]\n",
      "Step 518942  [5.405 sec/step, loss=0.07615, avg_loss=0.07470]\n",
      "Step 518943  [5.438 sec/step, loss=0.07548, avg_loss=0.07472]\n",
      "Step 518944  [5.422 sec/step, loss=0.07334, avg_loss=0.07469]\n",
      "Step 518945  [5.410 sec/step, loss=0.07633, avg_loss=0.07470]\n",
      "Step 518946  [5.423 sec/step, loss=0.07576, avg_loss=0.07473]\n",
      "Step 518947  [5.431 sec/step, loss=0.07623, avg_loss=0.07474]\n",
      "Step 518948  [5.429 sec/step, loss=0.07443, avg_loss=0.07472]\n",
      "Step 518949  [5.407 sec/step, loss=0.07489, avg_loss=0.07470]\n",
      "Step 518950  [5.365 sec/step, loss=0.06795, avg_loss=0.07462]\n",
      "Step 518951  [5.370 sec/step, loss=0.07663, avg_loss=0.07463]\n",
      "Step 518952  [5.380 sec/step, loss=0.07598, avg_loss=0.07464]\n",
      "Generated 32 batches of size 32 in 2.413 sec\n",
      "Step 518953  [5.409 sec/step, loss=0.07668, avg_loss=0.07474]\n",
      "Step 518954  [5.396 sec/step, loss=0.07434, avg_loss=0.07474]\n",
      "Step 518955  [5.395 sec/step, loss=0.07151, avg_loss=0.07470]\n",
      "Step 518956  [5.392 sec/step, loss=0.07404, avg_loss=0.07468]\n",
      "Step 518957  [5.374 sec/step, loss=0.07451, avg_loss=0.07466]\n",
      "Step 518958  [5.381 sec/step, loss=0.07430, avg_loss=0.07465]\n",
      "Step 518959  [5.381 sec/step, loss=0.07544, avg_loss=0.07467]\n",
      "Step 518960  [5.434 sec/step, loss=0.06664, avg_loss=0.07459]\n",
      "Step 518961  [5.437 sec/step, loss=0.07712, avg_loss=0.07460]\n",
      "Step 518962  [5.412 sec/step, loss=0.07081, avg_loss=0.07456]\n",
      "Step 518963  [5.449 sec/step, loss=0.07082, avg_loss=0.07451]\n",
      "Step 518964  [5.443 sec/step, loss=0.07188, avg_loss=0.07448]\n",
      "Step 518965  [5.450 sec/step, loss=0.07455, avg_loss=0.07446]\n",
      "Step 518966  [5.454 sec/step, loss=0.07658, avg_loss=0.07447]\n",
      "Step 518967  [5.450 sec/step, loss=0.07253, avg_loss=0.07443]\n",
      "Step 518968  [5.455 sec/step, loss=0.07596, avg_loss=0.07445]\n",
      "Step 518969  [5.447 sec/step, loss=0.07660, avg_loss=0.07445]\n",
      "Step 518970  [5.434 sec/step, loss=0.07523, avg_loss=0.07446]\n",
      "Step 518971  [5.403 sec/step, loss=0.07522, avg_loss=0.07448]\n",
      "Step 518972  [5.404 sec/step, loss=0.07345, avg_loss=0.07446]\n",
      "Step 518973  [5.422 sec/step, loss=0.07544, avg_loss=0.07451]\n",
      "Step 518974  [5.365 sec/step, loss=0.07502, avg_loss=0.07458]\n",
      "Step 518975  [5.348 sec/step, loss=0.07298, avg_loss=0.07454]\n",
      "Step 518976  [5.374 sec/step, loss=0.07408, avg_loss=0.07455]\n",
      "Step 518977  [5.361 sec/step, loss=0.07126, avg_loss=0.07449]\n",
      "Step 518978  [5.352 sec/step, loss=0.07374, avg_loss=0.07448]\n",
      "Step 518979  [5.352 sec/step, loss=0.07646, avg_loss=0.07447]\n",
      "Step 518980  [5.353 sec/step, loss=0.07241, avg_loss=0.07444]\n",
      "Step 518981  [5.337 sec/step, loss=0.07719, avg_loss=0.07447]\n",
      "Step 518982  [5.336 sec/step, loss=0.07584, avg_loss=0.07448]\n",
      "Step 518983  [5.336 sec/step, loss=0.07570, avg_loss=0.07450]\n",
      "Step 518984  [5.343 sec/step, loss=0.07458, avg_loss=0.07448]\n",
      "Generated 32 batches of size 32 in 2.400 sec\n",
      "Step 518985  [5.366 sec/step, loss=0.07505, avg_loss=0.07452]\n",
      "Step 518986  [5.366 sec/step, loss=0.06545, avg_loss=0.07452]\n",
      "Step 518987  [5.366 sec/step, loss=0.07723, avg_loss=0.07452]\n",
      "Step 518988  [5.367 sec/step, loss=0.07457, avg_loss=0.07455]\n",
      "Step 518989  [5.384 sec/step, loss=0.07591, avg_loss=0.07456]\n",
      "Step 518990  [5.417 sec/step, loss=0.07379, avg_loss=0.07455]\n",
      "Step 518991  [5.428 sec/step, loss=0.07724, avg_loss=0.07456]\n",
      "Step 518992  [5.428 sec/step, loss=0.07607, avg_loss=0.07456]\n",
      "Step 518993  [5.433 sec/step, loss=0.07684, avg_loss=0.07457]\n",
      "Step 518994  [5.430 sec/step, loss=0.07584, avg_loss=0.07458]\n",
      "Step 518995  [5.418 sec/step, loss=0.07491, avg_loss=0.07458]\n",
      "Step 518996  [5.427 sec/step, loss=0.07583, avg_loss=0.07458]\n",
      "Step 518997  [5.426 sec/step, loss=0.07601, avg_loss=0.07459]\n",
      "Step 518998  [5.442 sec/step, loss=0.07450, avg_loss=0.07459]\n",
      "Step 518999  [5.474 sec/step, loss=0.07669, avg_loss=0.07467]\n",
      "Step 519000  [5.471 sec/step, loss=0.07338, avg_loss=0.07468]\n",
      "Writing summary at step: 519000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-519000\n",
      "Saving audio and alignment...\n",
      "Input: mashar nay ghuur kur dzadz koo ddaykhaa guddaa khabbay muuuf bartdhtdhlaaizar miin tsaaquu sae piroojaa or sootsaa kub tdiin or miin tsham sae vaemp miin aa giroon~_______________________________________________________\n",
      "Step 519001  [5.466 sec/step, loss=0.07660, avg_loss=0.07468]\n",
      "Step 519002  [5.480 sec/step, loss=0.07592, avg_loss=0.07469]\n",
      "Step 519003  [5.484 sec/step, loss=0.07515, avg_loss=0.07470]\n",
      "Step 519004  [5.459 sec/step, loss=0.07616, avg_loss=0.07472]\n",
      "Step 519005  [5.459 sec/step, loss=0.07236, avg_loss=0.07471]\n",
      "Step 519006  [5.456 sec/step, loss=0.07485, avg_loss=0.07473]\n",
      "Step 519007  [5.449 sec/step, loss=0.07641, avg_loss=0.07474]\n",
      "Step 519008  [5.432 sec/step, loss=0.07218, avg_loss=0.07470]\n",
      "Step 519009  [5.444 sec/step, loss=0.07678, avg_loss=0.07471]\n",
      "Step 519010  [5.444 sec/step, loss=0.07425, avg_loss=0.07470]\n",
      "Step 519011  [5.452 sec/step, loss=0.07661, avg_loss=0.07471]\n",
      "Step 519012  [5.444 sec/step, loss=0.07431, avg_loss=0.07469]\n",
      "Step 519013  [5.434 sec/step, loss=0.07200, avg_loss=0.07464]\n",
      "Step 519014  [5.409 sec/step, loss=0.07405, avg_loss=0.07470]\n",
      "Generated 32 batches of size 32 in 2.497 sec\n",
      "Step 519015  [5.401 sec/step, loss=0.07552, avg_loss=0.07469]\n",
      "Step 519016  [5.386 sec/step, loss=0.06620, avg_loss=0.07461]\n",
      "Step 519017  [5.361 sec/step, loss=0.07488, avg_loss=0.07459]\n",
      "Step 519018  [5.350 sec/step, loss=0.07416, avg_loss=0.07456]\n",
      "Step 519019  [5.353 sec/step, loss=0.07663, avg_loss=0.07456]\n",
      "Step 519020  [5.350 sec/step, loss=0.07659, avg_loss=0.07459]\n",
      "Step 519021  [5.359 sec/step, loss=0.07746, avg_loss=0.07462]\n",
      "Step 519022  [5.356 sec/step, loss=0.07436, avg_loss=0.07460]\n",
      "Step 519023  [5.365 sec/step, loss=0.07514, avg_loss=0.07465]\n",
      "Step 519024  [5.381 sec/step, loss=0.07667, avg_loss=0.07470]\n",
      "Step 519025  [5.395 sec/step, loss=0.07514, avg_loss=0.07472]\n",
      "Step 519026  [5.385 sec/step, loss=0.07642, avg_loss=0.07472]\n",
      "Step 519027  [5.380 sec/step, loss=0.07395, avg_loss=0.07472]\n",
      "Step 519028  [5.382 sec/step, loss=0.07650, avg_loss=0.07473]\n",
      "Step 519029  [5.372 sec/step, loss=0.07230, avg_loss=0.07470]\n",
      "Step 519030  [5.387 sec/step, loss=0.07612, avg_loss=0.07474]\n",
      "Step 519031  [5.365 sec/step, loss=0.07305, avg_loss=0.07470]\n",
      "Step 519032  [5.374 sec/step, loss=0.07471, avg_loss=0.07470]\n",
      "Step 519033  [5.396 sec/step, loss=0.07358, avg_loss=0.07468]\n",
      "Step 519034  [5.400 sec/step, loss=0.07589, avg_loss=0.07471]\n",
      "Step 519035  [5.386 sec/step, loss=0.07178, avg_loss=0.07467]\n",
      "Step 519036  [5.375 sec/step, loss=0.07546, avg_loss=0.07466]\n",
      "Step 519037  [5.365 sec/step, loss=0.07481, avg_loss=0.07464]\n",
      "Step 519038  [5.356 sec/step, loss=0.07583, avg_loss=0.07463]\n",
      "Step 519039  [5.370 sec/step, loss=0.07647, avg_loss=0.07464]\n",
      "Step 519040  [5.350 sec/step, loss=0.06795, avg_loss=0.07457]\n",
      "Step 519041  [5.334 sec/step, loss=0.07239, avg_loss=0.07453]\n",
      "Step 519042  [5.345 sec/step, loss=0.07581, avg_loss=0.07453]\n",
      "Step 519043  [5.309 sec/step, loss=0.07440, avg_loss=0.07452]\n",
      "Step 519044  [5.338 sec/step, loss=0.07490, avg_loss=0.07453]\n",
      "Step 519045  [5.353 sec/step, loss=0.07497, avg_loss=0.07452]\n",
      "Step 519046  [5.347 sec/step, loss=0.07363, avg_loss=0.07450]\n",
      "Generated 32 batches of size 32 in 2.528 sec\n",
      "Step 519047  [5.336 sec/step, loss=0.07380, avg_loss=0.07447]\n",
      "Step 519048  [5.331 sec/step, loss=0.07637, avg_loss=0.07449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 519049  [5.337 sec/step, loss=0.07352, avg_loss=0.07448]\n",
      "Step 519050  [5.365 sec/step, loss=0.07657, avg_loss=0.07456]\n",
      "Step 519051  [5.365 sec/step, loss=0.07281, avg_loss=0.07453]\n",
      "Step 519052  [5.365 sec/step, loss=0.07541, avg_loss=0.07452]\n",
      "Step 519053  [5.356 sec/step, loss=0.07700, avg_loss=0.07452]\n",
      "Step 519054  [5.373 sec/step, loss=0.07482, avg_loss=0.07453]\n",
      "Step 519055  [5.425 sec/step, loss=0.07145, avg_loss=0.07453]\n",
      "Step 519056  [5.430 sec/step, loss=0.07508, avg_loss=0.07454]\n",
      "Step 519057  [5.418 sec/step, loss=0.06601, avg_loss=0.07445]\n",
      "Step 519058  [5.419 sec/step, loss=0.07566, avg_loss=0.07447]\n",
      "Step 519059  [5.444 sec/step, loss=0.07416, avg_loss=0.07445]\n",
      "Step 519060  [5.387 sec/step, loss=0.07520, avg_loss=0.07454]\n",
      "Step 519061  [5.426 sec/step, loss=0.06837, avg_loss=0.07445]\n",
      "Step 519062  [5.448 sec/step, loss=0.07669, avg_loss=0.07451]\n",
      "Step 519063  [5.417 sec/step, loss=0.07697, avg_loss=0.07457]\n",
      "Step 519064  [5.430 sec/step, loss=0.07691, avg_loss=0.07462]\n",
      "Step 519065  [5.419 sec/step, loss=0.07658, avg_loss=0.07464]\n",
      "Step 519066  [5.409 sec/step, loss=0.07602, avg_loss=0.07464]\n",
      "Step 519067  [5.399 sec/step, loss=0.07178, avg_loss=0.07463]\n",
      "Step 519068  [5.406 sec/step, loss=0.07473, avg_loss=0.07462]\n",
      "Step 519069  [5.409 sec/step, loss=0.07620, avg_loss=0.07461]\n",
      "Step 519070  [5.411 sec/step, loss=0.07579, avg_loss=0.07462]\n",
      "Step 519071  [5.411 sec/step, loss=0.07279, avg_loss=0.07459]\n",
      "Step 519072  [5.397 sec/step, loss=0.07349, avg_loss=0.07459]\n",
      "Step 519073  [5.387 sec/step, loss=0.07427, avg_loss=0.07458]\n",
      "Step 519074  [5.394 sec/step, loss=0.07368, avg_loss=0.07457]\n",
      "Step 519075  [5.390 sec/step, loss=0.07481, avg_loss=0.07459]\n",
      "Step 519076  [5.373 sec/step, loss=0.07656, avg_loss=0.07461]\n",
      "Step 519077  [5.403 sec/step, loss=0.07689, avg_loss=0.07467]\n",
      "Step 519078  [5.410 sec/step, loss=0.07244, avg_loss=0.07466]\n",
      "Generated 32 batches of size 32 in 2.571 sec\n",
      "Step 519079  [5.404 sec/step, loss=0.07619, avg_loss=0.07465]\n",
      "Step 519080  [5.409 sec/step, loss=0.07699, avg_loss=0.07470]\n",
      "Step 519081  [5.407 sec/step, loss=0.07503, avg_loss=0.07468]\n",
      "Step 519082  [5.415 sec/step, loss=0.07497, avg_loss=0.07467]\n",
      "Step 519083  [5.415 sec/step, loss=0.07256, avg_loss=0.07464]\n",
      "Step 519084  [5.417 sec/step, loss=0.07655, avg_loss=0.07466]\n",
      "Step 519085  [5.403 sec/step, loss=0.07387, avg_loss=0.07464]\n",
      "Step 519086  [5.420 sec/step, loss=0.07553, avg_loss=0.07475]\n",
      "Step 519087  [5.421 sec/step, loss=0.07720, avg_loss=0.07475]\n",
      "Step 519088  [5.434 sec/step, loss=0.07522, avg_loss=0.07475]\n",
      "Step 519089  [5.439 sec/step, loss=0.07707, avg_loss=0.07476]\n",
      "Step 519090  [5.438 sec/step, loss=0.07410, avg_loss=0.07477]\n",
      "Step 519091  [5.432 sec/step, loss=0.07362, avg_loss=0.07473]\n",
      "Step 519092  [5.491 sec/step, loss=0.07103, avg_loss=0.07468]\n",
      "Step 519093  [5.466 sec/step, loss=0.07476, avg_loss=0.07466]\n",
      "Step 519094  [5.459 sec/step, loss=0.07618, avg_loss=0.07466]\n",
      "Step 519095  [5.452 sec/step, loss=0.07428, avg_loss=0.07466]\n",
      "Step 519096  [5.442 sec/step, loss=0.07598, avg_loss=0.07466]\n",
      "Step 519097  [5.437 sec/step, loss=0.07554, avg_loss=0.07465]\n",
      "Step 519098  [5.406 sec/step, loss=0.06676, avg_loss=0.07458]\n",
      "Step 519099  [5.395 sec/step, loss=0.07649, avg_loss=0.07457]\n",
      "Step 519100  [5.412 sec/step, loss=0.07668, avg_loss=0.07461]\n",
      "Writing summary at step: 519100\n",
      "Step 519101  [5.407 sec/step, loss=0.07559, avg_loss=0.07460]\n",
      "Step 519102  [5.416 sec/step, loss=0.07359, avg_loss=0.07457]\n",
      "Step 519103  [5.416 sec/step, loss=0.07568, avg_loss=0.07458]\n",
      "Step 519104  [5.401 sec/step, loss=0.07147, avg_loss=0.07453]\n",
      "Step 519105  [5.407 sec/step, loss=0.07400, avg_loss=0.07455]\n",
      "Step 519106  [5.422 sec/step, loss=0.07470, avg_loss=0.07455]\n",
      "Step 519107  [5.426 sec/step, loss=0.07468, avg_loss=0.07453]\n",
      "Step 519108  [5.443 sec/step, loss=0.07636, avg_loss=0.07457]\n",
      "Step 519109  [5.418 sec/step, loss=0.07401, avg_loss=0.07454]\n",
      "Generated 32 batches of size 32 in 2.614 sec\n",
      "Step 519110  [5.426 sec/step, loss=0.07474, avg_loss=0.07455]\n",
      "Step 519111  [5.416 sec/step, loss=0.07433, avg_loss=0.07453]\n",
      "Step 519112  [5.428 sec/step, loss=0.07299, avg_loss=0.07451]\n",
      "Step 519113  [5.419 sec/step, loss=0.07140, avg_loss=0.07451]\n",
      "Step 519114  [5.403 sec/step, loss=0.07542, avg_loss=0.07452]\n",
      "Step 519115  [5.409 sec/step, loss=0.07737, avg_loss=0.07454]\n",
      "Step 519116  [5.436 sec/step, loss=0.07652, avg_loss=0.07464]\n",
      "Step 519117  [5.462 sec/step, loss=0.07685, avg_loss=0.07466]\n",
      "Step 519118  [5.468 sec/step, loss=0.07716, avg_loss=0.07469]\n",
      "Step 519119  [5.459 sec/step, loss=0.07401, avg_loss=0.07467]\n",
      "Step 519120  [5.449 sec/step, loss=0.07428, avg_loss=0.07464]\n",
      "Step 519121  [5.447 sec/step, loss=0.07650, avg_loss=0.07463]\n",
      "Step 519122  [5.469 sec/step, loss=0.07580, avg_loss=0.07465]\n",
      "Step 519123  [5.464 sec/step, loss=0.07495, avg_loss=0.07465]\n",
      "Step 519124  [5.453 sec/step, loss=0.07423, avg_loss=0.07462]\n",
      "Step 519125  [5.457 sec/step, loss=0.07683, avg_loss=0.07464]\n",
      "Step 519126  [5.439 sec/step, loss=0.07461, avg_loss=0.07462]\n",
      "Step 519127  [5.447 sec/step, loss=0.07325, avg_loss=0.07461]\n",
      "Step 519128  [5.439 sec/step, loss=0.07595, avg_loss=0.07461]\n",
      "Step 519129  [5.447 sec/step, loss=0.07507, avg_loss=0.07463]\n",
      "Step 519130  [5.493 sec/step, loss=0.06905, avg_loss=0.07456]\n",
      "Step 519131  [5.520 sec/step, loss=0.07668, avg_loss=0.07460]\n",
      "Step 519132  [5.505 sec/step, loss=0.07105, avg_loss=0.07456]\n",
      "Step 519133  [5.459 sec/step, loss=0.06590, avg_loss=0.07449]\n",
      "Step 519134  [5.459 sec/step, loss=0.07608, avg_loss=0.07449]\n",
      "Step 519135  [5.461 sec/step, loss=0.07579, avg_loss=0.07453]\n",
      "Step 519136  [5.464 sec/step, loss=0.07716, avg_loss=0.07455]\n",
      "Step 519137  [5.477 sec/step, loss=0.07402, avg_loss=0.07454]\n",
      "Step 519138  [5.480 sec/step, loss=0.07473, avg_loss=0.07453]\n",
      "Step 519139  [5.474 sec/step, loss=0.07332, avg_loss=0.07450]\n",
      "Step 519140  [5.494 sec/step, loss=0.07498, avg_loss=0.07457]\n",
      "Step 519141  [5.487 sec/step, loss=0.07254, avg_loss=0.07457]\n",
      "Generated 32 batches of size 32 in 2.533 sec\n",
      "Step 519142  [5.475 sec/step, loss=0.07374, avg_loss=0.07455]\n",
      "Step 519143  [5.495 sec/step, loss=0.07716, avg_loss=0.07457]\n",
      "Step 519144  [5.507 sec/step, loss=0.07316, avg_loss=0.07456]\n",
      "Step 519145  [5.503 sec/step, loss=0.07677, avg_loss=0.07457]\n",
      "Step 519146  [5.515 sec/step, loss=0.07664, avg_loss=0.07460]\n",
      "Step 519147  [5.533 sec/step, loss=0.07619, avg_loss=0.07463]\n",
      "Step 519148  [5.534 sec/step, loss=0.07551, avg_loss=0.07462]\n",
      "Step 519149  [5.534 sec/step, loss=0.07382, avg_loss=0.07462]\n",
      "Step 519150  [5.508 sec/step, loss=0.07222, avg_loss=0.07458]\n",
      "Step 519151  [5.514 sec/step, loss=0.07325, avg_loss=0.07458]\n",
      "Step 519152  [5.496 sec/step, loss=0.06579, avg_loss=0.07449]\n",
      "Step 519153  [5.499 sec/step, loss=0.07659, avg_loss=0.07448]\n",
      "Step 519154  [5.491 sec/step, loss=0.07551, avg_loss=0.07449]\n",
      "Step 519155  [5.445 sec/step, loss=0.07611, avg_loss=0.07454]\n",
      "Step 519156  [5.467 sec/step, loss=0.07368, avg_loss=0.07452]\n",
      "Step 519157  [5.483 sec/step, loss=0.07608, avg_loss=0.07462]\n",
      "Step 519158  [5.475 sec/step, loss=0.07540, avg_loss=0.07462]\n",
      "Step 519159  [5.461 sec/step, loss=0.07669, avg_loss=0.07465]\n",
      "Step 519160  [5.518 sec/step, loss=0.06793, avg_loss=0.07457]\n",
      "Step 519161  [5.458 sec/step, loss=0.07172, avg_loss=0.07461]\n",
      "Step 519162  [5.451 sec/step, loss=0.07290, avg_loss=0.07457]\n",
      "Step 519163  [5.449 sec/step, loss=0.07678, avg_loss=0.07457]\n",
      "Step 519164  [5.435 sec/step, loss=0.07402, avg_loss=0.07454]\n",
      "Step 519165  [5.440 sec/step, loss=0.07622, avg_loss=0.07453]\n",
      "Step 519166  [5.441 sec/step, loss=0.07459, avg_loss=0.07452]\n",
      "Step 519167  [5.469 sec/step, loss=0.07359, avg_loss=0.07454]\n",
      "Step 519168  [5.472 sec/step, loss=0.07438, avg_loss=0.07454]\n",
      "Step 519169  [5.469 sec/step, loss=0.07615, avg_loss=0.07453]\n",
      "Step 519170  [5.480 sec/step, loss=0.07642, avg_loss=0.07454]\n",
      "Step 519171  [5.502 sec/step, loss=0.07489, avg_loss=0.07456]\n",
      "Step 519172  [5.506 sec/step, loss=0.07493, avg_loss=0.07458]\n",
      "Step 519173  [5.516 sec/step, loss=0.07298, avg_loss=0.07456]\n",
      "Generated 32 batches of size 32 in 2.481 sec\n",
      "Step 519174  [5.536 sec/step, loss=0.07701, avg_loss=0.07460]\n",
      "Step 519175  [5.539 sec/step, loss=0.07378, avg_loss=0.07459]\n",
      "Step 519176  [5.530 sec/step, loss=0.07370, avg_loss=0.07456]\n",
      "Step 519177  [5.513 sec/step, loss=0.07500, avg_loss=0.07454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 519178  [5.507 sec/step, loss=0.07463, avg_loss=0.07456]\n",
      "Step 519179  [5.491 sec/step, loss=0.07159, avg_loss=0.07452]\n",
      "Step 519180  [5.480 sec/step, loss=0.07368, avg_loss=0.07448]\n",
      "Step 519181  [5.477 sec/step, loss=0.07460, avg_loss=0.07448]\n",
      "Step 519182  [5.471 sec/step, loss=0.07668, avg_loss=0.07449]\n",
      "Step 519183  [5.488 sec/step, loss=0.07416, avg_loss=0.07451]\n",
      "Step 519184  [5.488 sec/step, loss=0.07706, avg_loss=0.07452]\n",
      "Step 519185  [5.485 sec/step, loss=0.07499, avg_loss=0.07453]\n",
      "Step 519186  [5.487 sec/step, loss=0.07615, avg_loss=0.07453]\n",
      "Step 519187  [5.488 sec/step, loss=0.07511, avg_loss=0.07451]\n",
      "Step 519188  [5.471 sec/step, loss=0.07431, avg_loss=0.07450]\n",
      "Step 519189  [5.462 sec/step, loss=0.07593, avg_loss=0.07449]\n",
      "Step 519190  [5.446 sec/step, loss=0.07633, avg_loss=0.07451]\n",
      "Step 519191  [5.450 sec/step, loss=0.07459, avg_loss=0.07452]\n",
      "Step 519192  [5.403 sec/step, loss=0.07698, avg_loss=0.07458]\n",
      "Step 519193  [5.416 sec/step, loss=0.07577, avg_loss=0.07459]\n",
      "Step 519194  [5.422 sec/step, loss=0.07585, avg_loss=0.07459]\n",
      "Step 519195  [5.415 sec/step, loss=0.07344, avg_loss=0.07458]\n",
      "Step 519196  [5.414 sec/step, loss=0.07455, avg_loss=0.07457]\n",
      "Step 519197  [5.414 sec/step, loss=0.07493, avg_loss=0.07456]\n",
      "Step 519198  [5.440 sec/step, loss=0.07529, avg_loss=0.07465]\n",
      "Step 519199  [5.446 sec/step, loss=0.07554, avg_loss=0.07464]\n",
      "Step 519200  [5.431 sec/step, loss=0.07547, avg_loss=0.07463]\n",
      "Writing summary at step: 519200\n",
      "Step 519201  [5.411 sec/step, loss=0.06784, avg_loss=0.07455]\n",
      "Step 519202  [5.402 sec/step, loss=0.07480, avg_loss=0.07456]\n",
      "Step 519203  [5.394 sec/step, loss=0.07300, avg_loss=0.07453]\n",
      "Step 519204  [5.399 sec/step, loss=0.07047, avg_loss=0.07452]\n",
      "Generated 32 batches of size 32 in 2.410 sec\n",
      "Step 519205  [5.432 sec/step, loss=0.07540, avg_loss=0.07454]\n",
      "Step 519206  [5.418 sec/step, loss=0.07524, avg_loss=0.07454]\n",
      "Step 519207  [5.418 sec/step, loss=0.07188, avg_loss=0.07451]\n",
      "Step 519208  [5.420 sec/step, loss=0.07372, avg_loss=0.07449]\n",
      "Step 519209  [5.445 sec/step, loss=0.07633, avg_loss=0.07451]\n",
      "Step 519210  [5.442 sec/step, loss=0.07236, avg_loss=0.07449]\n",
      "Step 519211  [5.441 sec/step, loss=0.07606, avg_loss=0.07450]\n",
      "Step 519212  [5.426 sec/step, loss=0.07244, avg_loss=0.07450]\n",
      "Step 519213  [5.487 sec/step, loss=0.06752, avg_loss=0.07446]\n",
      "Step 519214  [5.488 sec/step, loss=0.07683, avg_loss=0.07447]\n",
      "Step 519215  [5.472 sec/step, loss=0.07383, avg_loss=0.07444]\n",
      "Step 519216  [5.478 sec/step, loss=0.07549, avg_loss=0.07443]\n",
      "Step 519217  [5.465 sec/step, loss=0.07268, avg_loss=0.07439]\n",
      "Step 519218  [5.466 sec/step, loss=0.07489, avg_loss=0.07436]\n",
      "Step 519219  [5.468 sec/step, loss=0.07540, avg_loss=0.07438]\n",
      "Step 519220  [5.471 sec/step, loss=0.07323, avg_loss=0.07437]\n",
      "Step 519221  [5.486 sec/step, loss=0.07285, avg_loss=0.07433]\n",
      "Step 519222  [5.520 sec/step, loss=0.06666, avg_loss=0.07424]\n",
      "Step 519223  [5.529 sec/step, loss=0.07663, avg_loss=0.07426]\n",
      "Step 519224  [5.539 sec/step, loss=0.07591, avg_loss=0.07427]\n",
      "Step 519225  [5.535 sec/step, loss=0.07196, avg_loss=0.07423]\n",
      "Step 519226  [5.559 sec/step, loss=0.07562, avg_loss=0.07424]\n",
      "Step 519227  [5.555 sec/step, loss=0.07391, avg_loss=0.07424]\n",
      "Step 519228  [5.536 sec/step, loss=0.07235, avg_loss=0.07421]\n",
      "Step 519229  [5.520 sec/step, loss=0.06555, avg_loss=0.07411]\n",
      "Step 519230  [5.486 sec/step, loss=0.07570, avg_loss=0.07418]\n",
      "Step 519231  [5.475 sec/step, loss=0.07438, avg_loss=0.07415]\n",
      "Step 519232  [5.489 sec/step, loss=0.07560, avg_loss=0.07420]\n",
      "Step 519233  [5.497 sec/step, loss=0.07493, avg_loss=0.07429]\n",
      "Step 519234  [5.509 sec/step, loss=0.07680, avg_loss=0.07430]\n",
      "Step 519235  [5.519 sec/step, loss=0.07663, avg_loss=0.07431]\n",
      "Step 519236  [5.512 sec/step, loss=0.07470, avg_loss=0.07428]\n",
      "Generated 32 batches of size 32 in 2.635 sec\n",
      "Step 519237  [5.507 sec/step, loss=0.07538, avg_loss=0.07429]\n",
      "Step 519238  [5.499 sec/step, loss=0.07608, avg_loss=0.07431]\n",
      "Step 519239  [5.484 sec/step, loss=0.07159, avg_loss=0.07429]\n",
      "Step 519240  [5.482 sec/step, loss=0.07611, avg_loss=0.07430]\n",
      "Step 519241  [5.480 sec/step, loss=0.07469, avg_loss=0.07432]\n",
      "Step 519242  [5.491 sec/step, loss=0.07713, avg_loss=0.07436]\n",
      "Step 519243  [5.487 sec/step, loss=0.07593, avg_loss=0.07435]\n",
      "Step 519244  [5.465 sec/step, loss=0.07645, avg_loss=0.07438]\n",
      "Step 519245  [5.454 sec/step, loss=0.07662, avg_loss=0.07438]\n",
      "Step 519246  [5.461 sec/step, loss=0.07683, avg_loss=0.07438]\n",
      "Step 519247  [5.445 sec/step, loss=0.07275, avg_loss=0.07434]\n",
      "Step 519248  [5.448 sec/step, loss=0.07505, avg_loss=0.07434]\n",
      "Step 519249  [5.450 sec/step, loss=0.07478, avg_loss=0.07435]\n",
      "Step 519250  [5.514 sec/step, loss=0.06738, avg_loss=0.07430]\n",
      "Step 519251  [5.519 sec/step, loss=0.07424, avg_loss=0.07431]\n",
      "Step 519252  [5.521 sec/step, loss=0.06663, avg_loss=0.07432]\n",
      "Step 519253  [5.542 sec/step, loss=0.07387, avg_loss=0.07429]\n",
      "Step 519254  [5.549 sec/step, loss=0.07479, avg_loss=0.07428]\n",
      "Step 519255  [5.538 sec/step, loss=0.07419, avg_loss=0.07427]\n",
      "Step 519256  [5.509 sec/step, loss=0.07393, avg_loss=0.07427]\n",
      "Step 519257  [5.498 sec/step, loss=0.07199, avg_loss=0.07423]\n",
      "Step 519258  [5.504 sec/step, loss=0.07620, avg_loss=0.07424]\n",
      "Step 519259  [5.490 sec/step, loss=0.07311, avg_loss=0.07420]\n",
      "Step 519260  [5.430 sec/step, loss=0.07494, avg_loss=0.07427]\n",
      "Step 519261  [5.440 sec/step, loss=0.07473, avg_loss=0.07430]\n",
      "Step 519262  [5.451 sec/step, loss=0.07709, avg_loss=0.07434]\n",
      "Step 519263  [5.447 sec/step, loss=0.07427, avg_loss=0.07432]\n",
      "Step 519264  [5.452 sec/step, loss=0.07471, avg_loss=0.07432]\n",
      "Step 519265  [5.448 sec/step, loss=0.07569, avg_loss=0.07432]\n",
      "Step 519266  [5.443 sec/step, loss=0.07430, avg_loss=0.07431]\n",
      "Step 519267  [5.424 sec/step, loss=0.07455, avg_loss=0.07432]\n",
      "Step 519268  [5.432 sec/step, loss=0.07412, avg_loss=0.07432]\n",
      "Generated 32 batches of size 32 in 2.442 sec\n",
      "Step 519269  [5.447 sec/step, loss=0.07424, avg_loss=0.07430]\n",
      "Step 519270  [5.443 sec/step, loss=0.07466, avg_loss=0.07429]\n",
      "Step 519271  [5.425 sec/step, loss=0.07360, avg_loss=0.07427]\n",
      "Step 519272  [5.426 sec/step, loss=0.07080, avg_loss=0.07423]\n",
      "Step 519273  [5.429 sec/step, loss=0.07640, avg_loss=0.07427]\n",
      "Step 519274  [5.418 sec/step, loss=0.07701, avg_loss=0.07427]\n",
      "Step 519275  [5.435 sec/step, loss=0.07688, avg_loss=0.07430]\n",
      "Step 519276  [5.446 sec/step, loss=0.07470, avg_loss=0.07431]\n",
      "Step 519277  [5.441 sec/step, loss=0.07628, avg_loss=0.07432]\n",
      "Step 519278  [5.468 sec/step, loss=0.07428, avg_loss=0.07432]\n",
      "Step 519279  [5.492 sec/step, loss=0.07666, avg_loss=0.07437]\n",
      "Step 519280  [5.490 sec/step, loss=0.07305, avg_loss=0.07436]\n",
      "Step 519281  [5.535 sec/step, loss=0.06638, avg_loss=0.07428]\n",
      "Step 519282  [5.542 sec/step, loss=0.07699, avg_loss=0.07428]\n",
      "Step 519283  [5.536 sec/step, loss=0.07665, avg_loss=0.07431]\n",
      "Step 519284  [5.520 sec/step, loss=0.07558, avg_loss=0.07429]\n",
      "Step 519285  [5.520 sec/step, loss=0.07432, avg_loss=0.07428]\n",
      "Step 519286  [5.517 sec/step, loss=0.07283, avg_loss=0.07425]\n",
      "Step 519287  [5.516 sec/step, loss=0.07476, avg_loss=0.07425]\n",
      "Step 519288  [5.524 sec/step, loss=0.07467, avg_loss=0.07425]\n",
      "Step 519289  [5.529 sec/step, loss=0.07661, avg_loss=0.07426]\n",
      "Step 519290  [5.528 sec/step, loss=0.07487, avg_loss=0.07424]\n",
      "Step 519291  [5.535 sec/step, loss=0.07671, avg_loss=0.07426]\n",
      "Step 519292  [5.531 sec/step, loss=0.07528, avg_loss=0.07425]\n",
      "Step 519293  [5.521 sec/step, loss=0.07363, avg_loss=0.07423]\n",
      "Step 519294  [5.524 sec/step, loss=0.07556, avg_loss=0.07422]\n",
      "Step 519295  [5.535 sec/step, loss=0.07470, avg_loss=0.07424]\n",
      "Step 519296  [5.541 sec/step, loss=0.07611, avg_loss=0.07425]\n",
      "Step 519297  [5.529 sec/step, loss=0.07106, avg_loss=0.07421]\n",
      "Step 519298  [5.523 sec/step, loss=0.07568, avg_loss=0.07422]\n",
      "Step 519299  [5.500 sec/step, loss=0.07176, avg_loss=0.07418]\n",
      "Step 519300  [5.531 sec/step, loss=0.07358, avg_loss=0.07416]\n",
      "Writing summary at step: 519300\n",
      "Generated 32 batches of size 32 in 2.448 sec\n",
      "Step 519301  [5.544 sec/step, loss=0.07447, avg_loss=0.07423]\n",
      "Step 519302  [5.540 sec/step, loss=0.07304, avg_loss=0.07421]\n",
      "Step 519303  [5.551 sec/step, loss=0.07667, avg_loss=0.07425]\n",
      "Step 519304  [5.562 sec/step, loss=0.07291, avg_loss=0.07427]\n",
      "Step 519305  [5.515 sec/step, loss=0.06563, avg_loss=0.07417]\n",
      "Step 519306  [5.520 sec/step, loss=0.07291, avg_loss=0.07415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 519307  [5.508 sec/step, loss=0.07436, avg_loss=0.07417]\n",
      "Step 519308  [5.504 sec/step, loss=0.07531, avg_loss=0.07419]\n",
      "Step 519309  [5.512 sec/step, loss=0.07399, avg_loss=0.07417]\n",
      "Step 519310  [5.524 sec/step, loss=0.07519, avg_loss=0.07419]\n",
      "Step 519311  [5.528 sec/step, loss=0.07177, avg_loss=0.07415]\n",
      "Step 519312  [5.548 sec/step, loss=0.07394, avg_loss=0.07417]\n",
      "Step 519313  [5.512 sec/step, loss=0.07594, avg_loss=0.07425]\n",
      "Step 519314  [5.551 sec/step, loss=0.06808, avg_loss=0.07416]\n",
      "Step 519315  [5.564 sec/step, loss=0.07523, avg_loss=0.07418]\n",
      "Step 519316  [5.534 sec/step, loss=0.07374, avg_loss=0.07416]\n",
      "Step 519317  [5.530 sec/step, loss=0.07438, avg_loss=0.07418]\n",
      "Step 519318  [5.530 sec/step, loss=0.07662, avg_loss=0.07419]\n",
      "Step 519319  [5.539 sec/step, loss=0.07691, avg_loss=0.07421]\n",
      "Step 519320  [5.544 sec/step, loss=0.07373, avg_loss=0.07421]\n",
      "Step 519321  [5.508 sec/step, loss=0.07170, avg_loss=0.07420]\n",
      "Step 519322  [5.462 sec/step, loss=0.07606, avg_loss=0.07430]\n",
      "Step 519323  [5.455 sec/step, loss=0.07566, avg_loss=0.07429]\n",
      "Step 519324  [5.448 sec/step, loss=0.07118, avg_loss=0.07424]\n",
      "Step 519325  [5.442 sec/step, loss=0.07473, avg_loss=0.07427]\n",
      "Step 519326  [5.423 sec/step, loss=0.07301, avg_loss=0.07424]\n",
      "Step 519327  [5.427 sec/step, loss=0.07557, avg_loss=0.07426]\n",
      "Step 519328  [5.433 sec/step, loss=0.07416, avg_loss=0.07428]\n",
      "Step 519329  [5.454 sec/step, loss=0.07662, avg_loss=0.07439]\n",
      "Step 519330  [5.435 sec/step, loss=0.07611, avg_loss=0.07439]\n",
      "Step 519331  [5.448 sec/step, loss=0.07453, avg_loss=0.07439]\n",
      "Generated 32 batches of size 32 in 2.439 sec\n",
      "Step 519332  [5.462 sec/step, loss=0.07676, avg_loss=0.07440]\n",
      "Step 519333  [5.475 sec/step, loss=0.07600, avg_loss=0.07441]\n",
      "Step 519334  [5.472 sec/step, loss=0.07650, avg_loss=0.07441]\n",
      "Step 519335  [5.444 sec/step, loss=0.06723, avg_loss=0.07432]\n",
      "Step 519336  [5.448 sec/step, loss=0.07628, avg_loss=0.07433]\n",
      "Step 519337  [5.450 sec/step, loss=0.07468, avg_loss=0.07433]\n",
      "Step 519338  [5.442 sec/step, loss=0.07328, avg_loss=0.07430]\n",
      "Step 519339  [5.449 sec/step, loss=0.07540, avg_loss=0.07434]\n",
      "Step 519340  [5.469 sec/step, loss=0.07347, avg_loss=0.07431]\n",
      "Step 519341  [5.458 sec/step, loss=0.06552, avg_loss=0.07422]\n",
      "Step 519342  [5.500 sec/step, loss=0.06575, avg_loss=0.07410]\n",
      "Step 519343  [5.508 sec/step, loss=0.07545, avg_loss=0.07410]\n",
      "Step 519344  [5.516 sec/step, loss=0.07661, avg_loss=0.07410]\n",
      "Step 519345  [5.510 sec/step, loss=0.07413, avg_loss=0.07408]\n",
      "Step 519346  [5.523 sec/step, loss=0.07321, avg_loss=0.07404]\n",
      "Step 519347  [5.526 sec/step, loss=0.07635, avg_loss=0.07408]\n",
      "Step 519348  [5.530 sec/step, loss=0.07633, avg_loss=0.07409]\n",
      "Step 519349  [5.528 sec/step, loss=0.07380, avg_loss=0.07408]\n",
      "Step 519350  [5.482 sec/step, loss=0.07539, avg_loss=0.07416]\n",
      "Step 519351  [5.470 sec/step, loss=0.07553, avg_loss=0.07417]\n",
      "Step 519352  [5.505 sec/step, loss=0.07611, avg_loss=0.07427]\n",
      "Step 519353  [5.478 sec/step, loss=0.07529, avg_loss=0.07428]\n",
      "Step 519354  [5.483 sec/step, loss=0.07591, avg_loss=0.07429]\n",
      "Step 519355  [5.479 sec/step, loss=0.07032, avg_loss=0.07425]\n",
      "Step 519356  [5.475 sec/step, loss=0.07471, avg_loss=0.07426]\n",
      "Step 519357  [5.496 sec/step, loss=0.07675, avg_loss=0.07431]\n",
      "Step 519358  [5.487 sec/step, loss=0.07341, avg_loss=0.07428]\n",
      "Step 519359  [5.479 sec/step, loss=0.07453, avg_loss=0.07430]\n",
      "Step 519360  [5.475 sec/step, loss=0.07052, avg_loss=0.07425]\n",
      "Step 519361  [5.480 sec/step, loss=0.07519, avg_loss=0.07426]\n",
      "Step 519362  [5.473 sec/step, loss=0.07391, avg_loss=0.07422]\n",
      "Step 519363  [5.465 sec/step, loss=0.07579, avg_loss=0.07424]\n",
      "Generated 32 batches of size 32 in 2.447 sec\n",
      "Step 519364  [5.468 sec/step, loss=0.07374, avg_loss=0.07423]\n",
      "Step 519365  [5.467 sec/step, loss=0.07561, avg_loss=0.07423]\n",
      "Step 519366  [5.480 sec/step, loss=0.07666, avg_loss=0.07425]\n",
      "Step 519367  [5.486 sec/step, loss=0.07429, avg_loss=0.07425]\n",
      "Step 519368  [5.469 sec/step, loss=0.07567, avg_loss=0.07427]\n",
      "Step 519369  [5.463 sec/step, loss=0.07532, avg_loss=0.07428]\n",
      "Step 519370  [5.451 sec/step, loss=0.07253, avg_loss=0.07425]\n",
      "Step 519371  [5.470 sec/step, loss=0.07597, avg_loss=0.07428]\n",
      "Step 519372  [5.483 sec/step, loss=0.07546, avg_loss=0.07432]\n",
      "Step 519373  [5.463 sec/step, loss=0.07431, avg_loss=0.07430]\n",
      "Step 519374  [5.463 sec/step, loss=0.07501, avg_loss=0.07428]\n",
      "Step 519375  [5.456 sec/step, loss=0.07578, avg_loss=0.07427]\n",
      "Step 519376  [5.453 sec/step, loss=0.07513, avg_loss=0.07428]\n",
      "Step 519377  [5.449 sec/step, loss=0.07321, avg_loss=0.07425]\n",
      "Step 519378  [5.429 sec/step, loss=0.07508, avg_loss=0.07425]\n",
      "Step 519379  [5.446 sec/step, loss=0.07351, avg_loss=0.07422]\n",
      "Step 519380  [5.449 sec/step, loss=0.07584, avg_loss=0.07425]\n",
      "Step 519381  [5.407 sec/step, loss=0.07658, avg_loss=0.07435]\n",
      "Step 519382  [5.385 sec/step, loss=0.07441, avg_loss=0.07433]\n",
      "Step 519383  [5.373 sec/step, loss=0.07419, avg_loss=0.07430]\n",
      "Step 519384  [5.382 sec/step, loss=0.07459, avg_loss=0.07429]\n",
      "Step 519385  [5.400 sec/step, loss=0.07636, avg_loss=0.07431]\n",
      "Step 519386  [5.419 sec/step, loss=0.07520, avg_loss=0.07434]\n",
      "Step 519387  [5.422 sec/step, loss=0.07625, avg_loss=0.07435]\n",
      "Step 519388  [5.424 sec/step, loss=0.07568, avg_loss=0.07436]\n",
      "Step 519389  [5.417 sec/step, loss=0.07530, avg_loss=0.07435]\n",
      "Step 519390  [5.404 sec/step, loss=0.07375, avg_loss=0.07434]\n",
      "Step 519391  [5.389 sec/step, loss=0.07124, avg_loss=0.07428]\n",
      "Step 519392  [5.370 sec/step, loss=0.07212, avg_loss=0.07425]\n",
      "Step 519393  [5.383 sec/step, loss=0.07582, avg_loss=0.07427]\n",
      "Step 519394  [5.387 sec/step, loss=0.07725, avg_loss=0.07429]\n",
      "Step 519395  [5.389 sec/step, loss=0.07399, avg_loss=0.07428]\n",
      "Generated 32 batches of size 32 in 2.425 sec\n",
      "Step 519396  [5.405 sec/step, loss=0.07631, avg_loss=0.07429]\n",
      "Step 519397  [5.415 sec/step, loss=0.07555, avg_loss=0.07433]\n",
      "Step 519398  [5.397 sec/step, loss=0.06662, avg_loss=0.07424]\n",
      "Step 519399  [5.416 sec/step, loss=0.07482, avg_loss=0.07427]\n",
      "Step 519400  [5.401 sec/step, loss=0.07467, avg_loss=0.07428]\n",
      "Writing summary at step: 519400\n",
      "Step 519401  [5.414 sec/step, loss=0.07556, avg_loss=0.07429]\n",
      "Step 519402  [5.459 sec/step, loss=0.06730, avg_loss=0.07423]\n",
      "Step 519403  [5.440 sec/step, loss=0.07222, avg_loss=0.07419]\n",
      "Step 519404  [5.451 sec/step, loss=0.07581, avg_loss=0.07422]\n",
      "Step 519405  [5.478 sec/step, loss=0.07640, avg_loss=0.07433]\n",
      "Step 519406  [5.476 sec/step, loss=0.07610, avg_loss=0.07436]\n",
      "Step 519407  [5.515 sec/step, loss=0.07338, avg_loss=0.07435]\n",
      "Step 519408  [5.505 sec/step, loss=0.07469, avg_loss=0.07434]\n",
      "Step 519409  [5.468 sec/step, loss=0.07168, avg_loss=0.07432]\n",
      "Step 519410  [5.452 sec/step, loss=0.07480, avg_loss=0.07432]\n",
      "Step 519411  [5.433 sec/step, loss=0.06679, avg_loss=0.07427]\n",
      "Step 519412  [5.428 sec/step, loss=0.07527, avg_loss=0.07428]\n",
      "Step 519413  [5.407 sec/step, loss=0.07421, avg_loss=0.07426]\n",
      "Step 519414  [5.352 sec/step, loss=0.07435, avg_loss=0.07432]\n",
      "Step 519415  [5.356 sec/step, loss=0.07686, avg_loss=0.07434]\n",
      "Step 519416  [5.376 sec/step, loss=0.07578, avg_loss=0.07436]\n",
      "Step 519417  [5.388 sec/step, loss=0.07489, avg_loss=0.07437]\n",
      "Step 519418  [5.383 sec/step, loss=0.07289, avg_loss=0.07433]\n",
      "Step 519419  [5.386 sec/step, loss=0.07692, avg_loss=0.07433]\n",
      "Step 519420  [5.384 sec/step, loss=0.07566, avg_loss=0.07435]\n",
      "Step 519421  [5.392 sec/step, loss=0.07510, avg_loss=0.07438]\n",
      "Step 519422  [5.386 sec/step, loss=0.07256, avg_loss=0.07435]\n",
      "Step 519423  [5.395 sec/step, loss=0.07423, avg_loss=0.07433]\n",
      "Step 519424  [5.396 sec/step, loss=0.07475, avg_loss=0.07437]\n",
      "Step 519425  [5.406 sec/step, loss=0.07609, avg_loss=0.07438]\n",
      "Step 519426  [5.419 sec/step, loss=0.07689, avg_loss=0.07442]\n",
      "Generated 32 batches of size 32 in 2.619 sec\n",
      "Step 519427  [5.421 sec/step, loss=0.07581, avg_loss=0.07442]\n",
      "Step 519428  [5.429 sec/step, loss=0.07330, avg_loss=0.07441]\n",
      "Step 519429  [5.424 sec/step, loss=0.07350, avg_loss=0.07438]\n",
      "Step 519430  [5.438 sec/step, loss=0.07695, avg_loss=0.07439]\n",
      "Step 519431  [5.437 sec/step, loss=0.07532, avg_loss=0.07440]\n",
      "Step 519432  [5.471 sec/step, loss=0.06734, avg_loss=0.07431]\n",
      "Step 519433  [5.454 sec/step, loss=0.07308, avg_loss=0.07428]\n",
      "Step 519434  [5.469 sec/step, loss=0.07395, avg_loss=0.07425]\n",
      "Step 519435  [5.490 sec/step, loss=0.07562, avg_loss=0.07433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 519436  [5.487 sec/step, loss=0.07616, avg_loss=0.07433]\n",
      "Step 519437  [5.485 sec/step, loss=0.07592, avg_loss=0.07435]\n",
      "Step 519438  [5.491 sec/step, loss=0.07281, avg_loss=0.07434]\n",
      "Step 519439  [5.484 sec/step, loss=0.07096, avg_loss=0.07430]\n",
      "Step 519440  [5.451 sec/step, loss=0.07469, avg_loss=0.07431]\n",
      "Step 519441  [5.471 sec/step, loss=0.07205, avg_loss=0.07437]\n",
      "Step 519442  [5.425 sec/step, loss=0.07521, avg_loss=0.07447]\n",
      "Step 519443  [5.422 sec/step, loss=0.07326, avg_loss=0.07445]\n",
      "Step 519444  [5.423 sec/step, loss=0.07661, avg_loss=0.07445]\n",
      "Step 519445  [5.444 sec/step, loss=0.07592, avg_loss=0.07446]\n",
      "Step 519446  [5.421 sec/step, loss=0.07505, avg_loss=0.07448]\n",
      "Step 519447  [5.419 sec/step, loss=0.07533, avg_loss=0.07447]\n",
      "Step 519448  [5.417 sec/step, loss=0.07522, avg_loss=0.07446]\n",
      "Step 519449  [5.473 sec/step, loss=0.06717, avg_loss=0.07440]\n",
      "Step 519450  [5.454 sec/step, loss=0.07232, avg_loss=0.07437]\n",
      "Step 519451  [5.456 sec/step, loss=0.07346, avg_loss=0.07434]\n",
      "Step 519452  [5.433 sec/step, loss=0.07583, avg_loss=0.07434]\n",
      "Step 519453  [5.435 sec/step, loss=0.07604, avg_loss=0.07435]\n",
      "Step 519454  [5.425 sec/step, loss=0.07552, avg_loss=0.07435]\n",
      "Step 519455  [5.461 sec/step, loss=0.07507, avg_loss=0.07439]\n",
      "Step 519456  [5.480 sec/step, loss=0.07646, avg_loss=0.07441]\n",
      "Step 519457  [5.471 sec/step, loss=0.07261, avg_loss=0.07437]\n",
      "Step 519458  [5.462 sec/step, loss=0.06681, avg_loss=0.07430]\n",
      "Generated 32 batches of size 32 in 2.463 sec\n",
      "Step 519459  [5.486 sec/step, loss=0.07694, avg_loss=0.07433]\n",
      "Step 519460  [5.513 sec/step, loss=0.07402, avg_loss=0.07436]\n",
      "Step 519461  [5.506 sec/step, loss=0.07451, avg_loss=0.07436]\n",
      "Step 519462  [5.511 sec/step, loss=0.07629, avg_loss=0.07438]\n",
      "Step 519463  [5.530 sec/step, loss=0.07605, avg_loss=0.07438]\n",
      "Step 519464  [5.517 sec/step, loss=0.07417, avg_loss=0.07439]\n",
      "Step 519465  [5.523 sec/step, loss=0.07573, avg_loss=0.07439]\n",
      "Step 519466  [5.507 sec/step, loss=0.07396, avg_loss=0.07436]\n",
      "Step 519467  [5.500 sec/step, loss=0.07571, avg_loss=0.07437]\n",
      "Step 519468  [5.480 sec/step, loss=0.06636, avg_loss=0.07428]\n",
      "Step 519469  [5.470 sec/step, loss=0.07523, avg_loss=0.07428]\n",
      "Step 519470  [5.465 sec/step, loss=0.07448, avg_loss=0.07430]\n",
      "Step 519471  [5.454 sec/step, loss=0.07458, avg_loss=0.07429]\n",
      "Step 519472  [5.465 sec/step, loss=0.07705, avg_loss=0.07430]\n",
      "Step 519473  [5.474 sec/step, loss=0.07561, avg_loss=0.07431]\n",
      "Step 519474  [5.468 sec/step, loss=0.07684, avg_loss=0.07433]\n",
      "Step 519475  [5.471 sec/step, loss=0.07602, avg_loss=0.07434]\n",
      "Step 519476  [5.471 sec/step, loss=0.07520, avg_loss=0.07434]\n",
      "Step 519477  [5.487 sec/step, loss=0.07624, avg_loss=0.07437]\n",
      "Step 519478  [5.499 sec/step, loss=0.07710, avg_loss=0.07439]\n",
      "Step 519479  [5.491 sec/step, loss=0.07402, avg_loss=0.07439]\n",
      "Step 519480  [5.490 sec/step, loss=0.07345, avg_loss=0.07437]\n",
      "Step 519481  [5.488 sec/step, loss=0.07673, avg_loss=0.07437]\n",
      "Step 519482  [5.482 sec/step, loss=0.07243, avg_loss=0.07435]\n",
      "Step 519483  [5.491 sec/step, loss=0.07519, avg_loss=0.07436]\n",
      "Step 519484  [5.494 sec/step, loss=0.07539, avg_loss=0.07437]\n",
      "Step 519485  [5.473 sec/step, loss=0.07479, avg_loss=0.07435]\n",
      "Step 519486  [5.469 sec/step, loss=0.07623, avg_loss=0.07436]\n",
      "Step 519487  [5.448 sec/step, loss=0.07427, avg_loss=0.07434]\n",
      "Step 519488  [5.446 sec/step, loss=0.07481, avg_loss=0.07433]\n",
      "Step 519489  [5.470 sec/step, loss=0.07404, avg_loss=0.07432]\n",
      "Step 519490  [5.469 sec/step, loss=0.07576, avg_loss=0.07434]\n",
      "Generated 32 batches of size 32 in 2.411 sec\n",
      "Step 519491  [5.481 sec/step, loss=0.07536, avg_loss=0.07438]\n",
      "Step 519492  [5.497 sec/step, loss=0.07590, avg_loss=0.07442]\n",
      "Step 519493  [5.503 sec/step, loss=0.07397, avg_loss=0.07440]\n",
      "Step 519494  [5.546 sec/step, loss=0.06654, avg_loss=0.07429]\n",
      "Step 519495  [5.550 sec/step, loss=0.07519, avg_loss=0.07431]\n",
      "Step 519496  [5.521 sec/step, loss=0.07203, avg_loss=0.07426]\n",
      "Step 519497  [5.516 sec/step, loss=0.07421, avg_loss=0.07425]\n",
      "Step 519498  [5.544 sec/step, loss=0.07649, avg_loss=0.07435]\n",
      "Step 519499  [5.537 sec/step, loss=0.07162, avg_loss=0.07432]\n",
      "Step 519500  [5.526 sec/step, loss=0.07657, avg_loss=0.07434]\n",
      "Writing summary at step: 519500\n",
      "Step 519501  [5.526 sec/step, loss=0.07685, avg_loss=0.07435]\n",
      "Step 519502  [5.493 sec/step, loss=0.07343, avg_loss=0.07441]\n",
      "Step 519503  [5.554 sec/step, loss=0.06642, avg_loss=0.07435]\n",
      "Step 519504  [5.569 sec/step, loss=0.07412, avg_loss=0.07434]\n",
      "Step 519505  [5.565 sec/step, loss=0.07672, avg_loss=0.07434]\n",
      "Step 519506  [5.570 sec/step, loss=0.07552, avg_loss=0.07433]\n",
      "Step 519507  [5.535 sec/step, loss=0.07446, avg_loss=0.07434]\n",
      "Step 519508  [5.541 sec/step, loss=0.07373, avg_loss=0.07433]\n",
      "Step 519509  [5.562 sec/step, loss=0.07444, avg_loss=0.07436]\n",
      "Step 519510  [5.569 sec/step, loss=0.07552, avg_loss=0.07437]\n",
      "Step 519511  [5.584 sec/step, loss=0.07291, avg_loss=0.07443]\n",
      "Step 519512  [5.591 sec/step, loss=0.07418, avg_loss=0.07442]\n",
      "Step 519513  [5.600 sec/step, loss=0.07222, avg_loss=0.07440]\n",
      "Step 519514  [5.608 sec/step, loss=0.07256, avg_loss=0.07438]\n",
      "Step 519515  [5.602 sec/step, loss=0.07542, avg_loss=0.07437]\n",
      "Step 519516  [5.613 sec/step, loss=0.07624, avg_loss=0.07437]\n",
      "Step 519517  [5.609 sec/step, loss=0.07543, avg_loss=0.07438]\n",
      "Step 519518  [5.602 sec/step, loss=0.07587, avg_loss=0.07441]\n",
      "Step 519519  [5.570 sec/step, loss=0.06518, avg_loss=0.07429]\n",
      "Step 519520  [5.577 sec/step, loss=0.07543, avg_loss=0.07429]\n",
      "Step 519521  [5.578 sec/step, loss=0.07410, avg_loss=0.07428]\n",
      "Generated 32 batches of size 32 in 2.458 sec\n",
      "Step 519522  [5.590 sec/step, loss=0.07554, avg_loss=0.07431]\n",
      "Step 519523  [5.574 sec/step, loss=0.07383, avg_loss=0.07430]\n",
      "Step 519524  [5.572 sec/step, loss=0.07579, avg_loss=0.07431]\n",
      "Step 519525  [5.582 sec/step, loss=0.07629, avg_loss=0.07432]\n",
      "Step 519526  [5.563 sec/step, loss=0.07208, avg_loss=0.07427]\n",
      "Step 519527  [5.569 sec/step, loss=0.07698, avg_loss=0.07428]\n",
      "Step 519528  [5.559 sec/step, loss=0.07458, avg_loss=0.07429]\n",
      "Step 519529  [5.548 sec/step, loss=0.07128, avg_loss=0.07427]\n",
      "Step 519530  [5.528 sec/step, loss=0.07447, avg_loss=0.07424]\n",
      "Step 519531  [5.519 sec/step, loss=0.07383, avg_loss=0.07423]\n",
      "Step 519532  [5.483 sec/step, loss=0.07706, avg_loss=0.07433]\n",
      "Step 519533  [5.515 sec/step, loss=0.07520, avg_loss=0.07435]\n",
      "Step 519534  [5.483 sec/step, loss=0.07171, avg_loss=0.07433]\n",
      "Step 519535  [5.489 sec/step, loss=0.07451, avg_loss=0.07431]\n",
      "Step 519536  [5.495 sec/step, loss=0.07665, avg_loss=0.07432]\n",
      "Step 519537  [5.495 sec/step, loss=0.07654, avg_loss=0.07433]\n",
      "Step 519538  [5.502 sec/step, loss=0.07584, avg_loss=0.07436]\n",
      "Step 519539  [5.511 sec/step, loss=0.07532, avg_loss=0.07440]\n",
      "Step 519540  [5.514 sec/step, loss=0.07356, avg_loss=0.07439]\n",
      "Step 519541  [5.496 sec/step, loss=0.06622, avg_loss=0.07433]\n",
      "Step 519542  [5.492 sec/step, loss=0.07451, avg_loss=0.07432]\n",
      "Step 519543  [5.479 sec/step, loss=0.07070, avg_loss=0.07430]\n",
      "Step 519544  [5.468 sec/step, loss=0.07497, avg_loss=0.07428]\n",
      "Step 519545  [5.437 sec/step, loss=0.07266, avg_loss=0.07425]\n",
      "Step 519546  [5.474 sec/step, loss=0.07019, avg_loss=0.07420]\n",
      "Step 519547  [5.471 sec/step, loss=0.07237, avg_loss=0.07417]\n",
      "Step 519548  [5.474 sec/step, loss=0.07549, avg_loss=0.07417]\n",
      "Step 519549  [5.436 sec/step, loss=0.07665, avg_loss=0.07427]\n",
      "Step 519550  [5.447 sec/step, loss=0.07455, avg_loss=0.07429]\n",
      "Step 519551  [5.460 sec/step, loss=0.07442, avg_loss=0.07430]\n",
      "Step 519552  [5.452 sec/step, loss=0.07485, avg_loss=0.07429]\n",
      "Step 519553  [5.450 sec/step, loss=0.07553, avg_loss=0.07428]\n",
      "Generated 32 batches of size 32 in 2.434 sec\n",
      "Step 519554  [5.463 sec/step, loss=0.07693, avg_loss=0.07430]\n",
      "Step 519555  [5.446 sec/step, loss=0.07605, avg_loss=0.07431]\n",
      "Step 519556  [5.436 sec/step, loss=0.07491, avg_loss=0.07429]\n",
      "Step 519557  [5.438 sec/step, loss=0.07546, avg_loss=0.07432]\n",
      "Step 519558  [5.447 sec/step, loss=0.07484, avg_loss=0.07440]\n",
      "Step 519559  [5.438 sec/step, loss=0.07563, avg_loss=0.07439]\n",
      "Step 519560  [5.431 sec/step, loss=0.07685, avg_loss=0.07442]\n",
      "Step 519561  [5.438 sec/step, loss=0.07512, avg_loss=0.07442]\n",
      "Step 519562  [5.456 sec/step, loss=0.07349, avg_loss=0.07440]\n",
      "Step 519563  [5.428 sec/step, loss=0.07211, avg_loss=0.07436]\n",
      "Step 519564  [5.441 sec/step, loss=0.07254, avg_loss=0.07434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 519565  [5.444 sec/step, loss=0.07370, avg_loss=0.07432]\n",
      "Step 519566  [5.466 sec/step, loss=0.07582, avg_loss=0.07434]\n",
      "Step 519567  [5.472 sec/step, loss=0.07539, avg_loss=0.07434]\n",
      "Step 519568  [5.516 sec/step, loss=0.07349, avg_loss=0.07441]\n",
      "Step 519569  [5.517 sec/step, loss=0.07630, avg_loss=0.07442]\n",
      "Step 519570  [5.528 sec/step, loss=0.07543, avg_loss=0.07443]\n",
      "Step 519571  [5.530 sec/step, loss=0.07556, avg_loss=0.07444]\n",
      "Step 519572  [5.500 sec/step, loss=0.06660, avg_loss=0.07433]\n",
      "Step 519573  [5.498 sec/step, loss=0.07333, avg_loss=0.07431]\n",
      "Step 519574  [5.504 sec/step, loss=0.07516, avg_loss=0.07429]\n",
      "Step 519575  [5.503 sec/step, loss=0.07652, avg_loss=0.07430]\n",
      "Step 519576  [5.504 sec/step, loss=0.07247, avg_loss=0.07427]\n",
      "Step 519577  [5.513 sec/step, loss=0.07600, avg_loss=0.07427]\n",
      "Step 519578  [5.512 sec/step, loss=0.07683, avg_loss=0.07426]\n",
      "Step 519579  [5.501 sec/step, loss=0.07603, avg_loss=0.07429]\n",
      "Step 519580  [5.501 sec/step, loss=0.07461, avg_loss=0.07430]\n",
      "Step 519581  [5.484 sec/step, loss=0.07105, avg_loss=0.07424]\n",
      "Step 519582  [5.491 sec/step, loss=0.07452, avg_loss=0.07426]\n",
      "Step 519583  [5.485 sec/step, loss=0.07426, avg_loss=0.07425]\n",
      "Step 519584  [5.526 sec/step, loss=0.06680, avg_loss=0.07417]\n",
      "Step 519585  [5.535 sec/step, loss=0.07518, avg_loss=0.07417]\n",
      "Generated 32 batches of size 32 in 2.423 sec\n",
      "Step 519586  [5.536 sec/step, loss=0.07640, avg_loss=0.07417]\n",
      "Step 519587  [5.548 sec/step, loss=0.07616, avg_loss=0.07419]\n",
      "Step 519588  [5.545 sec/step, loss=0.07572, avg_loss=0.07420]\n",
      "Step 519589  [5.524 sec/step, loss=0.07559, avg_loss=0.07421]\n",
      "Step 519590  [5.528 sec/step, loss=0.07219, avg_loss=0.07418]\n",
      "Step 519591  [5.521 sec/step, loss=0.07472, avg_loss=0.07417]\n",
      "Step 519592  [5.505 sec/step, loss=0.07246, avg_loss=0.07414]\n",
      "Step 519593  [5.507 sec/step, loss=0.07626, avg_loss=0.07416]\n",
      "Step 519594  [5.449 sec/step, loss=0.07393, avg_loss=0.07423]\n",
      "Step 519595  [5.462 sec/step, loss=0.07609, avg_loss=0.07424]\n",
      "Step 519596  [5.466 sec/step, loss=0.07314, avg_loss=0.07426]\n",
      "Step 519597  [5.484 sec/step, loss=0.07622, avg_loss=0.07428]\n",
      "Step 519598  [5.481 sec/step, loss=0.07661, avg_loss=0.07428]\n",
      "Step 519599  [5.469 sec/step, loss=0.07129, avg_loss=0.07427]\n",
      "Step 519600  [5.464 sec/step, loss=0.07403, avg_loss=0.07425]\n",
      "Writing summary at step: 519600\n",
      "Step 519601  [5.452 sec/step, loss=0.07477, avg_loss=0.07423]\n",
      "Step 519602  [5.425 sec/step, loss=0.07501, avg_loss=0.07424]\n",
      "Step 519603  [5.364 sec/step, loss=0.07468, avg_loss=0.07433]\n",
      "Step 519604  [5.345 sec/step, loss=0.07646, avg_loss=0.07435]\n",
      "Step 519605  [5.330 sec/step, loss=0.07037, avg_loss=0.07429]\n",
      "Step 519606  [5.326 sec/step, loss=0.07477, avg_loss=0.07428]\n",
      "Step 519607  [5.329 sec/step, loss=0.07221, avg_loss=0.07426]\n",
      "Step 519608  [5.332 sec/step, loss=0.07497, avg_loss=0.07427]\n",
      "Step 519609  [5.346 sec/step, loss=0.07497, avg_loss=0.07427]\n",
      "Step 519610  [5.350 sec/step, loss=0.07531, avg_loss=0.07427]\n",
      "Step 519611  [5.368 sec/step, loss=0.07599, avg_loss=0.07430]\n",
      "Step 519612  [5.339 sec/step, loss=0.06564, avg_loss=0.07422]\n",
      "Step 519613  [5.341 sec/step, loss=0.07451, avg_loss=0.07424]\n",
      "Step 519614  [5.388 sec/step, loss=0.06700, avg_loss=0.07418]\n",
      "Step 519615  [5.385 sec/step, loss=0.07274, avg_loss=0.07416]\n",
      "Step 519616  [5.380 sec/step, loss=0.07639, avg_loss=0.07416]\n",
      "Generated 32 batches of size 32 in 2.554 sec\n",
      "Step 519617  [5.374 sec/step, loss=0.07447, avg_loss=0.07415]\n",
      "Step 519618  [5.380 sec/step, loss=0.07626, avg_loss=0.07415]\n",
      "Step 519619  [5.397 sec/step, loss=0.07522, avg_loss=0.07425]\n",
      "Step 519620  [5.405 sec/step, loss=0.07656, avg_loss=0.07426]\n",
      "Step 519621  [5.414 sec/step, loss=0.07651, avg_loss=0.07429]\n",
      "Step 519622  [5.405 sec/step, loss=0.07540, avg_loss=0.07429]\n",
      "Step 519623  [5.406 sec/step, loss=0.07584, avg_loss=0.07431]\n",
      "Step 519624  [5.411 sec/step, loss=0.07517, avg_loss=0.07430]\n",
      "Step 519625  [5.410 sec/step, loss=0.07516, avg_loss=0.07429]\n",
      "Step 519626  [5.423 sec/step, loss=0.07497, avg_loss=0.07432]\n",
      "Step 519627  [5.425 sec/step, loss=0.07388, avg_loss=0.07429]\n",
      "Step 519628  [5.438 sec/step, loss=0.07575, avg_loss=0.07430]\n",
      "Step 519629  [5.452 sec/step, loss=0.07440, avg_loss=0.07433]\n",
      "Step 519630  [5.443 sec/step, loss=0.06739, avg_loss=0.07426]\n",
      "Step 519631  [5.440 sec/step, loss=0.07441, avg_loss=0.07427]\n",
      "Step 519632  [5.415 sec/step, loss=0.07471, avg_loss=0.07424]\n",
      "Step 519633  [5.383 sec/step, loss=0.07170, avg_loss=0.07421]\n",
      "Step 519634  [5.399 sec/step, loss=0.07526, avg_loss=0.07424]\n",
      "Step 519635  [5.392 sec/step, loss=0.07482, avg_loss=0.07425]\n",
      "Step 519636  [5.392 sec/step, loss=0.07647, avg_loss=0.07424]\n",
      "Step 519637  [5.405 sec/step, loss=0.07640, avg_loss=0.07424]\n",
      "Step 519638  [5.401 sec/step, loss=0.07506, avg_loss=0.07423]\n",
      "Step 519639  [5.416 sec/step, loss=0.07680, avg_loss=0.07425]\n",
      "Step 519640  [5.432 sec/step, loss=0.07680, avg_loss=0.07428]\n",
      "Step 519641  [5.460 sec/step, loss=0.07656, avg_loss=0.07438]\n",
      "Step 519642  [5.455 sec/step, loss=0.07401, avg_loss=0.07438]\n",
      "Step 519643  [5.454 sec/step, loss=0.07386, avg_loss=0.07441]\n",
      "Step 519644  [5.470 sec/step, loss=0.07355, avg_loss=0.07440]\n",
      "Step 519645  [5.486 sec/step, loss=0.07623, avg_loss=0.07443]\n",
      "Step 519646  [5.437 sec/step, loss=0.07424, avg_loss=0.07447]\n",
      "Step 519647  [5.448 sec/step, loss=0.07503, avg_loss=0.07450]\n",
      "Step 519648  [5.447 sec/step, loss=0.07646, avg_loss=0.07451]\n",
      "Generated 32 batches of size 32 in 2.574 sec\n",
      "Step 519649  [5.437 sec/step, loss=0.07587, avg_loss=0.07450]\n",
      "Step 519650  [5.467 sec/step, loss=0.07377, avg_loss=0.07449]\n",
      "Step 519651  [5.452 sec/step, loss=0.07551, avg_loss=0.07450]\n",
      "Step 519652  [5.513 sec/step, loss=0.06761, avg_loss=0.07443]\n",
      "Step 519653  [5.520 sec/step, loss=0.07537, avg_loss=0.07443]\n",
      "Step 519654  [5.491 sec/step, loss=0.07200, avg_loss=0.07438]\n",
      "Step 519655  [5.489 sec/step, loss=0.07532, avg_loss=0.07437]\n",
      "Step 519656  [5.487 sec/step, loss=0.07202, avg_loss=0.07435]\n",
      "Step 519657  [5.479 sec/step, loss=0.07394, avg_loss=0.07433]\n",
      "Step 519658  [5.488 sec/step, loss=0.07277, avg_loss=0.07431]\n",
      "Step 519659  [5.473 sec/step, loss=0.07498, avg_loss=0.07430]\n",
      "Step 519660  [5.494 sec/step, loss=0.07326, avg_loss=0.07427]\n",
      "Step 519661  [5.500 sec/step, loss=0.07662, avg_loss=0.07428]\n",
      "Step 519662  [5.471 sec/step, loss=0.07555, avg_loss=0.07430]\n",
      "Step 519663  [5.475 sec/step, loss=0.07405, avg_loss=0.07432]\n",
      "Step 519664  [5.465 sec/step, loss=0.07293, avg_loss=0.07433]\n",
      "Step 519665  [5.458 sec/step, loss=0.07251, avg_loss=0.07431]\n",
      "Step 519666  [5.455 sec/step, loss=0.07673, avg_loss=0.07432]\n",
      "Step 519667  [5.447 sec/step, loss=0.07393, avg_loss=0.07431]\n",
      "Step 519668  [5.408 sec/step, loss=0.07077, avg_loss=0.07428]\n",
      "Step 519669  [5.410 sec/step, loss=0.07582, avg_loss=0.07428]\n",
      "Step 519670  [5.419 sec/step, loss=0.07526, avg_loss=0.07427]\n",
      "Step 519671  [5.417 sec/step, loss=0.07405, avg_loss=0.07426]\n",
      "Step 519672  [5.449 sec/step, loss=0.07652, avg_loss=0.07436]\n",
      "Step 519673  [5.464 sec/step, loss=0.07520, avg_loss=0.07438]\n",
      "Step 519674  [5.439 sec/step, loss=0.06726, avg_loss=0.07430]\n",
      "Step 519675  [5.451 sec/step, loss=0.07578, avg_loss=0.07429]\n",
      "Step 519676  [5.451 sec/step, loss=0.07595, avg_loss=0.07433]\n",
      "Step 519677  [5.427 sec/step, loss=0.07390, avg_loss=0.07430]\n",
      "Step 519678  [5.421 sec/step, loss=0.07238, avg_loss=0.07426]\n",
      "Step 519679  [5.398 sec/step, loss=0.07145, avg_loss=0.07421]\n",
      "Step 519680  [5.411 sec/step, loss=0.07721, avg_loss=0.07424]\n",
      "Generated 32 batches of size 32 in 2.417 sec\n",
      "Step 519681  [5.432 sec/step, loss=0.07617, avg_loss=0.07429]\n",
      "Step 519682  [5.440 sec/step, loss=0.07550, avg_loss=0.07430]\n",
      "Step 519683  [5.438 sec/step, loss=0.07116, avg_loss=0.07427]\n",
      "Step 519684  [5.386 sec/step, loss=0.07561, avg_loss=0.07436]\n",
      "Step 519685  [5.438 sec/step, loss=0.06662, avg_loss=0.07427]\n",
      "Step 519686  [5.436 sec/step, loss=0.07651, avg_loss=0.07427]\n",
      "Step 519687  [5.439 sec/step, loss=0.07373, avg_loss=0.07425]\n",
      "Step 519688  [5.447 sec/step, loss=0.07483, avg_loss=0.07424]\n",
      "Step 519689  [5.446 sec/step, loss=0.07569, avg_loss=0.07424]\n",
      "Step 519690  [5.443 sec/step, loss=0.07427, avg_loss=0.07426]\n",
      "Step 519691  [5.442 sec/step, loss=0.07297, avg_loss=0.07425]\n",
      "Step 519692  [5.452 sec/step, loss=0.07411, avg_loss=0.07426]\n",
      "Step 519693  [5.447 sec/step, loss=0.07659, avg_loss=0.07427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 519694  [5.463 sec/step, loss=0.07682, avg_loss=0.07429]\n",
      "Step 519695  [5.429 sec/step, loss=0.06567, avg_loss=0.07419]\n",
      "Step 519696  [5.447 sec/step, loss=0.07612, avg_loss=0.07422]\n",
      "Step 519697  [5.447 sec/step, loss=0.07630, avg_loss=0.07422]\n",
      "Step 519698  [5.444 sec/step, loss=0.07561, avg_loss=0.07421]\n",
      "Step 519699  [5.460 sec/step, loss=0.07632, avg_loss=0.07426]\n",
      "Step 519700  [5.469 sec/step, loss=0.07534, avg_loss=0.07427]\n",
      "Writing summary at step: 519700\n",
      "Step 519701  [5.461 sec/step, loss=0.07467, avg_loss=0.07427]\n",
      "Step 519702  [5.484 sec/step, loss=0.07562, avg_loss=0.07428]\n",
      "Step 519703  [5.505 sec/step, loss=0.07379, avg_loss=0.07427]\n",
      "Step 519704  [5.493 sec/step, loss=0.07439, avg_loss=0.07425]\n",
      "Step 519705  [5.555 sec/step, loss=0.06669, avg_loss=0.07421]\n",
      "Step 519706  [5.547 sec/step, loss=0.07041, avg_loss=0.07417]\n",
      "Step 519707  [5.538 sec/step, loss=0.07374, avg_loss=0.07418]\n",
      "Step 519708  [5.539 sec/step, loss=0.07484, avg_loss=0.07418]\n",
      "Step 519709  [5.513 sec/step, loss=0.07502, avg_loss=0.07418]\n",
      "Step 519710  [5.526 sec/step, loss=0.07629, avg_loss=0.07419]\n",
      "Step 519711  [5.517 sec/step, loss=0.07684, avg_loss=0.07420]\n",
      "Generated 32 batches of size 32 in 2.465 sec\n",
      "Step 519712  [5.541 sec/step, loss=0.07586, avg_loss=0.07430]\n",
      "Step 519713  [5.533 sec/step, loss=0.07606, avg_loss=0.07432]\n",
      "Step 519714  [5.476 sec/step, loss=0.07446, avg_loss=0.07439]\n",
      "Step 519715  [5.474 sec/step, loss=0.07562, avg_loss=0.07442]\n",
      "Step 519716  [5.472 sec/step, loss=0.07569, avg_loss=0.07442]\n",
      "Step 519717  [5.477 sec/step, loss=0.07521, avg_loss=0.07442]\n",
      "Step 519718  [5.502 sec/step, loss=0.07386, avg_loss=0.07440]\n",
      "Step 519719  [5.513 sec/step, loss=0.07648, avg_loss=0.07441]\n",
      "Step 519720  [5.489 sec/step, loss=0.07204, avg_loss=0.07437]\n",
      "Step 519721  [5.485 sec/step, loss=0.07493, avg_loss=0.07435]\n",
      "Step 519722  [5.483 sec/step, loss=0.07378, avg_loss=0.07433]\n",
      "Step 519723  [5.485 sec/step, loss=0.07152, avg_loss=0.07429]\n",
      "Step 519724  [5.474 sec/step, loss=0.07380, avg_loss=0.07428]\n",
      "Step 519725  [5.471 sec/step, loss=0.07660, avg_loss=0.07429]\n",
      "Step 519726  [5.478 sec/step, loss=0.07696, avg_loss=0.07431]\n",
      "Step 519727  [5.486 sec/step, loss=0.07622, avg_loss=0.07434]\n",
      "Step 519728  [5.483 sec/step, loss=0.07500, avg_loss=0.07433]\n",
      "Step 519729  [5.488 sec/step, loss=0.07526, avg_loss=0.07434]\n",
      "Step 519730  [5.496 sec/step, loss=0.07446, avg_loss=0.07441]\n",
      "Step 519731  [5.481 sec/step, loss=0.07290, avg_loss=0.07439]\n",
      "Step 519732  [5.517 sec/step, loss=0.07347, avg_loss=0.07438]\n",
      "Step 519733  [5.542 sec/step, loss=0.07421, avg_loss=0.07440]\n",
      "Step 519734  [5.546 sec/step, loss=0.07520, avg_loss=0.07440]\n",
      "Step 519735  [5.544 sec/step, loss=0.07383, avg_loss=0.07439]\n",
      "Step 519736  [5.537 sec/step, loss=0.07584, avg_loss=0.07439]\n",
      "Step 519737  [5.527 sec/step, loss=0.07297, avg_loss=0.07435]\n",
      "Step 519738  [5.543 sec/step, loss=0.07595, avg_loss=0.07436]\n",
      "Step 519739  [5.581 sec/step, loss=0.06820, avg_loss=0.07428]\n",
      "Step 519740  [5.575 sec/step, loss=0.07534, avg_loss=0.07426]\n",
      "Step 519741  [5.564 sec/step, loss=0.07590, avg_loss=0.07426]\n",
      "Step 519742  [5.562 sec/step, loss=0.07428, avg_loss=0.07426]\n",
      "Step 519743  [5.556 sec/step, loss=0.07186, avg_loss=0.07424]\n",
      "Generated 32 batches of size 32 in 2.589 sec\n",
      "Step 519744  [5.539 sec/step, loss=0.07581, avg_loss=0.07426]\n",
      "Step 519745  [5.534 sec/step, loss=0.07378, avg_loss=0.07424]\n",
      "Step 519746  [5.547 sec/step, loss=0.07559, avg_loss=0.07425]\n",
      "Step 519747  [5.523 sec/step, loss=0.06702, avg_loss=0.07417]\n",
      "Step 519748  [5.516 sec/step, loss=0.07567, avg_loss=0.07416]\n",
      "Step 519749  [5.525 sec/step, loss=0.07691, avg_loss=0.07417]\n",
      "Step 519750  [5.508 sec/step, loss=0.07686, avg_loss=0.07420]\n",
      "Step 519751  [5.523 sec/step, loss=0.07636, avg_loss=0.07421]\n",
      "Step 519752  [5.469 sec/step, loss=0.07420, avg_loss=0.07428]\n",
      "Step 519753  [5.464 sec/step, loss=0.07497, avg_loss=0.07427]\n",
      "Step 519754  [5.472 sec/step, loss=0.07192, avg_loss=0.07427]\n",
      "Step 519755  [5.483 sec/step, loss=0.07600, avg_loss=0.07428]\n",
      "Step 519756  [5.485 sec/step, loss=0.07530, avg_loss=0.07431]\n",
      "Step 519757  [5.482 sec/step, loss=0.07159, avg_loss=0.07429]\n",
      "Step 519758  [5.489 sec/step, loss=0.07676, avg_loss=0.07433]\n",
      "Step 519759  [5.502 sec/step, loss=0.07560, avg_loss=0.07433]\n",
      "Step 519760  [5.475 sec/step, loss=0.07296, avg_loss=0.07433]\n",
      "Step 519761  [5.480 sec/step, loss=0.07614, avg_loss=0.07433]\n",
      "Step 519762  [5.508 sec/step, loss=0.07586, avg_loss=0.07433]\n",
      "Step 519763  [5.524 sec/step, loss=0.07421, avg_loss=0.07433]\n",
      "Step 519764  [5.512 sec/step, loss=0.06529, avg_loss=0.07426]\n",
      "Step 519765  [5.499 sec/step, loss=0.07450, avg_loss=0.07428]\n",
      "Step 519766  [5.482 sec/step, loss=0.07389, avg_loss=0.07425]\n",
      "Step 519767  [5.486 sec/step, loss=0.07502, avg_loss=0.07426]\n",
      "Step 519768  [5.502 sec/step, loss=0.07456, avg_loss=0.07430]\n",
      "Step 519769  [5.483 sec/step, loss=0.07061, avg_loss=0.07424]\n",
      "Step 519770  [5.464 sec/step, loss=0.07250, avg_loss=0.07422]\n",
      "Step 519771  [5.463 sec/step, loss=0.07513, avg_loss=0.07423]\n",
      "Step 519772  [5.461 sec/step, loss=0.07685, avg_loss=0.07423]\n",
      "Step 519773  [5.460 sec/step, loss=0.07678, avg_loss=0.07425]\n",
      "Step 519774  [5.526 sec/step, loss=0.06571, avg_loss=0.07423]\n",
      "Step 519775  [5.512 sec/step, loss=0.07496, avg_loss=0.07422]\n",
      "Generated 32 batches of size 32 in 2.588 sec\n",
      "Step 519776  [5.511 sec/step, loss=0.07612, avg_loss=0.07422]\n",
      "Step 519777  [5.511 sec/step, loss=0.07444, avg_loss=0.07423]\n",
      "Step 519778  [5.510 sec/step, loss=0.07477, avg_loss=0.07425]\n",
      "Step 519779  [5.522 sec/step, loss=0.07567, avg_loss=0.07430]\n",
      "Step 519780  [5.519 sec/step, loss=0.07627, avg_loss=0.07429]\n",
      "Step 519781  [5.515 sec/step, loss=0.07713, avg_loss=0.07430]\n",
      "Step 519782  [5.525 sec/step, loss=0.07631, avg_loss=0.07430]\n",
      "Step 519783  [5.536 sec/step, loss=0.07643, avg_loss=0.07436]\n",
      "Step 519784  [5.531 sec/step, loss=0.07462, avg_loss=0.07435]\n",
      "Step 519785  [5.507 sec/step, loss=0.07388, avg_loss=0.07442]\n",
      "Step 519786  [5.482 sec/step, loss=0.07300, avg_loss=0.07438]\n",
      "Step 519787  [5.477 sec/step, loss=0.07596, avg_loss=0.07441]\n",
      "Step 519788  [5.480 sec/step, loss=0.07691, avg_loss=0.07443]\n",
      "Step 519789  [5.491 sec/step, loss=0.07704, avg_loss=0.07444]\n",
      "Step 519790  [5.486 sec/step, loss=0.07458, avg_loss=0.07444]\n",
      "Step 519791  [5.477 sec/step, loss=0.07127, avg_loss=0.07443]\n",
      "Step 519792  [5.485 sec/step, loss=0.07520, avg_loss=0.07444]\n",
      "Step 519793  [5.496 sec/step, loss=0.07415, avg_loss=0.07441]\n",
      "Step 519794  [5.488 sec/step, loss=0.07613, avg_loss=0.07441]\n",
      "Step 519795  [5.506 sec/step, loss=0.07523, avg_loss=0.07450]\n",
      "Step 519796  [5.508 sec/step, loss=0.07422, avg_loss=0.07448]\n",
      "Step 519797  [5.505 sec/step, loss=0.07687, avg_loss=0.07449]\n",
      "Step 519798  [5.510 sec/step, loss=0.07445, avg_loss=0.07448]\n",
      "Step 519799  [5.506 sec/step, loss=0.07596, avg_loss=0.07447]\n",
      "Step 519800  [5.498 sec/step, loss=0.07643, avg_loss=0.07448]\n",
      "Writing summary at step: 519800\n",
      "Step 519801  [5.516 sec/step, loss=0.07435, avg_loss=0.07448]\n",
      "Step 519802  [5.503 sec/step, loss=0.07631, avg_loss=0.07449]\n",
      "Step 519803  [5.543 sec/step, loss=0.06788, avg_loss=0.07443]\n",
      "Step 519804  [5.539 sec/step, loss=0.07529, avg_loss=0.07444]\n",
      "Step 519805  [5.488 sec/step, loss=0.07468, avg_loss=0.07452]\n",
      "Step 519806  [5.478 sec/step, loss=0.07348, avg_loss=0.07455]\n",
      "Generated 32 batches of size 32 in 2.466 sec\n",
      "Step 519807  [5.498 sec/step, loss=0.07384, avg_loss=0.07455]\n",
      "Step 519808  [5.501 sec/step, loss=0.07524, avg_loss=0.07455]\n",
      "Step 519809  [5.499 sec/step, loss=0.07397, avg_loss=0.07454]\n",
      "Step 519810  [5.477 sec/step, loss=0.07399, avg_loss=0.07452]\n",
      "Step 519811  [5.471 sec/step, loss=0.07495, avg_loss=0.07450]\n",
      "Step 519812  [5.448 sec/step, loss=0.06687, avg_loss=0.07441]\n",
      "Step 519813  [5.457 sec/step, loss=0.07517, avg_loss=0.07440]\n",
      "Step 519814  [5.477 sec/step, loss=0.07691, avg_loss=0.07443]\n",
      "Step 519815  [5.490 sec/step, loss=0.07660, avg_loss=0.07444]\n",
      "Step 519816  [5.481 sec/step, loss=0.07611, avg_loss=0.07444]\n",
      "Step 519817  [5.495 sec/step, loss=0.07598, avg_loss=0.07445]\n",
      "Step 519818  [5.454 sec/step, loss=0.07169, avg_loss=0.07443]\n",
      "Step 519819  [5.452 sec/step, loss=0.07490, avg_loss=0.07441]\n",
      "Step 519820  [5.471 sec/step, loss=0.07692, avg_loss=0.07446]\n",
      "Step 519821  [5.461 sec/step, loss=0.07394, avg_loss=0.07445]\n",
      "Step 519822  [5.510 sec/step, loss=0.06674, avg_loss=0.07438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 519823  [5.505 sec/step, loss=0.07466, avg_loss=0.07441]\n",
      "Step 519824  [5.513 sec/step, loss=0.07342, avg_loss=0.07441]\n",
      "Step 519825  [5.532 sec/step, loss=0.07374, avg_loss=0.07438]\n",
      "Step 519826  [5.522 sec/step, loss=0.07568, avg_loss=0.07437]\n",
      "Step 519827  [5.517 sec/step, loss=0.07459, avg_loss=0.07435]\n",
      "Step 519828  [5.521 sec/step, loss=0.07567, avg_loss=0.07436]\n",
      "Step 519829  [5.524 sec/step, loss=0.07566, avg_loss=0.07436]\n",
      "Step 519830  [5.536 sec/step, loss=0.07478, avg_loss=0.07436]\n",
      "Step 519831  [5.548 sec/step, loss=0.07555, avg_loss=0.07439]\n",
      "Step 519832  [5.536 sec/step, loss=0.07704, avg_loss=0.07443]\n",
      "Step 519833  [5.516 sec/step, loss=0.07351, avg_loss=0.07442]\n",
      "Step 519834  [5.519 sec/step, loss=0.07689, avg_loss=0.07444]\n",
      "Step 519835  [5.514 sec/step, loss=0.07396, avg_loss=0.07444]\n",
      "Step 519836  [5.519 sec/step, loss=0.07663, avg_loss=0.07444]\n",
      "Step 519837  [5.519 sec/step, loss=0.07288, avg_loss=0.07444]\n",
      "Step 519838  [5.491 sec/step, loss=0.07089, avg_loss=0.07439]\n",
      "Generated 32 batches of size 32 in 2.536 sec\n",
      "Step 519839  [5.445 sec/step, loss=0.07505, avg_loss=0.07446]\n",
      "Step 519840  [5.440 sec/step, loss=0.07374, avg_loss=0.07445]\n",
      "Step 519841  [5.429 sec/step, loss=0.07457, avg_loss=0.07443]\n",
      "Step 519842  [5.443 sec/step, loss=0.07227, avg_loss=0.07441]\n",
      "Step 519843  [5.455 sec/step, loss=0.07543, avg_loss=0.07445]\n",
      "Step 519844  [5.461 sec/step, loss=0.07693, avg_loss=0.07446]\n",
      "Step 519845  [5.479 sec/step, loss=0.07651, avg_loss=0.07449]\n",
      "Step 519846  [5.456 sec/step, loss=0.06573, avg_loss=0.07439]\n",
      "Step 519847  [5.473 sec/step, loss=0.07246, avg_loss=0.07444]\n",
      "Step 519848  [5.471 sec/step, loss=0.07617, avg_loss=0.07445]\n",
      "Step 519849  [5.461 sec/step, loss=0.07591, avg_loss=0.07444]\n",
      "Step 519850  [5.456 sec/step, loss=0.07513, avg_loss=0.07442]\n",
      "Step 519851  [5.443 sec/step, loss=0.07582, avg_loss=0.07441]\n",
      "Step 519852  [5.437 sec/step, loss=0.07457, avg_loss=0.07442]\n",
      "Step 519853  [5.487 sec/step, loss=0.06600, avg_loss=0.07433]\n",
      "Step 519854  [5.486 sec/step, loss=0.07403, avg_loss=0.07435]\n",
      "Step 519855  [5.466 sec/step, loss=0.07545, avg_loss=0.07434]\n",
      "Step 519856  [5.471 sec/step, loss=0.07532, avg_loss=0.07434]\n",
      "Step 519857  [5.489 sec/step, loss=0.07612, avg_loss=0.07439]\n",
      "Step 519858  [5.491 sec/step, loss=0.07630, avg_loss=0.07439]\n",
      "Step 519859  [5.485 sec/step, loss=0.07546, avg_loss=0.07438]\n",
      "Step 519860  [5.469 sec/step, loss=0.06663, avg_loss=0.07432]\n",
      "Step 519861  [5.452 sec/step, loss=0.07311, avg_loss=0.07429]\n",
      "Step 519862  [5.417 sec/step, loss=0.07372, avg_loss=0.07427]\n",
      "Step 519863  [5.423 sec/step, loss=0.07572, avg_loss=0.07428]\n",
      "Step 519864  [5.452 sec/step, loss=0.07599, avg_loss=0.07439]\n",
      "Step 519865  [5.461 sec/step, loss=0.07517, avg_loss=0.07440]\n",
      "Step 519866  [5.470 sec/step, loss=0.07410, avg_loss=0.07440]\n",
      "Step 519867  [5.459 sec/step, loss=0.06996, avg_loss=0.07435]\n",
      "Step 519868  [5.472 sec/step, loss=0.07582, avg_loss=0.07436]\n",
      "Step 519869  [5.480 sec/step, loss=0.07399, avg_loss=0.07440]\n",
      "Step 519870  [5.496 sec/step, loss=0.07685, avg_loss=0.07444]\n",
      "Generated 32 batches of size 32 in 2.387 sec\n",
      "Step 519871  [5.511 sec/step, loss=0.07407, avg_loss=0.07443]\n",
      "Step 519872  [5.524 sec/step, loss=0.07589, avg_loss=0.07442]\n",
      "Step 519873  [5.523 sec/step, loss=0.07675, avg_loss=0.07442]\n",
      "Step 519874  [5.478 sec/step, loss=0.07611, avg_loss=0.07452]\n",
      "Step 519875  [5.475 sec/step, loss=0.07239, avg_loss=0.07450]\n",
      "Step 519876  [5.483 sec/step, loss=0.07497, avg_loss=0.07449]\n",
      "Step 519877  [5.486 sec/step, loss=0.07332, avg_loss=0.07447]\n",
      "Step 519878  [5.482 sec/step, loss=0.07366, avg_loss=0.07446]\n",
      "Step 519879  [5.470 sec/step, loss=0.07226, avg_loss=0.07443]\n",
      "Step 519880  [5.461 sec/step, loss=0.07431, avg_loss=0.07441]\n",
      "Step 519881  [5.464 sec/step, loss=0.07670, avg_loss=0.07441]\n",
      "Step 519882  [5.456 sec/step, loss=0.07443, avg_loss=0.07439]\n",
      "Step 519883  [5.452 sec/step, loss=0.07566, avg_loss=0.07438]\n",
      "Step 519884  [5.461 sec/step, loss=0.07558, avg_loss=0.07439]\n",
      "Step 519885  [5.423 sec/step, loss=0.07447, avg_loss=0.07439]\n",
      "Step 519886  [5.433 sec/step, loss=0.07451, avg_loss=0.07441]\n",
      "Step 519887  [5.426 sec/step, loss=0.07225, avg_loss=0.07437]\n",
      "Step 519888  [5.421 sec/step, loss=0.07500, avg_loss=0.07435]\n",
      "Step 519889  [5.410 sec/step, loss=0.07663, avg_loss=0.07435]\n",
      "Step 519890  [5.421 sec/step, loss=0.07441, avg_loss=0.07435]\n",
      "Step 519891  [5.421 sec/step, loss=0.07361, avg_loss=0.07437]\n",
      "Step 519892  [5.442 sec/step, loss=0.07341, avg_loss=0.07435]\n",
      "Step 519893  [5.474 sec/step, loss=0.06743, avg_loss=0.07429]\n",
      "Step 519894  [5.483 sec/step, loss=0.07617, avg_loss=0.07429]\n",
      "Step 519895  [5.471 sec/step, loss=0.07329, avg_loss=0.07427]\n",
      "Step 519896  [5.477 sec/step, loss=0.07575, avg_loss=0.07428]\n",
      "Step 519897  [5.452 sec/step, loss=0.06737, avg_loss=0.07419]\n",
      "Step 519898  [5.451 sec/step, loss=0.07488, avg_loss=0.07419]\n",
      "Step 519899  [5.450 sec/step, loss=0.07220, avg_loss=0.07415]\n",
      "Step 519900  [5.451 sec/step, loss=0.07601, avg_loss=0.07415]\n",
      "Writing summary at step: 519900\n",
      "Step 519901  [5.445 sec/step, loss=0.07653, avg_loss=0.07417]\n",
      "Generated 32 batches of size 32 in 2.595 sec\n",
      "Step 519902  [5.444 sec/step, loss=0.07554, avg_loss=0.07416]\n",
      "Step 519903  [5.379 sec/step, loss=0.07224, avg_loss=0.07421]\n",
      "Step 519904  [5.386 sec/step, loss=0.07538, avg_loss=0.07421]\n",
      "Step 519905  [5.393 sec/step, loss=0.07717, avg_loss=0.07423]\n",
      "Step 519906  [5.402 sec/step, loss=0.07410, avg_loss=0.07424]\n",
      "Step 519907  [5.408 sec/step, loss=0.07701, avg_loss=0.07427]\n",
      "Step 519908  [5.411 sec/step, loss=0.07694, avg_loss=0.07429]\n",
      "Step 519909  [5.426 sec/step, loss=0.07455, avg_loss=0.07429]\n",
      "Step 519910  [5.443 sec/step, loss=0.07608, avg_loss=0.07431]\n",
      "Step 519911  [5.451 sec/step, loss=0.07547, avg_loss=0.07432]\n",
      "Step 519912  [5.465 sec/step, loss=0.07187, avg_loss=0.07437]\n",
      "Step 519913  [5.464 sec/step, loss=0.07468, avg_loss=0.07436]\n",
      "Step 519914  [5.500 sec/step, loss=0.06629, avg_loss=0.07426]\n",
      "Step 519915  [5.497 sec/step, loss=0.07644, avg_loss=0.07426]\n",
      "Step 519916  [5.497 sec/step, loss=0.07505, avg_loss=0.07425]\n",
      "Step 519917  [5.488 sec/step, loss=0.07670, avg_loss=0.07425]\n",
      "Step 519918  [5.502 sec/step, loss=0.07358, avg_loss=0.07427]\n",
      "Step 519919  [5.497 sec/step, loss=0.07248, avg_loss=0.07425]\n",
      "Step 519920  [5.501 sec/step, loss=0.07409, avg_loss=0.07422]\n",
      "Step 519921  [5.531 sec/step, loss=0.07616, avg_loss=0.07424]\n",
      "Step 519922  [5.471 sec/step, loss=0.07178, avg_loss=0.07429]\n",
      "Step 519923  [5.470 sec/step, loss=0.07454, avg_loss=0.07429]\n",
      "Step 519924  [5.472 sec/step, loss=0.07572, avg_loss=0.07431]\n",
      "Step 519925  [5.432 sec/step, loss=0.07171, avg_loss=0.07429]\n",
      "Step 519926  [5.437 sec/step, loss=0.07668, avg_loss=0.07430]\n",
      "Step 519927  [5.426 sec/step, loss=0.07642, avg_loss=0.07432]\n",
      "Step 519928  [5.432 sec/step, loss=0.07710, avg_loss=0.07434]\n",
      "Step 519929  [5.443 sec/step, loss=0.07655, avg_loss=0.07435]\n",
      "Step 519930  [5.421 sec/step, loss=0.06524, avg_loss=0.07425]\n",
      "Step 519931  [5.422 sec/step, loss=0.07275, avg_loss=0.07422]\n",
      "Step 519932  [5.400 sec/step, loss=0.07145, avg_loss=0.07417]\n",
      "Step 519933  [5.425 sec/step, loss=0.07662, avg_loss=0.07420]\n",
      "Generated 32 batches of size 32 in 2.454 sec\n",
      "Step 519934  [5.431 sec/step, loss=0.07648, avg_loss=0.07419]\n",
      "Step 519935  [5.445 sec/step, loss=0.07534, avg_loss=0.07421]\n",
      "Step 519936  [5.439 sec/step, loss=0.07521, avg_loss=0.07419]\n",
      "Step 519937  [5.440 sec/step, loss=0.07335, avg_loss=0.07420]\n",
      "Step 519938  [5.442 sec/step, loss=0.07482, avg_loss=0.07424]\n",
      "Step 519939  [5.443 sec/step, loss=0.07614, avg_loss=0.07425]\n",
      "Step 519940  [5.442 sec/step, loss=0.07385, avg_loss=0.07425]\n",
      "Step 519941  [5.452 sec/step, loss=0.07612, avg_loss=0.07426]\n",
      "Step 519942  [5.442 sec/step, loss=0.07415, avg_loss=0.07428]\n",
      "Step 519943  [5.453 sec/step, loss=0.07611, avg_loss=0.07429]\n",
      "Step 519944  [5.455 sec/step, loss=0.07417, avg_loss=0.07426]\n",
      "Step 519945  [5.443 sec/step, loss=0.07535, avg_loss=0.07425]\n",
      "Step 519946  [5.462 sec/step, loss=0.07578, avg_loss=0.07435]\n",
      "Step 519947  [5.461 sec/step, loss=0.07493, avg_loss=0.07438]\n",
      "Step 519948  [5.458 sec/step, loss=0.07534, avg_loss=0.07437]\n",
      "Step 519949  [5.461 sec/step, loss=0.07542, avg_loss=0.07436]\n",
      "Step 519950  [5.471 sec/step, loss=0.07675, avg_loss=0.07438]\n",
      "Step 519951  [5.459 sec/step, loss=0.07087, avg_loss=0.07433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 519952  [5.466 sec/step, loss=0.07389, avg_loss=0.07432]\n",
      "Step 519953  [5.412 sec/step, loss=0.07340, avg_loss=0.07440]\n",
      "Step 519954  [5.420 sec/step, loss=0.07612, avg_loss=0.07442]\n",
      "Step 519955  [5.472 sec/step, loss=0.06716, avg_loss=0.07433]\n",
      "Step 519956  [5.459 sec/step, loss=0.07490, avg_loss=0.07433]\n",
      "Step 519957  [5.459 sec/step, loss=0.07531, avg_loss=0.07432]\n",
      "Step 519958  [5.433 sec/step, loss=0.07135, avg_loss=0.07427]\n",
      "Step 519959  [5.434 sec/step, loss=0.07558, avg_loss=0.07427]\n",
      "Step 519960  [5.453 sec/step, loss=0.07588, avg_loss=0.07437]\n",
      "Step 519961  [5.455 sec/step, loss=0.07278, avg_loss=0.07436]\n",
      "Step 519962  [5.456 sec/step, loss=0.07372, avg_loss=0.07436]\n",
      "Step 519963  [5.468 sec/step, loss=0.07368, avg_loss=0.07434]\n",
      "Step 519964  [5.467 sec/step, loss=0.07440, avg_loss=0.07433]\n",
      "Step 519965  [5.477 sec/step, loss=0.07488, avg_loss=0.07432]\n",
      "Generated 32 batches of size 32 in 2.608 sec\n",
      "Step 519966  [5.474 sec/step, loss=0.07317, avg_loss=0.07431]\n",
      "Step 519967  [5.467 sec/step, loss=0.06687, avg_loss=0.07428]\n",
      "Step 519968  [5.454 sec/step, loss=0.07534, avg_loss=0.07428]\n",
      "Step 519969  [5.459 sec/step, loss=0.07556, avg_loss=0.07429]\n",
      "Step 519970  [5.456 sec/step, loss=0.07606, avg_loss=0.07429]\n",
      "Step 519971  [5.445 sec/step, loss=0.07355, avg_loss=0.07428]\n",
      "Step 519972  [5.438 sec/step, loss=0.07601, avg_loss=0.07428]\n",
      "Step 519973  [5.440 sec/step, loss=0.07635, avg_loss=0.07428]\n",
      "Step 519974  [5.424 sec/step, loss=0.07481, avg_loss=0.07427]\n",
      "Step 519975  [5.415 sec/step, loss=0.07424, avg_loss=0.07428]\n",
      "Step 519976  [5.457 sec/step, loss=0.06660, avg_loss=0.07420]\n",
      "Step 519977  [5.471 sec/step, loss=0.07670, avg_loss=0.07423]\n",
      "Step 519978  [5.470 sec/step, loss=0.07530, avg_loss=0.07425]\n",
      "Step 519979  [5.477 sec/step, loss=0.07401, avg_loss=0.07427]\n",
      "Step 519980  [5.468 sec/step, loss=0.07260, avg_loss=0.07425]\n",
      "Step 519981  [5.457 sec/step, loss=0.07604, avg_loss=0.07424]\n",
      "Step 519982  [5.453 sec/step, loss=0.07573, avg_loss=0.07426]\n",
      "Step 519983  [5.475 sec/step, loss=0.07382, avg_loss=0.07424]\n",
      "Step 519984  [5.471 sec/step, loss=0.07274, avg_loss=0.07421]\n",
      "Step 519985  [5.472 sec/step, loss=0.07439, avg_loss=0.07421]\n",
      "Step 519986  [5.471 sec/step, loss=0.07431, avg_loss=0.07421]\n",
      "Step 519987  [5.475 sec/step, loss=0.07568, avg_loss=0.07424]\n",
      "Step 519988  [5.486 sec/step, loss=0.07640, avg_loss=0.07426]\n",
      "Step 519989  [5.488 sec/step, loss=0.07516, avg_loss=0.07424]\n",
      "Step 519990  [5.479 sec/step, loss=0.07416, avg_loss=0.07424]\n",
      "Step 519991  [5.492 sec/step, loss=0.07505, avg_loss=0.07425]\n",
      "Step 519992  [5.465 sec/step, loss=0.07410, avg_loss=0.07426]\n",
      "Step 519993  [5.411 sec/step, loss=0.07551, avg_loss=0.07434]\n",
      "Step 519994  [5.413 sec/step, loss=0.07419, avg_loss=0.07432]\n",
      "Step 519995  [5.436 sec/step, loss=0.07653, avg_loss=0.07435]\n",
      "Step 519996  [5.423 sec/step, loss=0.07581, avg_loss=0.07435]\n",
      "Step 519997  [5.447 sec/step, loss=0.07491, avg_loss=0.07443]\n",
      "Generated 32 batches of size 32 in 2.553 sec\n",
      "Step 519998  [5.437 sec/step, loss=0.07291, avg_loss=0.07441]\n",
      "Step 519999  [5.425 sec/step, loss=0.06694, avg_loss=0.07436]\n",
      "Step 520000  [5.434 sec/step, loss=0.07511, avg_loss=0.07435]\n",
      "Writing summary at step: 520000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-520000\n",
      "Saving audio and alignment...\n",
      "Input: naam baxsh kii atshtshaaii ood dii laeft budzhaanay kaa ishq hae~__________________________________\n",
      "Step 520001  [5.439 sec/step, loss=0.07652, avg_loss=0.07435]\n",
      "Step 520002  [5.460 sec/step, loss=0.07338, avg_loss=0.07433]\n",
      "Step 520003  [5.483 sec/step, loss=0.07348, avg_loss=0.07434]\n",
      "Step 520004  [5.487 sec/step, loss=0.07262, avg_loss=0.07431]\n",
      "Step 520005  [5.479 sec/step, loss=0.07587, avg_loss=0.07430]\n",
      "Step 520006  [5.534 sec/step, loss=0.06632, avg_loss=0.07422]\n",
      "Step 520007  [5.520 sec/step, loss=0.07489, avg_loss=0.07420]\n",
      "Step 520008  [5.508 sec/step, loss=0.07576, avg_loss=0.07419]\n",
      "Step 520009  [5.510 sec/step, loss=0.07610, avg_loss=0.07420]\n",
      "Step 520010  [5.493 sec/step, loss=0.07435, avg_loss=0.07419]\n",
      "Step 520011  [5.475 sec/step, loss=0.07479, avg_loss=0.07418]\n",
      "Step 520012  [5.505 sec/step, loss=0.07334, avg_loss=0.07419]\n",
      "Step 520013  [5.515 sec/step, loss=0.07644, avg_loss=0.07421]\n",
      "Step 520014  [5.454 sec/step, loss=0.07156, avg_loss=0.07426]\n",
      "Step 520015  [5.441 sec/step, loss=0.07381, avg_loss=0.07424]\n",
      "Step 520016  [5.440 sec/step, loss=0.07460, avg_loss=0.07423]\n",
      "Step 520017  [5.435 sec/step, loss=0.07543, avg_loss=0.07422]\n",
      "Step 520018  [5.420 sec/step, loss=0.07273, avg_loss=0.07421]\n",
      "Step 520019  [5.408 sec/step, loss=0.07462, avg_loss=0.07423]\n",
      "Step 520020  [5.379 sec/step, loss=0.06750, avg_loss=0.07417]\n",
      "Step 520021  [5.361 sec/step, loss=0.07585, avg_loss=0.07416]\n",
      "Step 520022  [5.381 sec/step, loss=0.07653, avg_loss=0.07421]\n",
      "Step 520023  [5.389 sec/step, loss=0.07611, avg_loss=0.07423]\n",
      "Step 520024  [5.380 sec/step, loss=0.07261, avg_loss=0.07420]\n",
      "Step 520025  [5.401 sec/step, loss=0.07698, avg_loss=0.07425]\n",
      "Step 520026  [5.395 sec/step, loss=0.07278, avg_loss=0.07421]\n",
      "Step 520027  [5.389 sec/step, loss=0.07562, avg_loss=0.07420]\n",
      "Generated 32 batches of size 32 in 2.393 sec\n",
      "Step 520028  [5.393 sec/step, loss=0.07660, avg_loss=0.07420]\n",
      "Step 520029  [5.383 sec/step, loss=0.07646, avg_loss=0.07420]\n",
      "Step 520030  [5.416 sec/step, loss=0.07548, avg_loss=0.07430]\n",
      "Step 520031  [5.426 sec/step, loss=0.07609, avg_loss=0.07433]\n",
      "Step 520032  [5.444 sec/step, loss=0.07358, avg_loss=0.07435]\n",
      "Step 520033  [5.433 sec/step, loss=0.07569, avg_loss=0.07434]\n",
      "Step 520034  [5.420 sec/step, loss=0.07449, avg_loss=0.07432]\n",
      "Step 520035  [5.411 sec/step, loss=0.07596, avg_loss=0.07433]\n",
      "Step 520036  [5.401 sec/step, loss=0.07384, avg_loss=0.07432]\n",
      "Step 520037  [5.403 sec/step, loss=0.07642, avg_loss=0.07435]\n",
      "Step 520038  [5.412 sec/step, loss=0.07447, avg_loss=0.07434]\n",
      "Step 520039  [5.401 sec/step, loss=0.07466, avg_loss=0.07433]\n",
      "Step 520040  [5.388 sec/step, loss=0.06481, avg_loss=0.07424]\n",
      "Step 520041  [5.384 sec/step, loss=0.07260, avg_loss=0.07420]\n",
      "Step 520042  [5.412 sec/step, loss=0.07340, avg_loss=0.07420]\n",
      "Step 520043  [5.406 sec/step, loss=0.07619, avg_loss=0.07420]\n",
      "Step 520044  [5.407 sec/step, loss=0.07638, avg_loss=0.07422]\n",
      "Step 520045  [5.391 sec/step, loss=0.07167, avg_loss=0.07418]\n",
      "Step 520046  [5.380 sec/step, loss=0.07452, avg_loss=0.07417]\n",
      "Step 520047  [5.388 sec/step, loss=0.07424, avg_loss=0.07416]\n",
      "Step 520048  [5.387 sec/step, loss=0.07532, avg_loss=0.07416]\n",
      "Step 520049  [5.393 sec/step, loss=0.07404, avg_loss=0.07415]\n",
      "Step 520050  [5.380 sec/step, loss=0.07514, avg_loss=0.07413]\n",
      "Step 520051  [5.394 sec/step, loss=0.07506, avg_loss=0.07417]\n",
      "Step 520052  [5.411 sec/step, loss=0.07657, avg_loss=0.07420]\n",
      "Step 520053  [5.413 sec/step, loss=0.07597, avg_loss=0.07423]\n",
      "Step 520054  [5.408 sec/step, loss=0.07577, avg_loss=0.07422]\n",
      "Step 520055  [5.375 sec/step, loss=0.07635, avg_loss=0.07431]\n",
      "Step 520056  [5.432 sec/step, loss=0.06735, avg_loss=0.07424]\n",
      "Step 520057  [5.428 sec/step, loss=0.07608, avg_loss=0.07425]\n",
      "Step 520058  [5.439 sec/step, loss=0.07587, avg_loss=0.07429]\n",
      "Step 520059  [5.429 sec/step, loss=0.07160, avg_loss=0.07425]\n",
      "Generated 32 batches of size 32 in 2.410 sec\n",
      "Step 520060  [5.443 sec/step, loss=0.07674, avg_loss=0.07426]\n",
      "Step 520061  [5.444 sec/step, loss=0.07423, avg_loss=0.07428]\n",
      "Step 520062  [5.450 sec/step, loss=0.07536, avg_loss=0.07429]\n",
      "Step 520063  [5.428 sec/step, loss=0.07541, avg_loss=0.07431]\n",
      "Step 520064  [5.421 sec/step, loss=0.07567, avg_loss=0.07432]\n",
      "Step 520065  [5.409 sec/step, loss=0.07110, avg_loss=0.07428]\n",
      "Step 520066  [5.416 sec/step, loss=0.07552, avg_loss=0.07431]\n",
      "Step 520067  [5.423 sec/step, loss=0.07464, avg_loss=0.07439]\n",
      "Step 520068  [5.410 sec/step, loss=0.07404, avg_loss=0.07437]\n",
      "Step 520069  [5.421 sec/step, loss=0.07647, avg_loss=0.07438]\n",
      "Step 520070  [5.408 sec/step, loss=0.07390, avg_loss=0.07436]\n",
      "Step 520071  [5.404 sec/step, loss=0.07414, avg_loss=0.07437]\n",
      "Step 520072  [5.392 sec/step, loss=0.07657, avg_loss=0.07437]\n",
      "Step 520073  [5.372 sec/step, loss=0.07423, avg_loss=0.07435]\n",
      "Step 520074  [5.388 sec/step, loss=0.07292, avg_loss=0.07433]\n",
      "Step 520075  [5.399 sec/step, loss=0.07238, avg_loss=0.07431]\n",
      "Step 520076  [5.344 sec/step, loss=0.07406, avg_loss=0.07439]\n",
      "Step 520077  [5.347 sec/step, loss=0.07430, avg_loss=0.07436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 520078  [5.343 sec/step, loss=0.07574, avg_loss=0.07437]\n",
      "Step 520079  [5.350 sec/step, loss=0.07554, avg_loss=0.07438]\n",
      "Step 520080  [5.356 sec/step, loss=0.07418, avg_loss=0.07440]\n",
      "Step 520081  [5.368 sec/step, loss=0.07653, avg_loss=0.07440]\n",
      "Step 520082  [5.396 sec/step, loss=0.07348, avg_loss=0.07438]\n",
      "Step 520083  [5.378 sec/step, loss=0.07582, avg_loss=0.07440]\n",
      "Step 520084  [5.376 sec/step, loss=0.07380, avg_loss=0.07441]\n",
      "Step 520085  [5.387 sec/step, loss=0.07505, avg_loss=0.07442]\n",
      "Step 520086  [5.386 sec/step, loss=0.07530, avg_loss=0.07443]\n",
      "Step 520087  [5.366 sec/step, loss=0.06597, avg_loss=0.07433]\n",
      "Step 520088  [5.361 sec/step, loss=0.07522, avg_loss=0.07432]\n",
      "Step 520089  [5.373 sec/step, loss=0.07609, avg_loss=0.07433]\n",
      "Step 520090  [5.366 sec/step, loss=0.07176, avg_loss=0.07431]\n",
      "Step 520091  [5.369 sec/step, loss=0.07714, avg_loss=0.07433]\n",
      "Generated 32 batches of size 32 in 2.331 sec\n",
      "Step 520092  [5.375 sec/step, loss=0.07607, avg_loss=0.07435]\n",
      "Step 520093  [5.389 sec/step, loss=0.07630, avg_loss=0.07435]\n",
      "Step 520094  [5.366 sec/step, loss=0.07460, avg_loss=0.07436]\n",
      "Step 520095  [5.357 sec/step, loss=0.07556, avg_loss=0.07435]\n",
      "Step 520096  [5.402 sec/step, loss=0.06733, avg_loss=0.07426]\n",
      "Step 520097  [5.406 sec/step, loss=0.07652, avg_loss=0.07428]\n",
      "Step 520098  [5.405 sec/step, loss=0.07454, avg_loss=0.07430]\n",
      "Step 520099  [5.407 sec/step, loss=0.07207, avg_loss=0.07435]\n",
      "Step 520100  [5.404 sec/step, loss=0.07634, avg_loss=0.07436]\n",
      "Writing summary at step: 520100\n",
      "Step 520101  [5.389 sec/step, loss=0.07408, avg_loss=0.07434]\n",
      "Step 520102  [5.370 sec/step, loss=0.07504, avg_loss=0.07435]\n",
      "Step 520103  [5.387 sec/step, loss=0.07302, avg_loss=0.07435]\n",
      "Step 520104  [5.383 sec/step, loss=0.07445, avg_loss=0.07437]\n",
      "Step 520105  [5.383 sec/step, loss=0.07453, avg_loss=0.07435]\n",
      "Step 520106  [5.348 sec/step, loss=0.07663, avg_loss=0.07446]\n",
      "Step 520107  [5.346 sec/step, loss=0.07352, avg_loss=0.07444]\n",
      "Step 520108  [5.345 sec/step, loss=0.07590, avg_loss=0.07444]\n",
      "Step 520109  [5.337 sec/step, loss=0.07587, avg_loss=0.07444]\n",
      "Step 520110  [5.358 sec/step, loss=0.07607, avg_loss=0.07446]\n",
      "Step 520111  [5.370 sec/step, loss=0.07645, avg_loss=0.07447]\n",
      "Step 520112  [5.349 sec/step, loss=0.07471, avg_loss=0.07449]\n",
      "Step 520113  [5.343 sec/step, loss=0.07444, avg_loss=0.07447]\n",
      "Step 520114  [5.349 sec/step, loss=0.07450, avg_loss=0.07450]\n",
      "Step 520115  [5.340 sec/step, loss=0.07193, avg_loss=0.07448]\n",
      "Step 520116  [5.351 sec/step, loss=0.07689, avg_loss=0.07450]\n",
      "Step 520117  [5.347 sec/step, loss=0.07355, avg_loss=0.07448]\n",
      "Step 520118  [5.366 sec/step, loss=0.07668, avg_loss=0.07452]\n",
      "Step 520119  [5.384 sec/step, loss=0.07668, avg_loss=0.07454]\n",
      "Step 520120  [5.402 sec/step, loss=0.07602, avg_loss=0.07463]\n",
      "Step 520121  [5.391 sec/step, loss=0.07559, avg_loss=0.07463]\n",
      "Step 520122  [5.371 sec/step, loss=0.07446, avg_loss=0.07460]\n",
      "Generated 32 batches of size 32 in 2.424 sec\n",
      "Step 520123  [5.374 sec/step, loss=0.07546, avg_loss=0.07460]\n",
      "Step 520124  [5.388 sec/step, loss=0.07719, avg_loss=0.07464]\n",
      "Step 520125  [5.371 sec/step, loss=0.07210, avg_loss=0.07460]\n",
      "Step 520126  [5.382 sec/step, loss=0.07417, avg_loss=0.07461]\n",
      "Step 520127  [5.398 sec/step, loss=0.07616, avg_loss=0.07461]\n",
      "Step 520128  [5.376 sec/step, loss=0.07487, avg_loss=0.07460]\n",
      "Step 520129  [5.352 sec/step, loss=0.06683, avg_loss=0.07450]\n",
      "Step 520130  [5.385 sec/step, loss=0.06677, avg_loss=0.07441]\n",
      "Step 520131  [5.382 sec/step, loss=0.07428, avg_loss=0.07440]\n",
      "Step 520132  [5.368 sec/step, loss=0.07449, avg_loss=0.07440]\n",
      "Step 520133  [5.348 sec/step, loss=0.07223, avg_loss=0.07437]\n",
      "Step 520134  [5.370 sec/step, loss=0.07399, avg_loss=0.07437]\n",
      "Step 520135  [5.419 sec/step, loss=0.06709, avg_loss=0.07428]\n",
      "Step 520136  [5.425 sec/step, loss=0.07627, avg_loss=0.07430]\n",
      "Step 520137  [5.408 sec/step, loss=0.07483, avg_loss=0.07428]\n",
      "Step 520138  [5.408 sec/step, loss=0.07496, avg_loss=0.07429]\n",
      "Step 520139  [5.414 sec/step, loss=0.07605, avg_loss=0.07430]\n",
      "Step 520140  [5.423 sec/step, loss=0.07246, avg_loss=0.07438]\n",
      "Step 520141  [5.423 sec/step, loss=0.07187, avg_loss=0.07437]\n",
      "Step 520142  [5.388 sec/step, loss=0.07131, avg_loss=0.07435]\n",
      "Step 520143  [5.386 sec/step, loss=0.07693, avg_loss=0.07436]\n",
      "Step 520144  [5.385 sec/step, loss=0.07710, avg_loss=0.07437]\n",
      "Step 520145  [5.411 sec/step, loss=0.07679, avg_loss=0.07442]\n",
      "Step 520146  [5.434 sec/step, loss=0.07642, avg_loss=0.07444]\n",
      "Step 520147  [5.408 sec/step, loss=0.06731, avg_loss=0.07437]\n",
      "Step 520148  [5.421 sec/step, loss=0.07652, avg_loss=0.07438]\n",
      "Step 520149  [5.411 sec/step, loss=0.07564, avg_loss=0.07440]\n",
      "Step 520150  [5.407 sec/step, loss=0.07592, avg_loss=0.07440]\n",
      "Step 520151  [5.412 sec/step, loss=0.07438, avg_loss=0.07440]\n",
      "Step 520152  [5.403 sec/step, loss=0.07553, avg_loss=0.07439]\n",
      "Step 520153  [5.421 sec/step, loss=0.07635, avg_loss=0.07439]\n",
      "Step 520154  [5.419 sec/step, loss=0.07361, avg_loss=0.07437]\n",
      "Generated 32 batches of size 32 in 2.407 sec\n",
      "Step 520155  [5.408 sec/step, loss=0.07535, avg_loss=0.07436]\n",
      "Step 520156  [5.363 sec/step, loss=0.07371, avg_loss=0.07442]\n",
      "Step 520157  [5.369 sec/step, loss=0.07434, avg_loss=0.07440]\n",
      "Step 520158  [5.377 sec/step, loss=0.07609, avg_loss=0.07441]\n",
      "Step 520159  [5.397 sec/step, loss=0.07637, avg_loss=0.07445]\n",
      "Step 520160  [5.389 sec/step, loss=0.07751, avg_loss=0.07446]\n",
      "Step 520161  [5.384 sec/step, loss=0.07438, avg_loss=0.07446]\n",
      "Step 520162  [5.378 sec/step, loss=0.07511, avg_loss=0.07446]\n",
      "Step 520163  [5.375 sec/step, loss=0.07557, avg_loss=0.07446]\n",
      "Step 520164  [5.420 sec/step, loss=0.06675, avg_loss=0.07437]\n",
      "Step 520165  [5.417 sec/step, loss=0.07399, avg_loss=0.07440]\n",
      "Step 520166  [5.423 sec/step, loss=0.07699, avg_loss=0.07442]\n",
      "Step 520167  [5.445 sec/step, loss=0.07486, avg_loss=0.07442]\n",
      "Step 520168  [5.464 sec/step, loss=0.07624, avg_loss=0.07444]\n",
      "Step 520169  [5.451 sec/step, loss=0.07397, avg_loss=0.07442]\n",
      "Step 520170  [5.461 sec/step, loss=0.07571, avg_loss=0.07443]\n",
      "Step 520171  [5.457 sec/step, loss=0.07384, avg_loss=0.07443]\n",
      "Step 520172  [5.435 sec/step, loss=0.06552, avg_loss=0.07432]\n",
      "Step 520173  [5.448 sec/step, loss=0.07540, avg_loss=0.07433]\n",
      "Step 520174  [5.428 sec/step, loss=0.07215, avg_loss=0.07432]\n",
      "Step 520175  [5.427 sec/step, loss=0.07293, avg_loss=0.07433]\n",
      "Step 520176  [5.432 sec/step, loss=0.07453, avg_loss=0.07433]\n",
      "Step 520177  [5.427 sec/step, loss=0.07649, avg_loss=0.07436]\n",
      "Step 520178  [5.444 sec/step, loss=0.07676, avg_loss=0.07437]\n",
      "Step 520179  [5.436 sec/step, loss=0.07465, avg_loss=0.07436]\n",
      "Step 520180  [5.453 sec/step, loss=0.07673, avg_loss=0.07438]\n",
      "Step 520181  [5.438 sec/step, loss=0.07438, avg_loss=0.07436]\n",
      "Step 520182  [5.403 sec/step, loss=0.07140, avg_loss=0.07434]\n",
      "Step 520183  [5.393 sec/step, loss=0.07456, avg_loss=0.07433]\n",
      "Step 520184  [5.406 sec/step, loss=0.07639, avg_loss=0.07435]\n",
      "Step 520185  [5.402 sec/step, loss=0.07550, avg_loss=0.07436]\n",
      "Step 520186  [5.404 sec/step, loss=0.07576, avg_loss=0.07436]\n",
      "Generated 32 batches of size 32 in 2.326 sec\n",
      "Step 520187  [5.428 sec/step, loss=0.07626, avg_loss=0.07447]\n",
      "Step 520188  [5.417 sec/step, loss=0.07251, avg_loss=0.07444]\n",
      "Step 520189  [5.406 sec/step, loss=0.07245, avg_loss=0.07440]\n",
      "Step 520190  [5.405 sec/step, loss=0.07219, avg_loss=0.07441]\n",
      "Step 520191  [5.416 sec/step, loss=0.07402, avg_loss=0.07438]\n",
      "Step 520192  [5.404 sec/step, loss=0.07008, avg_loss=0.07432]\n",
      "Step 520193  [5.403 sec/step, loss=0.07497, avg_loss=0.07430]\n",
      "Step 520194  [5.440 sec/step, loss=0.07347, avg_loss=0.07429]\n",
      "Step 520195  [5.444 sec/step, loss=0.07586, avg_loss=0.07429]\n",
      "Step 520196  [5.444 sec/step, loss=0.06651, avg_loss=0.07429]\n",
      "Step 520197  [5.449 sec/step, loss=0.07573, avg_loss=0.07428]\n",
      "Step 520198  [5.449 sec/step, loss=0.07179, avg_loss=0.07425]\n",
      "Step 520199  [5.459 sec/step, loss=0.07599, avg_loss=0.07429]\n",
      "Step 520200  [5.460 sec/step, loss=0.07546, avg_loss=0.07428]\n",
      "Writing summary at step: 520200\n",
      "Step 520201  [5.478 sec/step, loss=0.07662, avg_loss=0.07431]\n",
      "Step 520202  [5.479 sec/step, loss=0.07305, avg_loss=0.07429]\n",
      "Step 520203  [5.464 sec/step, loss=0.07518, avg_loss=0.07431]\n",
      "Step 520204  [5.468 sec/step, loss=0.07537, avg_loss=0.07432]\n",
      "Step 520205  [5.475 sec/step, loss=0.07621, avg_loss=0.07433]\n",
      "Step 520206  [5.458 sec/step, loss=0.07559, avg_loss=0.07432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 520207  [5.466 sec/step, loss=0.07279, avg_loss=0.07432]\n",
      "Step 520208  [5.457 sec/step, loss=0.07198, avg_loss=0.07428]\n",
      "Step 520209  [5.463 sec/step, loss=0.07507, avg_loss=0.07427]\n",
      "Step 520210  [5.461 sec/step, loss=0.07527, avg_loss=0.07426]\n",
      "Step 520211  [5.464 sec/step, loss=0.07633, avg_loss=0.07426]\n",
      "Step 520212  [5.459 sec/step, loss=0.07547, avg_loss=0.07427]\n",
      "Step 520213  [5.459 sec/step, loss=0.07664, avg_loss=0.07429]\n",
      "Step 520214  [5.463 sec/step, loss=0.07335, avg_loss=0.07428]\n",
      "Step 520215  [5.470 sec/step, loss=0.07421, avg_loss=0.07430]\n",
      "Step 520216  [5.452 sec/step, loss=0.07414, avg_loss=0.07427]\n",
      "Step 520217  [5.462 sec/step, loss=0.07670, avg_loss=0.07430]\n",
      "Generated 32 batches of size 32 in 2.434 sec\n",
      "Step 520218  [5.488 sec/step, loss=0.07232, avg_loss=0.07426]\n",
      "Step 520219  [5.477 sec/step, loss=0.07507, avg_loss=0.07425]\n",
      "Step 520220  [5.473 sec/step, loss=0.07279, avg_loss=0.07421]\n",
      "Step 520221  [5.466 sec/step, loss=0.07154, avg_loss=0.07417]\n",
      "Step 520222  [5.478 sec/step, loss=0.07548, avg_loss=0.07418]\n",
      "Step 520223  [5.464 sec/step, loss=0.07425, avg_loss=0.07417]\n",
      "Step 520224  [5.456 sec/step, loss=0.07503, avg_loss=0.07415]\n",
      "Step 520225  [5.468 sec/step, loss=0.07596, avg_loss=0.07419]\n",
      "Step 520226  [5.453 sec/step, loss=0.07415, avg_loss=0.07419]\n",
      "Step 520227  [5.453 sec/step, loss=0.07635, avg_loss=0.07419]\n",
      "Step 520228  [5.469 sec/step, loss=0.07644, avg_loss=0.07420]\n",
      "Step 520229  [5.484 sec/step, loss=0.07342, avg_loss=0.07427]\n",
      "Step 520230  [5.444 sec/step, loss=0.07460, avg_loss=0.07435]\n",
      "Step 520231  [5.426 sec/step, loss=0.07224, avg_loss=0.07433]\n",
      "Step 520232  [5.430 sec/step, loss=0.07564, avg_loss=0.07434]\n",
      "Step 520233  [5.452 sec/step, loss=0.07647, avg_loss=0.07438]\n",
      "Step 520234  [5.431 sec/step, loss=0.07619, avg_loss=0.07440]\n",
      "Step 520235  [5.385 sec/step, loss=0.07459, avg_loss=0.07448]\n",
      "Step 520236  [5.407 sec/step, loss=0.07388, avg_loss=0.07446]\n",
      "Step 520237  [5.407 sec/step, loss=0.07403, avg_loss=0.07445]\n",
      "Step 520238  [5.432 sec/step, loss=0.07576, avg_loss=0.07446]\n",
      "Step 520239  [5.428 sec/step, loss=0.07533, avg_loss=0.07445]\n",
      "Step 520240  [5.450 sec/step, loss=0.07679, avg_loss=0.07449]\n",
      "Step 520241  [5.451 sec/step, loss=0.07416, avg_loss=0.07451]\n",
      "Step 520242  [5.458 sec/step, loss=0.07273, avg_loss=0.07453]\n",
      "Step 520243  [5.449 sec/step, loss=0.07592, avg_loss=0.07452]\n",
      "Step 520244  [5.451 sec/step, loss=0.07465, avg_loss=0.07449]\n",
      "Step 520245  [5.433 sec/step, loss=0.07327, avg_loss=0.07446]\n",
      "Step 520246  [5.409 sec/step, loss=0.07124, avg_loss=0.07441]\n",
      "Step 520247  [5.425 sec/step, loss=0.07481, avg_loss=0.07448]\n",
      "Step 520248  [5.419 sec/step, loss=0.07500, avg_loss=0.07447]\n",
      "Step 520249  [5.400 sec/step, loss=0.06618, avg_loss=0.07437]\n",
      "Generated 32 batches of size 32 in 2.323 sec\n",
      "Step 520250  [5.408 sec/step, loss=0.07555, avg_loss=0.07437]\n",
      "Step 520251  [5.450 sec/step, loss=0.06729, avg_loss=0.07430]\n",
      "Step 520252  [5.445 sec/step, loss=0.07441, avg_loss=0.07429]\n",
      "Step 520253  [5.420 sec/step, loss=0.07458, avg_loss=0.07427]\n",
      "Step 520254  [5.433 sec/step, loss=0.07671, avg_loss=0.07430]\n",
      "Step 520255  [5.436 sec/step, loss=0.07665, avg_loss=0.07431]\n",
      "Step 520256  [5.437 sec/step, loss=0.07563, avg_loss=0.07433]\n",
      "Step 520257  [5.429 sec/step, loss=0.07556, avg_loss=0.07434]\n",
      "Step 520258  [5.418 sec/step, loss=0.07434, avg_loss=0.07433]\n",
      "Step 520259  [5.414 sec/step, loss=0.07590, avg_loss=0.07432]\n",
      "Step 520260  [5.420 sec/step, loss=0.07500, avg_loss=0.07430]\n",
      "Step 520261  [5.429 sec/step, loss=0.07200, avg_loss=0.07427]\n",
      "Step 520262  [5.447 sec/step, loss=0.07670, avg_loss=0.07429]\n",
      "Step 520263  [5.445 sec/step, loss=0.07288, avg_loss=0.07426]\n",
      "Step 520264  [5.396 sec/step, loss=0.07538, avg_loss=0.07435]\n",
      "Step 520265  [5.418 sec/step, loss=0.07656, avg_loss=0.07437]\n",
      "Step 520266  [5.396 sec/step, loss=0.07063, avg_loss=0.07431]\n",
      "Step 520267  [5.380 sec/step, loss=0.07371, avg_loss=0.07430]\n",
      "Step 520268  [5.379 sec/step, loss=0.07672, avg_loss=0.07430]\n",
      "Step 520269  [5.386 sec/step, loss=0.07674, avg_loss=0.07433]\n",
      "Step 520270  [5.388 sec/step, loss=0.07274, avg_loss=0.07430]\n",
      "Step 520271  [5.384 sec/step, loss=0.07420, avg_loss=0.07431]\n",
      "Step 520272  [5.409 sec/step, loss=0.07646, avg_loss=0.07442]\n",
      "Step 520273  [5.398 sec/step, loss=0.07462, avg_loss=0.07441]\n",
      "Step 520274  [5.416 sec/step, loss=0.07527, avg_loss=0.07444]\n",
      "Step 520275  [5.421 sec/step, loss=0.07584, avg_loss=0.07447]\n",
      "Step 520276  [5.420 sec/step, loss=0.07333, avg_loss=0.07446]\n",
      "Step 520277  [5.414 sec/step, loss=0.07474, avg_loss=0.07444]\n",
      "Step 520278  [5.398 sec/step, loss=0.07548, avg_loss=0.07443]\n",
      "Step 520279  [5.433 sec/step, loss=0.07442, avg_loss=0.07442]\n",
      "Step 520280  [5.475 sec/step, loss=0.06623, avg_loss=0.07432]\n",
      "Step 520281  [5.471 sec/step, loss=0.07412, avg_loss=0.07432]\n",
      "Generated 32 batches of size 32 in 2.448 sec\n",
      "Step 520282  [5.497 sec/step, loss=0.07663, avg_loss=0.07437]\n",
      "Step 520283  [5.483 sec/step, loss=0.06779, avg_loss=0.07430]\n",
      "Step 520284  [5.473 sec/step, loss=0.07545, avg_loss=0.07429]\n",
      "Step 520285  [5.466 sec/step, loss=0.07274, avg_loss=0.07426]\n",
      "Step 520286  [5.469 sec/step, loss=0.07626, avg_loss=0.07427]\n",
      "Step 520287  [5.484 sec/step, loss=0.07368, avg_loss=0.07424]\n",
      "Step 520288  [5.497 sec/step, loss=0.07475, avg_loss=0.07426]\n",
      "Step 520289  [5.504 sec/step, loss=0.07578, avg_loss=0.07430]\n",
      "Step 520290  [5.501 sec/step, loss=0.07213, avg_loss=0.07430]\n",
      "Step 520291  [5.476 sec/step, loss=0.07360, avg_loss=0.07429]\n",
      "Step 520292  [5.481 sec/step, loss=0.07563, avg_loss=0.07435]\n",
      "Step 520293  [5.460 sec/step, loss=0.07117, avg_loss=0.07431]\n",
      "Step 520294  [5.446 sec/step, loss=0.07709, avg_loss=0.07435]\n",
      "Step 520295  [5.465 sec/step, loss=0.07367, avg_loss=0.07432]\n",
      "Step 520296  [5.414 sec/step, loss=0.07204, avg_loss=0.07438]\n",
      "Step 520297  [5.406 sec/step, loss=0.07670, avg_loss=0.07439]\n",
      "Step 520298  [5.409 sec/step, loss=0.07408, avg_loss=0.07441]\n",
      "Step 520299  [5.403 sec/step, loss=0.07438, avg_loss=0.07440]\n",
      "Step 520300  [5.399 sec/step, loss=0.07478, avg_loss=0.07439]\n",
      "Writing summary at step: 520300\n",
      "Step 520301  [5.394 sec/step, loss=0.07592, avg_loss=0.07438]\n",
      "Step 520302  [5.382 sec/step, loss=0.07459, avg_loss=0.07440]\n",
      "Step 520303  [5.390 sec/step, loss=0.07564, avg_loss=0.07440]\n",
      "Step 520304  [5.373 sec/step, loss=0.07172, avg_loss=0.07437]\n",
      "Step 520305  [5.362 sec/step, loss=0.07422, avg_loss=0.07435]\n",
      "Step 520306  [5.369 sec/step, loss=0.07526, avg_loss=0.07434]\n",
      "Step 520307  [5.363 sec/step, loss=0.07608, avg_loss=0.07438]\n",
      "Step 520308  [5.388 sec/step, loss=0.07493, avg_loss=0.07441]\n",
      "Step 520309  [5.386 sec/step, loss=0.07670, avg_loss=0.07442]\n",
      "Step 520310  [5.383 sec/step, loss=0.07591, avg_loss=0.07443]\n",
      "Step 520311  [5.430 sec/step, loss=0.06826, avg_loss=0.07435]\n",
      "Step 520312  [5.410 sec/step, loss=0.06532, avg_loss=0.07425]\n",
      "Generated 32 batches of size 32 in 2.388 sec\n",
      "Step 520313  [5.413 sec/step, loss=0.07495, avg_loss=0.07423]\n",
      "Step 520314  [5.415 sec/step, loss=0.07169, avg_loss=0.07421]\n",
      "Step 520315  [5.425 sec/step, loss=0.07588, avg_loss=0.07423]\n",
      "Step 520316  [5.442 sec/step, loss=0.07638, avg_loss=0.07425]\n",
      "Step 520317  [5.447 sec/step, loss=0.07562, avg_loss=0.07424]\n",
      "Step 520318  [5.418 sec/step, loss=0.07542, avg_loss=0.07427]\n",
      "Step 520319  [5.425 sec/step, loss=0.07443, avg_loss=0.07427]\n",
      "Step 520320  [5.432 sec/step, loss=0.07617, avg_loss=0.07430]\n",
      "Step 520321  [5.455 sec/step, loss=0.07608, avg_loss=0.07434]\n",
      "Step 520322  [5.455 sec/step, loss=0.07503, avg_loss=0.07434]\n",
      "Step 520323  [5.477 sec/step, loss=0.07633, avg_loss=0.07436]\n",
      "Step 520324  [5.484 sec/step, loss=0.07640, avg_loss=0.07437]\n",
      "Step 520325  [5.492 sec/step, loss=0.07526, avg_loss=0.07437]\n",
      "Step 520326  [5.501 sec/step, loss=0.07627, avg_loss=0.07439]\n",
      "Step 520327  [5.485 sec/step, loss=0.07579, avg_loss=0.07438]\n",
      "Step 520328  [5.463 sec/step, loss=0.07234, avg_loss=0.07434]\n",
      "Step 520329  [5.470 sec/step, loss=0.07558, avg_loss=0.07436]\n",
      "Step 520330  [5.463 sec/step, loss=0.07617, avg_loss=0.07438]\n",
      "Step 520331  [5.467 sec/step, loss=0.07449, avg_loss=0.07440]\n",
      "Step 520332  [5.471 sec/step, loss=0.07488, avg_loss=0.07439]\n",
      "Step 520333  [5.463 sec/step, loss=0.07132, avg_loss=0.07434]\n",
      "Step 520334  [5.457 sec/step, loss=0.07488, avg_loss=0.07433]\n",
      "Step 520335  [5.448 sec/step, loss=0.07226, avg_loss=0.07431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 520336  [5.441 sec/step, loss=0.07648, avg_loss=0.07433]\n",
      "Step 520337  [5.467 sec/step, loss=0.07676, avg_loss=0.07436]\n",
      "Step 520338  [5.435 sec/step, loss=0.07492, avg_loss=0.07435]\n",
      "Step 520339  [5.447 sec/step, loss=0.07592, avg_loss=0.07436]\n",
      "Step 520340  [5.436 sec/step, loss=0.07548, avg_loss=0.07434]\n",
      "Step 520341  [5.438 sec/step, loss=0.07545, avg_loss=0.07436]\n",
      "Step 520342  [5.466 sec/step, loss=0.07587, avg_loss=0.07439]\n",
      "Step 520343  [5.462 sec/step, loss=0.07384, avg_loss=0.07437]\n",
      "Step 520344  [5.445 sec/step, loss=0.07425, avg_loss=0.07436]\n",
      "Generated 32 batches of size 32 in 2.878 sec\n",
      "Step 520345  [5.436 sec/step, loss=0.06744, avg_loss=0.07431]\n",
      "Step 520346  [5.450 sec/step, loss=0.07520, avg_loss=0.07434]\n",
      "Step 520347  [5.459 sec/step, loss=0.07731, avg_loss=0.07437]\n",
      "Step 520348  [5.444 sec/step, loss=0.07167, avg_loss=0.07434]\n",
      "Step 520349  [5.462 sec/step, loss=0.07371, avg_loss=0.07441]\n",
      "Step 520350  [5.507 sec/step, loss=0.06537, avg_loss=0.07431]\n",
      "Step 520351  [5.471 sec/step, loss=0.07475, avg_loss=0.07438]\n",
      "Step 520352  [5.483 sec/step, loss=0.07398, avg_loss=0.07438]\n",
      "Step 520353  [5.509 sec/step, loss=0.07579, avg_loss=0.07439]\n",
      "Step 520354  [5.510 sec/step, loss=0.07718, avg_loss=0.07440]\n",
      "Step 520355  [5.554 sec/step, loss=0.06709, avg_loss=0.07430]\n",
      "Step 520356  [5.535 sec/step, loss=0.07129, avg_loss=0.07426]\n",
      "Step 520357  [5.532 sec/step, loss=0.07173, avg_loss=0.07422]\n",
      "Step 520358  [5.528 sec/step, loss=0.07104, avg_loss=0.07419]\n",
      "Step 520359  [5.531 sec/step, loss=0.07614, avg_loss=0.07419]\n",
      "Step 520360  [5.532 sec/step, loss=0.07634, avg_loss=0.07420]\n",
      "Step 520361  [5.523 sec/step, loss=0.07388, avg_loss=0.07422]\n",
      "Step 520362  [5.515 sec/step, loss=0.07564, avg_loss=0.07421]\n",
      "Step 520363  [5.523 sec/step, loss=0.07643, avg_loss=0.07425]\n",
      "Step 520364  [5.526 sec/step, loss=0.07536, avg_loss=0.07425]\n",
      "Step 520365  [5.515 sec/step, loss=0.07519, avg_loss=0.07423]\n",
      "Step 520366  [5.551 sec/step, loss=0.07383, avg_loss=0.07426]\n",
      "Step 520367  [5.545 sec/step, loss=0.07422, avg_loss=0.07427]\n",
      "Step 520368  [5.533 sec/step, loss=0.07401, avg_loss=0.07424]\n",
      "Step 520369  [5.541 sec/step, loss=0.07607, avg_loss=0.07424]\n",
      "Step 520370  [5.554 sec/step, loss=0.07588, avg_loss=0.07427]\n",
      "Step 520371  [5.571 sec/step, loss=0.07358, avg_loss=0.07426]\n",
      "Step 520372  [5.565 sec/step, loss=0.07193, avg_loss=0.07422]\n",
      "Step 520373  [5.565 sec/step, loss=0.07360, avg_loss=0.07421]\n",
      "Step 520374  [5.571 sec/step, loss=0.07599, avg_loss=0.07421]\n",
      "Step 520375  [5.568 sec/step, loss=0.07554, avg_loss=0.07421]\n",
      "Step 520376  [5.564 sec/step, loss=0.07623, avg_loss=0.07424]\n",
      "Generated 32 batches of size 32 in 2.477 sec\n",
      "Step 520377  [5.583 sec/step, loss=0.07662, avg_loss=0.07426]\n",
      "Step 520378  [5.585 sec/step, loss=0.07165, avg_loss=0.07422]\n",
      "Step 520379  [5.561 sec/step, loss=0.07642, avg_loss=0.07424]\n",
      "Step 520380  [5.517 sec/step, loss=0.07229, avg_loss=0.07430]\n",
      "Step 520381  [5.522 sec/step, loss=0.07519, avg_loss=0.07431]\n",
      "Step 520382  [5.506 sec/step, loss=0.07467, avg_loss=0.07429]\n",
      "Step 520383  [5.532 sec/step, loss=0.07533, avg_loss=0.07437]\n",
      "Step 520384  [5.515 sec/step, loss=0.06586, avg_loss=0.07427]\n",
      "Step 520385  [5.518 sec/step, loss=0.07490, avg_loss=0.07429]\n",
      "Step 520386  [5.512 sec/step, loss=0.07612, avg_loss=0.07429]\n",
      "Step 520387  [5.487 sec/step, loss=0.07354, avg_loss=0.07429]\n",
      "Step 520388  [5.469 sec/step, loss=0.07433, avg_loss=0.07429]\n",
      "Step 520389  [5.439 sec/step, loss=0.06825, avg_loss=0.07421]\n",
      "Step 520390  [5.455 sec/step, loss=0.07628, avg_loss=0.07425]\n",
      "Step 520391  [5.451 sec/step, loss=0.07299, avg_loss=0.07425]\n",
      "Step 520392  [5.461 sec/step, loss=0.07562, avg_loss=0.07425]\n",
      "Step 520393  [5.457 sec/step, loss=0.07233, avg_loss=0.07426]\n",
      "Step 520394  [5.450 sec/step, loss=0.07555, avg_loss=0.07424]\n",
      "Step 520395  [5.416 sec/step, loss=0.07443, avg_loss=0.07425]\n",
      "Step 520396  [5.429 sec/step, loss=0.07489, avg_loss=0.07428]\n",
      "Step 520397  [5.425 sec/step, loss=0.07591, avg_loss=0.07427]\n",
      "Step 520398  [5.455 sec/step, loss=0.07598, avg_loss=0.07429]\n",
      "Step 520399  [5.481 sec/step, loss=0.07630, avg_loss=0.07431]\n",
      "Step 520400  [5.477 sec/step, loss=0.07586, avg_loss=0.07432]\n",
      "Writing summary at step: 520400\n",
      "Step 520401  [5.470 sec/step, loss=0.07182, avg_loss=0.07428]\n",
      "Step 520402  [5.474 sec/step, loss=0.07519, avg_loss=0.07428]\n",
      "Step 520403  [5.473 sec/step, loss=0.07683, avg_loss=0.07430]\n",
      "Step 520404  [5.486 sec/step, loss=0.07511, avg_loss=0.07433]\n",
      "Step 520405  [5.534 sec/step, loss=0.06986, avg_loss=0.07429]\n",
      "Step 520406  [5.539 sec/step, loss=0.07720, avg_loss=0.07430]\n",
      "Step 520407  [5.554 sec/step, loss=0.07484, avg_loss=0.07429]\n",
      "Generated 32 batches of size 32 in 2.434 sec\n",
      "Step 520408  [5.551 sec/step, loss=0.07582, avg_loss=0.07430]\n",
      "Step 520409  [5.551 sec/step, loss=0.07679, avg_loss=0.07430]\n",
      "Step 520410  [5.543 sec/step, loss=0.07553, avg_loss=0.07430]\n",
      "Step 520411  [5.481 sec/step, loss=0.07471, avg_loss=0.07436]\n",
      "Step 520412  [5.505 sec/step, loss=0.07464, avg_loss=0.07446]\n",
      "Step 520413  [5.494 sec/step, loss=0.07584, avg_loss=0.07447]\n",
      "Step 520414  [5.492 sec/step, loss=0.07321, avg_loss=0.07448]\n",
      "Step 520415  [5.503 sec/step, loss=0.07607, avg_loss=0.07448]\n",
      "Step 520416  [5.505 sec/step, loss=0.07663, avg_loss=0.07448]\n",
      "Step 520417  [5.502 sec/step, loss=0.07503, avg_loss=0.07448]\n",
      "Step 520418  [5.492 sec/step, loss=0.07422, avg_loss=0.07447]\n",
      "Step 520419  [5.491 sec/step, loss=0.07321, avg_loss=0.07445]\n",
      "Step 520420  [5.488 sec/step, loss=0.07387, avg_loss=0.07443]\n",
      "Step 520421  [5.503 sec/step, loss=0.07434, avg_loss=0.07441]\n",
      "Step 520422  [5.523 sec/step, loss=0.07401, avg_loss=0.07440]\n",
      "Step 520423  [5.511 sec/step, loss=0.07521, avg_loss=0.07439]\n",
      "Step 520424  [5.503 sec/step, loss=0.07116, avg_loss=0.07434]\n",
      "Step 520425  [5.499 sec/step, loss=0.07530, avg_loss=0.07434]\n",
      "Step 520426  [5.479 sec/step, loss=0.06667, avg_loss=0.07425]\n",
      "Step 520427  [5.488 sec/step, loss=0.07625, avg_loss=0.07425]\n",
      "Step 520428  [5.492 sec/step, loss=0.07446, avg_loss=0.07427]\n",
      "Step 520429  [5.489 sec/step, loss=0.07303, avg_loss=0.07425]\n",
      "Step 520430  [5.496 sec/step, loss=0.07657, avg_loss=0.07425]\n",
      "Step 520431  [5.491 sec/step, loss=0.07204, avg_loss=0.07422]\n",
      "Step 520432  [5.496 sec/step, loss=0.07640, avg_loss=0.07424]\n",
      "Step 520433  [5.495 sec/step, loss=0.07579, avg_loss=0.07428]\n",
      "Step 520434  [5.498 sec/step, loss=0.07603, avg_loss=0.07430]\n",
      "Step 520435  [5.492 sec/step, loss=0.07134, avg_loss=0.07429]\n",
      "Step 520436  [5.484 sec/step, loss=0.07560, avg_loss=0.07428]\n",
      "Step 520437  [5.472 sec/step, loss=0.07602, avg_loss=0.07427]\n",
      "Step 520438  [5.475 sec/step, loss=0.07446, avg_loss=0.07427]\n",
      "Step 520439  [5.480 sec/step, loss=0.07723, avg_loss=0.07428]\n",
      "Generated 32 batches of size 32 in 2.583 sec\n",
      "Step 520440  [5.478 sec/step, loss=0.07584, avg_loss=0.07428]\n",
      "Step 520441  [5.492 sec/step, loss=0.07666, avg_loss=0.07430]\n",
      "Step 520442  [5.462 sec/step, loss=0.07316, avg_loss=0.07427]\n",
      "Step 520443  [5.462 sec/step, loss=0.07447, avg_loss=0.07427]\n",
      "Step 520444  [5.472 sec/step, loss=0.07644, avg_loss=0.07430]\n",
      "Step 520445  [5.498 sec/step, loss=0.07458, avg_loss=0.07437]\n",
      "Step 520446  [5.546 sec/step, loss=0.06807, avg_loss=0.07430]\n",
      "Step 520447  [5.552 sec/step, loss=0.07687, avg_loss=0.07429]\n",
      "Step 520448  [5.572 sec/step, loss=0.07706, avg_loss=0.07435]\n",
      "Step 520449  [5.577 sec/step, loss=0.07520, avg_loss=0.07436]\n",
      "Step 520450  [5.538 sec/step, loss=0.07624, avg_loss=0.07447]\n",
      "Step 520451  [5.528 sec/step, loss=0.07577, avg_loss=0.07448]\n",
      "Step 520452  [5.525 sec/step, loss=0.07618, avg_loss=0.07450]\n",
      "Step 520453  [5.498 sec/step, loss=0.07418, avg_loss=0.07449]\n",
      "Step 520454  [5.481 sec/step, loss=0.07337, avg_loss=0.07445]\n",
      "Step 520455  [5.432 sec/step, loss=0.07540, avg_loss=0.07453]\n",
      "Step 520456  [5.497 sec/step, loss=0.06690, avg_loss=0.07449]\n",
      "Step 520457  [5.498 sec/step, loss=0.07516, avg_loss=0.07452]\n",
      "Step 520458  [5.518 sec/step, loss=0.07621, avg_loss=0.07457]\n",
      "Step 520459  [5.510 sec/step, loss=0.07495, avg_loss=0.07456]\n",
      "Step 520460  [5.493 sec/step, loss=0.07377, avg_loss=0.07453]\n",
      "Step 520461  [5.500 sec/step, loss=0.07683, avg_loss=0.07456]\n",
      "Step 520462  [5.482 sec/step, loss=0.06594, avg_loss=0.07447]\n",
      "Step 520463  [5.494 sec/step, loss=0.07546, avg_loss=0.07446]\n",
      "Step 520464  [5.490 sec/step, loss=0.07436, avg_loss=0.07445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 520465  [5.502 sec/step, loss=0.07488, avg_loss=0.07444]\n",
      "Step 520466  [5.471 sec/step, loss=0.07577, avg_loss=0.07446]\n",
      "Step 520467  [5.510 sec/step, loss=0.07355, avg_loss=0.07446]\n",
      "Step 520468  [5.521 sec/step, loss=0.07690, avg_loss=0.07449]\n",
      "Step 520469  [5.521 sec/step, loss=0.07528, avg_loss=0.07448]\n",
      "Step 520470  [5.491 sec/step, loss=0.07490, avg_loss=0.07447]\n",
      "Step 520471  [5.486 sec/step, loss=0.07443, avg_loss=0.07448]\n",
      "Generated 32 batches of size 32 in 2.507 sec\n",
      "Step 520472  [5.481 sec/step, loss=0.07493, avg_loss=0.07451]\n",
      "Step 520473  [5.494 sec/step, loss=0.07568, avg_loss=0.07453]\n",
      "Step 520474  [5.473 sec/step, loss=0.07144, avg_loss=0.07448]\n",
      "Step 520475  [5.454 sec/step, loss=0.07184, avg_loss=0.07445]\n",
      "Step 520476  [5.453 sec/step, loss=0.07386, avg_loss=0.07442]\n",
      "Step 520477  [5.447 sec/step, loss=0.07539, avg_loss=0.07441]\n",
      "Step 520478  [5.462 sec/step, loss=0.07664, avg_loss=0.07446]\n",
      "Step 520479  [5.456 sec/step, loss=0.07593, avg_loss=0.07445]\n",
      "Step 520480  [5.458 sec/step, loss=0.07414, avg_loss=0.07447]\n",
      "Step 520481  [5.468 sec/step, loss=0.07566, avg_loss=0.07448]\n",
      "Step 520482  [5.472 sec/step, loss=0.07565, avg_loss=0.07449]\n",
      "Step 520483  [5.466 sec/step, loss=0.07623, avg_loss=0.07450]\n",
      "Step 520484  [5.481 sec/step, loss=0.07377, avg_loss=0.07458]\n",
      "Step 520485  [5.486 sec/step, loss=0.07551, avg_loss=0.07458]\n",
      "Step 520486  [5.475 sec/step, loss=0.07228, avg_loss=0.07454]\n",
      "Step 520487  [5.476 sec/step, loss=0.07536, avg_loss=0.07456]\n",
      "Step 520488  [5.496 sec/step, loss=0.07457, avg_loss=0.07456]\n",
      "Step 520489  [5.506 sec/step, loss=0.07321, avg_loss=0.07461]\n",
      "Step 520490  [5.514 sec/step, loss=0.07681, avg_loss=0.07462]\n",
      "Step 520491  [5.531 sec/step, loss=0.07367, avg_loss=0.07463]\n",
      "Step 520492  [5.512 sec/step, loss=0.07441, avg_loss=0.07461]\n",
      "Step 520493  [5.516 sec/step, loss=0.07119, avg_loss=0.07460]\n",
      "Step 520494  [5.525 sec/step, loss=0.07669, avg_loss=0.07461]\n",
      "Step 520495  [5.528 sec/step, loss=0.07444, avg_loss=0.07461]\n",
      "Step 520496  [5.509 sec/step, loss=0.07463, avg_loss=0.07461]\n",
      "Step 520497  [5.509 sec/step, loss=0.07617, avg_loss=0.07461]\n",
      "Step 520498  [5.504 sec/step, loss=0.07352, avg_loss=0.07459]\n",
      "Step 520499  [5.501 sec/step, loss=0.07661, avg_loss=0.07459]\n",
      "Step 520500  [5.500 sec/step, loss=0.07236, avg_loss=0.07456]\n",
      "Writing summary at step: 520500\n",
      "Step 520501  [5.515 sec/step, loss=0.07732, avg_loss=0.07461]\n",
      "Step 520502  [5.532 sec/step, loss=0.07467, avg_loss=0.07461]\n",
      "Generated 32 batches of size 32 in 2.366 sec\n",
      "Step 520503  [5.524 sec/step, loss=0.07562, avg_loss=0.07460]\n",
      "Step 520504  [5.577 sec/step, loss=0.06650, avg_loss=0.07451]\n",
      "Step 520505  [5.543 sec/step, loss=0.07666, avg_loss=0.07458]\n",
      "Step 520506  [5.537 sec/step, loss=0.07440, avg_loss=0.07455]\n",
      "Step 520507  [5.523 sec/step, loss=0.07261, avg_loss=0.07453]\n",
      "Step 520508  [5.517 sec/step, loss=0.07519, avg_loss=0.07452]\n",
      "Step 520509  [5.510 sec/step, loss=0.07619, avg_loss=0.07451]\n",
      "Step 520510  [5.531 sec/step, loss=0.07454, avg_loss=0.07450]\n",
      "Step 520511  [5.543 sec/step, loss=0.07501, avg_loss=0.07451]\n",
      "Step 520512  [5.536 sec/step, loss=0.07520, avg_loss=0.07451]\n",
      "Step 520513  [5.541 sec/step, loss=0.07538, avg_loss=0.07451]\n",
      "Step 520514  [5.527 sec/step, loss=0.07279, avg_loss=0.07450]\n",
      "Step 520515  [5.536 sec/step, loss=0.07566, avg_loss=0.07450]\n",
      "Step 520516  [5.531 sec/step, loss=0.07425, avg_loss=0.07448]\n",
      "Step 520517  [5.531 sec/step, loss=0.07480, avg_loss=0.07447]\n",
      "Step 520518  [5.540 sec/step, loss=0.07623, avg_loss=0.07449]\n",
      "Step 520519  [5.531 sec/step, loss=0.07435, avg_loss=0.07451]\n",
      "Step 520520  [5.534 sec/step, loss=0.07589, avg_loss=0.07453]\n",
      "Step 520521  [5.509 sec/step, loss=0.07517, avg_loss=0.07453]\n",
      "Step 520522  [5.472 sec/step, loss=0.06619, avg_loss=0.07446]\n",
      "Step 520523  [5.479 sec/step, loss=0.07691, avg_loss=0.07447]\n",
      "Step 520524  [5.489 sec/step, loss=0.07686, avg_loss=0.07453]\n",
      "Step 520525  [5.489 sec/step, loss=0.07524, avg_loss=0.07453]\n",
      "Step 520526  [5.518 sec/step, loss=0.07605, avg_loss=0.07462]\n",
      "Step 520527  [5.528 sec/step, loss=0.07526, avg_loss=0.07461]\n",
      "Step 520528  [5.546 sec/step, loss=0.07643, avg_loss=0.07463]\n",
      "Step 520529  [5.535 sec/step, loss=0.07423, avg_loss=0.07464]\n",
      "Step 520530  [5.516 sec/step, loss=0.07335, avg_loss=0.07461]\n",
      "Step 520531  [5.529 sec/step, loss=0.07542, avg_loss=0.07465]\n",
      "Step 520532  [5.517 sec/step, loss=0.07294, avg_loss=0.07461]\n",
      "Step 520533  [5.510 sec/step, loss=0.07144, avg_loss=0.07457]\n",
      "Step 520534  [5.520 sec/step, loss=0.07646, avg_loss=0.07457]\n",
      "Generated 32 batches of size 32 in 2.430 sec\n",
      "Step 520535  [5.554 sec/step, loss=0.07550, avg_loss=0.07461]\n",
      "Step 520536  [5.561 sec/step, loss=0.07599, avg_loss=0.07462]\n",
      "Step 520537  [5.552 sec/step, loss=0.07484, avg_loss=0.07461]\n",
      "Step 520538  [5.555 sec/step, loss=0.07543, avg_loss=0.07462]\n",
      "Step 520539  [5.550 sec/step, loss=0.07526, avg_loss=0.07460]\n",
      "Step 520540  [5.548 sec/step, loss=0.07470, avg_loss=0.07458]\n",
      "Step 520541  [5.587 sec/step, loss=0.06669, avg_loss=0.07449]\n",
      "Step 520542  [5.589 sec/step, loss=0.07352, avg_loss=0.07449]\n",
      "Step 520543  [5.599 sec/step, loss=0.07203, avg_loss=0.07446]\n",
      "Step 520544  [5.596 sec/step, loss=0.07574, avg_loss=0.07446]\n",
      "Step 520545  [5.595 sec/step, loss=0.07442, avg_loss=0.07446]\n",
      "Step 520546  [5.595 sec/step, loss=0.06782, avg_loss=0.07445]\n",
      "Step 520547  [5.577 sec/step, loss=0.07380, avg_loss=0.07442]\n",
      "Step 520548  [5.571 sec/step, loss=0.07312, avg_loss=0.07438]\n",
      "Step 520549  [5.554 sec/step, loss=0.07099, avg_loss=0.07434]\n",
      "Step 520550  [5.545 sec/step, loss=0.07462, avg_loss=0.07433]\n",
      "Step 520551  [5.563 sec/step, loss=0.07328, avg_loss=0.07430]\n",
      "Step 520552  [5.556 sec/step, loss=0.07584, avg_loss=0.07430]\n",
      "Step 520553  [5.593 sec/step, loss=0.07350, avg_loss=0.07429]\n",
      "Step 520554  [5.589 sec/step, loss=0.07433, avg_loss=0.07430]\n",
      "Step 520555  [5.573 sec/step, loss=0.07263, avg_loss=0.07427]\n",
      "Step 520556  [5.529 sec/step, loss=0.07637, avg_loss=0.07437]\n",
      "Step 520557  [5.524 sec/step, loss=0.07390, avg_loss=0.07435]\n",
      "Step 520558  [5.497 sec/step, loss=0.06732, avg_loss=0.07427]\n",
      "Step 520559  [5.501 sec/step, loss=0.07589, avg_loss=0.07427]\n",
      "Step 520560  [5.502 sec/step, loss=0.07346, avg_loss=0.07427]\n",
      "Step 520561  [5.515 sec/step, loss=0.07663, avg_loss=0.07427]\n",
      "Step 520562  [5.543 sec/step, loss=0.07638, avg_loss=0.07437]\n",
      "Step 520563  [5.517 sec/step, loss=0.07443, avg_loss=0.07436]\n",
      "Step 520564  [5.518 sec/step, loss=0.07474, avg_loss=0.07437]\n",
      "Step 520565  [5.508 sec/step, loss=0.07281, avg_loss=0.07435]\n",
      "Step 520566  [5.526 sec/step, loss=0.07642, avg_loss=0.07435]\n",
      "Generated 32 batches of size 32 in 2.367 sec\n",
      "Step 520567  [5.508 sec/step, loss=0.07285, avg_loss=0.07435]\n",
      "Step 520568  [5.509 sec/step, loss=0.07615, avg_loss=0.07434]\n",
      "Step 520569  [5.512 sec/step, loss=0.07418, avg_loss=0.07433]\n",
      "Step 520570  [5.519 sec/step, loss=0.07562, avg_loss=0.07433]\n",
      "Step 520571  [5.511 sec/step, loss=0.07558, avg_loss=0.07435]\n",
      "Step 520572  [5.520 sec/step, loss=0.07503, avg_loss=0.07435]\n",
      "Step 520573  [5.525 sec/step, loss=0.07687, avg_loss=0.07436]\n",
      "Step 520574  [5.546 sec/step, loss=0.07620, avg_loss=0.07441]\n",
      "Step 520575  [5.550 sec/step, loss=0.07465, avg_loss=0.07443]\n",
      "Step 520576  [5.554 sec/step, loss=0.07435, avg_loss=0.07444]\n",
      "Step 520577  [5.543 sec/step, loss=0.07579, avg_loss=0.07444]\n",
      "Step 520578  [5.544 sec/step, loss=0.07447, avg_loss=0.07442]\n",
      "Step 520579  [5.535 sec/step, loss=0.07417, avg_loss=0.07440]\n",
      "Step 520580  [5.524 sec/step, loss=0.07337, avg_loss=0.07440]\n",
      "Step 520581  [5.520 sec/step, loss=0.07573, avg_loss=0.07440]\n",
      "Step 520582  [5.510 sec/step, loss=0.07247, avg_loss=0.07437]\n",
      "Step 520583  [5.508 sec/step, loss=0.07212, avg_loss=0.07432]\n",
      "Step 520584  [5.513 sec/step, loss=0.07439, avg_loss=0.07433]\n",
      "Step 520585  [5.527 sec/step, loss=0.07662, avg_loss=0.07434]\n",
      "Step 520586  [5.543 sec/step, loss=0.07559, avg_loss=0.07437]\n",
      "Step 520587  [5.595 sec/step, loss=0.06659, avg_loss=0.07429]\n",
      "Step 520588  [5.576 sec/step, loss=0.07481, avg_loss=0.07429]\n",
      "Step 520589  [5.573 sec/step, loss=0.07082, avg_loss=0.07427]\n",
      "Step 520590  [5.569 sec/step, loss=0.07550, avg_loss=0.07425]\n",
      "Step 520591  [5.574 sec/step, loss=0.07563, avg_loss=0.07427]\n",
      "Step 520592  [5.570 sec/step, loss=0.07149, avg_loss=0.07424]\n",
      "Step 520593  [5.573 sec/step, loss=0.07273, avg_loss=0.07426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 520594  [5.566 sec/step, loss=0.07647, avg_loss=0.07426]\n",
      "Step 520595  [5.576 sec/step, loss=0.07511, avg_loss=0.07426]\n",
      "Step 520596  [5.610 sec/step, loss=0.07280, avg_loss=0.07424]\n",
      "Step 520597  [5.586 sec/step, loss=0.06573, avg_loss=0.07414]\n",
      "Step 520598  [5.559 sec/step, loss=0.07138, avg_loss=0.07412]\n",
      "Generated 32 batches of size 32 in 2.376 sec\n",
      "Step 520599  [5.565 sec/step, loss=0.07636, avg_loss=0.07412]\n",
      "Step 520600  [5.580 sec/step, loss=0.07518, avg_loss=0.07414]\n",
      "Writing summary at step: 520600\n",
      "Step 520601  [5.581 sec/step, loss=0.07644, avg_loss=0.07414]\n",
      "Step 520602  [5.580 sec/step, loss=0.07453, avg_loss=0.07413]\n",
      "Step 520603  [5.567 sec/step, loss=0.07588, avg_loss=0.07414]\n",
      "Step 520604  [5.513 sec/step, loss=0.07554, avg_loss=0.07423]\n",
      "Step 520605  [5.512 sec/step, loss=0.07698, avg_loss=0.07423]\n",
      "Step 520606  [5.529 sec/step, loss=0.07618, avg_loss=0.07425]\n",
      "Step 520607  [5.525 sec/step, loss=0.07365, avg_loss=0.07426]\n",
      "Step 520608  [5.537 sec/step, loss=0.07633, avg_loss=0.07427]\n",
      "Step 520609  [5.538 sec/step, loss=0.07604, avg_loss=0.07427]\n",
      "Step 520610  [5.529 sec/step, loss=0.07633, avg_loss=0.07429]\n",
      "Step 520611  [5.523 sec/step, loss=0.07359, avg_loss=0.07427]\n",
      "Step 520612  [5.515 sec/step, loss=0.07169, avg_loss=0.07424]\n",
      "Step 520613  [5.521 sec/step, loss=0.07305, avg_loss=0.07421]\n",
      "Step 520614  [5.526 sec/step, loss=0.07111, avg_loss=0.07420]\n",
      "Step 520615  [5.501 sec/step, loss=0.07542, avg_loss=0.07419]\n",
      "Step 520616  [5.524 sec/step, loss=0.07402, avg_loss=0.07419]\n",
      "Step 520617  [5.516 sec/step, loss=0.07662, avg_loss=0.07421]\n",
      "Step 520618  [5.520 sec/step, loss=0.07488, avg_loss=0.07420]\n",
      "Step 520619  [5.522 sec/step, loss=0.07575, avg_loss=0.07421]\n",
      "Step 520620  [5.511 sec/step, loss=0.07432, avg_loss=0.07419]\n",
      "Step 520621  [5.518 sec/step, loss=0.07486, avg_loss=0.07419]\n",
      "Step 520622  [5.527 sec/step, loss=0.07461, avg_loss=0.07428]\n",
      "Step 520623  [5.526 sec/step, loss=0.07525, avg_loss=0.07426]\n",
      "Step 520624  [5.521 sec/step, loss=0.07532, avg_loss=0.07424]\n",
      "Step 520625  [5.501 sec/step, loss=0.07194, avg_loss=0.07421]\n",
      "Step 520626  [5.489 sec/step, loss=0.07521, avg_loss=0.07420]\n",
      "Step 520627  [5.475 sec/step, loss=0.07537, avg_loss=0.07420]\n",
      "Step 520628  [5.476 sec/step, loss=0.07727, avg_loss=0.07421]\n",
      "Step 520629  [5.483 sec/step, loss=0.07278, avg_loss=0.07420]\n",
      "Generated 32 batches of size 32 in 2.443 sec\n",
      "Step 520630  [5.506 sec/step, loss=0.07686, avg_loss=0.07423]\n",
      "Step 520631  [5.559 sec/step, loss=0.06694, avg_loss=0.07415]\n",
      "Step 520632  [5.546 sec/step, loss=0.06775, avg_loss=0.07410]\n",
      "Step 520633  [5.567 sec/step, loss=0.07718, avg_loss=0.07415]\n",
      "Step 520634  [5.560 sec/step, loss=0.07495, avg_loss=0.07414]\n",
      "Step 520635  [5.537 sec/step, loss=0.07423, avg_loss=0.07413]\n",
      "Step 520636  [5.537 sec/step, loss=0.07643, avg_loss=0.07413]\n",
      "Step 520637  [5.545 sec/step, loss=0.07421, avg_loss=0.07412]\n",
      "Step 520638  [5.566 sec/step, loss=0.07363, avg_loss=0.07411]\n",
      "Step 520639  [5.557 sec/step, loss=0.07440, avg_loss=0.07410]\n",
      "Step 520640  [5.551 sec/step, loss=0.07349, avg_loss=0.07409]\n",
      "Step 520641  [5.497 sec/step, loss=0.07257, avg_loss=0.07414]\n",
      "Step 520642  [5.508 sec/step, loss=0.07479, avg_loss=0.07416]\n",
      "Step 520643  [5.508 sec/step, loss=0.07510, avg_loss=0.07419]\n",
      "Step 520644  [5.535 sec/step, loss=0.07337, avg_loss=0.07416]\n",
      "Step 520645  [5.507 sec/step, loss=0.06604, avg_loss=0.07408]\n",
      "Step 520646  [5.461 sec/step, loss=0.07596, avg_loss=0.07416]\n",
      "Step 520647  [5.480 sec/step, loss=0.07701, avg_loss=0.07419]\n",
      "Step 520648  [5.493 sec/step, loss=0.07673, avg_loss=0.07423]\n",
      "Step 520649  [5.511 sec/step, loss=0.07479, avg_loss=0.07427]\n",
      "Step 520650  [5.521 sec/step, loss=0.07425, avg_loss=0.07426]\n",
      "Step 520651  [5.489 sec/step, loss=0.07162, avg_loss=0.07425]\n",
      "Step 520652  [5.486 sec/step, loss=0.07215, avg_loss=0.07421]\n",
      "Step 520653  [5.453 sec/step, loss=0.07467, avg_loss=0.07422]\n",
      "Step 520654  [5.474 sec/step, loss=0.07397, avg_loss=0.07422]\n",
      "Step 520655  [5.495 sec/step, loss=0.07669, avg_loss=0.07426]\n",
      "Step 520656  [5.489 sec/step, loss=0.07538, avg_loss=0.07425]\n",
      "Step 520657  [5.512 sec/step, loss=0.07579, avg_loss=0.07427]\n",
      "Step 520658  [5.534 sec/step, loss=0.07318, avg_loss=0.07433]\n",
      "Step 520659  [5.526 sec/step, loss=0.07595, avg_loss=0.07433]\n",
      "Step 520660  [5.530 sec/step, loss=0.07640, avg_loss=0.07436]\n",
      "Step 520661  [5.523 sec/step, loss=0.07715, avg_loss=0.07436]\n",
      "Generated 32 batches of size 32 in 2.722 sec\n",
      "Step 520662  [5.501 sec/step, loss=0.07243, avg_loss=0.07432]\n",
      "Step 520663  [5.514 sec/step, loss=0.07544, avg_loss=0.07433]\n",
      "Step 520664  [5.522 sec/step, loss=0.07570, avg_loss=0.07434]\n",
      "Step 520665  [5.504 sec/step, loss=0.07496, avg_loss=0.07436]\n",
      "Step 520666  [5.541 sec/step, loss=0.06720, avg_loss=0.07427]\n",
      "Step 520667  [5.534 sec/step, loss=0.07637, avg_loss=0.07431]\n",
      "Step 520668  [5.522 sec/step, loss=0.07539, avg_loss=0.07430]\n",
      "Step 520669  [5.500 sec/step, loss=0.07484, avg_loss=0.07431]\n",
      "Step 520670  [5.517 sec/step, loss=0.07694, avg_loss=0.07432]\n",
      "Step 520671  [5.505 sec/step, loss=0.07259, avg_loss=0.07429]\n",
      "Step 520672  [5.505 sec/step, loss=0.07577, avg_loss=0.07430]\n",
      "Step 520673  [5.502 sec/step, loss=0.07664, avg_loss=0.07429]\n",
      "Step 520674  [5.485 sec/step, loss=0.07462, avg_loss=0.07428]\n",
      "Step 520675  [5.507 sec/step, loss=0.07685, avg_loss=0.07430]\n",
      "Step 520676  [5.495 sec/step, loss=0.07183, avg_loss=0.07427]\n",
      "Step 520677  [5.495 sec/step, loss=0.07589, avg_loss=0.07428]\n",
      "Step 520678  [5.480 sec/step, loss=0.07196, avg_loss=0.07425]\n",
      "Step 520679  [5.499 sec/step, loss=0.07706, avg_loss=0.07428]\n",
      "Step 520680  [5.494 sec/step, loss=0.07083, avg_loss=0.07425]\n",
      "Step 520681  [5.502 sec/step, loss=0.07599, avg_loss=0.07426]\n",
      "Step 520682  [5.521 sec/step, loss=0.07626, avg_loss=0.07429]\n",
      "Step 520683  [5.539 sec/step, loss=0.07406, avg_loss=0.07431]\n",
      "Step 520684  [5.541 sec/step, loss=0.07566, avg_loss=0.07433]\n",
      "Step 520685  [5.540 sec/step, loss=0.07465, avg_loss=0.07431]\n",
      "Step 520686  [5.539 sec/step, loss=0.07350, avg_loss=0.07429]\n",
      "Step 520687  [5.485 sec/step, loss=0.07426, avg_loss=0.07436]\n",
      "Step 520688  [5.543 sec/step, loss=0.06701, avg_loss=0.07428]\n",
      "Step 520689  [5.554 sec/step, loss=0.07529, avg_loss=0.07433]\n",
      "Step 520690  [5.553 sec/step, loss=0.07489, avg_loss=0.07432]\n",
      "Step 520691  [5.546 sec/step, loss=0.07575, avg_loss=0.07432]\n",
      "Step 520692  [5.571 sec/step, loss=0.07576, avg_loss=0.07437]\n",
      "Step 520693  [5.573 sec/step, loss=0.07366, avg_loss=0.07438]\n",
      "Generated 32 batches of size 32 in 2.483 sec\n",
      "Step 520694  [5.599 sec/step, loss=0.07375, avg_loss=0.07435]\n",
      "Step 520695  [5.594 sec/step, loss=0.07411, avg_loss=0.07434]\n",
      "Step 520696  [5.564 sec/step, loss=0.07572, avg_loss=0.07437]\n",
      "Step 520697  [5.581 sec/step, loss=0.07592, avg_loss=0.07447]\n",
      "Step 520698  [5.593 sec/step, loss=0.07728, avg_loss=0.07453]\n",
      "Step 520699  [5.587 sec/step, loss=0.07498, avg_loss=0.07452]\n",
      "Step 520700  [5.567 sec/step, loss=0.07450, avg_loss=0.07451]\n",
      "Writing summary at step: 520700\n",
      "Step 520701  [5.557 sec/step, loss=0.07506, avg_loss=0.07450]\n",
      "Step 520702  [5.545 sec/step, loss=0.07541, avg_loss=0.07450]\n",
      "Step 520703  [5.551 sec/step, loss=0.07589, avg_loss=0.07450]\n",
      "Step 520704  [5.548 sec/step, loss=0.07387, avg_loss=0.07449]\n",
      "Step 520705  [5.554 sec/step, loss=0.07623, avg_loss=0.07448]\n",
      "Step 520706  [5.542 sec/step, loss=0.07636, avg_loss=0.07448]\n",
      "Step 520707  [5.573 sec/step, loss=0.07359, avg_loss=0.07448]\n",
      "Step 520708  [5.566 sec/step, loss=0.07498, avg_loss=0.07447]\n",
      "Step 520709  [5.568 sec/step, loss=0.07545, avg_loss=0.07446]\n",
      "Step 520710  [5.557 sec/step, loss=0.07541, avg_loss=0.07445]\n",
      "Step 520711  [5.572 sec/step, loss=0.07441, avg_loss=0.07446]\n",
      "Step 520712  [5.580 sec/step, loss=0.07523, avg_loss=0.07450]\n",
      "Step 520713  [5.620 sec/step, loss=0.06611, avg_loss=0.07443]\n",
      "Step 520714  [5.631 sec/step, loss=0.07471, avg_loss=0.07446]\n",
      "Step 520715  [5.625 sec/step, loss=0.07401, avg_loss=0.07445]\n",
      "Step 520716  [5.612 sec/step, loss=0.07452, avg_loss=0.07445]\n",
      "Step 520717  [5.602 sec/step, loss=0.07423, avg_loss=0.07443]\n",
      "Step 520718  [5.592 sec/step, loss=0.07409, avg_loss=0.07442]\n",
      "Step 520719  [5.615 sec/step, loss=0.07355, avg_loss=0.07440]\n",
      "Step 520720  [5.613 sec/step, loss=0.07104, avg_loss=0.07437]\n",
      "Step 520721  [5.601 sec/step, loss=0.07288, avg_loss=0.07435]\n",
      "Step 520722  [5.617 sec/step, loss=0.07427, avg_loss=0.07434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 520723  [5.597 sec/step, loss=0.07345, avg_loss=0.07433]\n",
      "Step 520724  [5.581 sec/step, loss=0.07334, avg_loss=0.07431]\n",
      "Generated 32 batches of size 32 in 2.937 sec\n",
      "Step 520725  [5.585 sec/step, loss=0.06736, avg_loss=0.07426]\n",
      "Step 520726  [5.594 sec/step, loss=0.07662, avg_loss=0.07427]\n",
      "Step 520727  [5.598 sec/step, loss=0.07638, avg_loss=0.07428]\n",
      "Step 520728  [5.592 sec/step, loss=0.07625, avg_loss=0.07427]\n",
      "Step 520729  [5.592 sec/step, loss=0.07541, avg_loss=0.07430]\n",
      "Step 520730  [5.588 sec/step, loss=0.07592, avg_loss=0.07429]\n",
      "Step 520731  [5.550 sec/step, loss=0.07680, avg_loss=0.07439]\n",
      "Step 520732  [5.563 sec/step, loss=0.07569, avg_loss=0.07447]\n",
      "Step 520733  [5.551 sec/step, loss=0.07377, avg_loss=0.07443]\n",
      "Step 520734  [5.595 sec/step, loss=0.06634, avg_loss=0.07435]\n",
      "Step 520735  [5.585 sec/step, loss=0.07439, avg_loss=0.07435]\n",
      "Step 520736  [5.559 sec/step, loss=0.07092, avg_loss=0.07430]\n",
      "Step 520737  [5.582 sec/step, loss=0.07480, avg_loss=0.07430]\n",
      "Step 520738  [5.573 sec/step, loss=0.07639, avg_loss=0.07433]\n",
      "Step 520739  [5.573 sec/step, loss=0.07196, avg_loss=0.07430]\n",
      "Step 520740  [5.592 sec/step, loss=0.07575, avg_loss=0.07433]\n",
      "Step 520741  [5.584 sec/step, loss=0.07067, avg_loss=0.07431]\n",
      "Step 520742  [5.583 sec/step, loss=0.07720, avg_loss=0.07433]\n",
      "Step 520743  [5.600 sec/step, loss=0.07552, avg_loss=0.07434]\n",
      "Step 520744  [5.584 sec/step, loss=0.07670, avg_loss=0.07437]\n",
      "Step 520745  [5.605 sec/step, loss=0.07507, avg_loss=0.07446]\n",
      "Step 520746  [5.617 sec/step, loss=0.07586, avg_loss=0.07446]\n",
      "Step 520747  [5.608 sec/step, loss=0.07552, avg_loss=0.07444]\n",
      "Step 520748  [5.594 sec/step, loss=0.07432, avg_loss=0.07442]\n",
      "Step 520749  [5.593 sec/step, loss=0.07405, avg_loss=0.07441]\n",
      "Step 520750  [5.590 sec/step, loss=0.07633, avg_loss=0.07443]\n",
      "Step 520751  [5.597 sec/step, loss=0.07338, avg_loss=0.07445]\n",
      "Step 520752  [5.608 sec/step, loss=0.07599, avg_loss=0.07449]\n",
      "Step 520753  [5.614 sec/step, loss=0.07527, avg_loss=0.07450]\n",
      "Step 520754  [5.594 sec/step, loss=0.07308, avg_loss=0.07449]\n",
      "Step 520755  [5.590 sec/step, loss=0.07334, avg_loss=0.07445]\n",
      "Step 520756  [5.587 sec/step, loss=0.07333, avg_loss=0.07443]\n",
      "Generated 32 batches of size 32 in 2.577 sec\n",
      "Step 520757  [5.570 sec/step, loss=0.07472, avg_loss=0.07442]\n",
      "Step 520758  [5.568 sec/step, loss=0.07460, avg_loss=0.07444]\n",
      "Step 520759  [5.581 sec/step, loss=0.07669, avg_loss=0.07444]\n",
      "Step 520760  [5.587 sec/step, loss=0.07572, avg_loss=0.07444]\n",
      "Step 520761  [5.576 sec/step, loss=0.07622, avg_loss=0.07443]\n",
      "Step 520762  [5.589 sec/step, loss=0.07579, avg_loss=0.07446]\n",
      "Step 520763  [5.567 sec/step, loss=0.06556, avg_loss=0.07436]\n",
      "Step 520764  [5.562 sec/step, loss=0.07533, avg_loss=0.07436]\n",
      "Step 520765  [5.565 sec/step, loss=0.07428, avg_loss=0.07435]\n",
      "Step 520766  [5.519 sec/step, loss=0.07519, avg_loss=0.07443]\n",
      "Step 520767  [5.524 sec/step, loss=0.07692, avg_loss=0.07444]\n",
      "Step 520768  [5.527 sec/step, loss=0.07449, avg_loss=0.07443]\n",
      "Step 520769  [5.550 sec/step, loss=0.07668, avg_loss=0.07445]\n",
      "Step 520770  [5.530 sec/step, loss=0.07474, avg_loss=0.07442]\n",
      "Step 520771  [5.568 sec/step, loss=0.07465, avg_loss=0.07445]\n",
      "Step 520772  [5.567 sec/step, loss=0.07449, avg_loss=0.07443]\n",
      "Step 520773  [5.557 sec/step, loss=0.07427, avg_loss=0.07441]\n",
      "Step 520774  [5.549 sec/step, loss=0.07183, avg_loss=0.07438]\n",
      "Step 520775  [5.544 sec/step, loss=0.07519, avg_loss=0.07436]\n",
      "Step 520776  [5.566 sec/step, loss=0.07464, avg_loss=0.07439]\n",
      "Step 520777  [5.572 sec/step, loss=0.07670, avg_loss=0.07440]\n",
      "Step 520778  [5.573 sec/step, loss=0.07291, avg_loss=0.07441]\n",
      "Step 520779  [5.572 sec/step, loss=0.07648, avg_loss=0.07440]\n",
      "Step 520780  [5.578 sec/step, loss=0.07106, avg_loss=0.07441]\n",
      "Step 520781  [5.578 sec/step, loss=0.07660, avg_loss=0.07441]\n",
      "Step 520782  [5.559 sec/step, loss=0.07621, avg_loss=0.07441]\n",
      "Step 520783  [5.549 sec/step, loss=0.07483, avg_loss=0.07442]\n",
      "Step 520784  [5.529 sec/step, loss=0.06607, avg_loss=0.07432]\n",
      "Step 520785  [5.520 sec/step, loss=0.07564, avg_loss=0.07433]\n",
      "Step 520786  [5.547 sec/step, loss=0.07370, avg_loss=0.07434]\n",
      "Step 520787  [5.540 sec/step, loss=0.07076, avg_loss=0.07430]\n",
      "Step 520788  [5.485 sec/step, loss=0.07420, avg_loss=0.07437]\n",
      "Generated 32 batches of size 32 in 2.456 sec\n",
      "Step 520789  [5.501 sec/step, loss=0.07411, avg_loss=0.07436]\n",
      "Step 520790  [5.498 sec/step, loss=0.07497, avg_loss=0.07436]\n",
      "Step 520791  [5.547 sec/step, loss=0.06762, avg_loss=0.07428]\n",
      "Step 520792  [5.527 sec/step, loss=0.07289, avg_loss=0.07425]\n",
      "Step 520793  [5.529 sec/step, loss=0.07391, avg_loss=0.07425]\n",
      "Step 520794  [5.509 sec/step, loss=0.07650, avg_loss=0.07428]\n",
      "Step 520795  [5.512 sec/step, loss=0.07498, avg_loss=0.07429]\n",
      "Step 520796  [5.519 sec/step, loss=0.07616, avg_loss=0.07429]\n",
      "Step 520797  [5.511 sec/step, loss=0.07439, avg_loss=0.07428]\n",
      "Step 520798  [5.485 sec/step, loss=0.07183, avg_loss=0.07422]\n",
      "Step 520799  [5.474 sec/step, loss=0.07493, avg_loss=0.07422]\n",
      "Step 520800  [5.479 sec/step, loss=0.07425, avg_loss=0.07422]\n",
      "Writing summary at step: 520800\n",
      "Step 520801  [5.485 sec/step, loss=0.07318, avg_loss=0.07420]\n",
      "Step 520802  [5.468 sec/step, loss=0.06535, avg_loss=0.07410]\n",
      "Step 520803  [5.459 sec/step, loss=0.07452, avg_loss=0.07409]\n",
      "Step 520804  [5.461 sec/step, loss=0.07594, avg_loss=0.07411]\n",
      "Step 520805  [5.435 sec/step, loss=0.07164, avg_loss=0.07406]\n",
      "Step 520806  [5.432 sec/step, loss=0.07474, avg_loss=0.07405]\n",
      "Step 520807  [5.404 sec/step, loss=0.07162, avg_loss=0.07403]\n",
      "Step 520808  [5.399 sec/step, loss=0.07536, avg_loss=0.07403]\n",
      "Step 520809  [5.397 sec/step, loss=0.07610, avg_loss=0.07404]\n",
      "Step 520810  [5.385 sec/step, loss=0.07458, avg_loss=0.07403]\n",
      "Step 520811  [5.377 sec/step, loss=0.07553, avg_loss=0.07404]\n",
      "Step 520812  [5.383 sec/step, loss=0.07628, avg_loss=0.07405]\n",
      "Step 520813  [5.348 sec/step, loss=0.07594, avg_loss=0.07415]\n",
      "Step 520814  [5.353 sec/step, loss=0.07695, avg_loss=0.07417]\n",
      "Step 520815  [5.360 sec/step, loss=0.07501, avg_loss=0.07418]\n",
      "Step 520816  [5.352 sec/step, loss=0.07428, avg_loss=0.07418]\n",
      "Step 520817  [5.363 sec/step, loss=0.07313, avg_loss=0.07417]\n",
      "Step 520818  [5.390 sec/step, loss=0.07346, avg_loss=0.07416]\n",
      "Step 520819  [5.381 sec/step, loss=0.07685, avg_loss=0.07420]\n",
      "Generated 32 batches of size 32 in 2.610 sec\n",
      "Step 520820  [5.395 sec/step, loss=0.07370, avg_loss=0.07422]\n",
      "Step 520821  [5.408 sec/step, loss=0.07634, avg_loss=0.07426]\n",
      "Step 520822  [5.391 sec/step, loss=0.07077, avg_loss=0.07422]\n",
      "Step 520823  [5.412 sec/step, loss=0.07484, avg_loss=0.07424]\n",
      "Step 520824  [5.437 sec/step, loss=0.07634, avg_loss=0.07427]\n",
      "Step 520825  [5.499 sec/step, loss=0.06739, avg_loss=0.07427]\n",
      "Step 520826  [5.486 sec/step, loss=0.07367, avg_loss=0.07424]\n",
      "Step 520827  [5.488 sec/step, loss=0.07655, avg_loss=0.07424]\n",
      "Step 520828  [5.505 sec/step, loss=0.07562, avg_loss=0.07423]\n",
      "Step 520829  [5.520 sec/step, loss=0.07643, avg_loss=0.07424]\n",
      "Step 520830  [5.511 sec/step, loss=0.07497, avg_loss=0.07423]\n",
      "Step 520831  [5.510 sec/step, loss=0.07639, avg_loss=0.07423]\n",
      "Step 520832  [5.510 sec/step, loss=0.07559, avg_loss=0.07423]\n",
      "Step 520833  [5.562 sec/step, loss=0.06566, avg_loss=0.07415]\n",
      "Step 520834  [5.517 sec/step, loss=0.07558, avg_loss=0.07424]\n",
      "Step 520835  [5.545 sec/step, loss=0.07629, avg_loss=0.07426]\n",
      "Step 520836  [5.555 sec/step, loss=0.07364, avg_loss=0.07428]\n",
      "Step 520837  [5.536 sec/step, loss=0.07521, avg_loss=0.07429]\n",
      "Step 520838  [5.532 sec/step, loss=0.07653, avg_loss=0.07429]\n",
      "Step 520839  [5.532 sec/step, loss=0.07328, avg_loss=0.07430]\n",
      "Step 520840  [5.509 sec/step, loss=0.07426, avg_loss=0.07429]\n",
      "Step 520841  [5.526 sec/step, loss=0.07337, avg_loss=0.07432]\n",
      "Step 520842  [5.504 sec/step, loss=0.07260, avg_loss=0.07427]\n",
      "Step 520843  [5.493 sec/step, loss=0.07746, avg_loss=0.07429]\n",
      "Step 520844  [5.471 sec/step, loss=0.07236, avg_loss=0.07425]\n",
      "Step 520845  [5.457 sec/step, loss=0.07405, avg_loss=0.07424]\n",
      "Step 520846  [5.450 sec/step, loss=0.07638, avg_loss=0.07424]\n",
      "Step 520847  [5.428 sec/step, loss=0.06781, avg_loss=0.07416]\n",
      "Step 520848  [5.422 sec/step, loss=0.07406, avg_loss=0.07416]\n",
      "Step 520849  [5.428 sec/step, loss=0.07643, avg_loss=0.07418]\n",
      "Step 520850  [5.434 sec/step, loss=0.07590, avg_loss=0.07418]\n",
      "Step 520851  [5.465 sec/step, loss=0.07583, avg_loss=0.07421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 32 batches of size 32 in 2.415 sec\n",
      "Step 520852  [5.464 sec/step, loss=0.07544, avg_loss=0.07420]\n",
      "Step 520853  [5.468 sec/step, loss=0.07562, avg_loss=0.07420]\n",
      "Step 520854  [5.487 sec/step, loss=0.07594, avg_loss=0.07423]\n",
      "Step 520855  [5.476 sec/step, loss=0.07418, avg_loss=0.07424]\n",
      "Step 520856  [5.478 sec/step, loss=0.07196, avg_loss=0.07423]\n",
      "Step 520857  [5.477 sec/step, loss=0.07518, avg_loss=0.07423]\n",
      "Step 520858  [5.478 sec/step, loss=0.07371, avg_loss=0.07422]\n",
      "Step 520859  [5.473 sec/step, loss=0.07530, avg_loss=0.07421]\n",
      "Step 520860  [5.468 sec/step, loss=0.07612, avg_loss=0.07421]\n",
      "Step 520861  [5.474 sec/step, loss=0.07600, avg_loss=0.07421]\n",
      "Step 520862  [5.461 sec/step, loss=0.07177, avg_loss=0.07417]\n",
      "Step 520863  [5.473 sec/step, loss=0.07295, avg_loss=0.07424]\n",
      "Step 520864  [5.479 sec/step, loss=0.07663, avg_loss=0.07426]\n",
      "Step 520865  [5.485 sec/step, loss=0.07124, avg_loss=0.07423]\n",
      "Step 520866  [5.495 sec/step, loss=0.07644, avg_loss=0.07424]\n",
      "Step 520867  [5.475 sec/step, loss=0.07434, avg_loss=0.07421]\n",
      "Step 520868  [5.483 sec/step, loss=0.07595, avg_loss=0.07423]\n",
      "Step 520869  [5.454 sec/step, loss=0.07290, avg_loss=0.07419]\n",
      "Step 520870  [5.466 sec/step, loss=0.07560, avg_loss=0.07420]\n",
      "Step 520871  [5.492 sec/step, loss=0.06822, avg_loss=0.07413]\n",
      "Step 520872  [5.494 sec/step, loss=0.07488, avg_loss=0.07414]\n",
      "Step 520873  [5.509 sec/step, loss=0.07427, avg_loss=0.07414]\n",
      "Step 520874  [5.508 sec/step, loss=0.06668, avg_loss=0.07409]\n",
      "Step 520875  [5.504 sec/step, loss=0.07564, avg_loss=0.07409]\n",
      "Step 520876  [5.504 sec/step, loss=0.07490, avg_loss=0.07409]\n",
      "Step 520877  [5.492 sec/step, loss=0.07602, avg_loss=0.07409]\n",
      "Step 520878  [5.490 sec/step, loss=0.07391, avg_loss=0.07410]\n",
      "Step 520879  [5.502 sec/step, loss=0.07337, avg_loss=0.07407]\n",
      "Step 520880  [5.506 sec/step, loss=0.07490, avg_loss=0.07410]\n",
      "Step 520881  [5.502 sec/step, loss=0.07697, avg_loss=0.07411]\n",
      "Step 520882  [5.510 sec/step, loss=0.07453, avg_loss=0.07409]\n",
      "Step 520883  [5.517 sec/step, loss=0.07664, avg_loss=0.07411]\n",
      "Generated 32 batches of size 32 in 2.633 sec\n",
      "Step 520884  [5.539 sec/step, loss=0.07534, avg_loss=0.07420]\n",
      "Step 520885  [5.535 sec/step, loss=0.07486, avg_loss=0.07419]\n",
      "Step 520886  [5.510 sec/step, loss=0.07499, avg_loss=0.07421]\n",
      "Step 520887  [5.514 sec/step, loss=0.07435, avg_loss=0.07424]\n",
      "Step 520888  [5.529 sec/step, loss=0.07502, avg_loss=0.07425]\n",
      "Step 520889  [5.540 sec/step, loss=0.07380, avg_loss=0.07425]\n",
      "Step 520890  [5.532 sec/step, loss=0.07429, avg_loss=0.07424]\n",
      "Step 520891  [5.481 sec/step, loss=0.07351, avg_loss=0.07430]\n",
      "Step 520892  [5.500 sec/step, loss=0.07634, avg_loss=0.07433]\n",
      "Step 520893  [5.504 sec/step, loss=0.07426, avg_loss=0.07434]\n",
      "Step 520894  [5.478 sec/step, loss=0.07157, avg_loss=0.07429]\n",
      "Step 520895  [5.471 sec/step, loss=0.07590, avg_loss=0.07430]\n",
      "Step 520896  [5.471 sec/step, loss=0.07521, avg_loss=0.07429]\n",
      "Step 520897  [5.472 sec/step, loss=0.07458, avg_loss=0.07429]\n",
      "Step 520898  [5.467 sec/step, loss=0.06564, avg_loss=0.07423]\n",
      "Step 520899  [5.517 sec/step, loss=0.06562, avg_loss=0.07414]\n",
      "Step 520900  [5.526 sec/step, loss=0.07542, avg_loss=0.07415]\n",
      "Writing summary at step: 520900\n",
      "Step 520901  [5.528 sec/step, loss=0.07589, avg_loss=0.07417]\n",
      "Step 520902  [5.552 sec/step, loss=0.07527, avg_loss=0.07427]\n",
      "Step 520903  [5.551 sec/step, loss=0.07469, avg_loss=0.07427]\n",
      "Step 520904  [5.562 sec/step, loss=0.07672, avg_loss=0.07428]\n",
      "Step 520905  [5.599 sec/step, loss=0.07253, avg_loss=0.07429]\n",
      "Step 520906  [5.600 sec/step, loss=0.07481, avg_loss=0.07429]\n",
      "Step 520907  [5.608 sec/step, loss=0.07624, avg_loss=0.07434]\n",
      "Step 520908  [5.610 sec/step, loss=0.07617, avg_loss=0.07435]\n",
      "Step 520909  [5.597 sec/step, loss=0.07119, avg_loss=0.07430]\n",
      "Step 520910  [5.605 sec/step, loss=0.07448, avg_loss=0.07430]\n",
      "Step 520911  [5.614 sec/step, loss=0.07695, avg_loss=0.07431]\n",
      "Step 520912  [5.619 sec/step, loss=0.07693, avg_loss=0.07432]\n",
      "Step 520913  [5.622 sec/step, loss=0.07703, avg_loss=0.07433]\n",
      "Step 520914  [5.605 sec/step, loss=0.07381, avg_loss=0.07430]\n",
      "Generated 32 batches of size 32 in 2.437 sec\n",
      "Step 520915  [5.612 sec/step, loss=0.07280, avg_loss=0.07427]\n",
      "Step 520916  [5.604 sec/step, loss=0.07308, avg_loss=0.07426]\n",
      "Step 520917  [5.597 sec/step, loss=0.07412, avg_loss=0.07427]\n",
      "Step 520918  [5.582 sec/step, loss=0.07688, avg_loss=0.07431]\n",
      "Step 520919  [5.574 sec/step, loss=0.07583, avg_loss=0.07430]\n",
      "Step 520920  [5.580 sec/step, loss=0.07489, avg_loss=0.07431]\n",
      "Step 520921  [5.567 sec/step, loss=0.07573, avg_loss=0.07430]\n",
      "Step 520922  [5.571 sec/step, loss=0.07419, avg_loss=0.07434]\n",
      "Step 520923  [5.581 sec/step, loss=0.07603, avg_loss=0.07435]\n",
      "Step 520924  [5.577 sec/step, loss=0.07668, avg_loss=0.07435]\n",
      "Step 520925  [5.515 sec/step, loss=0.07497, avg_loss=0.07443]\n",
      "Step 520926  [5.516 sec/step, loss=0.07229, avg_loss=0.07441]\n",
      "Step 520927  [5.511 sec/step, loss=0.07342, avg_loss=0.07438]\n",
      "Step 520928  [5.489 sec/step, loss=0.07450, avg_loss=0.07437]\n",
      "Step 520929  [5.480 sec/step, loss=0.07551, avg_loss=0.07436]\n",
      "Step 520930  [5.479 sec/step, loss=0.07566, avg_loss=0.07437]\n",
      "Step 520931  [5.451 sec/step, loss=0.06720, avg_loss=0.07428]\n",
      "Step 520932  [5.446 sec/step, loss=0.07458, avg_loss=0.07427]\n",
      "Step 520933  [5.393 sec/step, loss=0.07419, avg_loss=0.07435]\n",
      "Step 520934  [5.394 sec/step, loss=0.07652, avg_loss=0.07436]\n",
      "Step 520935  [5.390 sec/step, loss=0.07662, avg_loss=0.07437]\n",
      "Step 520936  [5.398 sec/step, loss=0.07655, avg_loss=0.07439]\n",
      "Step 520937  [5.443 sec/step, loss=0.06704, avg_loss=0.07431]\n",
      "Step 520938  [5.421 sec/step, loss=0.07317, avg_loss=0.07428]\n",
      "Step 520939  [5.442 sec/step, loss=0.07359, avg_loss=0.07428]\n",
      "Step 520940  [5.466 sec/step, loss=0.07473, avg_loss=0.07429]\n",
      "Step 520941  [5.472 sec/step, loss=0.07686, avg_loss=0.07432]\n",
      "Step 520942  [5.495 sec/step, loss=0.07404, avg_loss=0.07434]\n",
      "Step 520943  [5.487 sec/step, loss=0.07520, avg_loss=0.07431]\n",
      "Step 520944  [5.487 sec/step, loss=0.07228, avg_loss=0.07431]\n",
      "Step 520945  [5.497 sec/step, loss=0.07591, avg_loss=0.07433]\n",
      "Step 520946  [5.484 sec/step, loss=0.07232, avg_loss=0.07429]\n",
      "Generated 32 batches of size 32 in 2.375 sec\n",
      "Step 520947  [5.516 sec/step, loss=0.07513, avg_loss=0.07436]\n",
      "Step 520948  [5.523 sec/step, loss=0.07218, avg_loss=0.07435]\n",
      "Step 520949  [5.504 sec/step, loss=0.07371, avg_loss=0.07432]\n",
      "Step 520950  [5.498 sec/step, loss=0.07610, avg_loss=0.07432]\n",
      "Step 520951  [5.469 sec/step, loss=0.07592, avg_loss=0.07432]\n",
      "Step 520952  [5.475 sec/step, loss=0.07651, avg_loss=0.07433]\n",
      "Step 520953  [5.476 sec/step, loss=0.07654, avg_loss=0.07434]\n",
      "Step 520954  [5.470 sec/step, loss=0.07539, avg_loss=0.07434]\n",
      "Step 520955  [5.505 sec/step, loss=0.07635, avg_loss=0.07436]\n",
      "Step 520956  [5.491 sec/step, loss=0.07223, avg_loss=0.07436]\n",
      "Step 520957  [5.510 sec/step, loss=0.07507, avg_loss=0.07436]\n",
      "Step 520958  [5.497 sec/step, loss=0.07454, avg_loss=0.07437]\n",
      "Step 520959  [5.488 sec/step, loss=0.07566, avg_loss=0.07437]\n",
      "Step 520960  [5.487 sec/step, loss=0.07461, avg_loss=0.07436]\n",
      "Step 520961  [5.486 sec/step, loss=0.07542, avg_loss=0.07435]\n",
      "Step 520962  [5.502 sec/step, loss=0.07542, avg_loss=0.07439]\n",
      "Step 520963  [5.504 sec/step, loss=0.07561, avg_loss=0.07441]\n",
      "Step 520964  [5.490 sec/step, loss=0.07414, avg_loss=0.07439]\n",
      "Step 520965  [5.502 sec/step, loss=0.07643, avg_loss=0.07444]\n",
      "Step 520966  [5.497 sec/step, loss=0.07679, avg_loss=0.07444]\n",
      "Step 520967  [5.508 sec/step, loss=0.07528, avg_loss=0.07445]\n",
      "Step 520968  [5.511 sec/step, loss=0.07642, avg_loss=0.07446]\n",
      "Step 520969  [5.521 sec/step, loss=0.07456, avg_loss=0.07447]\n",
      "Step 520970  [5.506 sec/step, loss=0.07053, avg_loss=0.07442]\n",
      "Step 520971  [5.460 sec/step, loss=0.07400, avg_loss=0.07448]\n",
      "Step 520972  [5.462 sec/step, loss=0.07670, avg_loss=0.07450]\n",
      "Step 520973  [5.460 sec/step, loss=0.07675, avg_loss=0.07452]\n",
      "Step 520974  [5.527 sec/step, loss=0.06717, avg_loss=0.07453]\n",
      "Step 520975  [5.516 sec/step, loss=0.07391, avg_loss=0.07451]\n",
      "Step 520976  [5.506 sec/step, loss=0.07363, avg_loss=0.07450]\n",
      "Step 520977  [5.536 sec/step, loss=0.07388, avg_loss=0.07448]\n",
      "Step 520978  [5.544 sec/step, loss=0.07534, avg_loss=0.07449]\n",
      "Generated 32 batches of size 32 in 2.457 sec\n",
      "Step 520979  [5.540 sec/step, loss=0.07402, avg_loss=0.07450]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 520980  [5.548 sec/step, loss=0.07676, avg_loss=0.07452]\n",
      "Step 520981  [5.536 sec/step, loss=0.07344, avg_loss=0.07448]\n",
      "Step 520982  [5.548 sec/step, loss=0.07644, avg_loss=0.07450]\n",
      "Step 520983  [5.524 sec/step, loss=0.07420, avg_loss=0.07448]\n",
      "Step 520984  [5.523 sec/step, loss=0.07532, avg_loss=0.07448]\n",
      "Step 520985  [5.533 sec/step, loss=0.07412, avg_loss=0.07447]\n",
      "Step 520986  [5.511 sec/step, loss=0.06595, avg_loss=0.07438]\n",
      "Step 520987  [5.522 sec/step, loss=0.07674, avg_loss=0.07440]\n",
      "Step 520988  [5.509 sec/step, loss=0.07152, avg_loss=0.07437]\n",
      "Step 520989  [5.491 sec/step, loss=0.07629, avg_loss=0.07439]\n",
      "Step 520990  [5.507 sec/step, loss=0.07677, avg_loss=0.07442]\n",
      "Step 520991  [5.496 sec/step, loss=0.07472, avg_loss=0.07443]\n",
      "Step 520992  [5.479 sec/step, loss=0.07352, avg_loss=0.07440]\n",
      "Step 520993  [5.490 sec/step, loss=0.07646, avg_loss=0.07442]\n",
      "Step 520994  [5.492 sec/step, loss=0.07164, avg_loss=0.07442]\n",
      "Step 520995  [5.481 sec/step, loss=0.07223, avg_loss=0.07439]\n",
      "Step 520996  [5.479 sec/step, loss=0.07516, avg_loss=0.07439]\n",
      "Step 520997  [5.474 sec/step, loss=0.06640, avg_loss=0.07430]\n",
      "Step 520998  [5.484 sec/step, loss=0.07426, avg_loss=0.07439]\n",
      "Step 520999  [5.440 sec/step, loss=0.07417, avg_loss=0.07448]\n",
      "Step 521000  [5.435 sec/step, loss=0.07476, avg_loss=0.07447]\n",
      "Writing summary at step: 521000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-521000\n",
      "Saving audio and alignment...\n",
      "/home/itu/Desktop/UrduTTSNew/UrduTTS/tvenv/lib/python3.6/site-packages/matplotlib/pyplot.py:524: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "Input: anvkhay or baybaak rafiiq kay uddaas hoonay sae vaaqiyii ddil koo ddhatskaa lagaa~_________________________________\n",
      "Step 521001  [5.422 sec/step, loss=0.07340, avg_loss=0.07444]\n",
      "Step 521002  [5.423 sec/step, loss=0.07694, avg_loss=0.07446]\n",
      "Step 521003  [5.426 sec/step, loss=0.07451, avg_loss=0.07446]\n",
      "Step 521004  [5.422 sec/step, loss=0.07562, avg_loss=0.07445]\n",
      "Step 521005  [5.418 sec/step, loss=0.07547, avg_loss=0.07448]\n",
      "Step 521006  [5.464 sec/step, loss=0.06509, avg_loss=0.07438]\n",
      "Step 521007  [5.458 sec/step, loss=0.07575, avg_loss=0.07438]\n",
      "Step 521008  [5.452 sec/step, loss=0.07597, avg_loss=0.07437]\n",
      "Generated 32 batches of size 32 in 2.496 sec\n",
      "Step 521009  [5.467 sec/step, loss=0.07528, avg_loss=0.07441]\n",
      "Step 521010  [5.475 sec/step, loss=0.07451, avg_loss=0.07442]\n",
      "Step 521011  [5.465 sec/step, loss=0.07629, avg_loss=0.07441]\n",
      "Step 521012  [5.471 sec/step, loss=0.07553, avg_loss=0.07439]\n",
      "Step 521013  [5.461 sec/step, loss=0.07676, avg_loss=0.07439]\n",
      "Step 521014  [5.489 sec/step, loss=0.07607, avg_loss=0.07441]\n",
      "Step 521015  [5.478 sec/step, loss=0.07196, avg_loss=0.07441]\n",
      "Step 521016  [5.483 sec/step, loss=0.07362, avg_loss=0.07441]\n",
      "Step 521017  [5.501 sec/step, loss=0.07600, avg_loss=0.07443]\n",
      "Step 521018  [5.483 sec/step, loss=0.07385, avg_loss=0.07440]\n",
      "Step 521019  [5.480 sec/step, loss=0.07627, avg_loss=0.07440]\n",
      "Step 521020  [5.468 sec/step, loss=0.07449, avg_loss=0.07440]\n",
      "Step 521021  [5.459 sec/step, loss=0.07436, avg_loss=0.07439]\n",
      "Step 521022  [5.467 sec/step, loss=0.07534, avg_loss=0.07440]\n",
      "Step 521023  [5.458 sec/step, loss=0.07671, avg_loss=0.07440]\n",
      "Step 521024  [5.447 sec/step, loss=0.07481, avg_loss=0.07439]\n",
      "Step 521025  [5.466 sec/step, loss=0.07639, avg_loss=0.07440]\n",
      "Step 521026  [5.497 sec/step, loss=0.07254, avg_loss=0.07440]\n",
      "Step 521027  [5.477 sec/step, loss=0.07197, avg_loss=0.07439]\n",
      "Step 521028  [5.493 sec/step, loss=0.07699, avg_loss=0.07441]\n",
      "Step 521029  [5.502 sec/step, loss=0.07648, avg_loss=0.07442]\n",
      "Step 521030  [5.511 sec/step, loss=0.07586, avg_loss=0.07443]\n",
      "Step 521031  [5.540 sec/step, loss=0.07517, avg_loss=0.07450]\n",
      "Step 521032  [5.558 sec/step, loss=0.07661, avg_loss=0.07453]\n",
      "Step 521033  [5.561 sec/step, loss=0.07574, avg_loss=0.07454]\n",
      "Step 521034  [5.548 sec/step, loss=0.07420, avg_loss=0.07452]\n",
      "Step 521035  [5.554 sec/step, loss=0.07432, avg_loss=0.07449]\n",
      "Step 521036  [5.550 sec/step, loss=0.07202, avg_loss=0.07445]\n",
      "Step 521037  [5.496 sec/step, loss=0.07585, avg_loss=0.07454]\n",
      "Step 521038  [5.507 sec/step, loss=0.07306, avg_loss=0.07454]\n",
      "Step 521039  [5.491 sec/step, loss=0.07487, avg_loss=0.07455]\n",
      "Step 521040  [5.479 sec/step, loss=0.07289, avg_loss=0.07453]\n",
      "Generated 32 batches of size 32 in 2.905 sec\n",
      "Step 521041  [5.456 sec/step, loss=0.06655, avg_loss=0.07443]\n",
      "Step 521042  [5.444 sec/step, loss=0.07437, avg_loss=0.07443]\n",
      "Step 521043  [5.448 sec/step, loss=0.07218, avg_loss=0.07440]\n",
      "Step 521044  [5.454 sec/step, loss=0.07570, avg_loss=0.07443]\n",
      "Step 521045  [5.458 sec/step, loss=0.07326, avg_loss=0.07441]\n",
      "Step 521046  [5.471 sec/step, loss=0.07618, avg_loss=0.07445]\n",
      "Step 521047  [5.467 sec/step, loss=0.07517, avg_loss=0.07445]\n",
      "Step 521048  [5.514 sec/step, loss=0.06806, avg_loss=0.07441]\n",
      "Step 521049  [5.511 sec/step, loss=0.07167, avg_loss=0.07439]\n",
      "Step 521050  [5.502 sec/step, loss=0.07535, avg_loss=0.07438]\n",
      "Step 521051  [5.501 sec/step, loss=0.07427, avg_loss=0.07436]\n",
      "Step 521052  [5.498 sec/step, loss=0.07652, avg_loss=0.07436]\n",
      "Step 521053  [5.483 sec/step, loss=0.07400, avg_loss=0.07434]\n",
      "Step 521054  [5.504 sec/step, loss=0.07254, avg_loss=0.07431]\n",
      "Step 521055  [5.494 sec/step, loss=0.07567, avg_loss=0.07430]\n",
      "Step 521056  [5.514 sec/step, loss=0.07641, avg_loss=0.07434]\n",
      "Step 521057  [5.494 sec/step, loss=0.07441, avg_loss=0.07434]\n",
      "Step 521058  [5.513 sec/step, loss=0.07624, avg_loss=0.07435]\n",
      "Step 521059  [5.526 sec/step, loss=0.07392, avg_loss=0.07434]\n",
      "Step 521060  [5.533 sec/step, loss=0.07702, avg_loss=0.07436]\n",
      "Step 521061  [5.532 sec/step, loss=0.07611, avg_loss=0.07437]\n",
      "Step 521062  [5.528 sec/step, loss=0.07545, avg_loss=0.07437]\n",
      "Step 521063  [5.528 sec/step, loss=0.07216, avg_loss=0.07433]\n",
      "Step 521064  [5.546 sec/step, loss=0.07547, avg_loss=0.07435]\n",
      "Step 521065  [5.525 sec/step, loss=0.07476, avg_loss=0.07433]\n",
      "Step 521066  [5.498 sec/step, loss=0.06599, avg_loss=0.07422]\n",
      "Step 521067  [5.502 sec/step, loss=0.07550, avg_loss=0.07422]\n",
      "Step 521068  [5.501 sec/step, loss=0.07565, avg_loss=0.07422]\n",
      "Step 521069  [5.507 sec/step, loss=0.07559, avg_loss=0.07423]\n",
      "Step 521070  [5.510 sec/step, loss=0.07430, avg_loss=0.07426]\n",
      "Step 521071  [5.555 sec/step, loss=0.06626, avg_loss=0.07419]\n",
      "Step 521072  [5.552 sec/step, loss=0.07535, avg_loss=0.07417]\n",
      "Generated 32 batches of size 32 in 2.571 sec\n",
      "Step 521073  [5.546 sec/step, loss=0.07435, avg_loss=0.07415]\n",
      "Step 521074  [5.493 sec/step, loss=0.07591, avg_loss=0.07424]\n",
      "Step 521075  [5.515 sec/step, loss=0.07609, avg_loss=0.07426]\n",
      "Step 521076  [5.503 sec/step, loss=0.07170, avg_loss=0.07424]\n",
      "Step 521077  [5.462 sec/step, loss=0.07225, avg_loss=0.07422]\n",
      "Step 521078  [5.451 sec/step, loss=0.07396, avg_loss=0.07421]\n",
      "Step 521079  [5.440 sec/step, loss=0.07496, avg_loss=0.07422]\n",
      "Step 521080  [5.428 sec/step, loss=0.07564, avg_loss=0.07421]\n",
      "Step 521081  [5.437 sec/step, loss=0.07455, avg_loss=0.07422]\n",
      "Step 521082  [5.409 sec/step, loss=0.07306, avg_loss=0.07418]\n",
      "Step 521083  [5.420 sec/step, loss=0.07554, avg_loss=0.07420]\n",
      "Step 521084  [5.420 sec/step, loss=0.07600, avg_loss=0.07420]\n",
      "Step 521085  [5.420 sec/step, loss=0.07700, avg_loss=0.07423]\n",
      "Step 521086  [5.437 sec/step, loss=0.07431, avg_loss=0.07432]\n",
      "Step 521087  [5.447 sec/step, loss=0.07705, avg_loss=0.07432]\n",
      "Step 521088  [5.470 sec/step, loss=0.07406, avg_loss=0.07435]\n",
      "Step 521089  [5.464 sec/step, loss=0.07314, avg_loss=0.07431]\n",
      "Step 521090  [5.439 sec/step, loss=0.06637, avg_loss=0.07421]\n",
      "Step 521091  [5.459 sec/step, loss=0.07702, avg_loss=0.07423]\n",
      "Step 521092  [5.463 sec/step, loss=0.07425, avg_loss=0.07424]\n",
      "Step 521093  [5.455 sec/step, loss=0.07229, avg_loss=0.07420]\n",
      "Step 521094  [5.468 sec/step, loss=0.07446, avg_loss=0.07423]\n",
      "Step 521095  [5.475 sec/step, loss=0.07486, avg_loss=0.07425]\n",
      "Step 521096  [5.470 sec/step, loss=0.07254, avg_loss=0.07423]\n",
      "Step 521097  [5.487 sec/step, loss=0.07552, avg_loss=0.07432]\n",
      "Step 521098  [5.546 sec/step, loss=0.06733, avg_loss=0.07425]\n",
      "Step 521099  [5.533 sec/step, loss=0.07201, avg_loss=0.07423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 521100  [5.527 sec/step, loss=0.07126, avg_loss=0.07419]\n",
      "Writing summary at step: 521100\n",
      "Step 521101  [5.535 sec/step, loss=0.07666, avg_loss=0.07422]\n",
      "Step 521102  [5.526 sec/step, loss=0.07464, avg_loss=0.07420]\n",
      "Step 521103  [5.543 sec/step, loss=0.07484, avg_loss=0.07420]\n",
      "Generated 32 batches of size 32 in 2.418 sec\n",
      "Step 521104  [5.544 sec/step, loss=0.07496, avg_loss=0.07420]\n",
      "Step 521105  [5.535 sec/step, loss=0.07725, avg_loss=0.07422]\n",
      "Step 521106  [5.492 sec/step, loss=0.07712, avg_loss=0.07434]\n",
      "Step 521107  [5.476 sec/step, loss=0.07331, avg_loss=0.07431]\n",
      "Step 521108  [5.491 sec/step, loss=0.07561, avg_loss=0.07431]\n",
      "Step 521109  [5.488 sec/step, loss=0.07599, avg_loss=0.07431]\n",
      "Step 521110  [5.488 sec/step, loss=0.07555, avg_loss=0.07433]\n",
      "Step 521111  [5.485 sec/step, loss=0.07565, avg_loss=0.07432]\n",
      "Step 521112  [5.498 sec/step, loss=0.07385, avg_loss=0.07430]\n",
      "Step 521113  [5.539 sec/step, loss=0.06687, avg_loss=0.07420]\n",
      "Step 521114  [5.520 sec/step, loss=0.07480, avg_loss=0.07419]\n",
      "Step 521115  [5.527 sec/step, loss=0.07320, avg_loss=0.07420]\n",
      "Step 521116  [5.521 sec/step, loss=0.07489, avg_loss=0.07422]\n",
      "Step 521117  [5.512 sec/step, loss=0.07503, avg_loss=0.07421]\n",
      "Step 521118  [5.531 sec/step, loss=0.07709, avg_loss=0.07424]\n",
      "Step 521119  [5.533 sec/step, loss=0.07485, avg_loss=0.07422]\n",
      "Step 521120  [5.527 sec/step, loss=0.07426, avg_loss=0.07422]\n",
      "Step 521121  [5.547 sec/step, loss=0.07501, avg_loss=0.07423]\n",
      "Step 521122  [5.544 sec/step, loss=0.07501, avg_loss=0.07423]\n",
      "Step 521123  [5.532 sec/step, loss=0.07249, avg_loss=0.07418]\n",
      "Step 521124  [5.546 sec/step, loss=0.07530, avg_loss=0.07419]\n",
      "Step 521125  [5.535 sec/step, loss=0.07619, avg_loss=0.07419]\n",
      "Step 521126  [5.492 sec/step, loss=0.07144, avg_loss=0.07417]\n",
      "Step 521127  [5.511 sec/step, loss=0.07566, avg_loss=0.07421]\n",
      "Step 521128  [5.501 sec/step, loss=0.07589, avg_loss=0.07420]\n",
      "Step 521129  [5.479 sec/step, loss=0.07303, avg_loss=0.07417]\n",
      "Step 521130  [5.490 sec/step, loss=0.07581, avg_loss=0.07417]\n",
      "Step 521131  [5.487 sec/step, loss=0.07641, avg_loss=0.07418]\n",
      "Step 521132  [5.466 sec/step, loss=0.07047, avg_loss=0.07412]\n",
      "Step 521133  [5.461 sec/step, loss=0.07326, avg_loss=0.07409]\n",
      "Step 521134  [5.470 sec/step, loss=0.07607, avg_loss=0.07411]\n",
      "Step 521135  [5.446 sec/step, loss=0.07409, avg_loss=0.07411]\n",
      "Generated 32 batches of size 32 in 2.459 sec\n",
      "Step 521136  [5.479 sec/step, loss=0.07255, avg_loss=0.07411]\n",
      "Step 521137  [5.471 sec/step, loss=0.07466, avg_loss=0.07410]\n",
      "Step 521138  [5.484 sec/step, loss=0.07384, avg_loss=0.07411]\n",
      "Step 521139  [5.488 sec/step, loss=0.07356, avg_loss=0.07410]\n",
      "Step 521140  [5.481 sec/step, loss=0.07454, avg_loss=0.07411]\n",
      "Step 521141  [5.502 sec/step, loss=0.07649, avg_loss=0.07421]\n",
      "Step 521142  [5.516 sec/step, loss=0.07590, avg_loss=0.07423]\n",
      "Step 521143  [5.495 sec/step, loss=0.06641, avg_loss=0.07417]\n",
      "Step 521144  [5.516 sec/step, loss=0.07581, avg_loss=0.07417]\n",
      "Step 521145  [5.524 sec/step, loss=0.07593, avg_loss=0.07420]\n",
      "Step 521146  [5.519 sec/step, loss=0.07468, avg_loss=0.07418]\n",
      "Step 521147  [5.512 sec/step, loss=0.07384, avg_loss=0.07417]\n",
      "Step 521148  [5.465 sec/step, loss=0.07542, avg_loss=0.07424]\n",
      "Step 521149  [5.485 sec/step, loss=0.07673, avg_loss=0.07429]\n",
      "Step 521150  [5.467 sec/step, loss=0.06507, avg_loss=0.07419]\n",
      "Step 521151  [5.466 sec/step, loss=0.07559, avg_loss=0.07420]\n",
      "Step 521152  [5.450 sec/step, loss=0.07443, avg_loss=0.07418]\n",
      "Step 521153  [5.466 sec/step, loss=0.07641, avg_loss=0.07421]\n",
      "Step 521154  [5.441 sec/step, loss=0.07580, avg_loss=0.07424]\n",
      "Step 521155  [5.425 sec/step, loss=0.07467, avg_loss=0.07423]\n",
      "Step 521156  [5.415 sec/step, loss=0.07317, avg_loss=0.07420]\n",
      "Step 521157  [5.408 sec/step, loss=0.07429, avg_loss=0.07420]\n",
      "Step 521158  [5.408 sec/step, loss=0.07705, avg_loss=0.07420]\n",
      "Step 521159  [5.392 sec/step, loss=0.07392, avg_loss=0.07420]\n",
      "Step 521160  [5.385 sec/step, loss=0.07521, avg_loss=0.07419]\n",
      "Step 521161  [5.402 sec/step, loss=0.07616, avg_loss=0.07419]\n",
      "Step 521162  [5.427 sec/step, loss=0.07308, avg_loss=0.07416]\n",
      "Step 521163  [5.434 sec/step, loss=0.07554, avg_loss=0.07420]\n",
      "Step 521164  [5.408 sec/step, loss=0.07408, avg_loss=0.07418]\n",
      "Step 521165  [5.416 sec/step, loss=0.07547, avg_loss=0.07419]\n",
      "Step 521166  [5.423 sec/step, loss=0.07129, avg_loss=0.07424]\n",
      "Step 521167  [5.462 sec/step, loss=0.06888, avg_loss=0.07418]\n",
      "Generated 32 batches of size 32 in 2.759 sec\n",
      "Step 521168  [5.442 sec/step, loss=0.07145, avg_loss=0.07413]\n",
      "Step 521169  [5.444 sec/step, loss=0.07583, avg_loss=0.07414]\n",
      "Step 521170  [5.462 sec/step, loss=0.07727, avg_loss=0.07417]\n",
      "Step 521171  [5.411 sec/step, loss=0.07361, avg_loss=0.07424]\n",
      "Step 521172  [5.398 sec/step, loss=0.07382, avg_loss=0.07423]\n",
      "Step 521173  [5.403 sec/step, loss=0.07571, avg_loss=0.07424]\n",
      "Step 521174  [5.419 sec/step, loss=0.07590, avg_loss=0.07424]\n",
      "Step 521175  [5.415 sec/step, loss=0.07398, avg_loss=0.07422]\n",
      "Step 521176  [5.425 sec/step, loss=0.07497, avg_loss=0.07425]\n",
      "Step 521177  [5.425 sec/step, loss=0.07252, avg_loss=0.07425]\n",
      "Step 521178  [5.456 sec/step, loss=0.07411, avg_loss=0.07425]\n",
      "Step 521179  [5.465 sec/step, loss=0.07635, avg_loss=0.07427]\n",
      "Step 521180  [5.487 sec/step, loss=0.07400, avg_loss=0.07425]\n",
      "Step 521181  [5.485 sec/step, loss=0.07553, avg_loss=0.07426]\n",
      "Step 521182  [5.480 sec/step, loss=0.06639, avg_loss=0.07420]\n",
      "Step 521183  [5.485 sec/step, loss=0.07673, avg_loss=0.07421]\n",
      "Step 521184  [5.485 sec/step, loss=0.07311, avg_loss=0.07418]\n",
      "Step 521185  [5.528 sec/step, loss=0.06756, avg_loss=0.07408]\n",
      "Step 521186  [5.523 sec/step, loss=0.07447, avg_loss=0.07409]\n",
      "Step 521187  [5.509 sec/step, loss=0.07406, avg_loss=0.07406]\n",
      "Step 521188  [5.493 sec/step, loss=0.07538, avg_loss=0.07407]\n",
      "Step 521189  [5.488 sec/step, loss=0.07531, avg_loss=0.07409]\n",
      "Step 521190  [5.511 sec/step, loss=0.07310, avg_loss=0.07416]\n",
      "Step 521191  [5.501 sec/step, loss=0.07603, avg_loss=0.07415]\n",
      "Step 521192  [5.497 sec/step, loss=0.07383, avg_loss=0.07414]\n",
      "Step 521193  [5.504 sec/step, loss=0.07525, avg_loss=0.07417]\n",
      "Step 521194  [5.516 sec/step, loss=0.07614, avg_loss=0.07419]\n",
      "Step 521195  [5.538 sec/step, loss=0.07499, avg_loss=0.07419]\n",
      "Step 521196  [5.541 sec/step, loss=0.07567, avg_loss=0.07422]\n",
      "Step 521197  [5.530 sec/step, loss=0.07240, avg_loss=0.07419]\n",
      "Step 521198  [5.469 sec/step, loss=0.07462, avg_loss=0.07426]\n",
      "Step 521199  [5.486 sec/step, loss=0.07654, avg_loss=0.07431]\n",
      "Generated 32 batches of size 32 in 2.439 sec\n",
      "Step 521200  [5.506 sec/step, loss=0.07700, avg_loss=0.07437]\n",
      "Writing summary at step: 521200\n",
      "Step 521201  [5.495 sec/step, loss=0.07446, avg_loss=0.07434]\n",
      "Step 521202  [5.502 sec/step, loss=0.07684, avg_loss=0.07437]\n",
      "Step 521203  [5.498 sec/step, loss=0.07684, avg_loss=0.07439]\n",
      "Step 521204  [5.493 sec/step, loss=0.07525, avg_loss=0.07439]\n",
      "Step 521205  [5.476 sec/step, loss=0.07449, avg_loss=0.07436]\n",
      "Step 521206  [5.465 sec/step, loss=0.07562, avg_loss=0.07435]\n",
      "Step 521207  [5.476 sec/step, loss=0.07378, avg_loss=0.07435]\n",
      "Step 521208  [5.464 sec/step, loss=0.07573, avg_loss=0.07435]\n",
      "Step 521209  [5.466 sec/step, loss=0.07536, avg_loss=0.07435]\n",
      "Step 521210  [5.458 sec/step, loss=0.07224, avg_loss=0.07431]\n",
      "Step 521211  [5.472 sec/step, loss=0.07678, avg_loss=0.07433]\n",
      "Step 521212  [5.444 sec/step, loss=0.07504, avg_loss=0.07434]\n",
      "Step 521213  [5.397 sec/step, loss=0.07603, avg_loss=0.07443]\n",
      "Step 521214  [5.395 sec/step, loss=0.07454, avg_loss=0.07443]\n",
      "Step 521215  [5.401 sec/step, loss=0.07662, avg_loss=0.07446]\n",
      "Step 521216  [5.400 sec/step, loss=0.07368, avg_loss=0.07445]\n",
      "Step 521217  [5.396 sec/step, loss=0.07585, avg_loss=0.07446]\n",
      "Step 521218  [5.392 sec/step, loss=0.07622, avg_loss=0.07445]\n",
      "Step 521219  [5.400 sec/step, loss=0.07488, avg_loss=0.07445]\n",
      "Step 521220  [5.421 sec/step, loss=0.07718, avg_loss=0.07448]\n",
      "Step 521221  [5.420 sec/step, loss=0.07659, avg_loss=0.07449]\n",
      "Step 521222  [5.424 sec/step, loss=0.07531, avg_loss=0.07450]\n",
      "Step 521223  [5.442 sec/step, loss=0.07707, avg_loss=0.07454]\n",
      "Step 521224  [5.439 sec/step, loss=0.07547, avg_loss=0.07454]\n",
      "Step 521225  [5.434 sec/step, loss=0.07460, avg_loss=0.07453]\n",
      "Step 521226  [5.450 sec/step, loss=0.07622, avg_loss=0.07458]\n",
      "Step 521227  [5.446 sec/step, loss=0.07602, avg_loss=0.07458]\n",
      "Step 521228  [5.433 sec/step, loss=0.07286, avg_loss=0.07455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 521229  [5.437 sec/step, loss=0.07437, avg_loss=0.07456]\n",
      "Step 521230  [5.416 sec/step, loss=0.07623, avg_loss=0.07457]\n",
      "Generated 32 batches of size 32 in 2.643 sec\n",
      "Step 521231  [5.463 sec/step, loss=0.06761, avg_loss=0.07448]\n",
      "Step 521232  [5.459 sec/step, loss=0.07356, avg_loss=0.07451]\n",
      "Step 521233  [5.485 sec/step, loss=0.07543, avg_loss=0.07453]\n",
      "Step 521234  [5.496 sec/step, loss=0.07515, avg_loss=0.07452]\n",
      "Step 521235  [5.529 sec/step, loss=0.07491, avg_loss=0.07453]\n",
      "Step 521236  [5.489 sec/step, loss=0.07199, avg_loss=0.07452]\n",
      "Step 521237  [5.483 sec/step, loss=0.06660, avg_loss=0.07444]\n",
      "Step 521238  [5.469 sec/step, loss=0.07467, avg_loss=0.07445]\n",
      "Step 521239  [5.454 sec/step, loss=0.07498, avg_loss=0.07447]\n",
      "Step 521240  [5.484 sec/step, loss=0.07525, avg_loss=0.07447]\n",
      "Step 521241  [5.476 sec/step, loss=0.07585, avg_loss=0.07447]\n",
      "Step 521242  [5.461 sec/step, loss=0.07670, avg_loss=0.07447]\n",
      "Step 521243  [5.478 sec/step, loss=0.07406, avg_loss=0.07455]\n",
      "Step 521244  [5.457 sec/step, loss=0.07231, avg_loss=0.07452]\n",
      "Step 521245  [5.433 sec/step, loss=0.07490, avg_loss=0.07451]\n",
      "Step 521246  [5.431 sec/step, loss=0.07658, avg_loss=0.07452]\n",
      "Step 521247  [5.441 sec/step, loss=0.07614, avg_loss=0.07455]\n",
      "Step 521248  [5.439 sec/step, loss=0.07544, avg_loss=0.07455]\n",
      "Step 521249  [5.435 sec/step, loss=0.07638, avg_loss=0.07454]\n",
      "Step 521250  [5.465 sec/step, loss=0.07724, avg_loss=0.07467]\n",
      "Step 521251  [5.453 sec/step, loss=0.06718, avg_loss=0.07458]\n",
      "Step 521252  [5.471 sec/step, loss=0.07673, avg_loss=0.07460]\n",
      "Step 521253  [5.458 sec/step, loss=0.07514, avg_loss=0.07459]\n",
      "Step 521254  [5.450 sec/step, loss=0.07452, avg_loss=0.07458]\n",
      "Step 521255  [5.438 sec/step, loss=0.07083, avg_loss=0.07454]\n",
      "Step 521256  [5.439 sec/step, loss=0.07435, avg_loss=0.07455]\n",
      "Step 521257  [5.454 sec/step, loss=0.07648, avg_loss=0.07457]\n",
      "Step 521258  [5.457 sec/step, loss=0.07694, avg_loss=0.07457]\n",
      "Step 521259  [5.458 sec/step, loss=0.07536, avg_loss=0.07459]\n",
      "Step 521260  [5.473 sec/step, loss=0.07609, avg_loss=0.07460]\n",
      "Step 521261  [5.462 sec/step, loss=0.07701, avg_loss=0.07461]\n",
      "Step 521262  [5.436 sec/step, loss=0.07472, avg_loss=0.07462]\n",
      "Generated 32 batches of size 32 in 2.590 sec\n",
      "Step 521263  [5.428 sec/step, loss=0.07352, avg_loss=0.07460]\n",
      "Step 521264  [5.425 sec/step, loss=0.07165, avg_loss=0.07458]\n",
      "Step 521265  [5.433 sec/step, loss=0.07464, avg_loss=0.07457]\n",
      "Step 521266  [5.452 sec/step, loss=0.07688, avg_loss=0.07463]\n",
      "Step 521267  [5.414 sec/step, loss=0.07374, avg_loss=0.07467]\n",
      "Step 521268  [5.473 sec/step, loss=0.06631, avg_loss=0.07462]\n",
      "Step 521269  [5.480 sec/step, loss=0.07495, avg_loss=0.07461]\n",
      "Step 521270  [5.474 sec/step, loss=0.07579, avg_loss=0.07460]\n",
      "Step 521271  [5.478 sec/step, loss=0.07621, avg_loss=0.07462]\n",
      "Step 521272  [5.475 sec/step, loss=0.07154, avg_loss=0.07460]\n",
      "Step 521273  [5.466 sec/step, loss=0.07420, avg_loss=0.07459]\n",
      "Step 521274  [5.460 sec/step, loss=0.07651, avg_loss=0.07459]\n",
      "Step 521275  [5.456 sec/step, loss=0.07641, avg_loss=0.07462]\n",
      "Step 521276  [5.441 sec/step, loss=0.06564, avg_loss=0.07452]\n",
      "Step 521277  [5.466 sec/step, loss=0.07683, avg_loss=0.07457]\n",
      "Step 521278  [5.437 sec/step, loss=0.07257, avg_loss=0.07455]\n",
      "Step 521279  [5.437 sec/step, loss=0.07441, avg_loss=0.07453]\n",
      "Step 521280  [5.409 sec/step, loss=0.07435, avg_loss=0.07454]\n",
      "Step 521281  [5.401 sec/step, loss=0.07637, avg_loss=0.07454]\n",
      "Step 521282  [5.420 sec/step, loss=0.07292, avg_loss=0.07461]\n",
      "Step 521283  [5.428 sec/step, loss=0.07582, avg_loss=0.07460]\n",
      "Step 521284  [5.437 sec/step, loss=0.07549, avg_loss=0.07462]\n",
      "Step 521285  [5.385 sec/step, loss=0.07517, avg_loss=0.07470]\n",
      "Step 521286  [5.388 sec/step, loss=0.07401, avg_loss=0.07470]\n",
      "Step 521287  [5.389 sec/step, loss=0.07530, avg_loss=0.07471]\n",
      "Step 521288  [5.390 sec/step, loss=0.07321, avg_loss=0.07469]\n",
      "Step 521289  [5.440 sec/step, loss=0.06740, avg_loss=0.07461]\n",
      "Step 521290  [5.447 sec/step, loss=0.07667, avg_loss=0.07464]\n",
      "Step 521291  [5.457 sec/step, loss=0.07427, avg_loss=0.07462]\n",
      "Step 521292  [5.469 sec/step, loss=0.07502, avg_loss=0.07464]\n",
      "Step 521293  [5.457 sec/step, loss=0.07506, avg_loss=0.07463]\n",
      "Step 521294  [5.433 sec/step, loss=0.07460, avg_loss=0.07462]\n",
      "Generated 32 batches of size 32 in 2.467 sec\n",
      "Step 521295  [5.435 sec/step, loss=0.07681, avg_loss=0.07464]\n",
      "Step 521296  [5.442 sec/step, loss=0.07686, avg_loss=0.07465]\n",
      "Step 521297  [5.458 sec/step, loss=0.07595, avg_loss=0.07469]\n",
      "Step 521298  [5.455 sec/step, loss=0.07156, avg_loss=0.07465]\n",
      "Step 521299  [5.448 sec/step, loss=0.07689, avg_loss=0.07466]\n",
      "Step 521300  [5.426 sec/step, loss=0.07364, avg_loss=0.07462]\n",
      "Writing summary at step: 521300\n",
      "Step 521301  [5.457 sec/step, loss=0.07377, avg_loss=0.07462]\n",
      "Step 521302  [5.448 sec/step, loss=0.07592, avg_loss=0.07461]\n",
      "Step 521303  [5.450 sec/step, loss=0.07627, avg_loss=0.07460]\n",
      "Step 521304  [5.448 sec/step, loss=0.07203, avg_loss=0.07457]\n",
      "Step 521305  [5.453 sec/step, loss=0.07553, avg_loss=0.07458]\n",
      "Step 521306  [5.457 sec/step, loss=0.07578, avg_loss=0.07458]\n",
      "Step 521307  [5.463 sec/step, loss=0.07374, avg_loss=0.07458]\n",
      "Step 521308  [5.455 sec/step, loss=0.07489, avg_loss=0.07457]\n",
      "Step 521309  [5.445 sec/step, loss=0.07480, avg_loss=0.07457]\n",
      "Step 521310  [5.453 sec/step, loss=0.07689, avg_loss=0.07461]\n",
      "Step 521311  [5.433 sec/step, loss=0.07410, avg_loss=0.07459]\n",
      "Step 521312  [5.443 sec/step, loss=0.07673, avg_loss=0.07460]\n",
      "Step 521313  [5.446 sec/step, loss=0.07454, avg_loss=0.07459]\n",
      "Step 521314  [5.497 sec/step, loss=0.06748, avg_loss=0.07452]\n",
      "Step 521315  [5.502 sec/step, loss=0.07656, avg_loss=0.07452]\n",
      "Step 521316  [5.505 sec/step, loss=0.07368, avg_loss=0.07452]\n",
      "Step 521317  [5.496 sec/step, loss=0.07487, avg_loss=0.07451]\n",
      "Step 521318  [5.497 sec/step, loss=0.07475, avg_loss=0.07449]\n",
      "Step 521319  [5.486 sec/step, loss=0.07508, avg_loss=0.07450]\n",
      "Step 521320  [5.469 sec/step, loss=0.07345, avg_loss=0.07446]\n",
      "Step 521321  [5.465 sec/step, loss=0.07616, avg_loss=0.07445]\n",
      "Step 521322  [5.488 sec/step, loss=0.07372, avg_loss=0.07444]\n",
      "Step 521323  [5.496 sec/step, loss=0.07367, avg_loss=0.07440]\n",
      "Step 521324  [5.491 sec/step, loss=0.07475, avg_loss=0.07440]\n",
      "Step 521325  [5.499 sec/step, loss=0.07499, avg_loss=0.07440]\n",
      "Generated 32 batches of size 32 in 2.597 sec\n",
      "Step 521326  [5.499 sec/step, loss=0.07567, avg_loss=0.07440]\n",
      "Step 521327  [5.482 sec/step, loss=0.06668, avg_loss=0.07430]\n",
      "Step 521328  [5.507 sec/step, loss=0.07680, avg_loss=0.07434]\n",
      "Step 521329  [5.500 sec/step, loss=0.07170, avg_loss=0.07432]\n",
      "Step 521330  [5.506 sec/step, loss=0.07572, avg_loss=0.07431]\n",
      "Step 521331  [5.447 sec/step, loss=0.07584, avg_loss=0.07439]\n",
      "Step 521332  [5.447 sec/step, loss=0.07321, avg_loss=0.07439]\n",
      "Step 521333  [5.433 sec/step, loss=0.07684, avg_loss=0.07440]\n",
      "Step 521334  [5.430 sec/step, loss=0.07389, avg_loss=0.07439]\n",
      "Step 521335  [5.387 sec/step, loss=0.07278, avg_loss=0.07437]\n",
      "Step 521336  [5.399 sec/step, loss=0.07440, avg_loss=0.07439]\n",
      "Step 521337  [5.407 sec/step, loss=0.07421, avg_loss=0.07447]\n",
      "Step 521338  [5.410 sec/step, loss=0.07496, avg_loss=0.07447]\n",
      "Step 521339  [5.422 sec/step, loss=0.07612, avg_loss=0.07448]\n",
      "Step 521340  [5.406 sec/step, loss=0.07404, avg_loss=0.07447]\n",
      "Step 521341  [5.414 sec/step, loss=0.07661, avg_loss=0.07448]\n",
      "Step 521342  [5.407 sec/step, loss=0.07218, avg_loss=0.07443]\n",
      "Step 521343  [5.417 sec/step, loss=0.07667, avg_loss=0.07446]\n",
      "Step 521344  [5.431 sec/step, loss=0.07651, avg_loss=0.07450]\n",
      "Step 521345  [5.455 sec/step, loss=0.07675, avg_loss=0.07452]\n",
      "Step 521346  [5.460 sec/step, loss=0.07667, avg_loss=0.07452]\n",
      "Step 521347  [5.463 sec/step, loss=0.07384, avg_loss=0.07450]\n",
      "Step 521348  [5.477 sec/step, loss=0.07687, avg_loss=0.07451]\n",
      "Step 521349  [5.456 sec/step, loss=0.06778, avg_loss=0.07443]\n",
      "Step 521350  [5.449 sec/step, loss=0.07578, avg_loss=0.07441]\n",
      "Step 521351  [5.461 sec/step, loss=0.07420, avg_loss=0.07448]\n",
      "Step 521352  [5.450 sec/step, loss=0.07550, avg_loss=0.07447]\n",
      "Step 521353  [5.450 sec/step, loss=0.07395, avg_loss=0.07446]\n",
      "Step 521354  [5.454 sec/step, loss=0.07592, avg_loss=0.07447]\n",
      "Step 521355  [5.469 sec/step, loss=0.07223, avg_loss=0.07449]\n",
      "Step 521356  [5.468 sec/step, loss=0.07355, avg_loss=0.07448]\n",
      "Step 521357  [5.460 sec/step, loss=0.07620, avg_loss=0.07448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 32 batches of size 32 in 2.426 sec\n",
      "Step 521358  [5.453 sec/step, loss=0.07559, avg_loss=0.07446]\n",
      "Step 521359  [5.465 sec/step, loss=0.07578, avg_loss=0.07447]\n",
      "Step 521360  [5.474 sec/step, loss=0.07416, avg_loss=0.07445]\n",
      "Step 521361  [5.459 sec/step, loss=0.07108, avg_loss=0.07439]\n",
      "Step 521362  [5.458 sec/step, loss=0.07458, avg_loss=0.07439]\n",
      "Step 521363  [5.474 sec/step, loss=0.07439, avg_loss=0.07439]\n",
      "Step 521364  [5.485 sec/step, loss=0.07543, avg_loss=0.07443]\n",
      "Step 521365  [5.529 sec/step, loss=0.06712, avg_loss=0.07436]\n",
      "Step 521366  [5.514 sec/step, loss=0.07440, avg_loss=0.07433]\n",
      "Step 521367  [5.518 sec/step, loss=0.07690, avg_loss=0.07436]\n",
      "Step 521368  [5.476 sec/step, loss=0.07551, avg_loss=0.07446]\n",
      "Step 521369  [5.483 sec/step, loss=0.07631, avg_loss=0.07447]\n",
      "Step 521370  [5.481 sec/step, loss=0.07562, avg_loss=0.07447]\n",
      "Step 521371  [5.479 sec/step, loss=0.07362, avg_loss=0.07444]\n",
      "Step 521372  [5.482 sec/step, loss=0.07314, avg_loss=0.07446]\n",
      "Step 521373  [5.465 sec/step, loss=0.06594, avg_loss=0.07438]\n",
      "Step 521374  [5.471 sec/step, loss=0.07678, avg_loss=0.07438]\n",
      "Step 521375  [5.467 sec/step, loss=0.07266, avg_loss=0.07434]\n",
      "Step 521376  [5.481 sec/step, loss=0.07540, avg_loss=0.07444]\n",
      "Step 521377  [5.471 sec/step, loss=0.07556, avg_loss=0.07443]\n",
      "Step 521378  [5.486 sec/step, loss=0.07462, avg_loss=0.07445]\n",
      "Step 521379  [5.471 sec/step, loss=0.07572, avg_loss=0.07446]\n",
      "Step 521380  [5.483 sec/step, loss=0.07517, avg_loss=0.07447]\n",
      "Step 521381  [5.483 sec/step, loss=0.07161, avg_loss=0.07442]\n",
      "Step 521382  [5.485 sec/step, loss=0.07562, avg_loss=0.07445]\n",
      "Step 521383  [5.484 sec/step, loss=0.07681, avg_loss=0.07446]\n",
      "Step 521384  [5.456 sec/step, loss=0.07133, avg_loss=0.07441]\n",
      "Step 521385  [5.468 sec/step, loss=0.07643, avg_loss=0.07443]\n",
      "Step 521386  [5.467 sec/step, loss=0.07298, avg_loss=0.07442]\n",
      "Step 521387  [5.463 sec/step, loss=0.07359, avg_loss=0.07440]\n",
      "Step 521388  [5.500 sec/step, loss=0.06921, avg_loss=0.07436]\n",
      "Step 521389  [5.451 sec/step, loss=0.07603, avg_loss=0.07445]\n",
      "Generated 32 batches of size 32 in 2.392 sec\n",
      "Step 521390  [5.454 sec/step, loss=0.07677, avg_loss=0.07445]\n",
      "Step 521391  [5.433 sec/step, loss=0.07453, avg_loss=0.07445]\n",
      "Step 521392  [5.417 sec/step, loss=0.07098, avg_loss=0.07441]\n",
      "Step 521393  [5.419 sec/step, loss=0.07598, avg_loss=0.07442]\n",
      "Step 521394  [5.430 sec/step, loss=0.07428, avg_loss=0.07442]\n",
      "Step 521395  [5.425 sec/step, loss=0.07651, avg_loss=0.07441]\n",
      "Step 521396  [5.407 sec/step, loss=0.07432, avg_loss=0.07439]\n",
      "Step 521397  [5.410 sec/step, loss=0.07702, avg_loss=0.07440]\n",
      "Step 521398  [5.450 sec/step, loss=0.07376, avg_loss=0.07442]\n",
      "Step 521399  [5.497 sec/step, loss=0.06753, avg_loss=0.07433]\n",
      "Step 521400  [5.530 sec/step, loss=0.07590, avg_loss=0.07435]\n",
      "Writing summary at step: 521400\n",
      "Step 521401  [5.494 sec/step, loss=0.07447, avg_loss=0.07436]\n",
      "Step 521402  [5.499 sec/step, loss=0.07581, avg_loss=0.07435]\n",
      "Step 521403  [5.488 sec/step, loss=0.07483, avg_loss=0.07434]\n",
      "Step 521404  [5.502 sec/step, loss=0.07589, avg_loss=0.07438]\n",
      "Step 521405  [5.492 sec/step, loss=0.07104, avg_loss=0.07433]\n",
      "Step 521406  [5.502 sec/step, loss=0.07650, avg_loss=0.07434]\n",
      "Step 521407  [5.483 sec/step, loss=0.06575, avg_loss=0.07426]\n",
      "Step 521408  [5.502 sec/step, loss=0.07664, avg_loss=0.07428]\n",
      "Step 521409  [5.524 sec/step, loss=0.07490, avg_loss=0.07428]\n",
      "Step 521410  [5.517 sec/step, loss=0.07481, avg_loss=0.07426]\n",
      "Step 521411  [5.529 sec/step, loss=0.07431, avg_loss=0.07426]\n",
      "Step 521412  [5.514 sec/step, loss=0.07420, avg_loss=0.07424]\n",
      "Step 521413  [5.510 sec/step, loss=0.07588, avg_loss=0.07425]\n",
      "Step 521414  [5.467 sec/step, loss=0.07465, avg_loss=0.07432]\n",
      "Step 521415  [5.453 sec/step, loss=0.07403, avg_loss=0.07430]\n",
      "Step 521416  [5.466 sec/step, loss=0.07669, avg_loss=0.07433]\n",
      "Step 521417  [5.470 sec/step, loss=0.07575, avg_loss=0.07433]\n",
      "Step 521418  [5.470 sec/step, loss=0.07422, avg_loss=0.07433]\n",
      "Step 521419  [5.475 sec/step, loss=0.07594, avg_loss=0.07434]\n",
      "Step 521420  [5.480 sec/step, loss=0.07577, avg_loss=0.07436]\n",
      "Generated 32 batches of size 32 in 2.657 sec\n",
      "Step 521421  [5.478 sec/step, loss=0.07608, avg_loss=0.07436]\n",
      "Step 521422  [5.444 sec/step, loss=0.07454, avg_loss=0.07437]\n",
      "Step 521423  [5.421 sec/step, loss=0.07220, avg_loss=0.07435]\n",
      "Step 521424  [5.410 sec/step, loss=0.07334, avg_loss=0.07434]\n",
      "Step 521425  [5.396 sec/step, loss=0.07281, avg_loss=0.07432]\n",
      "Step 521426  [5.403 sec/step, loss=0.07712, avg_loss=0.07433]\n",
      "Step 521427  [5.424 sec/step, loss=0.07530, avg_loss=0.07442]\n",
      "Step 521428  [5.420 sec/step, loss=0.07357, avg_loss=0.07439]\n",
      "Step 521429  [5.447 sec/step, loss=0.07642, avg_loss=0.07443]\n",
      "Step 521430  [5.438 sec/step, loss=0.07414, avg_loss=0.07442]\n",
      "Step 521431  [5.492 sec/step, loss=0.06691, avg_loss=0.07433]\n",
      "Step 521432  [5.510 sec/step, loss=0.07253, avg_loss=0.07432]\n",
      "Step 521433  [5.499 sec/step, loss=0.07488, avg_loss=0.07430]\n",
      "Step 521434  [5.479 sec/step, loss=0.07200, avg_loss=0.07428]\n",
      "Step 521435  [5.478 sec/step, loss=0.07212, avg_loss=0.07428]\n",
      "Step 521436  [5.464 sec/step, loss=0.07426, avg_loss=0.07428]\n",
      "Step 521437  [5.474 sec/step, loss=0.07201, avg_loss=0.07425]\n",
      "Step 521438  [5.494 sec/step, loss=0.07361, avg_loss=0.07424]\n",
      "Step 521439  [5.492 sec/step, loss=0.07572, avg_loss=0.07424]\n",
      "Step 521440  [5.474 sec/step, loss=0.07401, avg_loss=0.07424]\n",
      "Step 521441  [5.473 sec/step, loss=0.07651, avg_loss=0.07423]\n",
      "Step 521442  [5.487 sec/step, loss=0.07486, avg_loss=0.07426]\n",
      "Step 521443  [5.489 sec/step, loss=0.07452, avg_loss=0.07424]\n",
      "Step 521444  [5.475 sec/step, loss=0.07391, avg_loss=0.07421]\n",
      "Step 521445  [5.464 sec/step, loss=0.07554, avg_loss=0.07420]\n",
      "Step 521446  [5.485 sec/step, loss=0.07306, avg_loss=0.07417]\n",
      "Step 521447  [5.476 sec/step, loss=0.07672, avg_loss=0.07419]\n",
      "Step 521448  [5.456 sec/step, loss=0.07253, avg_loss=0.07415]\n",
      "Step 521449  [5.472 sec/step, loss=0.07477, avg_loss=0.07422]\n",
      "Step 521450  [5.473 sec/step, loss=0.07603, avg_loss=0.07422]\n",
      "Step 521451  [5.484 sec/step, loss=0.07555, avg_loss=0.07424]\n",
      "Step 521452  [5.489 sec/step, loss=0.07273, avg_loss=0.07421]\n",
      "Generated 32 batches of size 32 in 2.509 sec\n",
      "Step 521453  [5.495 sec/step, loss=0.07590, avg_loss=0.07423]\n",
      "Step 521454  [5.481 sec/step, loss=0.06706, avg_loss=0.07414]\n",
      "Step 521455  [5.491 sec/step, loss=0.07653, avg_loss=0.07418]\n",
      "Step 521456  [5.505 sec/step, loss=0.07672, avg_loss=0.07421]\n",
      "Step 521457  [5.519 sec/step, loss=0.07680, avg_loss=0.07422]\n",
      "Step 521458  [5.513 sec/step, loss=0.07556, avg_loss=0.07422]\n",
      "Step 521459  [5.502 sec/step, loss=0.07240, avg_loss=0.07419]\n",
      "Step 521460  [5.485 sec/step, loss=0.07656, avg_loss=0.07421]\n",
      "Step 521461  [5.503 sec/step, loss=0.07427, avg_loss=0.07424]\n",
      "Step 521462  [5.501 sec/step, loss=0.07584, avg_loss=0.07425]\n",
      "Step 521463  [5.515 sec/step, loss=0.07246, avg_loss=0.07424]\n",
      "Step 521464  [5.511 sec/step, loss=0.07441, avg_loss=0.07423]\n",
      "Step 521465  [5.473 sec/step, loss=0.07612, avg_loss=0.07432]\n",
      "Step 521466  [5.494 sec/step, loss=0.07602, avg_loss=0.07433]\n",
      "Step 521467  [5.487 sec/step, loss=0.07648, avg_loss=0.07433]\n",
      "Step 521468  [5.472 sec/step, loss=0.07458, avg_loss=0.07432]\n",
      "Step 521469  [5.459 sec/step, loss=0.07558, avg_loss=0.07431]\n",
      "Step 521470  [5.466 sec/step, loss=0.07643, avg_loss=0.07432]\n",
      "Step 521471  [5.470 sec/step, loss=0.07516, avg_loss=0.07433]\n",
      "Step 521472  [5.496 sec/step, loss=0.07615, avg_loss=0.07436]\n",
      "Step 521473  [5.511 sec/step, loss=0.07191, avg_loss=0.07442]\n",
      "Step 521474  [5.487 sec/step, loss=0.07421, avg_loss=0.07440]\n",
      "Step 521475  [5.483 sec/step, loss=0.07516, avg_loss=0.07442]\n",
      "Step 521476  [5.487 sec/step, loss=0.07563, avg_loss=0.07443]\n",
      "Step 521477  [5.494 sec/step, loss=0.07702, avg_loss=0.07444]\n",
      "Step 521478  [5.468 sec/step, loss=0.07214, avg_loss=0.07442]\n",
      "Step 521479  [5.476 sec/step, loss=0.07317, avg_loss=0.07439]\n",
      "Step 521480  [5.453 sec/step, loss=0.06624, avg_loss=0.07430]\n",
      "Step 521481  [5.471 sec/step, loss=0.07493, avg_loss=0.07433]\n",
      "Step 521482  [5.475 sec/step, loss=0.07634, avg_loss=0.07434]\n",
      "Step 521483  [5.460 sec/step, loss=0.07475, avg_loss=0.07432]\n",
      "Step 521484  [5.482 sec/step, loss=0.07412, avg_loss=0.07435]\n",
      "Generated 32 batches of size 32 in 2.467 sec\n",
      "Step 521485  [5.476 sec/step, loss=0.07531, avg_loss=0.07434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 521486  [5.470 sec/step, loss=0.07179, avg_loss=0.07433]\n",
      "Step 521487  [5.480 sec/step, loss=0.07278, avg_loss=0.07432]\n",
      "Step 521488  [5.432 sec/step, loss=0.07468, avg_loss=0.07437]\n",
      "Step 521489  [5.432 sec/step, loss=0.07493, avg_loss=0.07436]\n",
      "Step 521490  [5.427 sec/step, loss=0.07610, avg_loss=0.07435]\n",
      "Step 521491  [5.440 sec/step, loss=0.07591, avg_loss=0.07437]\n",
      "Step 521492  [5.448 sec/step, loss=0.07392, avg_loss=0.07440]\n",
      "Step 521493  [5.497 sec/step, loss=0.06635, avg_loss=0.07430]\n",
      "Step 521494  [5.497 sec/step, loss=0.07521, avg_loss=0.07431]\n",
      "Step 521495  [5.477 sec/step, loss=0.07467, avg_loss=0.07429]\n",
      "Step 521496  [5.492 sec/step, loss=0.07412, avg_loss=0.07429]\n",
      "Step 521497  [5.476 sec/step, loss=0.07398, avg_loss=0.07426]\n",
      "Step 521498  [5.447 sec/step, loss=0.07374, avg_loss=0.07426]\n",
      "Step 521499  [5.399 sec/step, loss=0.07599, avg_loss=0.07434]\n",
      "Step 521500  [5.370 sec/step, loss=0.07539, avg_loss=0.07434]\n",
      "Writing summary at step: 521500\n",
      "Step 521501  [5.377 sec/step, loss=0.07424, avg_loss=0.07434]\n",
      "Step 521502  [5.394 sec/step, loss=0.07310, avg_loss=0.07431]\n",
      "Step 521503  [5.395 sec/step, loss=0.07584, avg_loss=0.07432]\n",
      "Step 521504  [5.369 sec/step, loss=0.07277, avg_loss=0.07429]\n",
      "Step 521505  [5.387 sec/step, loss=0.07445, avg_loss=0.07432]\n",
      "Step 521506  [5.379 sec/step, loss=0.07507, avg_loss=0.07431]\n",
      "Step 521507  [5.405 sec/step, loss=0.07455, avg_loss=0.07440]\n",
      "Step 521508  [5.395 sec/step, loss=0.07492, avg_loss=0.07438]\n",
      "Step 521509  [5.390 sec/step, loss=0.07691, avg_loss=0.07440]\n",
      "Step 521510  [5.416 sec/step, loss=0.07384, avg_loss=0.07439]\n",
      "Step 521511  [5.417 sec/step, loss=0.07626, avg_loss=0.07441]\n",
      "Step 521512  [5.437 sec/step, loss=0.07654, avg_loss=0.07443]\n",
      "Step 521513  [5.448 sec/step, loss=0.07345, avg_loss=0.07441]\n",
      "Step 521514  [5.439 sec/step, loss=0.07544, avg_loss=0.07442]\n",
      "Step 521515  [5.442 sec/step, loss=0.07571, avg_loss=0.07443]\n",
      "Generated 32 batches of size 32 in 2.413 sec\n",
      "Step 521516  [5.439 sec/step, loss=0.07525, avg_loss=0.07442]\n",
      "Step 521517  [5.426 sec/step, loss=0.06616, avg_loss=0.07432]\n",
      "Step 521518  [5.406 sec/step, loss=0.07221, avg_loss=0.07430]\n",
      "Step 521519  [5.410 sec/step, loss=0.07655, avg_loss=0.07431]\n",
      "Step 521520  [5.403 sec/step, loss=0.07430, avg_loss=0.07429]\n",
      "Step 521521  [5.398 sec/step, loss=0.07391, avg_loss=0.07427]\n",
      "Step 521522  [5.456 sec/step, loss=0.06635, avg_loss=0.07419]\n",
      "Step 521523  [5.451 sec/step, loss=0.07656, avg_loss=0.07423]\n",
      "Step 521524  [5.460 sec/step, loss=0.07553, avg_loss=0.07426]\n",
      "Step 521525  [5.470 sec/step, loss=0.07240, avg_loss=0.07425]\n",
      "Step 521526  [5.471 sec/step, loss=0.07610, avg_loss=0.07424]\n",
      "Step 521527  [5.467 sec/step, loss=0.07548, avg_loss=0.07424]\n",
      "Step 521528  [5.445 sec/step, loss=0.07071, avg_loss=0.07421]\n",
      "Step 521529  [5.420 sec/step, loss=0.07441, avg_loss=0.07419]\n",
      "Step 521530  [5.435 sec/step, loss=0.07650, avg_loss=0.07422]\n",
      "Step 521531  [5.381 sec/step, loss=0.07568, avg_loss=0.07431]\n",
      "Step 521532  [5.394 sec/step, loss=0.07609, avg_loss=0.07434]\n",
      "Step 521533  [5.420 sec/step, loss=0.07328, avg_loss=0.07433]\n",
      "Step 521534  [5.481 sec/step, loss=0.06702, avg_loss=0.07428]\n",
      "Step 521535  [5.506 sec/step, loss=0.07464, avg_loss=0.07430]\n",
      "Step 521536  [5.500 sec/step, loss=0.06656, avg_loss=0.07422]\n",
      "Step 521537  [5.495 sec/step, loss=0.07416, avg_loss=0.07425]\n",
      "Step 521538  [5.484 sec/step, loss=0.07254, avg_loss=0.07423]\n",
      "Step 521539  [5.496 sec/step, loss=0.07476, avg_loss=0.07422]\n",
      "Step 521540  [5.493 sec/step, loss=0.07463, avg_loss=0.07423]\n",
      "Step 521541  [5.485 sec/step, loss=0.07521, avg_loss=0.07422]\n",
      "Step 521542  [5.481 sec/step, loss=0.07405, avg_loss=0.07421]\n",
      "Step 521543  [5.477 sec/step, loss=0.07602, avg_loss=0.07423]\n",
      "Step 521544  [5.495 sec/step, loss=0.07670, avg_loss=0.07425]\n",
      "Step 521545  [5.485 sec/step, loss=0.07414, avg_loss=0.07424]\n",
      "Step 521546  [5.444 sec/step, loss=0.07168, avg_loss=0.07423]\n",
      "Step 521547  [5.430 sec/step, loss=0.07582, avg_loss=0.07422]\n",
      "Generated 32 batches of size 32 in 2.379 sec\n",
      "Step 521548  [5.446 sec/step, loss=0.07649, avg_loss=0.07426]\n",
      "Step 521549  [5.451 sec/step, loss=0.07194, avg_loss=0.07423]\n",
      "Step 521550  [5.462 sec/step, loss=0.07597, avg_loss=0.07423]\n",
      "Step 521551  [5.467 sec/step, loss=0.07657, avg_loss=0.07424]\n",
      "Step 521552  [5.456 sec/step, loss=0.07445, avg_loss=0.07425]\n",
      "Step 521553  [5.455 sec/step, loss=0.07629, avg_loss=0.07426]\n",
      "Step 521554  [5.473 sec/step, loss=0.07319, avg_loss=0.07432]\n",
      "Step 521555  [5.465 sec/step, loss=0.07589, avg_loss=0.07431]\n",
      "Step 521556  [5.460 sec/step, loss=0.07541, avg_loss=0.07430]\n",
      "Step 521557  [5.455 sec/step, loss=0.07660, avg_loss=0.07430]\n",
      "Step 521558  [5.467 sec/step, loss=0.07589, avg_loss=0.07430]\n",
      "Step 521559  [5.482 sec/step, loss=0.07461, avg_loss=0.07432]\n",
      "Step 521560  [5.474 sec/step, loss=0.07586, avg_loss=0.07432]\n",
      "Step 521561  [5.469 sec/step, loss=0.07520, avg_loss=0.07433]\n",
      "Step 521562  [5.482 sec/step, loss=0.07648, avg_loss=0.07433]\n",
      "Step 521563  [5.454 sec/step, loss=0.07503, avg_loss=0.07436]\n",
      "Step 521564  [5.464 sec/step, loss=0.07185, avg_loss=0.07433]\n",
      "Step 521565  [5.452 sec/step, loss=0.07388, avg_loss=0.07431]\n",
      "Step 521566  [5.447 sec/step, loss=0.07654, avg_loss=0.07431]\n",
      "Step 521567  [5.430 sec/step, loss=0.07195, avg_loss=0.07427]\n",
      "Step 521568  [5.441 sec/step, loss=0.07584, avg_loss=0.07428]\n",
      "Step 521569  [5.426 sec/step, loss=0.07440, avg_loss=0.07427]\n",
      "Step 521570  [5.443 sec/step, loss=0.07347, avg_loss=0.07424]\n",
      "Step 521571  [5.432 sec/step, loss=0.07008, avg_loss=0.07419]\n",
      "Step 521572  [5.403 sec/step, loss=0.07165, avg_loss=0.07414]\n",
      "Step 521573  [5.398 sec/step, loss=0.07409, avg_loss=0.07417]\n",
      "Step 521574  [5.459 sec/step, loss=0.06737, avg_loss=0.07410]\n",
      "Step 521575  [5.457 sec/step, loss=0.07408, avg_loss=0.07409]\n",
      "Step 521576  [5.457 sec/step, loss=0.07580, avg_loss=0.07409]\n",
      "Step 521577  [5.453 sec/step, loss=0.07516, avg_loss=0.07407]\n",
      "Step 521578  [5.474 sec/step, loss=0.07600, avg_loss=0.07411]\n",
      "Step 521579  [5.465 sec/step, loss=0.07435, avg_loss=0.07412]\n",
      "Generated 32 batches of size 32 in 2.513 sec\n",
      "Step 521580  [5.486 sec/step, loss=0.07538, avg_loss=0.07421]\n",
      "Step 521581  [5.468 sec/step, loss=0.07369, avg_loss=0.07420]\n",
      "Step 521582  [5.456 sec/step, loss=0.07564, avg_loss=0.07419]\n",
      "Step 521583  [5.471 sec/step, loss=0.07665, avg_loss=0.07421]\n",
      "Step 521584  [5.446 sec/step, loss=0.06687, avg_loss=0.07414]\n",
      "Step 521585  [5.452 sec/step, loss=0.07496, avg_loss=0.07414]\n",
      "Step 521586  [5.465 sec/step, loss=0.07632, avg_loss=0.07418]\n",
      "Step 521587  [5.478 sec/step, loss=0.07321, avg_loss=0.07419]\n",
      "Step 521588  [5.497 sec/step, loss=0.07657, avg_loss=0.07420]\n",
      "Step 521589  [5.503 sec/step, loss=0.07444, avg_loss=0.07420]\n",
      "Step 521590  [5.487 sec/step, loss=0.07173, avg_loss=0.07416]\n",
      "Step 521591  [5.504 sec/step, loss=0.07302, avg_loss=0.07413]\n",
      "Step 521592  [5.504 sec/step, loss=0.07387, avg_loss=0.07413]\n",
      "Step 521593  [5.446 sec/step, loss=0.07419, avg_loss=0.07420]\n",
      "Step 521594  [5.445 sec/step, loss=0.07330, avg_loss=0.07419]\n",
      "Step 521595  [5.469 sec/step, loss=0.07439, avg_loss=0.07418]\n",
      "Step 521596  [5.467 sec/step, loss=0.07499, avg_loss=0.07419]\n",
      "Step 521597  [5.473 sec/step, loss=0.07405, avg_loss=0.07419]\n",
      "Step 521598  [5.464 sec/step, loss=0.07434, avg_loss=0.07420]\n",
      "Step 521599  [5.445 sec/step, loss=0.06641, avg_loss=0.07410]\n",
      "Step 521600  [5.441 sec/step, loss=0.07388, avg_loss=0.07409]\n",
      "Writing summary at step: 521600\n",
      "Step 521601  [5.438 sec/step, loss=0.07600, avg_loss=0.07411]\n",
      "Step 521602  [5.418 sec/step, loss=0.07152, avg_loss=0.07409]\n",
      "Step 521603  [5.425 sec/step, loss=0.07486, avg_loss=0.07408]\n",
      "Step 521604  [5.425 sec/step, loss=0.07231, avg_loss=0.07408]\n",
      "Step 521605  [5.426 sec/step, loss=0.07394, avg_loss=0.07407]\n",
      "Step 521606  [5.417 sec/step, loss=0.07379, avg_loss=0.07406]\n",
      "Step 521607  [5.409 sec/step, loss=0.07622, avg_loss=0.07407]\n",
      "Step 521608  [5.460 sec/step, loss=0.06666, avg_loss=0.07399]\n",
      "Step 521609  [5.458 sec/step, loss=0.07691, avg_loss=0.07399]\n",
      "Step 521610  [5.421 sec/step, loss=0.07153, avg_loss=0.07397]\n",
      "Generated 32 batches of size 32 in 2.442 sec\n",
      "Step 521611  [5.434 sec/step, loss=0.07638, avg_loss=0.07397]\n",
      "Step 521612  [5.425 sec/step, loss=0.07518, avg_loss=0.07396]\n",
      "Step 521613  [5.417 sec/step, loss=0.07489, avg_loss=0.07397]\n",
      "Step 521614  [5.421 sec/step, loss=0.07554, avg_loss=0.07397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 521615  [5.420 sec/step, loss=0.07507, avg_loss=0.07396]\n",
      "Step 521616  [5.412 sec/step, loss=0.07522, avg_loss=0.07396]\n",
      "Step 521617  [5.437 sec/step, loss=0.07651, avg_loss=0.07407]\n",
      "Step 521618  [5.462 sec/step, loss=0.07696, avg_loss=0.07412]\n",
      "Step 521619  [5.482 sec/step, loss=0.07334, avg_loss=0.07408]\n",
      "Step 521620  [5.481 sec/step, loss=0.07034, avg_loss=0.07404]\n",
      "Step 521621  [5.482 sec/step, loss=0.07229, avg_loss=0.07403]\n",
      "Step 521622  [5.412 sec/step, loss=0.06497, avg_loss=0.07401]\n",
      "Step 521623  [5.428 sec/step, loss=0.07731, avg_loss=0.07402]\n",
      "Step 521624  [5.425 sec/step, loss=0.07576, avg_loss=0.07402]\n",
      "Step 521625  [5.419 sec/step, loss=0.07473, avg_loss=0.07405]\n",
      "Step 521626  [5.460 sec/step, loss=0.06694, avg_loss=0.07396]\n",
      "Step 521627  [5.462 sec/step, loss=0.07561, avg_loss=0.07396]\n",
      "Step 521628  [5.475 sec/step, loss=0.07237, avg_loss=0.07397]\n",
      "Step 521629  [5.506 sec/step, loss=0.07396, avg_loss=0.07397]\n",
      "Step 521630  [5.509 sec/step, loss=0.07445, avg_loss=0.07395]\n",
      "Step 521631  [5.514 sec/step, loss=0.07572, avg_loss=0.07395]\n",
      "Step 521632  [5.488 sec/step, loss=0.07164, avg_loss=0.07390]\n",
      "Step 521633  [5.463 sec/step, loss=0.07572, avg_loss=0.07393]\n",
      "Step 521634  [5.412 sec/step, loss=0.07477, avg_loss=0.07401]\n",
      "Step 521635  [5.387 sec/step, loss=0.07242, avg_loss=0.07398]\n",
      "Step 521636  [5.400 sec/step, loss=0.07449, avg_loss=0.07406]\n",
      "Step 521637  [5.412 sec/step, loss=0.07564, avg_loss=0.07408]\n",
      "Step 521638  [5.409 sec/step, loss=0.07335, avg_loss=0.07409]\n",
      "Step 521639  [5.392 sec/step, loss=0.07480, avg_loss=0.07409]\n",
      "Step 521640  [5.410 sec/step, loss=0.07631, avg_loss=0.07410]\n",
      "Step 521641  [5.403 sec/step, loss=0.07444, avg_loss=0.07410]\n",
      "Step 521642  [5.429 sec/step, loss=0.07392, avg_loss=0.07409]\n",
      "Generated 32 batches of size 32 in 2.384 sec\n",
      "Step 521643  [5.439 sec/step, loss=0.07678, avg_loss=0.07410]\n",
      "Step 521644  [5.425 sec/step, loss=0.07536, avg_loss=0.07409]\n",
      "Step 521645  [5.438 sec/step, loss=0.07675, avg_loss=0.07411]\n",
      "Step 521646  [5.457 sec/step, loss=0.07522, avg_loss=0.07415]\n",
      "Step 521647  [5.473 sec/step, loss=0.07670, avg_loss=0.07416]\n",
      "Step 521648  [5.473 sec/step, loss=0.07683, avg_loss=0.07416]\n",
      "Step 521649  [5.470 sec/step, loss=0.07530, avg_loss=0.07420]\n",
      "Step 521650  [5.451 sec/step, loss=0.07395, avg_loss=0.07418]\n",
      "Step 521651  [5.450 sec/step, loss=0.07634, avg_loss=0.07417]\n",
      "Step 521652  [5.465 sec/step, loss=0.07659, avg_loss=0.07419]\n",
      "Step 521653  [5.477 sec/step, loss=0.07612, avg_loss=0.07419]\n",
      "Step 521654  [5.487 sec/step, loss=0.07635, avg_loss=0.07422]\n",
      "Step 521655  [5.491 sec/step, loss=0.07514, avg_loss=0.07422]\n",
      "Step 521656  [5.482 sec/step, loss=0.07193, avg_loss=0.07418]\n",
      "Step 521657  [5.475 sec/step, loss=0.07539, avg_loss=0.07417]\n",
      "Step 521658  [5.467 sec/step, loss=0.07582, avg_loss=0.07417]\n",
      "Step 521659  [5.453 sec/step, loss=0.07510, avg_loss=0.07417]\n",
      "Step 521660  [5.444 sec/step, loss=0.07329, avg_loss=0.07415]\n",
      "Step 521661  [5.433 sec/step, loss=0.07389, avg_loss=0.07414]\n",
      "Step 521662  [5.426 sec/step, loss=0.07406, avg_loss=0.07411]\n",
      "Step 521663  [5.416 sec/step, loss=0.07065, avg_loss=0.07407]\n",
      "Step 521664  [5.431 sec/step, loss=0.07612, avg_loss=0.07411]\n",
      "Step 521665  [5.458 sec/step, loss=0.07312, avg_loss=0.07410]\n",
      "Step 521666  [5.450 sec/step, loss=0.07474, avg_loss=0.07408]\n",
      "Step 521667  [5.479 sec/step, loss=0.07570, avg_loss=0.07412]\n",
      "Step 521668  [5.527 sec/step, loss=0.06748, avg_loss=0.07404]\n",
      "Step 521669  [5.533 sec/step, loss=0.07409, avg_loss=0.07404]\n",
      "Step 521670  [5.504 sec/step, loss=0.07579, avg_loss=0.07406]\n",
      "Step 521671  [5.509 sec/step, loss=0.07326, avg_loss=0.07409]\n",
      "Step 521672  [5.518 sec/step, loss=0.07549, avg_loss=0.07413]\n",
      "Step 521673  [5.511 sec/step, loss=0.07198, avg_loss=0.07411]\n",
      "Step 521674  [5.444 sec/step, loss=0.06802, avg_loss=0.07411]\n",
      "Generated 32 batches of size 32 in 2.428 sec\n",
      "Step 521675  [5.458 sec/step, loss=0.07533, avg_loss=0.07413]\n",
      "Step 521676  [5.457 sec/step, loss=0.07426, avg_loss=0.07411]\n",
      "Step 521677  [5.461 sec/step, loss=0.07722, avg_loss=0.07413]\n",
      "Step 521678  [5.461 sec/step, loss=0.07463, avg_loss=0.07412]\n",
      "Step 521679  [5.473 sec/step, loss=0.07616, avg_loss=0.07414]\n",
      "Step 521680  [5.461 sec/step, loss=0.07487, avg_loss=0.07413]\n",
      "Step 521681  [5.468 sec/step, loss=0.07495, avg_loss=0.07414]\n",
      "Step 521682  [5.482 sec/step, loss=0.07413, avg_loss=0.07413]\n",
      "Step 521683  [5.462 sec/step, loss=0.07367, avg_loss=0.07410]\n",
      "Step 521684  [5.477 sec/step, loss=0.07430, avg_loss=0.07417]\n",
      "Step 521685  [5.478 sec/step, loss=0.07637, avg_loss=0.07419]\n",
      "Step 521686  [5.470 sec/step, loss=0.07079, avg_loss=0.07413]\n",
      "Step 521687  [5.451 sec/step, loss=0.07631, avg_loss=0.07416]\n",
      "Step 521688  [5.463 sec/step, loss=0.07508, avg_loss=0.07415]\n",
      "Step 521689  [5.455 sec/step, loss=0.07355, avg_loss=0.07414]\n",
      "Step 521690  [5.465 sec/step, loss=0.07494, avg_loss=0.07417]\n",
      "Step 521691  [5.453 sec/step, loss=0.07457, avg_loss=0.07419]\n",
      "Step 521692  [5.461 sec/step, loss=0.07480, avg_loss=0.07420]\n",
      "Step 521693  [5.483 sec/step, loss=0.07567, avg_loss=0.07421]\n",
      "Step 521694  [5.475 sec/step, loss=0.07189, avg_loss=0.07420]\n",
      "Step 521695  [5.456 sec/step, loss=0.07410, avg_loss=0.07419]\n",
      "Step 521696  [5.442 sec/step, loss=0.07304, avg_loss=0.07417]\n",
      "Step 521697  [5.429 sec/step, loss=0.07315, avg_loss=0.07417]\n",
      "Step 521698  [5.453 sec/step, loss=0.07662, avg_loss=0.07419]\n",
      "Step 521699  [5.489 sec/step, loss=0.07580, avg_loss=0.07428]\n",
      "Step 521700  [5.504 sec/step, loss=0.07668, avg_loss=0.07431]\n",
      "Writing summary at step: 521700\n",
      "Step 521701  [5.504 sec/step, loss=0.07605, avg_loss=0.07431]\n",
      "Step 521702  [5.505 sec/step, loss=0.07552, avg_loss=0.07435]\n",
      "Step 521703  [5.494 sec/step, loss=0.07532, avg_loss=0.07436]\n",
      "Step 521704  [5.505 sec/step, loss=0.07174, avg_loss=0.07435]\n",
      "Step 521705  [5.504 sec/step, loss=0.07378, avg_loss=0.07435]\n",
      "Generated 32 batches of size 32 in 2.477 sec\n",
      "Step 521706  [5.514 sec/step, loss=0.07507, avg_loss=0.07436]\n",
      "Step 521707  [5.496 sec/step, loss=0.06556, avg_loss=0.07425]\n",
      "Step 521708  [5.442 sec/step, loss=0.07402, avg_loss=0.07433]\n",
      "Step 521709  [5.481 sec/step, loss=0.06615, avg_loss=0.07422]\n",
      "Step 521710  [5.503 sec/step, loss=0.07654, avg_loss=0.07427]\n",
      "Step 521711  [5.487 sec/step, loss=0.07596, avg_loss=0.07427]\n",
      "Step 521712  [5.487 sec/step, loss=0.07595, avg_loss=0.07427]\n",
      "Step 521713  [5.486 sec/step, loss=0.07447, avg_loss=0.07427]\n",
      "Step 521714  [5.480 sec/step, loss=0.07414, avg_loss=0.07426]\n",
      "Step 521715  [5.478 sec/step, loss=0.07211, avg_loss=0.07423]\n",
      "Step 521716  [5.466 sec/step, loss=0.07009, avg_loss=0.07417]\n",
      "Step 521717  [5.453 sec/step, loss=0.07323, avg_loss=0.07414]\n",
      "Step 521718  [5.456 sec/step, loss=0.07593, avg_loss=0.07413]\n",
      "Step 521719  [5.420 sec/step, loss=0.07327, avg_loss=0.07413]\n",
      "Step 521720  [5.435 sec/step, loss=0.07521, avg_loss=0.07418]\n",
      "Step 521721  [5.486 sec/step, loss=0.06690, avg_loss=0.07413]\n",
      "Step 521722  [5.505 sec/step, loss=0.07570, avg_loss=0.07423]\n",
      "Step 521723  [5.497 sec/step, loss=0.07253, avg_loss=0.07419]\n",
      "Step 521724  [5.481 sec/step, loss=0.07159, avg_loss=0.07414]\n",
      "Step 521725  [5.491 sec/step, loss=0.07560, avg_loss=0.07415]\n",
      "Step 521726  [5.465 sec/step, loss=0.07417, avg_loss=0.07422]\n",
      "Step 521727  [5.461 sec/step, loss=0.07506, avg_loss=0.07422]\n",
      "Step 521728  [5.460 sec/step, loss=0.07514, avg_loss=0.07425]\n",
      "Step 521729  [5.434 sec/step, loss=0.07357, avg_loss=0.07424]\n",
      "Step 521730  [5.433 sec/step, loss=0.07478, avg_loss=0.07425]\n",
      "Step 521731  [5.434 sec/step, loss=0.07544, avg_loss=0.07424]\n",
      "Step 521732  [5.444 sec/step, loss=0.07509, avg_loss=0.07428]\n",
      "Step 521733  [5.457 sec/step, loss=0.07658, avg_loss=0.07429]\n",
      "Step 521734  [5.469 sec/step, loss=0.07657, avg_loss=0.07430]\n",
      "Step 521735  [5.465 sec/step, loss=0.06564, avg_loss=0.07424]\n",
      "Step 521736  [5.458 sec/step, loss=0.07416, avg_loss=0.07423]\n",
      "Step 521737  [5.452 sec/step, loss=0.07513, avg_loss=0.07423]\n",
      "Generated 32 batches of size 32 in 2.390 sec\n",
      "Step 521738  [5.458 sec/step, loss=0.07609, avg_loss=0.07426]\n",
      "Step 521739  [5.475 sec/step, loss=0.07645, avg_loss=0.07427]\n",
      "Step 521740  [5.478 sec/step, loss=0.07620, avg_loss=0.07427]\n",
      "Step 521741  [5.494 sec/step, loss=0.07526, avg_loss=0.07428]\n",
      "Step 521742  [5.476 sec/step, loss=0.07631, avg_loss=0.07430]\n",
      "Step 521743  [5.453 sec/step, loss=0.07280, avg_loss=0.07426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 521744  [5.452 sec/step, loss=0.07547, avg_loss=0.07426]\n",
      "Step 521745  [5.439 sec/step, loss=0.07416, avg_loss=0.07424]\n",
      "Step 521746  [5.438 sec/step, loss=0.07528, avg_loss=0.07424]\n",
      "Step 521747  [5.431 sec/step, loss=0.07576, avg_loss=0.07423]\n",
      "Step 521748  [5.417 sec/step, loss=0.07551, avg_loss=0.07422]\n",
      "Step 521749  [5.418 sec/step, loss=0.07512, avg_loss=0.07421]\n",
      "Step 521750  [5.441 sec/step, loss=0.07331, avg_loss=0.07421]\n",
      "Step 521751  [5.431 sec/step, loss=0.07631, avg_loss=0.07421]\n",
      "Step 521752  [5.421 sec/step, loss=0.07400, avg_loss=0.07418]\n",
      "Step 521753  [5.391 sec/step, loss=0.06652, avg_loss=0.07409]\n",
      "Step 521754  [5.385 sec/step, loss=0.07532, avg_loss=0.07408]\n",
      "Step 521755  [5.365 sec/step, loss=0.07438, avg_loss=0.07407]\n",
      "Step 521756  [5.365 sec/step, loss=0.07269, avg_loss=0.07408]\n",
      "Step 521757  [5.358 sec/step, loss=0.07388, avg_loss=0.07406]\n",
      "Step 521758  [5.364 sec/step, loss=0.07607, avg_loss=0.07406]\n",
      "Step 521759  [5.375 sec/step, loss=0.07595, avg_loss=0.07407]\n",
      "Step 521760  [5.387 sec/step, loss=0.07513, avg_loss=0.07409]\n",
      "Step 521761  [5.384 sec/step, loss=0.07288, avg_loss=0.07408]\n",
      "Step 521762  [5.392 sec/step, loss=0.07637, avg_loss=0.07410]\n",
      "Step 521763  [5.404 sec/step, loss=0.07625, avg_loss=0.07416]\n",
      "Step 521764  [5.373 sec/step, loss=0.07236, avg_loss=0.07412]\n",
      "Step 521765  [5.344 sec/step, loss=0.07164, avg_loss=0.07411]\n",
      "Step 521766  [5.344 sec/step, loss=0.07219, avg_loss=0.07408]\n",
      "Step 521767  [5.325 sec/step, loss=0.07552, avg_loss=0.07408]\n",
      "Step 521768  [5.265 sec/step, loss=0.07462, avg_loss=0.07415]\n",
      "Step 521769  [5.261 sec/step, loss=0.07430, avg_loss=0.07415]\n",
      "Generated 32 batches of size 32 in 2.328 sec\n",
      "Step 521770  [5.265 sec/step, loss=0.07517, avg_loss=0.07415]\n",
      "Step 521771  [5.269 sec/step, loss=0.07509, avg_loss=0.07416]\n",
      "Step 521772  [5.283 sec/step, loss=0.07602, avg_loss=0.07417]\n",
      "Step 521773  [5.310 sec/step, loss=0.07683, avg_loss=0.07422]\n",
      "Step 521774  [5.351 sec/step, loss=0.07357, avg_loss=0.07427]\n",
      "Step 521775  [5.391 sec/step, loss=0.06622, avg_loss=0.07418]\n",
      "Step 521776  [5.402 sec/step, loss=0.07423, avg_loss=0.07418]\n",
      "Step 521777  [5.400 sec/step, loss=0.07543, avg_loss=0.07416]\n",
      "Step 521778  [5.404 sec/step, loss=0.07471, avg_loss=0.07417]\n",
      "Step 521779  [5.413 sec/step, loss=0.07553, avg_loss=0.07416]\n",
      "Step 521780  [5.430 sec/step, loss=0.07658, avg_loss=0.07418]\n",
      "Step 521781  [5.426 sec/step, loss=0.07187, avg_loss=0.07415]\n",
      "Step 521782  [5.424 sec/step, loss=0.07686, avg_loss=0.07417]\n",
      "Step 521783  [5.442 sec/step, loss=0.07539, avg_loss=0.07419]\n",
      "Step 521784  [5.447 sec/step, loss=0.07567, avg_loss=0.07420]\n",
      "Step 521785  [5.436 sec/step, loss=0.07601, avg_loss=0.07420]\n",
      "Step 521786  [5.440 sec/step, loss=0.07470, avg_loss=0.07424]\n",
      "Step 521787  [5.440 sec/step, loss=0.07586, avg_loss=0.07423]\n",
      "Step 521788  [5.408 sec/step, loss=0.07455, avg_loss=0.07423]\n",
      "Step 521789  [5.394 sec/step, loss=0.07108, avg_loss=0.07420]\n",
      "Step 521790  [5.438 sec/step, loss=0.06687, avg_loss=0.07412]\n",
      "Step 521791  [5.456 sec/step, loss=0.07546, avg_loss=0.07413]\n",
      "Step 521792  [5.456 sec/step, loss=0.07562, avg_loss=0.07414]\n",
      "Step 521793  [5.453 sec/step, loss=0.07663, avg_loss=0.07415]\n",
      "Step 521794  [5.467 sec/step, loss=0.07491, avg_loss=0.07418]\n",
      "Step 521795  [5.469 sec/step, loss=0.07621, avg_loss=0.07420]\n",
      "Step 521796  [5.483 sec/step, loss=0.07504, avg_loss=0.07422]\n",
      "Step 521797  [5.497 sec/step, loss=0.07459, avg_loss=0.07424]\n",
      "Step 521798  [5.494 sec/step, loss=0.07402, avg_loss=0.07421]\n",
      "Step 521799  [5.474 sec/step, loss=0.07206, avg_loss=0.07417]\n",
      "Step 521800  [5.455 sec/step, loss=0.07442, avg_loss=0.07415]\n",
      "Writing summary at step: 521800\n",
      "Generated 32 batches of size 32 in 2.401 sec\n",
      "Step 521801  [5.476 sec/step, loss=0.07633, avg_loss=0.07415]\n",
      "Step 521802  [5.477 sec/step, loss=0.07580, avg_loss=0.07416]\n",
      "Step 521803  [5.463 sec/step, loss=0.06553, avg_loss=0.07406]\n",
      "Step 521804  [5.456 sec/step, loss=0.07130, avg_loss=0.07405]\n",
      "Step 521805  [5.441 sec/step, loss=0.07355, avg_loss=0.07405]\n",
      "Step 521806  [5.434 sec/step, loss=0.07537, avg_loss=0.07405]\n",
      "Step 521807  [5.447 sec/step, loss=0.07307, avg_loss=0.07413]\n",
      "Step 521808  [5.443 sec/step, loss=0.07438, avg_loss=0.07413]\n",
      "Step 521809  [5.401 sec/step, loss=0.07571, avg_loss=0.07423]\n",
      "Step 521810  [5.394 sec/step, loss=0.07506, avg_loss=0.07421]\n",
      "Step 521811  [5.395 sec/step, loss=0.07512, avg_loss=0.07421]\n",
      "Step 521812  [5.387 sec/step, loss=0.07411, avg_loss=0.07419]\n",
      "Step 521813  [5.392 sec/step, loss=0.07342, avg_loss=0.07418]\n",
      "Step 521814  [5.392 sec/step, loss=0.07197, avg_loss=0.07415]\n",
      "Step 521815  [5.389 sec/step, loss=0.07507, avg_loss=0.07418]\n",
      "Step 521816  [5.427 sec/step, loss=0.07396, avg_loss=0.07422]\n",
      "Step 521817  [5.431 sec/step, loss=0.07606, avg_loss=0.07425]\n",
      "Step 521818  [5.412 sec/step, loss=0.07563, avg_loss=0.07425]\n",
      "Step 521819  [5.422 sec/step, loss=0.07616, avg_loss=0.07428]\n",
      "Step 521820  [5.402 sec/step, loss=0.07112, avg_loss=0.07424]\n",
      "Step 521821  [5.364 sec/step, loss=0.07660, avg_loss=0.07433]\n",
      "Step 521822  [5.383 sec/step, loss=0.07330, avg_loss=0.07431]\n",
      "Step 521823  [5.379 sec/step, loss=0.07281, avg_loss=0.07431]\n",
      "Step 521824  [5.386 sec/step, loss=0.07378, avg_loss=0.07433]\n",
      "Step 521825  [5.385 sec/step, loss=0.07449, avg_loss=0.07432]\n",
      "Step 521826  [5.351 sec/step, loss=0.07401, avg_loss=0.07432]\n",
      "Step 521827  [5.353 sec/step, loss=0.07476, avg_loss=0.07432]\n",
      "Step 521828  [5.356 sec/step, loss=0.07340, avg_loss=0.07430]\n",
      "Step 521829  [5.369 sec/step, loss=0.07512, avg_loss=0.07432]\n",
      "Step 521830  [5.357 sec/step, loss=0.07367, avg_loss=0.07431]\n",
      "Step 521831  [5.345 sec/step, loss=0.07458, avg_loss=0.07430]\n",
      "Step 521832  [5.345 sec/step, loss=0.07515, avg_loss=0.07430]\n",
      "Generated 32 batches of size 32 in 2.395 sec\n",
      "Step 521833  [5.348 sec/step, loss=0.07604, avg_loss=0.07429]\n",
      "Step 521834  [5.347 sec/step, loss=0.07651, avg_loss=0.07429]\n",
      "Step 521835  [5.377 sec/step, loss=0.07661, avg_loss=0.07440]\n",
      "Step 521836  [5.371 sec/step, loss=0.06677, avg_loss=0.07433]\n",
      "Step 521837  [5.383 sec/step, loss=0.07675, avg_loss=0.07434]\n",
      "Step 521838  [5.377 sec/step, loss=0.07528, avg_loss=0.07434]\n",
      "Step 521839  [5.414 sec/step, loss=0.06620, avg_loss=0.07423]\n",
      "Step 521840  [5.389 sec/step, loss=0.07252, avg_loss=0.07420]\n",
      "Step 521841  [5.388 sec/step, loss=0.07663, avg_loss=0.07421]\n",
      "Step 521842  [5.387 sec/step, loss=0.07570, avg_loss=0.07420]\n",
      "Step 521843  [5.406 sec/step, loss=0.07642, avg_loss=0.07424]\n",
      "Step 521844  [5.406 sec/step, loss=0.07564, avg_loss=0.07424]\n",
      "Step 521845  [5.395 sec/step, loss=0.06589, avg_loss=0.07416]\n",
      "Step 521846  [5.390 sec/step, loss=0.07483, avg_loss=0.07415]\n",
      "Step 521847  [5.409 sec/step, loss=0.07373, avg_loss=0.07413]\n",
      "Step 521848  [5.422 sec/step, loss=0.07643, avg_loss=0.07414]\n",
      "Step 521849  [5.421 sec/step, loss=0.07565, avg_loss=0.07415]\n",
      "Step 521850  [5.451 sec/step, loss=0.06756, avg_loss=0.07409]\n",
      "Step 521851  [5.450 sec/step, loss=0.07563, avg_loss=0.07408]\n",
      "Step 521852  [5.456 sec/step, loss=0.07600, avg_loss=0.07410]\n",
      "Step 521853  [5.473 sec/step, loss=0.07442, avg_loss=0.07418]\n",
      "Step 521854  [5.471 sec/step, loss=0.07298, avg_loss=0.07416]\n",
      "Step 521855  [5.486 sec/step, loss=0.07456, avg_loss=0.07416]\n",
      "Step 521856  [5.504 sec/step, loss=0.07641, avg_loss=0.07420]\n",
      "Step 521857  [5.504 sec/step, loss=0.07412, avg_loss=0.07420]\n",
      "Step 521858  [5.485 sec/step, loss=0.07174, avg_loss=0.07416]\n",
      "Step 521859  [5.486 sec/step, loss=0.07631, avg_loss=0.07416]\n",
      "Step 521860  [5.474 sec/step, loss=0.07434, avg_loss=0.07415]\n",
      "Step 521861  [5.488 sec/step, loss=0.07504, avg_loss=0.07417]\n",
      "Step 521862  [5.470 sec/step, loss=0.07454, avg_loss=0.07416]\n",
      "Step 521863  [5.465 sec/step, loss=0.07548, avg_loss=0.07415]\n",
      "Step 521864  [5.475 sec/step, loss=0.07228, avg_loss=0.07415]\n",
      "Generated 32 batches of size 32 in 2.529 sec\n",
      "Step 521865  [5.479 sec/step, loss=0.07382, avg_loss=0.07417]\n",
      "Step 521866  [5.483 sec/step, loss=0.07580, avg_loss=0.07421]\n",
      "Step 521867  [5.474 sec/step, loss=0.07222, avg_loss=0.07417]\n",
      "Step 521868  [5.488 sec/step, loss=0.07665, avg_loss=0.07419]\n",
      "Step 521869  [5.510 sec/step, loss=0.07480, avg_loss=0.07420]\n",
      "Step 521870  [5.526 sec/step, loss=0.07322, avg_loss=0.07418]\n",
      "Step 521871  [5.515 sec/step, loss=0.07073, avg_loss=0.07414]\n",
      "Step 521872  [5.505 sec/step, loss=0.07226, avg_loss=0.07410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 521873  [5.503 sec/step, loss=0.07393, avg_loss=0.07407]\n",
      "Step 521874  [5.467 sec/step, loss=0.07406, avg_loss=0.07407]\n",
      "Step 521875  [5.418 sec/step, loss=0.07499, avg_loss=0.07416]\n",
      "Step 521876  [5.407 sec/step, loss=0.07591, avg_loss=0.07418]\n",
      "Step 521877  [5.400 sec/step, loss=0.07550, avg_loss=0.07418]\n",
      "Step 521878  [5.374 sec/step, loss=0.06659, avg_loss=0.07410]\n",
      "Step 521879  [5.370 sec/step, loss=0.07588, avg_loss=0.07410]\n",
      "Step 521880  [5.359 sec/step, loss=0.07399, avg_loss=0.07408]\n",
      "Step 521881  [5.357 sec/step, loss=0.07189, avg_loss=0.07408]\n",
      "Step 521882  [5.342 sec/step, loss=0.07410, avg_loss=0.07405]\n",
      "Step 521883  [5.345 sec/step, loss=0.07520, avg_loss=0.07405]\n",
      "Step 521884  [5.352 sec/step, loss=0.07401, avg_loss=0.07403]\n",
      "Step 521885  [5.365 sec/step, loss=0.07380, avg_loss=0.07401]\n",
      "Step 521886  [5.368 sec/step, loss=0.07295, avg_loss=0.07399]\n",
      "Step 521887  [5.375 sec/step, loss=0.07440, avg_loss=0.07398]\n",
      "Step 521888  [5.375 sec/step, loss=0.07399, avg_loss=0.07397]\n",
      "Step 521889  [5.391 sec/step, loss=0.07594, avg_loss=0.07402]\n",
      "Step 521890  [5.345 sec/step, loss=0.07467, avg_loss=0.07410]\n",
      "Step 521891  [5.306 sec/step, loss=0.07249, avg_loss=0.07407]\n",
      "Step 521892  [5.310 sec/step, loss=0.07673, avg_loss=0.07408]\n",
      "Step 521893  [5.294 sec/step, loss=0.07384, avg_loss=0.07405]\n",
      "Step 521894  [5.284 sec/step, loss=0.07308, avg_loss=0.07403]\n",
      "Step 521895  [5.302 sec/step, loss=0.07645, avg_loss=0.07403]\n",
      "Step 521896  [5.304 sec/step, loss=0.07484, avg_loss=0.07403]\n",
      "Generated 32 batches of size 32 in 2.344 sec\n",
      "Step 521897  [5.319 sec/step, loss=0.07636, avg_loss=0.07405]\n",
      "Step 521898  [5.308 sec/step, loss=0.07530, avg_loss=0.07406]\n",
      "Step 521899  [5.298 sec/step, loss=0.07209, avg_loss=0.07406]\n",
      "Step 521900  [5.317 sec/step, loss=0.07668, avg_loss=0.07409]\n",
      "Writing summary at step: 521900\n",
      "Step 521901  [5.350 sec/step, loss=0.06625, avg_loss=0.07398]\n",
      "Step 521902  [5.352 sec/step, loss=0.07496, avg_loss=0.07398]\n",
      "Step 521903  [5.367 sec/step, loss=0.07566, avg_loss=0.07408]\n",
      "Step 521904  [5.403 sec/step, loss=0.07311, avg_loss=0.07410]\n",
      "Step 521905  [5.423 sec/step, loss=0.07587, avg_loss=0.07412]\n",
      "Step 521906  [5.438 sec/step, loss=0.07655, avg_loss=0.07413]\n",
      "Step 521907  [5.449 sec/step, loss=0.07646, avg_loss=0.07416]\n",
      "Step 521908  [5.461 sec/step, loss=0.07577, avg_loss=0.07418]\n",
      "Step 521909  [5.454 sec/step, loss=0.07517, avg_loss=0.07417]\n",
      "Step 521910  [5.454 sec/step, loss=0.07469, avg_loss=0.07417]\n",
      "Step 521911  [5.468 sec/step, loss=0.07635, avg_loss=0.07418]\n",
      "Step 521912  [5.468 sec/step, loss=0.07184, avg_loss=0.07416]\n",
      "Step 521913  [5.455 sec/step, loss=0.07460, avg_loss=0.07417]\n",
      "Step 521914  [5.466 sec/step, loss=0.07631, avg_loss=0.07421]\n",
      "Step 521915  [5.478 sec/step, loss=0.07571, avg_loss=0.07422]\n",
      "Step 521916  [5.460 sec/step, loss=0.07605, avg_loss=0.07424]\n",
      "Step 521917  [5.441 sec/step, loss=0.06555, avg_loss=0.07414]\n",
      "Step 521918  [5.443 sec/step, loss=0.07532, avg_loss=0.07413]\n",
      "Step 521919  [5.466 sec/step, loss=0.07588, avg_loss=0.07413]\n",
      "Step 521920  [5.473 sec/step, loss=0.07565, avg_loss=0.07418]\n",
      "Step 521921  [5.464 sec/step, loss=0.07583, avg_loss=0.07417]\n",
      "Step 521922  [5.430 sec/step, loss=0.07118, avg_loss=0.07415]\n",
      "Step 521923  [5.431 sec/step, loss=0.07544, avg_loss=0.07417]\n",
      "Step 521924  [5.489 sec/step, loss=0.06599, avg_loss=0.07410]\n",
      "Step 521925  [5.489 sec/step, loss=0.07385, avg_loss=0.07409]\n",
      "Step 521926  [5.504 sec/step, loss=0.07527, avg_loss=0.07410]\n",
      "Step 521927  [5.492 sec/step, loss=0.07440, avg_loss=0.07410]\n",
      "Generated 32 batches of size 32 in 2.629 sec\n",
      "Step 521928  [5.482 sec/step, loss=0.07144, avg_loss=0.07408]\n",
      "Step 521929  [5.465 sec/step, loss=0.07340, avg_loss=0.07406]\n",
      "Step 521930  [5.467 sec/step, loss=0.07476, avg_loss=0.07407]\n",
      "Step 521931  [5.467 sec/step, loss=0.07451, avg_loss=0.07407]\n",
      "Step 521932  [5.463 sec/step, loss=0.07310, avg_loss=0.07405]\n",
      "Step 521933  [5.458 sec/step, loss=0.07396, avg_loss=0.07403]\n",
      "Step 521934  [5.449 sec/step, loss=0.07618, avg_loss=0.07403]\n",
      "Step 521935  [5.448 sec/step, loss=0.07409, avg_loss=0.07400]\n",
      "Step 521936  [5.475 sec/step, loss=0.07664, avg_loss=0.07410]\n",
      "Step 521937  [5.468 sec/step, loss=0.07505, avg_loss=0.07408]\n",
      "Step 521938  [5.458 sec/step, loss=0.07224, avg_loss=0.07405]\n",
      "Step 521939  [5.400 sec/step, loss=0.07257, avg_loss=0.07412]\n",
      "Step 521940  [5.422 sec/step, loss=0.07563, avg_loss=0.07415]\n",
      "Step 521941  [5.431 sec/step, loss=0.07584, avg_loss=0.07414]\n",
      "Step 521942  [5.420 sec/step, loss=0.07462, avg_loss=0.07413]\n",
      "Step 521943  [5.414 sec/step, loss=0.07710, avg_loss=0.07414]\n",
      "Step 521944  [5.415 sec/step, loss=0.07436, avg_loss=0.07412]\n",
      "Step 521945  [5.458 sec/step, loss=0.07344, avg_loss=0.07420]\n",
      "Step 521946  [5.460 sec/step, loss=0.07599, avg_loss=0.07421]\n",
      "Step 521947  [5.446 sec/step, loss=0.07633, avg_loss=0.07424]\n",
      "Step 521948  [5.448 sec/step, loss=0.07417, avg_loss=0.07421]\n",
      "Step 521949  [5.456 sec/step, loss=0.07633, avg_loss=0.07422]\n",
      "Step 521950  [5.419 sec/step, loss=0.07650, avg_loss=0.07431]\n",
      "Step 521951  [5.411 sec/step, loss=0.07418, avg_loss=0.07430]\n",
      "Step 521952  [5.455 sec/step, loss=0.06699, avg_loss=0.07421]\n",
      "Step 521953  [5.444 sec/step, loss=0.07488, avg_loss=0.07421]\n",
      "Step 521954  [5.441 sec/step, loss=0.07326, avg_loss=0.07421]\n",
      "Step 521955  [5.420 sec/step, loss=0.06565, avg_loss=0.07412]\n",
      "Step 521956  [5.395 sec/step, loss=0.07062, avg_loss=0.07407]\n",
      "Step 521957  [5.399 sec/step, loss=0.07477, avg_loss=0.07407]\n",
      "Step 521958  [5.414 sec/step, loss=0.07490, avg_loss=0.07410]\n",
      "Step 521959  [5.400 sec/step, loss=0.07418, avg_loss=0.07408]\n",
      "Generated 32 batches of size 32 in 2.536 sec\n",
      "Step 521960  [5.410 sec/step, loss=0.07377, avg_loss=0.07408]\n",
      "Step 521961  [5.406 sec/step, loss=0.07555, avg_loss=0.07408]\n",
      "Step 521962  [5.428 sec/step, loss=0.07606, avg_loss=0.07410]\n",
      "Step 521963  [5.433 sec/step, loss=0.07567, avg_loss=0.07410]\n",
      "Step 521964  [5.435 sec/step, loss=0.07500, avg_loss=0.07413]\n",
      "Step 521965  [5.436 sec/step, loss=0.07575, avg_loss=0.07415]\n",
      "Step 521966  [5.440 sec/step, loss=0.07637, avg_loss=0.07415]\n",
      "Step 521967  [5.459 sec/step, loss=0.07341, avg_loss=0.07416]\n",
      "Step 521968  [5.439 sec/step, loss=0.07239, avg_loss=0.07412]\n",
      "Step 521969  [5.430 sec/step, loss=0.07463, avg_loss=0.07412]\n",
      "Step 521970  [5.394 sec/step, loss=0.06765, avg_loss=0.07406]\n",
      "Step 521971  [5.413 sec/step, loss=0.07368, avg_loss=0.07409]\n",
      "Step 521972  [5.408 sec/step, loss=0.07417, avg_loss=0.07411]\n",
      "Step 521973  [5.390 sec/step, loss=0.07059, avg_loss=0.07408]\n",
      "Step 521974  [5.412 sec/step, loss=0.07586, avg_loss=0.07410]\n",
      "Step 521975  [5.440 sec/step, loss=0.07368, avg_loss=0.07408]\n",
      "Step 521976  [5.440 sec/step, loss=0.07497, avg_loss=0.07407]\n",
      "Step 521977  [5.440 sec/step, loss=0.07542, avg_loss=0.07407]\n",
      "Step 521978  [5.446 sec/step, loss=0.07231, avg_loss=0.07413]\n",
      "Step 521979  [5.430 sec/step, loss=0.07563, avg_loss=0.07413]\n",
      "Step 521980  [5.443 sec/step, loss=0.07456, avg_loss=0.07413]\n",
      "Step 521981  [5.461 sec/step, loss=0.07608, avg_loss=0.07418]\n",
      "Step 521982  [5.473 sec/step, loss=0.07435, avg_loss=0.07418]\n",
      "Step 521983  [5.449 sec/step, loss=0.07460, avg_loss=0.07417]\n",
      "Step 521984  [5.434 sec/step, loss=0.07349, avg_loss=0.07417]\n",
      "Step 521985  [5.426 sec/step, loss=0.07563, avg_loss=0.07418]\n",
      "Step 521986  [5.417 sec/step, loss=0.07413, avg_loss=0.07420]\n",
      "Step 521987  [5.426 sec/step, loss=0.07575, avg_loss=0.07421]\n",
      "Step 521988  [5.431 sec/step, loss=0.07451, avg_loss=0.07422]\n",
      "Step 521989  [5.441 sec/step, loss=0.07631, avg_loss=0.07422]\n",
      "Step 521990  [5.433 sec/step, loss=0.07558, avg_loss=0.07423]\n",
      "Step 521991  [5.449 sec/step, loss=0.07532, avg_loss=0.07426]\n",
      "Generated 32 batches of size 32 in 2.608 sec\n",
      "Step 521992  [5.442 sec/step, loss=0.07194, avg_loss=0.07421]\n",
      "Step 521993  [5.455 sec/step, loss=0.07721, avg_loss=0.07424]\n",
      "Step 521994  [5.454 sec/step, loss=0.07417, avg_loss=0.07425]\n",
      "Step 521995  [5.452 sec/step, loss=0.07643, avg_loss=0.07425]\n",
      "Step 521996  [5.455 sec/step, loss=0.07452, avg_loss=0.07425]\n",
      "Step 521997  [5.491 sec/step, loss=0.06627, avg_loss=0.07415]\n",
      "Step 521998  [5.478 sec/step, loss=0.07245, avg_loss=0.07412]\n",
      "Step 521999  [5.491 sec/step, loss=0.07615, avg_loss=0.07416]\n",
      "Step 522000  [5.489 sec/step, loss=0.07656, avg_loss=0.07416]\n",
      "Writing summary at step: 522000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-522000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving audio and alignment...\n",
      "Input: kaandzoo gadzar kii shaedoo daraaiing miin kunvayn sordz or koothrii koo khirrkii kay ggallaaf sae dzatsayn~________________________________\n",
      "Step 522001  [5.435 sec/step, loss=0.07297, avg_loss=0.07423]\n",
      "Step 522002  [5.433 sec/step, loss=0.07550, avg_loss=0.07423]\n",
      "Step 522003  [5.434 sec/step, loss=0.07568, avg_loss=0.07423]\n",
      "Step 522004  [5.422 sec/step, loss=0.07668, avg_loss=0.07427]\n",
      "Step 522005  [5.425 sec/step, loss=0.07564, avg_loss=0.07427]\n",
      "Step 522006  [5.419 sec/step, loss=0.07611, avg_loss=0.07426]\n",
      "Step 522007  [5.409 sec/step, loss=0.07553, avg_loss=0.07425]\n",
      "Step 522008  [5.393 sec/step, loss=0.07483, avg_loss=0.07424]\n",
      "Step 522009  [5.397 sec/step, loss=0.07612, avg_loss=0.07425]\n",
      "Step 522010  [5.386 sec/step, loss=0.07415, avg_loss=0.07425]\n",
      "Step 522011  [5.363 sec/step, loss=0.07352, avg_loss=0.07422]\n",
      "Step 522012  [5.356 sec/step, loss=0.07260, avg_loss=0.07423]\n",
      "Step 522013  [5.372 sec/step, loss=0.07653, avg_loss=0.07425]\n",
      "Step 522014  [5.372 sec/step, loss=0.07682, avg_loss=0.07425]\n",
      "Step 522015  [5.362 sec/step, loss=0.07513, avg_loss=0.07424]\n",
      "Step 522016  [5.335 sec/step, loss=0.06636, avg_loss=0.07415]\n",
      "Step 522017  [5.360 sec/step, loss=0.07515, avg_loss=0.07424]\n",
      "Step 522018  [5.360 sec/step, loss=0.07384, avg_loss=0.07423]\n",
      "Step 522019  [5.345 sec/step, loss=0.07381, avg_loss=0.07421]\n",
      "Step 522020  [5.349 sec/step, loss=0.07585, avg_loss=0.07421]\n",
      "Step 522021  [5.351 sec/step, loss=0.07487, avg_loss=0.07420]\n",
      "Generated 32 batches of size 32 in 2.626 sec\n",
      "Step 522022  [5.366 sec/step, loss=0.07552, avg_loss=0.07424]\n",
      "Step 522023  [5.417 sec/step, loss=0.06707, avg_loss=0.07416]\n",
      "Step 522024  [5.353 sec/step, loss=0.07288, avg_loss=0.07423]\n",
      "Step 522025  [5.363 sec/step, loss=0.07639, avg_loss=0.07425]\n",
      "Step 522026  [5.363 sec/step, loss=0.07562, avg_loss=0.07426]\n",
      "Step 522027  [5.366 sec/step, loss=0.07392, avg_loss=0.07425]\n",
      "Step 522028  [5.375 sec/step, loss=0.07326, avg_loss=0.07427]\n",
      "Step 522029  [5.377 sec/step, loss=0.07431, avg_loss=0.07428]\n",
      "Step 522030  [5.382 sec/step, loss=0.07573, avg_loss=0.07429]\n",
      "Step 522031  [5.389 sec/step, loss=0.07371, avg_loss=0.07428]\n",
      "Step 522032  [5.378 sec/step, loss=0.06615, avg_loss=0.07421]\n",
      "Step 522033  [5.383 sec/step, loss=0.07643, avg_loss=0.07424]\n",
      "Step 522034  [5.383 sec/step, loss=0.07528, avg_loss=0.07423]\n",
      "Step 522035  [5.384 sec/step, loss=0.07652, avg_loss=0.07425]\n",
      "Step 522036  [5.364 sec/step, loss=0.07120, avg_loss=0.07420]\n",
      "Step 522037  [5.357 sec/step, loss=0.07634, avg_loss=0.07421]\n",
      "Step 522038  [5.357 sec/step, loss=0.07163, avg_loss=0.07421]\n",
      "Step 522039  [5.359 sec/step, loss=0.07256, avg_loss=0.07421]\n",
      "Step 522040  [5.347 sec/step, loss=0.07553, avg_loss=0.07420]\n",
      "Step 522041  [5.322 sec/step, loss=0.07401, avg_loss=0.07419]\n",
      "Step 522042  [5.326 sec/step, loss=0.07291, avg_loss=0.07417]\n",
      "Step 522043  [5.315 sec/step, loss=0.07551, avg_loss=0.07415]\n",
      "Step 522044  [5.323 sec/step, loss=0.07658, avg_loss=0.07418]\n",
      "Step 522045  [5.312 sec/step, loss=0.07543, avg_loss=0.07419]\n",
      "Step 522046  [5.318 sec/step, loss=0.07684, avg_loss=0.07420]\n",
      "Step 522047  [5.310 sec/step, loss=0.07492, avg_loss=0.07419]\n",
      "Step 522048  [5.305 sec/step, loss=0.07508, avg_loss=0.07420]\n",
      "Step 522049  [5.304 sec/step, loss=0.07548, avg_loss=0.07419]\n",
      "Step 522050  [5.339 sec/step, loss=0.06639, avg_loss=0.07409]\n",
      "Step 522051  [5.358 sec/step, loss=0.07654, avg_loss=0.07411]\n",
      "Step 522052  [5.308 sec/step, loss=0.07549, avg_loss=0.07420]\n",
      "Step 522053  [5.332 sec/step, loss=0.07638, avg_loss=0.07421]\n",
      "Generated 32 batches of size 32 in 2.536 sec\n",
      "Step 522054  [5.330 sec/step, loss=0.07442, avg_loss=0.07422]\n",
      "Step 522055  [5.352 sec/step, loss=0.07544, avg_loss=0.07432]\n",
      "Step 522056  [5.352 sec/step, loss=0.07440, avg_loss=0.07436]\n",
      "Step 522057  [5.354 sec/step, loss=0.07514, avg_loss=0.07436]\n",
      "Step 522058  [5.360 sec/step, loss=0.07377, avg_loss=0.07435]\n",
      "Step 522059  [5.369 sec/step, loss=0.07432, avg_loss=0.07435]\n",
      "Step 522060  [5.353 sec/step, loss=0.07168, avg_loss=0.07433]\n",
      "Step 522061  [5.344 sec/step, loss=0.07419, avg_loss=0.07432]\n",
      "Step 522062  [5.355 sec/step, loss=0.07344, avg_loss=0.07429]\n",
      "Step 522063  [5.353 sec/step, loss=0.07456, avg_loss=0.07428]\n",
      "Step 522064  [5.347 sec/step, loss=0.07452, avg_loss=0.07428]\n",
      "Step 522065  [5.355 sec/step, loss=0.07656, avg_loss=0.07428]\n",
      "Step 522066  [5.329 sec/step, loss=0.06639, avg_loss=0.07419]\n",
      "Step 522067  [5.321 sec/step, loss=0.07218, avg_loss=0.07417]\n",
      "Step 522068  [5.324 sec/step, loss=0.07453, avg_loss=0.07419]\n",
      "Step 522069  [5.326 sec/step, loss=0.07627, avg_loss=0.07421]\n",
      "Step 522070  [5.355 sec/step, loss=0.07472, avg_loss=0.07428]\n",
      "Step 522071  [5.342 sec/step, loss=0.07399, avg_loss=0.07428]\n",
      "Step 522072  [5.344 sec/step, loss=0.07541, avg_loss=0.07430]\n",
      "Step 522073  [5.353 sec/step, loss=0.07533, avg_loss=0.07434]\n",
      "Step 522074  [5.327 sec/step, loss=0.07243, avg_loss=0.07431]\n",
      "Step 522075  [5.299 sec/step, loss=0.07463, avg_loss=0.07432]\n",
      "Step 522076  [5.306 sec/step, loss=0.07524, avg_loss=0.07432]\n",
      "Step 522077  [5.353 sec/step, loss=0.06725, avg_loss=0.07424]\n",
      "Step 522078  [5.382 sec/step, loss=0.07457, avg_loss=0.07426]\n",
      "Step 522079  [5.389 sec/step, loss=0.07533, avg_loss=0.07426]\n",
      "Step 522080  [5.370 sec/step, loss=0.07338, avg_loss=0.07425]\n",
      "Step 522081  [5.353 sec/step, loss=0.07480, avg_loss=0.07424]\n",
      "Step 522082  [5.352 sec/step, loss=0.07520, avg_loss=0.07424]\n",
      "Step 522083  [5.385 sec/step, loss=0.07391, avg_loss=0.07424]\n",
      "Step 522084  [5.394 sec/step, loss=0.07560, avg_loss=0.07426]\n",
      "Step 522085  [5.384 sec/step, loss=0.07602, avg_loss=0.07426]\n",
      "Generated 32 batches of size 32 in 2.391 sec\n",
      "Step 522086  [5.403 sec/step, loss=0.07566, avg_loss=0.07428]\n",
      "Step 522087  [5.383 sec/step, loss=0.07278, avg_loss=0.07425]\n",
      "Step 522088  [5.372 sec/step, loss=0.07163, avg_loss=0.07422]\n",
      "Step 522089  [5.376 sec/step, loss=0.07649, avg_loss=0.07422]\n",
      "Step 522090  [5.376 sec/step, loss=0.07523, avg_loss=0.07422]\n",
      "Step 522091  [5.386 sec/step, loss=0.07499, avg_loss=0.07421]\n",
      "Step 522092  [5.393 sec/step, loss=0.07651, avg_loss=0.07426]\n",
      "Step 522093  [5.380 sec/step, loss=0.07448, avg_loss=0.07423]\n",
      "Step 522094  [5.397 sec/step, loss=0.07629, avg_loss=0.07425]\n",
      "Step 522095  [5.399 sec/step, loss=0.07640, avg_loss=0.07425]\n",
      "Step 522096  [5.384 sec/step, loss=0.07579, avg_loss=0.07427]\n",
      "Step 522097  [5.335 sec/step, loss=0.07554, avg_loss=0.07436]\n",
      "Step 522098  [5.347 sec/step, loss=0.07362, avg_loss=0.07437]\n",
      "Step 522099  [5.354 sec/step, loss=0.07652, avg_loss=0.07437]\n",
      "Step 522100  [5.349 sec/step, loss=0.07634, avg_loss=0.07437]\n",
      "Writing summary at step: 522100\n",
      "Step 522101  [5.355 sec/step, loss=0.07577, avg_loss=0.07440]\n",
      "Step 522102  [5.350 sec/step, loss=0.07527, avg_loss=0.07440]\n",
      "Step 522103  [5.349 sec/step, loss=0.07100, avg_loss=0.07435]\n",
      "Step 522104  [5.334 sec/step, loss=0.07518, avg_loss=0.07434]\n",
      "Step 522105  [5.325 sec/step, loss=0.07436, avg_loss=0.07432]\n",
      "Step 522106  [5.321 sec/step, loss=0.07148, avg_loss=0.07428]\n",
      "Step 522107  [5.337 sec/step, loss=0.07613, avg_loss=0.07428]\n",
      "Step 522108  [5.337 sec/step, loss=0.07110, avg_loss=0.07425]\n",
      "Step 522109  [5.328 sec/step, loss=0.07302, avg_loss=0.07421]\n",
      "Step 522110  [5.330 sec/step, loss=0.07410, avg_loss=0.07421]\n",
      "Step 522111  [5.347 sec/step, loss=0.07700, avg_loss=0.07425]\n",
      "Step 522112  [5.357 sec/step, loss=0.07603, avg_loss=0.07428]\n",
      "Step 522113  [5.329 sec/step, loss=0.06548, avg_loss=0.07417]\n",
      "Step 522114  [5.308 sec/step, loss=0.07150, avg_loss=0.07412]\n",
      "Step 522115  [5.320 sec/step, loss=0.07600, avg_loss=0.07413]\n",
      "Step 522116  [5.329 sec/step, loss=0.07427, avg_loss=0.07421]\n",
      "Generated 32 batches of size 32 in 2.482 sec\n",
      "Step 522117  [5.319 sec/step, loss=0.07431, avg_loss=0.07420]\n",
      "Step 522118  [5.336 sec/step, loss=0.07534, avg_loss=0.07421]\n",
      "Step 522119  [5.315 sec/step, loss=0.07424, avg_loss=0.07422]\n",
      "Step 522120  [5.318 sec/step, loss=0.07352, avg_loss=0.07419]\n",
      "Step 522121  [5.365 sec/step, loss=0.06626, avg_loss=0.07411]\n",
      "Step 522122  [5.390 sec/step, loss=0.07249, avg_loss=0.07408]\n",
      "Step 522123  [5.345 sec/step, loss=0.07500, avg_loss=0.07416]\n",
      "Step 522124  [5.368 sec/step, loss=0.07640, avg_loss=0.07419]\n",
      "Step 522125  [5.363 sec/step, loss=0.07540, avg_loss=0.07418]\n",
      "Step 522126  [5.358 sec/step, loss=0.07349, avg_loss=0.07416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 522127  [5.376 sec/step, loss=0.07526, avg_loss=0.07418]\n",
      "Step 522128  [5.385 sec/step, loss=0.07399, avg_loss=0.07418]\n",
      "Step 522129  [5.394 sec/step, loss=0.07484, avg_loss=0.07419]\n",
      "Step 522130  [5.403 sec/step, loss=0.07645, avg_loss=0.07420]\n",
      "Step 522131  [5.384 sec/step, loss=0.06604, avg_loss=0.07412]\n",
      "Step 522132  [5.397 sec/step, loss=0.07546, avg_loss=0.07421]\n",
      "Step 522133  [5.375 sec/step, loss=0.07237, avg_loss=0.07417]\n",
      "Step 522134  [5.423 sec/step, loss=0.06602, avg_loss=0.07408]\n",
      "Step 522135  [5.426 sec/step, loss=0.07601, avg_loss=0.07407]\n",
      "Step 522136  [5.437 sec/step, loss=0.07554, avg_loss=0.07412]\n",
      "Step 522137  [5.425 sec/step, loss=0.07456, avg_loss=0.07410]\n",
      "Step 522138  [5.432 sec/step, loss=0.07448, avg_loss=0.07413]\n",
      "Step 522139  [5.446 sec/step, loss=0.07665, avg_loss=0.07417]\n",
      "Step 522140  [5.445 sec/step, loss=0.07438, avg_loss=0.07416]\n",
      "Step 522141  [5.463 sec/step, loss=0.07329, avg_loss=0.07415]\n",
      "Step 522142  [5.474 sec/step, loss=0.07634, avg_loss=0.07418]\n",
      "Step 522143  [5.476 sec/step, loss=0.07406, avg_loss=0.07417]\n",
      "Step 522144  [5.462 sec/step, loss=0.07400, avg_loss=0.07414]\n",
      "Step 522145  [5.466 sec/step, loss=0.07616, avg_loss=0.07415]\n",
      "Step 522146  [5.462 sec/step, loss=0.07558, avg_loss=0.07414]\n",
      "Step 522147  [5.484 sec/step, loss=0.07561, avg_loss=0.07414]\n",
      "Step 522148  [5.465 sec/step, loss=0.07137, avg_loss=0.07411]\n",
      "Generated 32 batches of size 32 in 2.479 sec\n",
      "Step 522149  [5.462 sec/step, loss=0.07518, avg_loss=0.07410]\n",
      "Step 522150  [5.409 sec/step, loss=0.07410, avg_loss=0.07418]\n",
      "Step 522151  [5.404 sec/step, loss=0.07527, avg_loss=0.07417]\n",
      "Step 522152  [5.395 sec/step, loss=0.07423, avg_loss=0.07416]\n",
      "Step 522153  [5.368 sec/step, loss=0.07215, avg_loss=0.07411]\n",
      "Step 522154  [5.371 sec/step, loss=0.07583, avg_loss=0.07413]\n",
      "Step 522155  [5.370 sec/step, loss=0.07532, avg_loss=0.07413]\n",
      "Step 522156  [5.377 sec/step, loss=0.07534, avg_loss=0.07414]\n",
      "Step 522157  [5.384 sec/step, loss=0.07678, avg_loss=0.07415]\n",
      "Step 522158  [5.369 sec/step, loss=0.07409, avg_loss=0.07416]\n",
      "Step 522159  [5.377 sec/step, loss=0.07392, avg_loss=0.07415]\n",
      "Step 522160  [5.387 sec/step, loss=0.07559, avg_loss=0.07419]\n",
      "Step 522161  [5.381 sec/step, loss=0.07212, avg_loss=0.07417]\n",
      "Step 522162  [5.363 sec/step, loss=0.07629, avg_loss=0.07420]\n",
      "Step 522163  [5.376 sec/step, loss=0.07672, avg_loss=0.07422]\n",
      "Step 522164  [5.388 sec/step, loss=0.07641, avg_loss=0.07424]\n",
      "Step 522165  [5.427 sec/step, loss=0.06731, avg_loss=0.07415]\n",
      "Step 522166  [5.440 sec/step, loss=0.07428, avg_loss=0.07423]\n",
      "Step 522167  [5.459 sec/step, loss=0.07596, avg_loss=0.07426]\n",
      "Step 522168  [5.474 sec/step, loss=0.07358, avg_loss=0.07425]\n",
      "Step 522169  [5.468 sec/step, loss=0.07565, avg_loss=0.07425]\n",
      "Step 522170  [5.464 sec/step, loss=0.07677, avg_loss=0.07427]\n",
      "Step 522171  [5.461 sec/step, loss=0.07087, avg_loss=0.07424]\n",
      "Step 522172  [5.450 sec/step, loss=0.07446, avg_loss=0.07423]\n",
      "Step 522173  [5.453 sec/step, loss=0.07526, avg_loss=0.07423]\n",
      "Step 522174  [5.490 sec/step, loss=0.07412, avg_loss=0.07424]\n",
      "Step 522175  [5.489 sec/step, loss=0.07478, avg_loss=0.07425]\n",
      "Step 522176  [5.475 sec/step, loss=0.07388, avg_loss=0.07423]\n",
      "Step 522177  [5.427 sec/step, loss=0.07340, avg_loss=0.07429]\n",
      "Step 522178  [5.393 sec/step, loss=0.06610, avg_loss=0.07421]\n",
      "Step 522179  [5.399 sec/step, loss=0.07656, avg_loss=0.07422]\n",
      "Step 522180  [5.415 sec/step, loss=0.07551, avg_loss=0.07424]\n",
      "Generated 32 batches of size 32 in 2.366 sec\n",
      "Step 522181  [5.421 sec/step, loss=0.07500, avg_loss=0.07424]\n",
      "Step 522182  [5.418 sec/step, loss=0.07635, avg_loss=0.07426]\n",
      "Step 522183  [5.399 sec/step, loss=0.07295, avg_loss=0.07425]\n",
      "Step 522184  [5.383 sec/step, loss=0.07182, avg_loss=0.07421]\n",
      "Step 522185  [5.392 sec/step, loss=0.07481, avg_loss=0.07420]\n",
      "Step 522186  [5.408 sec/step, loss=0.07450, avg_loss=0.07418]\n",
      "Step 522187  [5.422 sec/step, loss=0.07498, avg_loss=0.07421]\n",
      "Step 522188  [5.433 sec/step, loss=0.07560, avg_loss=0.07425]\n",
      "Step 522189  [5.419 sec/step, loss=0.07341, avg_loss=0.07422]\n",
      "Step 522190  [5.418 sec/step, loss=0.07130, avg_loss=0.07418]\n",
      "Step 522191  [5.408 sec/step, loss=0.07452, avg_loss=0.07417]\n",
      "Step 522192  [5.411 sec/step, loss=0.07628, avg_loss=0.07417]\n",
      "Step 522193  [5.404 sec/step, loss=0.07144, avg_loss=0.07414]\n",
      "Step 522194  [5.377 sec/step, loss=0.07272, avg_loss=0.07410]\n",
      "Step 522195  [5.353 sec/step, loss=0.07442, avg_loss=0.07408]\n",
      "Step 522196  [5.364 sec/step, loss=0.07473, avg_loss=0.07407]\n",
      "Step 522197  [5.363 sec/step, loss=0.07544, avg_loss=0.07407]\n",
      "Step 522198  [5.390 sec/step, loss=0.07258, avg_loss=0.07406]\n",
      "Step 522199  [5.400 sec/step, loss=0.07641, avg_loss=0.07406]\n",
      "Step 522200  [5.404 sec/step, loss=0.07478, avg_loss=0.07404]\n",
      "Writing summary at step: 522200\n",
      "Step 522201  [5.393 sec/step, loss=0.07392, avg_loss=0.07403]\n",
      "Step 522202  [5.393 sec/step, loss=0.07400, avg_loss=0.07401]\n",
      "Step 522203  [5.409 sec/step, loss=0.07647, avg_loss=0.07407]\n",
      "Step 522204  [5.416 sec/step, loss=0.07651, avg_loss=0.07408]\n",
      "Step 522205  [5.405 sec/step, loss=0.07589, avg_loss=0.07410]\n",
      "Step 522206  [5.413 sec/step, loss=0.07432, avg_loss=0.07413]\n",
      "Step 522207  [5.404 sec/step, loss=0.07506, avg_loss=0.07411]\n",
      "Step 522208  [5.397 sec/step, loss=0.06705, avg_loss=0.07407]\n",
      "Step 522209  [5.408 sec/step, loss=0.07656, avg_loss=0.07411]\n",
      "Step 522210  [5.425 sec/step, loss=0.07654, avg_loss=0.07413]\n",
      "Step 522211  [5.422 sec/step, loss=0.07523, avg_loss=0.07412]\n",
      "Generated 32 batches of size 32 in 2.631 sec\n",
      "Step 522212  [5.476 sec/step, loss=0.06576, avg_loss=0.07401]\n",
      "Step 522213  [5.503 sec/step, loss=0.07686, avg_loss=0.07413]\n",
      "Step 522214  [5.518 sec/step, loss=0.07544, avg_loss=0.07417]\n",
      "Step 522215  [5.510 sec/step, loss=0.07616, avg_loss=0.07417]\n",
      "Step 522216  [5.519 sec/step, loss=0.07344, avg_loss=0.07416]\n",
      "Step 522217  [5.515 sec/step, loss=0.07473, avg_loss=0.07416]\n",
      "Step 522218  [5.494 sec/step, loss=0.07306, avg_loss=0.07414]\n",
      "Step 522219  [5.505 sec/step, loss=0.07447, avg_loss=0.07414]\n",
      "Step 522220  [5.506 sec/step, loss=0.07415, avg_loss=0.07415]\n",
      "Step 522221  [5.473 sec/step, loss=0.07525, avg_loss=0.07424]\n",
      "Step 522222  [5.460 sec/step, loss=0.07602, avg_loss=0.07427]\n",
      "Step 522223  [5.452 sec/step, loss=0.07163, avg_loss=0.07424]\n",
      "Step 522224  [5.457 sec/step, loss=0.07483, avg_loss=0.07423]\n",
      "Step 522225  [5.448 sec/step, loss=0.07581, avg_loss=0.07423]\n",
      "Step 522226  [5.456 sec/step, loss=0.07691, avg_loss=0.07426]\n",
      "Step 522227  [5.445 sec/step, loss=0.07478, avg_loss=0.07426]\n",
      "Step 522228  [5.438 sec/step, loss=0.07386, avg_loss=0.07426]\n",
      "Step 522229  [5.434 sec/step, loss=0.07586, avg_loss=0.07427]\n",
      "Step 522230  [5.416 sec/step, loss=0.07560, avg_loss=0.07426]\n",
      "Step 522231  [5.427 sec/step, loss=0.07281, avg_loss=0.07433]\n",
      "Step 522232  [5.442 sec/step, loss=0.07550, avg_loss=0.07433]\n",
      "Step 522233  [5.445 sec/step, loss=0.07483, avg_loss=0.07435]\n",
      "Step 522234  [5.406 sec/step, loss=0.07632, avg_loss=0.07446]\n",
      "Step 522235  [5.393 sec/step, loss=0.07503, avg_loss=0.07445]\n",
      "Step 522236  [5.443 sec/step, loss=0.06739, avg_loss=0.07436]\n",
      "Step 522237  [5.480 sec/step, loss=0.07314, avg_loss=0.07435]\n",
      "Step 522238  [5.460 sec/step, loss=0.06505, avg_loss=0.07426]\n",
      "Step 522239  [5.448 sec/step, loss=0.07024, avg_loss=0.07419]\n",
      "Step 522240  [5.458 sec/step, loss=0.07473, avg_loss=0.07419]\n",
      "Step 522241  [5.441 sec/step, loss=0.07416, avg_loss=0.07420]\n",
      "Step 522242  [5.426 sec/step, loss=0.07478, avg_loss=0.07419]\n",
      "Step 522243  [5.415 sec/step, loss=0.07032, avg_loss=0.07415]\n",
      "Generated 32 batches of size 32 in 2.394 sec\n",
      "Step 522244  [5.433 sec/step, loss=0.07613, avg_loss=0.07417]\n",
      "Step 522245  [5.424 sec/step, loss=0.07688, avg_loss=0.07418]\n",
      "Step 522246  [5.429 sec/step, loss=0.07628, avg_loss=0.07419]\n",
      "Step 522247  [5.404 sec/step, loss=0.07616, avg_loss=0.07419]\n",
      "Step 522248  [5.416 sec/step, loss=0.07339, avg_loss=0.07421]\n",
      "Step 522249  [5.400 sec/step, loss=0.07476, avg_loss=0.07421]\n",
      "Step 522250  [5.409 sec/step, loss=0.07419, avg_loss=0.07421]\n",
      "Step 522251  [5.388 sec/step, loss=0.07091, avg_loss=0.07416]\n",
      "Step 522252  [5.399 sec/step, loss=0.07602, avg_loss=0.07418]\n",
      "Step 522253  [5.412 sec/step, loss=0.07460, avg_loss=0.07421]\n",
      "Step 522254  [5.423 sec/step, loss=0.07621, avg_loss=0.07421]\n",
      "Step 522255  [5.415 sec/step, loss=0.07550, avg_loss=0.07421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 522256  [5.426 sec/step, loss=0.07590, avg_loss=0.07422]\n",
      "Step 522257  [5.419 sec/step, loss=0.07583, avg_loss=0.07421]\n",
      "Step 522258  [5.435 sec/step, loss=0.07451, avg_loss=0.07421]\n",
      "Step 522259  [5.438 sec/step, loss=0.07700, avg_loss=0.07424]\n",
      "Step 522260  [5.447 sec/step, loss=0.07661, avg_loss=0.07425]\n",
      "Step 522261  [5.462 sec/step, loss=0.07259, avg_loss=0.07426]\n",
      "Step 522262  [5.454 sec/step, loss=0.07526, avg_loss=0.07425]\n",
      "Step 522263  [5.447 sec/step, loss=0.07527, avg_loss=0.07423]\n",
      "Step 522264  [5.431 sec/step, loss=0.07211, avg_loss=0.07419]\n",
      "Step 522265  [5.391 sec/step, loss=0.07458, avg_loss=0.07426]\n",
      "Step 522266  [5.393 sec/step, loss=0.07397, avg_loss=0.07426]\n",
      "Step 522267  [5.385 sec/step, loss=0.07657, avg_loss=0.07427]\n",
      "Step 522268  [5.405 sec/step, loss=0.07353, avg_loss=0.07427]\n",
      "Step 522269  [5.414 sec/step, loss=0.07602, avg_loss=0.07427]\n",
      "Step 522270  [5.402 sec/step, loss=0.07309, avg_loss=0.07423]\n",
      "Step 522271  [5.392 sec/step, loss=0.07206, avg_loss=0.07424]\n",
      "Step 522272  [5.399 sec/step, loss=0.07207, avg_loss=0.07422]\n",
      "Step 522273  [5.390 sec/step, loss=0.07544, avg_loss=0.07422]\n",
      "Step 522274  [5.387 sec/step, loss=0.07405, avg_loss=0.07422]\n",
      "Step 522275  [5.379 sec/step, loss=0.07136, avg_loss=0.07419]\n",
      "Generated 32 batches of size 32 in 2.369 sec\n",
      "Step 522276  [5.390 sec/step, loss=0.07565, avg_loss=0.07421]\n",
      "Step 522277  [5.439 sec/step, loss=0.06653, avg_loss=0.07414]\n",
      "Step 522278  [5.459 sec/step, loss=0.07485, avg_loss=0.07422]\n",
      "Step 522279  [5.440 sec/step, loss=0.07503, avg_loss=0.07421]\n",
      "Step 522280  [5.429 sec/step, loss=0.07424, avg_loss=0.07420]\n",
      "Step 522281  [5.435 sec/step, loss=0.07649, avg_loss=0.07421]\n",
      "Step 522282  [5.427 sec/step, loss=0.07391, avg_loss=0.07419]\n",
      "Step 522283  [5.405 sec/step, loss=0.06672, avg_loss=0.07412]\n",
      "Step 522284  [5.420 sec/step, loss=0.07472, avg_loss=0.07415]\n",
      "Step 522285  [5.402 sec/step, loss=0.07303, avg_loss=0.07414]\n",
      "Step 522286  [5.370 sec/step, loss=0.07426, avg_loss=0.07413]\n",
      "Step 522287  [5.358 sec/step, loss=0.07492, avg_loss=0.07413]\n",
      "Step 522288  [5.363 sec/step, loss=0.07507, avg_loss=0.07413]\n",
      "Step 522289  [5.368 sec/step, loss=0.07531, avg_loss=0.07415]\n",
      "Step 522290  [5.366 sec/step, loss=0.07391, avg_loss=0.07417]\n",
      "Step 522291  [5.354 sec/step, loss=0.07430, avg_loss=0.07417]\n",
      "Step 522292  [5.339 sec/step, loss=0.07562, avg_loss=0.07416]\n",
      "Step 522293  [5.369 sec/step, loss=0.07280, avg_loss=0.07418]\n",
      "Step 522294  [5.395 sec/step, loss=0.07432, avg_loss=0.07419]\n",
      "Step 522295  [5.403 sec/step, loss=0.07204, avg_loss=0.07417]\n",
      "Step 522296  [5.404 sec/step, loss=0.07682, avg_loss=0.07419]\n",
      "Step 522297  [5.397 sec/step, loss=0.07428, avg_loss=0.07418]\n",
      "Step 522298  [5.379 sec/step, loss=0.07653, avg_loss=0.07422]\n",
      "Step 522299  [5.358 sec/step, loss=0.07302, avg_loss=0.07418]\n",
      "Step 522300  [5.337 sec/step, loss=0.06730, avg_loss=0.07411]\n",
      "Writing summary at step: 522300\n",
      "Step 522301  [5.354 sec/step, loss=0.07412, avg_loss=0.07411]\n",
      "Step 522302  [5.366 sec/step, loss=0.07671, avg_loss=0.07414]\n",
      "Step 522303  [5.359 sec/step, loss=0.07618, avg_loss=0.07414]\n",
      "Step 522304  [5.353 sec/step, loss=0.07561, avg_loss=0.07413]\n",
      "Step 522305  [5.356 sec/step, loss=0.07584, avg_loss=0.07413]\n",
      "Step 522306  [5.356 sec/step, loss=0.07701, avg_loss=0.07415]\n",
      "Generated 32 batches of size 32 in 2.338 sec\n",
      "Step 522307  [5.369 sec/step, loss=0.07633, avg_loss=0.07417]\n",
      "Step 522308  [5.397 sec/step, loss=0.07687, avg_loss=0.07426]\n",
      "Step 522309  [5.432 sec/step, loss=0.06958, avg_loss=0.07419]\n",
      "Step 522310  [5.420 sec/step, loss=0.07509, avg_loss=0.07418]\n",
      "Step 522311  [5.404 sec/step, loss=0.07425, avg_loss=0.07417]\n",
      "Step 522312  [5.377 sec/step, loss=0.07382, avg_loss=0.07425]\n",
      "Step 522313  [5.371 sec/step, loss=0.07367, avg_loss=0.07422]\n",
      "Step 522314  [5.372 sec/step, loss=0.07315, avg_loss=0.07420]\n",
      "Step 522315  [5.359 sec/step, loss=0.07170, avg_loss=0.07415]\n",
      "Step 522316  [5.359 sec/step, loss=0.07318, avg_loss=0.07415]\n",
      "Step 522317  [5.364 sec/step, loss=0.07560, avg_loss=0.07416]\n",
      "Step 522318  [5.365 sec/step, loss=0.07124, avg_loss=0.07414]\n",
      "Step 522319  [5.358 sec/step, loss=0.07369, avg_loss=0.07413]\n",
      "Step 522320  [5.341 sec/step, loss=0.07175, avg_loss=0.07411]\n",
      "Step 522321  [5.334 sec/step, loss=0.07309, avg_loss=0.07409]\n",
      "Step 522322  [5.334 sec/step, loss=0.07638, avg_loss=0.07409]\n",
      "Step 522323  [5.357 sec/step, loss=0.07598, avg_loss=0.07413]\n",
      "Step 522324  [5.327 sec/step, loss=0.06598, avg_loss=0.07404]\n",
      "Step 522325  [5.337 sec/step, loss=0.07656, avg_loss=0.07405]\n",
      "Step 522326  [5.333 sec/step, loss=0.07359, avg_loss=0.07402]\n",
      "Step 522327  [5.339 sec/step, loss=0.07567, avg_loss=0.07403]\n",
      "Step 522328  [5.329 sec/step, loss=0.07398, avg_loss=0.07403]\n",
      "Step 522329  [5.318 sec/step, loss=0.07446, avg_loss=0.07402]\n",
      "Step 522330  [5.326 sec/step, loss=0.07545, avg_loss=0.07401]\n",
      "Step 522331  [5.344 sec/step, loss=0.07642, avg_loss=0.07405]\n",
      "Step 522332  [5.331 sec/step, loss=0.07507, avg_loss=0.07405]\n",
      "Step 522333  [5.332 sec/step, loss=0.07585, avg_loss=0.07406]\n",
      "Step 522334  [5.316 sec/step, loss=0.07230, avg_loss=0.07402]\n",
      "Step 522335  [5.315 sec/step, loss=0.07557, avg_loss=0.07402]\n",
      "Step 522336  [5.274 sec/step, loss=0.07456, avg_loss=0.07409]\n",
      "Step 522337  [5.234 sec/step, loss=0.07214, avg_loss=0.07408]\n",
      "Step 522338  [5.256 sec/step, loss=0.07404, avg_loss=0.07417]\n",
      "Generated 32 batches of size 32 in 2.628 sec\n",
      "Step 522339  [5.314 sec/step, loss=0.06669, avg_loss=0.07414]\n",
      "Step 522340  [5.307 sec/step, loss=0.07582, avg_loss=0.07415]\n",
      "Step 522341  [5.313 sec/step, loss=0.07528, avg_loss=0.07416]\n",
      "Step 522342  [5.326 sec/step, loss=0.07673, avg_loss=0.07418]\n",
      "Step 522343  [5.352 sec/step, loss=0.07543, avg_loss=0.07423]\n",
      "Step 522344  [5.334 sec/step, loss=0.07447, avg_loss=0.07421]\n",
      "Step 522345  [5.327 sec/step, loss=0.07489, avg_loss=0.07419]\n",
      "Step 522346  [5.331 sec/step, loss=0.07688, avg_loss=0.07420]\n",
      "Step 522347  [5.357 sec/step, loss=0.07335, avg_loss=0.07417]\n",
      "Step 522348  [5.359 sec/step, loss=0.07306, avg_loss=0.07417]\n",
      "Step 522349  [5.377 sec/step, loss=0.07598, avg_loss=0.07418]\n",
      "Step 522350  [5.362 sec/step, loss=0.07104, avg_loss=0.07415]\n",
      "Step 522351  [5.362 sec/step, loss=0.07166, avg_loss=0.07416]\n",
      "Step 522352  [5.370 sec/step, loss=0.07499, avg_loss=0.07415]\n",
      "Step 522353  [5.389 sec/step, loss=0.07374, avg_loss=0.07414]\n",
      "Step 522354  [5.373 sec/step, loss=0.07555, avg_loss=0.07413]\n",
      "Step 522355  [5.399 sec/step, loss=0.07423, avg_loss=0.07412]\n",
      "Step 522356  [5.406 sec/step, loss=0.07507, avg_loss=0.07411]\n",
      "Step 522357  [5.411 sec/step, loss=0.07648, avg_loss=0.07412]\n",
      "Step 522358  [5.391 sec/step, loss=0.07460, avg_loss=0.07412]\n",
      "Step 522359  [5.380 sec/step, loss=0.07509, avg_loss=0.07410]\n",
      "Step 522360  [5.385 sec/step, loss=0.07374, avg_loss=0.07407]\n",
      "Step 522361  [5.433 sec/step, loss=0.06728, avg_loss=0.07402]\n",
      "Step 522362  [5.444 sec/step, loss=0.07667, avg_loss=0.07403]\n",
      "Step 522363  [5.441 sec/step, loss=0.07237, avg_loss=0.07400]\n",
      "Step 522364  [5.457 sec/step, loss=0.07255, avg_loss=0.07401]\n",
      "Step 522365  [5.447 sec/step, loss=0.07491, avg_loss=0.07401]\n",
      "Step 522366  [5.446 sec/step, loss=0.07407, avg_loss=0.07401]\n",
      "Step 522367  [5.465 sec/step, loss=0.07419, avg_loss=0.07399]\n",
      "Step 522368  [5.446 sec/step, loss=0.07539, avg_loss=0.07400]\n",
      "Step 522369  [5.446 sec/step, loss=0.07675, avg_loss=0.07401]\n",
      "Step 522370  [5.446 sec/step, loss=0.07215, avg_loss=0.07400]\n",
      "Generated 32 batches of size 32 in 2.834 sec\n",
      "Step 522371  [5.447 sec/step, loss=0.06534, avg_loss=0.07393]\n",
      "Step 522372  [5.444 sec/step, loss=0.07307, avg_loss=0.07394]\n",
      "Step 522373  [5.439 sec/step, loss=0.07465, avg_loss=0.07394]\n",
      "Step 522374  [5.420 sec/step, loss=0.07416, avg_loss=0.07394]\n",
      "Step 522375  [5.431 sec/step, loss=0.07505, avg_loss=0.07398]\n",
      "Step 522376  [5.428 sec/step, loss=0.07630, avg_loss=0.07398]\n",
      "Step 522377  [5.376 sec/step, loss=0.07102, avg_loss=0.07403]\n",
      "Step 522378  [5.369 sec/step, loss=0.07583, avg_loss=0.07404]\n",
      "Step 522379  [5.385 sec/step, loss=0.07623, avg_loss=0.07405]\n",
      "Step 522380  [5.393 sec/step, loss=0.07541, avg_loss=0.07406]\n",
      "Step 522381  [5.383 sec/step, loss=0.07540, avg_loss=0.07405]\n",
      "Step 522382  [5.382 sec/step, loss=0.07394, avg_loss=0.07405]\n",
      "Step 522383  [5.394 sec/step, loss=0.07389, avg_loss=0.07412]\n",
      "Step 522384  [5.395 sec/step, loss=0.07693, avg_loss=0.07414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 522385  [5.433 sec/step, loss=0.07546, avg_loss=0.07417]\n",
      "Step 522386  [5.435 sec/step, loss=0.07420, avg_loss=0.07417]\n",
      "Step 522387  [5.450 sec/step, loss=0.07451, avg_loss=0.07416]\n",
      "Step 522388  [5.496 sec/step, loss=0.06607, avg_loss=0.07407]\n",
      "Step 522389  [5.478 sec/step, loss=0.07224, avg_loss=0.07404]\n",
      "Step 522390  [5.481 sec/step, loss=0.07546, avg_loss=0.07406]\n",
      "Step 522391  [5.506 sec/step, loss=0.07565, avg_loss=0.07407]\n",
      "Step 522392  [5.500 sec/step, loss=0.07227, avg_loss=0.07404]\n",
      "Step 522393  [5.479 sec/step, loss=0.07227, avg_loss=0.07403]\n",
      "Step 522394  [5.477 sec/step, loss=0.07620, avg_loss=0.07405]\n",
      "Step 522395  [5.482 sec/step, loss=0.07561, avg_loss=0.07409]\n",
      "Step 522396  [5.482 sec/step, loss=0.07704, avg_loss=0.07409]\n",
      "Step 522397  [5.479 sec/step, loss=0.07471, avg_loss=0.07409]\n",
      "Step 522398  [5.473 sec/step, loss=0.07529, avg_loss=0.07408]\n",
      "Step 522399  [5.475 sec/step, loss=0.07558, avg_loss=0.07411]\n",
      "Step 522400  [5.503 sec/step, loss=0.07444, avg_loss=0.07418]\n",
      "Writing summary at step: 522400\n",
      "Step 522401  [5.501 sec/step, loss=0.07635, avg_loss=0.07420]\n",
      "Generated 32 batches of size 32 in 2.500 sec\n",
      "Step 522402  [5.489 sec/step, loss=0.07452, avg_loss=0.07418]\n",
      "Step 522403  [5.464 sec/step, loss=0.06765, avg_loss=0.07409]\n",
      "Step 522404  [5.459 sec/step, loss=0.07398, avg_loss=0.07408]\n",
      "Step 522405  [5.471 sec/step, loss=0.07666, avg_loss=0.07408]\n",
      "Step 522406  [5.463 sec/step, loss=0.07524, avg_loss=0.07407]\n",
      "Step 522407  [5.452 sec/step, loss=0.07444, avg_loss=0.07405]\n",
      "Step 522408  [5.442 sec/step, loss=0.07223, avg_loss=0.07400]\n",
      "Step 522409  [5.407 sec/step, loss=0.07376, avg_loss=0.07404]\n",
      "Step 522410  [5.425 sec/step, loss=0.07513, avg_loss=0.07404]\n",
      "Step 522411  [5.436 sec/step, loss=0.07551, avg_loss=0.07406]\n",
      "Step 522412  [5.410 sec/step, loss=0.07477, avg_loss=0.07407]\n",
      "Step 522413  [5.415 sec/step, loss=0.07370, avg_loss=0.07407]\n",
      "Step 522414  [5.413 sec/step, loss=0.07490, avg_loss=0.07408]\n",
      "Step 522415  [5.422 sec/step, loss=0.07219, avg_loss=0.07409]\n",
      "Step 522416  [5.420 sec/step, loss=0.07532, avg_loss=0.07411]\n",
      "Step 522417  [5.415 sec/step, loss=0.07425, avg_loss=0.07410]\n",
      "Step 522418  [5.431 sec/step, loss=0.07644, avg_loss=0.07415]\n",
      "Step 522419  [5.445 sec/step, loss=0.07668, avg_loss=0.07418]\n",
      "Step 522420  [5.470 sec/step, loss=0.07561, avg_loss=0.07422]\n",
      "Step 522421  [5.458 sec/step, loss=0.07351, avg_loss=0.07422]\n",
      "Step 522422  [5.446 sec/step, loss=0.07557, avg_loss=0.07421]\n",
      "Step 522423  [5.417 sec/step, loss=0.07440, avg_loss=0.07420]\n",
      "Step 522424  [5.442 sec/step, loss=0.07631, avg_loss=0.07430]\n",
      "Step 522425  [5.438 sec/step, loss=0.07546, avg_loss=0.07429]\n",
      "Step 522426  [5.451 sec/step, loss=0.07559, avg_loss=0.07431]\n",
      "Step 522427  [5.442 sec/step, loss=0.07374, avg_loss=0.07429]\n",
      "Step 522428  [5.457 sec/step, loss=0.07574, avg_loss=0.07431]\n",
      "Step 522429  [5.472 sec/step, loss=0.07519, avg_loss=0.07432]\n",
      "Step 522430  [5.461 sec/step, loss=0.07402, avg_loss=0.07430]\n",
      "Step 522431  [5.455 sec/step, loss=0.07461, avg_loss=0.07428]\n",
      "Step 522432  [5.446 sec/step, loss=0.07133, avg_loss=0.07425]\n",
      "Step 522433  [5.442 sec/step, loss=0.07594, avg_loss=0.07425]\n",
      "Generated 32 batches of size 32 in 2.364 sec\n",
      "Step 522434  [5.462 sec/step, loss=0.07615, avg_loss=0.07429]\n",
      "Step 522435  [5.475 sec/step, loss=0.07658, avg_loss=0.07430]\n",
      "Step 522436  [5.515 sec/step, loss=0.06737, avg_loss=0.07422]\n",
      "Step 522437  [5.515 sec/step, loss=0.07112, avg_loss=0.07421]\n",
      "Step 522438  [5.513 sec/step, loss=0.07155, avg_loss=0.07419]\n",
      "Step 522439  [5.443 sec/step, loss=0.06549, avg_loss=0.07418]\n",
      "Step 522440  [5.471 sec/step, loss=0.07379, avg_loss=0.07416]\n",
      "Step 522441  [5.472 sec/step, loss=0.07327, avg_loss=0.07414]\n",
      "Step 522442  [5.471 sec/step, loss=0.07632, avg_loss=0.07413]\n",
      "Step 522443  [5.467 sec/step, loss=0.07409, avg_loss=0.07412]\n",
      "Step 522444  [5.474 sec/step, loss=0.07517, avg_loss=0.07413]\n",
      "Step 522445  [5.485 sec/step, loss=0.07660, avg_loss=0.07414]\n",
      "Step 522446  [5.477 sec/step, loss=0.07514, avg_loss=0.07413]\n",
      "Step 522447  [5.452 sec/step, loss=0.07597, avg_loss=0.07415]\n",
      "Step 522448  [5.464 sec/step, loss=0.07658, avg_loss=0.07419]\n",
      "Step 522449  [5.506 sec/step, loss=0.06561, avg_loss=0.07408]\n",
      "Step 522450  [5.521 sec/step, loss=0.07522, avg_loss=0.07412]\n",
      "Step 522451  [5.536 sec/step, loss=0.07383, avg_loss=0.07415]\n",
      "Step 522452  [5.525 sec/step, loss=0.07478, avg_loss=0.07414]\n",
      "Step 522453  [5.509 sec/step, loss=0.07526, avg_loss=0.07416]\n",
      "Step 522454  [5.527 sec/step, loss=0.07395, avg_loss=0.07414]\n",
      "Step 522455  [5.503 sec/step, loss=0.07326, avg_loss=0.07413]\n",
      "Step 522456  [5.478 sec/step, loss=0.07279, avg_loss=0.07411]\n",
      "Step 522457  [5.464 sec/step, loss=0.07065, avg_loss=0.07405]\n",
      "Step 522458  [5.480 sec/step, loss=0.07488, avg_loss=0.07406]\n",
      "Step 522459  [5.462 sec/step, loss=0.07295, avg_loss=0.07403]\n",
      "Step 522460  [5.451 sec/step, loss=0.07467, avg_loss=0.07404]\n",
      "Step 522461  [5.411 sec/step, loss=0.07634, avg_loss=0.07413]\n",
      "Step 522462  [5.396 sec/step, loss=0.07392, avg_loss=0.07411]\n",
      "Step 522463  [5.402 sec/step, loss=0.07459, avg_loss=0.07413]\n",
      "Step 522464  [5.379 sec/step, loss=0.06779, avg_loss=0.07408]\n",
      "Step 522465  [5.390 sec/step, loss=0.07644, avg_loss=0.07410]\n",
      "Generated 32 batches of size 32 in 2.542 sec\n",
      "Step 522466  [5.394 sec/step, loss=0.07547, avg_loss=0.07411]\n",
      "Step 522467  [5.362 sec/step, loss=0.07060, avg_loss=0.07407]\n",
      "Step 522468  [5.354 sec/step, loss=0.07544, avg_loss=0.07407]\n",
      "Step 522469  [5.369 sec/step, loss=0.07568, avg_loss=0.07406]\n",
      "Step 522470  [5.391 sec/step, loss=0.07635, avg_loss=0.07411]\n",
      "Step 522471  [5.412 sec/step, loss=0.07620, avg_loss=0.07421]\n",
      "Step 522472  [5.412 sec/step, loss=0.07420, avg_loss=0.07423]\n",
      "Step 522473  [5.428 sec/step, loss=0.07515, avg_loss=0.07423]\n",
      "Step 522474  [5.423 sec/step, loss=0.07390, avg_loss=0.07423]\n",
      "Step 522475  [5.410 sec/step, loss=0.07065, avg_loss=0.07418]\n",
      "Step 522476  [5.459 sec/step, loss=0.06642, avg_loss=0.07409]\n",
      "Step 522477  [5.469 sec/step, loss=0.07652, avg_loss=0.07414]\n",
      "Step 522478  [5.472 sec/step, loss=0.07573, avg_loss=0.07414]\n",
      "Step 522479  [5.475 sec/step, loss=0.07648, avg_loss=0.07414]\n",
      "Step 522480  [5.456 sec/step, loss=0.07205, avg_loss=0.07411]\n",
      "Step 522481  [5.461 sec/step, loss=0.07628, avg_loss=0.07412]\n",
      "Step 522482  [5.473 sec/step, loss=0.07518, avg_loss=0.07413]\n",
      "Step 522483  [5.472 sec/step, loss=0.07216, avg_loss=0.07411]\n",
      "Step 522484  [5.464 sec/step, loss=0.07404, avg_loss=0.07408]\n",
      "Step 522485  [5.466 sec/step, loss=0.07289, avg_loss=0.07406]\n",
      "Step 522486  [5.476 sec/step, loss=0.07618, avg_loss=0.07408]\n",
      "Step 522487  [5.471 sec/step, loss=0.07504, avg_loss=0.07408]\n",
      "Step 522488  [5.422 sec/step, loss=0.07611, avg_loss=0.07418]\n",
      "Step 522489  [5.438 sec/step, loss=0.07585, avg_loss=0.07422]\n",
      "Step 522490  [5.440 sec/step, loss=0.07453, avg_loss=0.07421]\n",
      "Step 522491  [5.422 sec/step, loss=0.07408, avg_loss=0.07419]\n",
      "Step 522492  [5.450 sec/step, loss=0.07543, avg_loss=0.07423]\n",
      "Step 522493  [5.462 sec/step, loss=0.07435, avg_loss=0.07425]\n",
      "Step 522494  [5.465 sec/step, loss=0.07527, avg_loss=0.07424]\n",
      "Step 522495  [5.474 sec/step, loss=0.07612, avg_loss=0.07424]\n",
      "Step 522496  [5.466 sec/step, loss=0.07509, avg_loss=0.07422]\n",
      "Step 522497  [5.466 sec/step, loss=0.07438, avg_loss=0.07422]\n",
      "Generated 32 batches of size 32 in 2.552 sec\n",
      "Step 522498  [5.462 sec/step, loss=0.07367, avg_loss=0.07420]\n",
      "Step 522499  [5.463 sec/step, loss=0.07373, avg_loss=0.07419]\n",
      "Step 522500  [5.456 sec/step, loss=0.07489, avg_loss=0.07419]\n",
      "Writing summary at step: 522500\n",
      "Step 522501  [5.446 sec/step, loss=0.07573, avg_loss=0.07418]\n",
      "Step 522502  [5.455 sec/step, loss=0.07621, avg_loss=0.07420]\n",
      "Step 522503  [5.466 sec/step, loss=0.07480, avg_loss=0.07427]\n",
      "Step 522504  [5.484 sec/step, loss=0.07680, avg_loss=0.07430]\n",
      "Step 522505  [5.454 sec/step, loss=0.06699, avg_loss=0.07420]\n",
      "Step 522506  [5.445 sec/step, loss=0.07340, avg_loss=0.07418]\n",
      "Step 522507  [5.490 sec/step, loss=0.06817, avg_loss=0.07412]\n",
      "Step 522508  [5.485 sec/step, loss=0.07607, avg_loss=0.07416]\n",
      "Step 522509  [5.484 sec/step, loss=0.07583, avg_loss=0.07418]\n",
      "Step 522510  [5.477 sec/step, loss=0.07702, avg_loss=0.07420]\n",
      "Step 522511  [5.473 sec/step, loss=0.07553, avg_loss=0.07420]\n",
      "Step 522512  [5.469 sec/step, loss=0.07324, avg_loss=0.07418]\n",
      "Step 522513  [5.456 sec/step, loss=0.07528, avg_loss=0.07420]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 522514  [5.470 sec/step, loss=0.07648, avg_loss=0.07422]\n",
      "Step 522515  [5.499 sec/step, loss=0.07406, avg_loss=0.07424]\n",
      "Step 522516  [5.487 sec/step, loss=0.07155, avg_loss=0.07420]\n",
      "Step 522517  [5.496 sec/step, loss=0.07607, avg_loss=0.07422]\n",
      "Step 522518  [5.475 sec/step, loss=0.07452, avg_loss=0.07420]\n",
      "Step 522519  [5.459 sec/step, loss=0.07527, avg_loss=0.07418]\n",
      "Step 522520  [5.455 sec/step, loss=0.07477, avg_loss=0.07417]\n",
      "Step 522521  [5.473 sec/step, loss=0.07689, avg_loss=0.07421]\n",
      "Step 522522  [5.470 sec/step, loss=0.07448, avg_loss=0.07420]\n",
      "Step 522523  [5.491 sec/step, loss=0.07697, avg_loss=0.07422]\n",
      "Step 522524  [5.463 sec/step, loss=0.06553, avg_loss=0.07411]\n",
      "Step 522525  [5.479 sec/step, loss=0.07642, avg_loss=0.07412]\n",
      "Step 522526  [5.454 sec/step, loss=0.07324, avg_loss=0.07410]\n",
      "Step 522527  [5.467 sec/step, loss=0.07494, avg_loss=0.07411]\n",
      "Step 522528  [5.461 sec/step, loss=0.07496, avg_loss=0.07411]\n",
      "Generated 32 batches of size 32 in 2.394 sec\n",
      "Step 522529  [5.466 sec/step, loss=0.07548, avg_loss=0.07411]\n",
      "Step 522530  [5.482 sec/step, loss=0.07709, avg_loss=0.07414]\n",
      "Step 522531  [5.490 sec/step, loss=0.07461, avg_loss=0.07414]\n",
      "Step 522532  [5.497 sec/step, loss=0.07440, avg_loss=0.07417]\n",
      "Step 522533  [5.503 sec/step, loss=0.07244, avg_loss=0.07413]\n",
      "Step 522534  [5.491 sec/step, loss=0.07579, avg_loss=0.07413]\n",
      "Step 522535  [5.485 sec/step, loss=0.07611, avg_loss=0.07413]\n",
      "Step 522536  [5.438 sec/step, loss=0.07531, avg_loss=0.07421]\n",
      "Step 522537  [5.442 sec/step, loss=0.07112, avg_loss=0.07421]\n",
      "Step 522538  [5.422 sec/step, loss=0.06784, avg_loss=0.07417]\n",
      "Step 522539  [5.428 sec/step, loss=0.07437, avg_loss=0.07426]\n",
      "Step 522540  [5.398 sec/step, loss=0.07409, avg_loss=0.07426]\n",
      "Step 522541  [5.398 sec/step, loss=0.07477, avg_loss=0.07428]\n",
      "Step 522542  [5.414 sec/step, loss=0.07448, avg_loss=0.07426]\n",
      "Step 522543  [5.390 sec/step, loss=0.07265, avg_loss=0.07424]\n",
      "Step 522544  [5.400 sec/step, loss=0.07656, avg_loss=0.07426]\n",
      "Step 522545  [5.390 sec/step, loss=0.07547, avg_loss=0.07424]\n",
      "Step 522546  [5.378 sec/step, loss=0.07406, avg_loss=0.07423]\n",
      "Step 522547  [5.382 sec/step, loss=0.07228, avg_loss=0.07420]\n",
      "Step 522548  [5.368 sec/step, loss=0.07496, avg_loss=0.07418]\n",
      "Step 522549  [5.314 sec/step, loss=0.07408, avg_loss=0.07427]\n",
      "Step 522550  [5.314 sec/step, loss=0.07563, avg_loss=0.07427]\n",
      "Step 522551  [5.314 sec/step, loss=0.07584, avg_loss=0.07429]\n",
      "Step 522552  [5.324 sec/step, loss=0.07630, avg_loss=0.07431]\n",
      "Step 522553  [5.322 sec/step, loss=0.07322, avg_loss=0.07428]\n",
      "Step 522554  [5.311 sec/step, loss=0.07327, avg_loss=0.07428]\n",
      "Step 522555  [5.303 sec/step, loss=0.07170, avg_loss=0.07426]\n",
      "Step 522556  [5.309 sec/step, loss=0.07506, avg_loss=0.07429]\n",
      "Step 522557  [5.325 sec/step, loss=0.07665, avg_loss=0.07435]\n",
      "Step 522558  [5.320 sec/step, loss=0.07588, avg_loss=0.07436]\n",
      "Step 522559  [5.343 sec/step, loss=0.07684, avg_loss=0.07439]\n",
      "Step 522560  [5.355 sec/step, loss=0.07473, avg_loss=0.07439]\n",
      "Generated 32 batches of size 32 in 2.389 sec\n",
      "Step 522561  [5.357 sec/step, loss=0.07621, avg_loss=0.07439]\n",
      "Step 522562  [5.357 sec/step, loss=0.07620, avg_loss=0.07442]\n",
      "Step 522563  [5.366 sec/step, loss=0.07690, avg_loss=0.07444]\n",
      "Step 522564  [5.393 sec/step, loss=0.07455, avg_loss=0.07451]\n",
      "Step 522565  [5.379 sec/step, loss=0.07203, avg_loss=0.07446]\n",
      "Step 522566  [5.402 sec/step, loss=0.07425, avg_loss=0.07445]\n",
      "Step 522567  [5.458 sec/step, loss=0.06792, avg_loss=0.07442]\n",
      "Step 522568  [5.452 sec/step, loss=0.07386, avg_loss=0.07441]\n",
      "Step 522569  [5.443 sec/step, loss=0.07647, avg_loss=0.07442]\n",
      "Step 522570  [5.418 sec/step, loss=0.07417, avg_loss=0.07439]\n",
      "Step 522571  [5.426 sec/step, loss=0.07628, avg_loss=0.07439]\n",
      "Step 522572  [5.422 sec/step, loss=0.07470, avg_loss=0.07440]\n",
      "Step 522573  [5.416 sec/step, loss=0.07514, avg_loss=0.07440]\n",
      "Step 522574  [5.446 sec/step, loss=0.07579, avg_loss=0.07442]\n",
      "Step 522575  [5.440 sec/step, loss=0.06621, avg_loss=0.07437]\n",
      "Step 522576  [5.390 sec/step, loss=0.07575, avg_loss=0.07447]\n",
      "Step 522577  [5.368 sec/step, loss=0.07194, avg_loss=0.07442]\n",
      "Step 522578  [5.366 sec/step, loss=0.07395, avg_loss=0.07440]\n",
      "Step 522579  [5.353 sec/step, loss=0.07168, avg_loss=0.07436]\n",
      "Step 522580  [5.363 sec/step, loss=0.07436, avg_loss=0.07438]\n",
      "Step 522581  [5.347 sec/step, loss=0.07176, avg_loss=0.07433]\n",
      "Step 522582  [5.350 sec/step, loss=0.07707, avg_loss=0.07435]\n",
      "Step 522583  [5.355 sec/step, loss=0.07475, avg_loss=0.07438]\n",
      "Step 522584  [5.367 sec/step, loss=0.07401, avg_loss=0.07438]\n",
      "Step 522585  [5.390 sec/step, loss=0.06605, avg_loss=0.07431]\n",
      "Step 522586  [5.387 sec/step, loss=0.07476, avg_loss=0.07430]\n",
      "Step 522587  [5.386 sec/step, loss=0.07556, avg_loss=0.07430]\n",
      "Step 522588  [5.395 sec/step, loss=0.07656, avg_loss=0.07431]\n",
      "Step 522589  [5.407 sec/step, loss=0.07604, avg_loss=0.07431]\n",
      "Step 522590  [5.424 sec/step, loss=0.07540, avg_loss=0.07432]\n",
      "Step 522591  [5.424 sec/step, loss=0.07303, avg_loss=0.07431]\n",
      "Step 522592  [5.411 sec/step, loss=0.07354, avg_loss=0.07429]\n",
      "Generated 32 batches of size 32 in 2.377 sec\n",
      "Step 522593  [5.410 sec/step, loss=0.07670, avg_loss=0.07431]\n",
      "Step 522594  [5.401 sec/step, loss=0.07566, avg_loss=0.07431]\n",
      "Step 522595  [5.383 sec/step, loss=0.07396, avg_loss=0.07429]\n",
      "Step 522596  [5.394 sec/step, loss=0.07545, avg_loss=0.07430]\n",
      "Step 522597  [5.408 sec/step, loss=0.07586, avg_loss=0.07431]\n",
      "Step 522598  [5.411 sec/step, loss=0.07618, avg_loss=0.07434]\n",
      "Step 522599  [5.418 sec/step, loss=0.07524, avg_loss=0.07435]\n",
      "Step 522600  [5.413 sec/step, loss=0.07510, avg_loss=0.07435]\n",
      "Writing summary at step: 522600\n",
      "Step 522601  [5.424 sec/step, loss=0.07534, avg_loss=0.07435]\n",
      "Step 522602  [5.421 sec/step, loss=0.07562, avg_loss=0.07434]\n",
      "Step 522603  [5.421 sec/step, loss=0.07438, avg_loss=0.07434]\n",
      "Step 522604  [5.394 sec/step, loss=0.07171, avg_loss=0.07429]\n",
      "Step 522605  [5.416 sec/step, loss=0.07403, avg_loss=0.07436]\n",
      "Step 522606  [5.417 sec/step, loss=0.07376, avg_loss=0.07436]\n",
      "Step 522607  [5.362 sec/step, loss=0.07314, avg_loss=0.07441]\n",
      "Step 522608  [5.355 sec/step, loss=0.07430, avg_loss=0.07439]\n",
      "Step 522609  [5.361 sec/step, loss=0.07643, avg_loss=0.07440]\n",
      "Step 522610  [5.359 sec/step, loss=0.07654, avg_loss=0.07440]\n",
      "Step 522611  [5.369 sec/step, loss=0.07586, avg_loss=0.07440]\n",
      "Step 522612  [5.382 sec/step, loss=0.07595, avg_loss=0.07443]\n",
      "Step 522613  [5.434 sec/step, loss=0.06607, avg_loss=0.07433]\n",
      "Step 522614  [5.417 sec/step, loss=0.07551, avg_loss=0.07432]\n",
      "Step 522615  [5.417 sec/step, loss=0.07399, avg_loss=0.07432]\n",
      "Step 522616  [5.434 sec/step, loss=0.07594, avg_loss=0.07437]\n",
      "Step 522617  [5.432 sec/step, loss=0.07210, avg_loss=0.07433]\n",
      "Step 522618  [5.428 sec/step, loss=0.07080, avg_loss=0.07429]\n",
      "Step 522619  [5.451 sec/step, loss=0.07453, avg_loss=0.07428]\n",
      "Step 522620  [5.444 sec/step, loss=0.07534, avg_loss=0.07429]\n",
      "Step 522621  [5.437 sec/step, loss=0.07552, avg_loss=0.07428]\n",
      "Step 522622  [5.437 sec/step, loss=0.07372, avg_loss=0.07427]\n",
      "Step 522623  [5.424 sec/step, loss=0.07587, avg_loss=0.07426]\n",
      "Generated 32 batches of size 32 in 2.395 sec\n",
      "Step 522624  [5.460 sec/step, loss=0.07653, avg_loss=0.07437]\n",
      "Step 522625  [5.436 sec/step, loss=0.07384, avg_loss=0.07434]\n",
      "Step 522626  [5.446 sec/step, loss=0.07540, avg_loss=0.07436]\n",
      "Step 522627  [5.436 sec/step, loss=0.07151, avg_loss=0.07433]\n",
      "Step 522628  [5.452 sec/step, loss=0.07308, avg_loss=0.07431]\n",
      "Step 522629  [5.426 sec/step, loss=0.06575, avg_loss=0.07421]\n",
      "Step 522630  [5.419 sec/step, loss=0.07502, avg_loss=0.07419]\n",
      "Step 522631  [5.412 sec/step, loss=0.07517, avg_loss=0.07420]\n",
      "Step 522632  [5.424 sec/step, loss=0.07546, avg_loss=0.07421]\n",
      "Step 522633  [5.411 sec/step, loss=0.07438, avg_loss=0.07423]\n",
      "Step 522634  [5.394 sec/step, loss=0.07149, avg_loss=0.07418]\n",
      "Step 522635  [5.385 sec/step, loss=0.07088, avg_loss=0.07413]\n",
      "Step 522636  [5.371 sec/step, loss=0.07088, avg_loss=0.07409]\n",
      "Step 522637  [5.396 sec/step, loss=0.07576, avg_loss=0.07413]\n",
      "Step 522638  [5.396 sec/step, loss=0.06566, avg_loss=0.07411]\n",
      "Step 522639  [5.406 sec/step, loss=0.07307, avg_loss=0.07410]\n",
      "Step 522640  [5.421 sec/step, loss=0.07414, avg_loss=0.07410]\n",
      "Step 522641  [5.434 sec/step, loss=0.07367, avg_loss=0.07409]\n",
      "Step 522642  [5.412 sec/step, loss=0.07542, avg_loss=0.07410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 522643  [5.427 sec/step, loss=0.07534, avg_loss=0.07412]\n",
      "Step 522644  [5.466 sec/step, loss=0.06559, avg_loss=0.07401]\n",
      "Step 522645  [5.460 sec/step, loss=0.07389, avg_loss=0.07400]\n",
      "Step 522646  [5.468 sec/step, loss=0.07426, avg_loss=0.07400]\n",
      "Step 522647  [5.465 sec/step, loss=0.07598, avg_loss=0.07404]\n",
      "Step 522648  [5.469 sec/step, loss=0.07505, avg_loss=0.07404]\n",
      "Step 522649  [5.473 sec/step, loss=0.07477, avg_loss=0.07405]\n",
      "Step 522650  [5.479 sec/step, loss=0.07490, avg_loss=0.07404]\n",
      "Step 522651  [5.486 sec/step, loss=0.07654, avg_loss=0.07405]\n",
      "Step 522652  [5.489 sec/step, loss=0.07696, avg_loss=0.07405]\n",
      "Step 522653  [5.483 sec/step, loss=0.07216, avg_loss=0.07404]\n",
      "Step 522654  [5.474 sec/step, loss=0.07491, avg_loss=0.07406]\n",
      "Step 522655  [5.480 sec/step, loss=0.07563, avg_loss=0.07410]\n",
      "Generated 32 batches of size 32 in 2.406 sec\n",
      "Step 522656  [5.493 sec/step, loss=0.07465, avg_loss=0.07409]\n",
      "Step 522657  [5.497 sec/step, loss=0.07663, avg_loss=0.07409]\n",
      "Step 522658  [5.486 sec/step, loss=0.07285, avg_loss=0.07406]\n",
      "Step 522659  [5.499 sec/step, loss=0.07430, avg_loss=0.07404]\n",
      "Step 522660  [5.496 sec/step, loss=0.07645, avg_loss=0.07405]\n",
      "Step 522661  [5.492 sec/step, loss=0.07449, avg_loss=0.07404]\n",
      "Step 522662  [5.512 sec/step, loss=0.07594, avg_loss=0.07403]\n",
      "Step 522663  [5.502 sec/step, loss=0.07538, avg_loss=0.07402]\n",
      "Step 522664  [5.488 sec/step, loss=0.07537, avg_loss=0.07403]\n",
      "Step 522665  [5.493 sec/step, loss=0.07243, avg_loss=0.07403]\n",
      "Step 522666  [5.468 sec/step, loss=0.07430, avg_loss=0.07403]\n",
      "Step 522667  [5.431 sec/step, loss=0.07623, avg_loss=0.07412]\n",
      "Step 522668  [5.447 sec/step, loss=0.07478, avg_loss=0.07412]\n",
      "Step 522669  [5.433 sec/step, loss=0.07537, avg_loss=0.07411]\n",
      "Step 522670  [5.446 sec/step, loss=0.07511, avg_loss=0.07412]\n",
      "Step 522671  [5.428 sec/step, loss=0.07268, avg_loss=0.07409]\n",
      "Step 522672  [5.466 sec/step, loss=0.07385, avg_loss=0.07408]\n",
      "Step 522673  [5.468 sec/step, loss=0.07260, avg_loss=0.07405]\n",
      "Step 522674  [5.436 sec/step, loss=0.07424, avg_loss=0.07404]\n",
      "Step 522675  [5.457 sec/step, loss=0.07487, avg_loss=0.07412]\n",
      "Step 522676  [5.440 sec/step, loss=0.06678, avg_loss=0.07403]\n",
      "Step 522677  [5.454 sec/step, loss=0.07512, avg_loss=0.07407]\n",
      "Step 522678  [5.462 sec/step, loss=0.07655, avg_loss=0.07409]\n",
      "Step 522679  [5.463 sec/step, loss=0.07182, avg_loss=0.07409]\n",
      "Step 522680  [5.473 sec/step, loss=0.07590, avg_loss=0.07411]\n",
      "Step 522681  [5.494 sec/step, loss=0.07308, avg_loss=0.07412]\n",
      "Step 522682  [5.484 sec/step, loss=0.07548, avg_loss=0.07411]\n",
      "Step 522683  [5.494 sec/step, loss=0.07676, avg_loss=0.07413]\n",
      "Step 522684  [5.480 sec/step, loss=0.07394, avg_loss=0.07413]\n",
      "Step 522685  [5.441 sec/step, loss=0.07620, avg_loss=0.07423]\n",
      "Step 522686  [5.450 sec/step, loss=0.07603, avg_loss=0.07424]\n",
      "Step 522687  [5.443 sec/step, loss=0.07590, avg_loss=0.07424]\n",
      "Generated 32 batches of size 32 in 2.379 sec\n",
      "Step 522688  [5.445 sec/step, loss=0.07663, avg_loss=0.07424]\n",
      "Step 522689  [5.422 sec/step, loss=0.07275, avg_loss=0.07421]\n",
      "Step 522690  [5.399 sec/step, loss=0.06994, avg_loss=0.07416]\n",
      "Step 522691  [5.397 sec/step, loss=0.07425, avg_loss=0.07417]\n",
      "Step 522692  [5.443 sec/step, loss=0.06815, avg_loss=0.07412]\n",
      "Step 522693  [5.426 sec/step, loss=0.07119, avg_loss=0.07406]\n",
      "Step 522694  [5.407 sec/step, loss=0.07313, avg_loss=0.07403]\n",
      "Step 522695  [5.419 sec/step, loss=0.07492, avg_loss=0.07404]\n",
      "Step 522696  [5.430 sec/step, loss=0.07369, avg_loss=0.07403]\n",
      "Step 522697  [5.420 sec/step, loss=0.07433, avg_loss=0.07401]\n",
      "Step 522698  [5.407 sec/step, loss=0.07395, avg_loss=0.07399]\n",
      "Step 522699  [5.411 sec/step, loss=0.07516, avg_loss=0.07399]\n",
      "Step 522700  [5.418 sec/step, loss=0.07629, avg_loss=0.07400]\n",
      "Writing summary at step: 522700\n",
      "Step 522701  [5.413 sec/step, loss=0.07505, avg_loss=0.07400]\n",
      "Step 522702  [5.422 sec/step, loss=0.07380, avg_loss=0.07398]\n",
      "Step 522703  [5.440 sec/step, loss=0.07631, avg_loss=0.07400]\n",
      "Step 522704  [5.454 sec/step, loss=0.07302, avg_loss=0.07401]\n",
      "Step 522705  [5.461 sec/step, loss=0.07624, avg_loss=0.07403]\n",
      "Step 522706  [5.464 sec/step, loss=0.07441, avg_loss=0.07404]\n",
      "Step 522707  [5.467 sec/step, loss=0.07579, avg_loss=0.07407]\n",
      "Step 522708  [5.459 sec/step, loss=0.06652, avg_loss=0.07399]\n",
      "Step 522709  [5.433 sec/step, loss=0.07158, avg_loss=0.07394]\n",
      "Step 522710  [5.475 sec/step, loss=0.06636, avg_loss=0.07384]\n",
      "Step 522711  [5.462 sec/step, loss=0.07504, avg_loss=0.07383]\n",
      "Step 522712  [5.458 sec/step, loss=0.07470, avg_loss=0.07382]\n",
      "Step 522713  [5.410 sec/step, loss=0.07594, avg_loss=0.07392]\n",
      "Step 522714  [5.413 sec/step, loss=0.07462, avg_loss=0.07391]\n",
      "Step 522715  [5.401 sec/step, loss=0.07613, avg_loss=0.07393]\n",
      "Step 522716  [5.394 sec/step, loss=0.07307, avg_loss=0.07390]\n",
      "Step 522717  [5.383 sec/step, loss=0.07122, avg_loss=0.07389]\n",
      "Step 522718  [5.400 sec/step, loss=0.07433, avg_loss=0.07393]\n",
      "Generated 32 batches of size 32 in 2.434 sec\n",
      "Step 522719  [5.392 sec/step, loss=0.07482, avg_loss=0.07393]\n",
      "Step 522720  [5.385 sec/step, loss=0.07372, avg_loss=0.07391]\n",
      "Step 522721  [5.387 sec/step, loss=0.07660, avg_loss=0.07392]\n",
      "Step 522722  [5.390 sec/step, loss=0.07523, avg_loss=0.07394]\n",
      "Step 522723  [5.411 sec/step, loss=0.07515, avg_loss=0.07393]\n",
      "Step 522724  [5.391 sec/step, loss=0.07015, avg_loss=0.07387]\n",
      "Step 522725  [5.411 sec/step, loss=0.07639, avg_loss=0.07389]\n",
      "Step 522726  [5.400 sec/step, loss=0.07358, avg_loss=0.07388]\n",
      "Step 522727  [5.404 sec/step, loss=0.07555, avg_loss=0.07392]\n",
      "Step 522728  [5.369 sec/step, loss=0.07221, avg_loss=0.07391]\n",
      "Step 522729  [5.398 sec/step, loss=0.07409, avg_loss=0.07399]\n",
      "Step 522730  [5.393 sec/step, loss=0.07522, avg_loss=0.07399]\n",
      "Step 522731  [5.437 sec/step, loss=0.06662, avg_loss=0.07391]\n",
      "Step 522732  [5.424 sec/step, loss=0.07533, avg_loss=0.07391]\n",
      "Step 522733  [5.426 sec/step, loss=0.07416, avg_loss=0.07390]\n",
      "Step 522734  [5.451 sec/step, loss=0.07636, avg_loss=0.07395]\n",
      "Step 522735  [5.443 sec/step, loss=0.07438, avg_loss=0.07399]\n",
      "Step 522736  [5.466 sec/step, loss=0.07657, avg_loss=0.07404]\n",
      "Step 522737  [5.447 sec/step, loss=0.07403, avg_loss=0.07403]\n",
      "Step 522738  [5.491 sec/step, loss=0.07381, avg_loss=0.07411]\n",
      "Step 522739  [5.500 sec/step, loss=0.07691, avg_loss=0.07415]\n",
      "Step 522740  [5.489 sec/step, loss=0.07331, avg_loss=0.07414]\n",
      "Step 522741  [5.484 sec/step, loss=0.07531, avg_loss=0.07416]\n",
      "Step 522742  [5.483 sec/step, loss=0.07619, avg_loss=0.07416]\n",
      "Step 522743  [5.481 sec/step, loss=0.07446, avg_loss=0.07415]\n",
      "Step 522744  [5.416 sec/step, loss=0.06567, avg_loss=0.07416]\n",
      "Step 522745  [5.424 sec/step, loss=0.07515, avg_loss=0.07417]\n",
      "Step 522746  [5.419 sec/step, loss=0.07400, avg_loss=0.07417]\n",
      "Step 522747  [5.421 sec/step, loss=0.07295, avg_loss=0.07413]\n",
      "Step 522748  [5.431 sec/step, loss=0.07666, avg_loss=0.07415]\n",
      "Step 522749  [5.421 sec/step, loss=0.07217, avg_loss=0.07412]\n",
      "Step 522750  [5.420 sec/step, loss=0.07372, avg_loss=0.07411]\n",
      "Generated 32 batches of size 32 in 2.419 sec\n",
      "Step 522751  [5.425 sec/step, loss=0.07637, avg_loss=0.07411]\n",
      "Step 522752  [5.415 sec/step, loss=0.07592, avg_loss=0.07410]\n",
      "Step 522753  [5.418 sec/step, loss=0.07302, avg_loss=0.07411]\n",
      "Step 522754  [5.417 sec/step, loss=0.07401, avg_loss=0.07410]\n",
      "Step 522755  [5.425 sec/step, loss=0.07560, avg_loss=0.07410]\n",
      "Step 522756  [5.415 sec/step, loss=0.07452, avg_loss=0.07410]\n",
      "Step 522757  [5.412 sec/step, loss=0.07585, avg_loss=0.07409]\n",
      "Step 522758  [5.443 sec/step, loss=0.07393, avg_loss=0.07410]\n",
      "Step 522759  [5.423 sec/step, loss=0.07530, avg_loss=0.07411]\n",
      "Step 522760  [5.411 sec/step, loss=0.07115, avg_loss=0.07406]\n",
      "Step 522761  [5.416 sec/step, loss=0.07380, avg_loss=0.07405]\n",
      "Step 522762  [5.450 sec/step, loss=0.06622, avg_loss=0.07395]\n",
      "Step 522763  [5.445 sec/step, loss=0.07491, avg_loss=0.07395]\n",
      "Step 522764  [5.457 sec/step, loss=0.07623, avg_loss=0.07396]\n",
      "Step 522765  [5.448 sec/step, loss=0.07364, avg_loss=0.07397]\n",
      "Step 522766  [5.442 sec/step, loss=0.07468, avg_loss=0.07397]\n",
      "Step 522767  [5.453 sec/step, loss=0.07560, avg_loss=0.07397]\n",
      "Step 522768  [5.445 sec/step, loss=0.07450, avg_loss=0.07397]\n",
      "Step 522769  [5.442 sec/step, loss=0.07501, avg_loss=0.07396]\n",
      "Step 522770  [5.419 sec/step, loss=0.06556, avg_loss=0.07387]\n",
      "Step 522771  [5.417 sec/step, loss=0.07574, avg_loss=0.07390]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 522772  [5.389 sec/step, loss=0.07462, avg_loss=0.07390]\n",
      "Step 522773  [5.379 sec/step, loss=0.07123, avg_loss=0.07389]\n",
      "Step 522774  [5.391 sec/step, loss=0.07536, avg_loss=0.07390]\n",
      "Step 522775  [5.389 sec/step, loss=0.07543, avg_loss=0.07391]\n",
      "Step 522776  [5.408 sec/step, loss=0.07585, avg_loss=0.07400]\n",
      "Step 522777  [5.423 sec/step, loss=0.07499, avg_loss=0.07400]\n",
      "Step 522778  [5.405 sec/step, loss=0.07162, avg_loss=0.07395]\n",
      "Step 522779  [5.420 sec/step, loss=0.07570, avg_loss=0.07399]\n",
      "Step 522780  [5.426 sec/step, loss=0.07650, avg_loss=0.07399]\n",
      "Step 522781  [5.420 sec/step, loss=0.07479, avg_loss=0.07401]\n",
      "Step 522782  [5.433 sec/step, loss=0.07605, avg_loss=0.07402]\n",
      "Generated 32 batches of size 32 in 2.359 sec\n",
      "Step 522783  [5.430 sec/step, loss=0.07595, avg_loss=0.07401]\n",
      "Step 522784  [5.430 sec/step, loss=0.07315, avg_loss=0.07400]\n",
      "Step 522785  [5.423 sec/step, loss=0.07500, avg_loss=0.07399]\n",
      "Step 522786  [5.413 sec/step, loss=0.07415, avg_loss=0.07397]\n",
      "Step 522787  [5.430 sec/step, loss=0.07609, avg_loss=0.07397]\n",
      "Step 522788  [5.415 sec/step, loss=0.07375, avg_loss=0.07394]\n",
      "Step 522789  [5.411 sec/step, loss=0.07281, avg_loss=0.07394]\n",
      "Step 522790  [5.426 sec/step, loss=0.07609, avg_loss=0.07400]\n",
      "Step 522791  [5.442 sec/step, loss=0.07538, avg_loss=0.07401]\n",
      "Step 522792  [5.394 sec/step, loss=0.07525, avg_loss=0.07409]\n",
      "Step 522793  [5.410 sec/step, loss=0.07441, avg_loss=0.07412]\n",
      "Step 522794  [5.416 sec/step, loss=0.07351, avg_loss=0.07412]\n",
      "Step 522795  [5.407 sec/step, loss=0.07261, avg_loss=0.07410]\n",
      "Step 522796  [5.392 sec/step, loss=0.07677, avg_loss=0.07413]\n",
      "Step 522797  [5.405 sec/step, loss=0.07625, avg_loss=0.07415]\n",
      "Step 522798  [5.419 sec/step, loss=0.07512, avg_loss=0.07416]\n",
      "Step 522799  [5.422 sec/step, loss=0.07675, avg_loss=0.07418]\n",
      "Step 522800  [5.404 sec/step, loss=0.07020, avg_loss=0.07412]\n",
      "Writing summary at step: 522800\n",
      "Step 522801  [5.400 sec/step, loss=0.07528, avg_loss=0.07412]\n",
      "Step 522802  [5.397 sec/step, loss=0.07613, avg_loss=0.07414]\n",
      "Step 522803  [5.382 sec/step, loss=0.07573, avg_loss=0.07414]\n",
      "Step 522804  [5.368 sec/step, loss=0.07092, avg_loss=0.07411]\n",
      "Step 522805  [5.374 sec/step, loss=0.07530, avg_loss=0.07410]\n",
      "Step 522806  [5.372 sec/step, loss=0.07542, avg_loss=0.07412]\n",
      "Step 522807  [5.379 sec/step, loss=0.07338, avg_loss=0.07409]\n",
      "Step 522808  [5.379 sec/step, loss=0.06655, avg_loss=0.07409]\n",
      "Step 522809  [5.442 sec/step, loss=0.06742, avg_loss=0.07405]\n",
      "Step 522810  [5.388 sec/step, loss=0.07165, avg_loss=0.07410]\n",
      "Step 522811  [5.391 sec/step, loss=0.07597, avg_loss=0.07411]\n",
      "Step 522812  [5.386 sec/step, loss=0.07453, avg_loss=0.07411]\n",
      "Step 522813  [5.392 sec/step, loss=0.07370, avg_loss=0.07409]\n",
      "Generated 32 batches of size 32 in 2.576 sec\n",
      "Step 522814  [5.388 sec/step, loss=0.07421, avg_loss=0.07408]\n",
      "Step 522815  [5.385 sec/step, loss=0.07626, avg_loss=0.07408]\n",
      "Step 522816  [5.400 sec/step, loss=0.07631, avg_loss=0.07412]\n",
      "Step 522817  [5.434 sec/step, loss=0.07311, avg_loss=0.07414]\n",
      "Step 522818  [5.428 sec/step, loss=0.07312, avg_loss=0.07412]\n",
      "Step 522819  [5.412 sec/step, loss=0.07419, avg_loss=0.07412]\n",
      "Step 522820  [5.416 sec/step, loss=0.07589, avg_loss=0.07414]\n",
      "Step 522821  [5.422 sec/step, loss=0.07587, avg_loss=0.07413]\n",
      "Step 522822  [5.423 sec/step, loss=0.07518, avg_loss=0.07413]\n",
      "Step 522823  [5.401 sec/step, loss=0.07438, avg_loss=0.07412]\n",
      "Step 522824  [5.416 sec/step, loss=0.07686, avg_loss=0.07419]\n",
      "Step 522825  [5.410 sec/step, loss=0.07630, avg_loss=0.07419]\n",
      "Step 522826  [5.414 sec/step, loss=0.07285, avg_loss=0.07418]\n",
      "Step 522827  [5.413 sec/step, loss=0.07264, avg_loss=0.07415]\n",
      "Step 522828  [5.435 sec/step, loss=0.07700, avg_loss=0.07420]\n",
      "Step 522829  [5.432 sec/step, loss=0.07417, avg_loss=0.07420]\n",
      "Step 522830  [5.455 sec/step, loss=0.07375, avg_loss=0.07419]\n",
      "Step 522831  [5.394 sec/step, loss=0.07197, avg_loss=0.07424]\n",
      "Step 522832  [5.401 sec/step, loss=0.07285, avg_loss=0.07422]\n",
      "Step 522833  [5.434 sec/step, loss=0.07585, avg_loss=0.07423]\n",
      "Step 522834  [5.421 sec/step, loss=0.07427, avg_loss=0.07421]\n",
      "Step 522835  [5.429 sec/step, loss=0.07242, avg_loss=0.07419]\n",
      "Step 522836  [5.468 sec/step, loss=0.06607, avg_loss=0.07409]\n",
      "Step 522837  [5.458 sec/step, loss=0.07248, avg_loss=0.07407]\n",
      "Step 522838  [5.432 sec/step, loss=0.07547, avg_loss=0.07409]\n",
      "Step 522839  [5.430 sec/step, loss=0.07601, avg_loss=0.07408]\n",
      "Step 522840  [5.433 sec/step, loss=0.07549, avg_loss=0.07410]\n",
      "Step 522841  [5.434 sec/step, loss=0.07621, avg_loss=0.07411]\n",
      "Step 522842  [5.447 sec/step, loss=0.07408, avg_loss=0.07409]\n",
      "Step 522843  [5.445 sec/step, loss=0.07566, avg_loss=0.07410]\n",
      "Step 522844  [5.454 sec/step, loss=0.07437, avg_loss=0.07419]\n",
      "Step 522845  [5.442 sec/step, loss=0.07371, avg_loss=0.07417]\n",
      "Generated 32 batches of size 32 in 2.510 sec\n",
      "Step 522846  [5.451 sec/step, loss=0.07479, avg_loss=0.07418]\n",
      "Step 522847  [5.449 sec/step, loss=0.07582, avg_loss=0.07421]\n",
      "Step 522848  [5.440 sec/step, loss=0.07549, avg_loss=0.07420]\n",
      "Step 522849  [5.434 sec/step, loss=0.06656, avg_loss=0.07414]\n",
      "Step 522850  [5.422 sec/step, loss=0.07560, avg_loss=0.07416]\n",
      "Step 522851  [5.423 sec/step, loss=0.07634, avg_loss=0.07416]\n",
      "Step 522852  [5.408 sec/step, loss=0.07423, avg_loss=0.07414]\n",
      "Step 522853  [5.414 sec/step, loss=0.07462, avg_loss=0.07416]\n",
      "Step 522854  [5.432 sec/step, loss=0.07626, avg_loss=0.07418]\n",
      "Step 522855  [5.410 sec/step, loss=0.06657, avg_loss=0.07409]\n",
      "Step 522856  [5.413 sec/step, loss=0.07310, avg_loss=0.07408]\n",
      "Step 522857  [5.412 sec/step, loss=0.07644, avg_loss=0.07408]\n",
      "Step 522858  [5.384 sec/step, loss=0.07415, avg_loss=0.07409]\n",
      "Step 522859  [5.388 sec/step, loss=0.07207, avg_loss=0.07405]\n",
      "Step 522860  [5.403 sec/step, loss=0.07622, avg_loss=0.07411]\n",
      "Step 522861  [5.436 sec/step, loss=0.07036, avg_loss=0.07407]\n",
      "Step 522862  [5.403 sec/step, loss=0.07371, avg_loss=0.07415]\n",
      "Step 522863  [5.416 sec/step, loss=0.07464, avg_loss=0.07414]\n",
      "Step 522864  [5.397 sec/step, loss=0.07266, avg_loss=0.07411]\n",
      "Step 522865  [5.409 sec/step, loss=0.07488, avg_loss=0.07412]\n",
      "Step 522866  [5.409 sec/step, loss=0.07153, avg_loss=0.07409]\n",
      "Step 522867  [5.381 sec/step, loss=0.07566, avg_loss=0.07409]\n",
      "Step 522868  [5.381 sec/step, loss=0.07357, avg_loss=0.07408]\n",
      "Step 522869  [5.378 sec/step, loss=0.07385, avg_loss=0.07407]\n",
      "Step 522870  [5.383 sec/step, loss=0.07291, avg_loss=0.07414]\n",
      "Step 522871  [5.410 sec/step, loss=0.07422, avg_loss=0.07413]\n",
      "Step 522872  [5.405 sec/step, loss=0.06969, avg_loss=0.07408]\n",
      "Step 522873  [5.425 sec/step, loss=0.07660, avg_loss=0.07413]\n",
      "Step 522874  [5.427 sec/step, loss=0.07707, avg_loss=0.07415]\n",
      "Step 522875  [5.425 sec/step, loss=0.07544, avg_loss=0.07415]\n",
      "Step 522876  [5.416 sec/step, loss=0.07425, avg_loss=0.07413]\n",
      "Step 522877  [5.415 sec/step, loss=0.07687, avg_loss=0.07415]\n",
      "Generated 32 batches of size 32 in 2.385 sec\n",
      "Step 522878  [5.435 sec/step, loss=0.07544, avg_loss=0.07419]\n",
      "Step 522879  [5.431 sec/step, loss=0.07637, avg_loss=0.07420]\n",
      "Step 522880  [5.420 sec/step, loss=0.07463, avg_loss=0.07418]\n",
      "Step 522881  [5.416 sec/step, loss=0.07261, avg_loss=0.07415]\n",
      "Step 522882  [5.414 sec/step, loss=0.07652, avg_loss=0.07416]\n",
      "Step 522883  [5.417 sec/step, loss=0.07500, avg_loss=0.07415]\n",
      "Step 522884  [5.414 sec/step, loss=0.07475, avg_loss=0.07417]\n",
      "Step 522885  [5.413 sec/step, loss=0.07570, avg_loss=0.07417]\n",
      "Step 522886  [5.410 sec/step, loss=0.07504, avg_loss=0.07418]\n",
      "Step 522887  [5.402 sec/step, loss=0.07615, avg_loss=0.07418]\n",
      "Step 522888  [5.411 sec/step, loss=0.07487, avg_loss=0.07419]\n",
      "Step 522889  [5.423 sec/step, loss=0.07559, avg_loss=0.07422]\n",
      "Step 522890  [5.406 sec/step, loss=0.07320, avg_loss=0.07419]\n",
      "Step 522891  [5.399 sec/step, loss=0.07487, avg_loss=0.07419]\n",
      "Step 522892  [5.422 sec/step, loss=0.07547, avg_loss=0.07419]\n",
      "Step 522893  [5.403 sec/step, loss=0.07049, avg_loss=0.07415]\n",
      "Step 522894  [5.428 sec/step, loss=0.07530, avg_loss=0.07417]\n",
      "Step 522895  [5.431 sec/step, loss=0.07307, avg_loss=0.07417]\n",
      "Step 522896  [5.422 sec/step, loss=0.07523, avg_loss=0.07416]\n",
      "Step 522897  [5.426 sec/step, loss=0.07367, avg_loss=0.07413]\n",
      "Step 522898  [5.423 sec/step, loss=0.07570, avg_loss=0.07414]\n",
      "Step 522899  [5.409 sec/step, loss=0.07175, avg_loss=0.07409]\n",
      "Step 522900  [5.419 sec/step, loss=0.07430, avg_loss=0.07413]\n",
      "Writing summary at step: 522900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 522901  [5.429 sec/step, loss=0.07654, avg_loss=0.07414]\n",
      "Step 522902  [5.417 sec/step, loss=0.07332, avg_loss=0.07411]\n",
      "Step 522903  [5.431 sec/step, loss=0.07649, avg_loss=0.07412]\n",
      "Step 522904  [5.454 sec/step, loss=0.07337, avg_loss=0.07415]\n",
      "Step 522905  [5.450 sec/step, loss=0.07593, avg_loss=0.07415]\n",
      "Step 522906  [5.454 sec/step, loss=0.07574, avg_loss=0.07415]\n",
      "Step 522907  [5.457 sec/step, loss=0.07681, avg_loss=0.07419]\n",
      "Step 522908  [5.479 sec/step, loss=0.07453, avg_loss=0.07427]\n",
      "Generated 32 batches of size 32 in 2.611 sec\n",
      "Step 522909  [5.430 sec/step, loss=0.07386, avg_loss=0.07433]\n",
      "Step 522910  [5.426 sec/step, loss=0.07437, avg_loss=0.07436]\n",
      "Step 522911  [5.410 sec/step, loss=0.07112, avg_loss=0.07431]\n",
      "Step 522912  [5.399 sec/step, loss=0.07443, avg_loss=0.07431]\n",
      "Step 522913  [5.375 sec/step, loss=0.06598, avg_loss=0.07423]\n",
      "Step 522914  [5.392 sec/step, loss=0.07664, avg_loss=0.07426]\n",
      "Step 522915  [5.429 sec/step, loss=0.06663, avg_loss=0.07416]\n",
      "Step 522916  [5.415 sec/step, loss=0.07525, avg_loss=0.07415]\n",
      "Step 522917  [5.396 sec/step, loss=0.07591, avg_loss=0.07418]\n",
      "Step 522918  [5.407 sec/step, loss=0.07444, avg_loss=0.07419]\n",
      "Step 522919  [5.417 sec/step, loss=0.07500, avg_loss=0.07420]\n",
      "Step 522920  [5.409 sec/step, loss=0.07470, avg_loss=0.07419]\n",
      "Step 522921  [5.402 sec/step, loss=0.07650, avg_loss=0.07419]\n",
      "Step 522922  [5.399 sec/step, loss=0.07578, avg_loss=0.07420]\n",
      "Step 522923  [5.422 sec/step, loss=0.07306, avg_loss=0.07419]\n",
      "Step 522924  [5.409 sec/step, loss=0.07240, avg_loss=0.07414]\n",
      "Step 522925  [5.388 sec/step, loss=0.07344, avg_loss=0.07411]\n",
      "Step 522926  [5.396 sec/step, loss=0.07476, avg_loss=0.07413]\n",
      "Step 522927  [5.404 sec/step, loss=0.07548, avg_loss=0.07416]\n",
      "Step 522928  [5.399 sec/step, loss=0.07161, avg_loss=0.07411]\n",
      "Step 522929  [5.397 sec/step, loss=0.07491, avg_loss=0.07412]\n",
      "Step 522930  [5.393 sec/step, loss=0.07684, avg_loss=0.07415]\n",
      "Step 522931  [5.406 sec/step, loss=0.07535, avg_loss=0.07418]\n",
      "Step 522932  [5.415 sec/step, loss=0.07643, avg_loss=0.07422]\n",
      "Step 522933  [5.386 sec/step, loss=0.07317, avg_loss=0.07419]\n",
      "Step 522934  [5.385 sec/step, loss=0.07396, avg_loss=0.07419]\n",
      "Step 522935  [5.384 sec/step, loss=0.07550, avg_loss=0.07422]\n",
      "Step 522936  [5.322 sec/step, loss=0.07152, avg_loss=0.07427]\n",
      "Step 522937  [5.341 sec/step, loss=0.07509, avg_loss=0.07430]\n",
      "Step 522938  [5.353 sec/step, loss=0.07594, avg_loss=0.07430]\n",
      "Step 522939  [5.337 sec/step, loss=0.07474, avg_loss=0.07429]\n",
      "Step 522940  [5.317 sec/step, loss=0.06699, avg_loss=0.07420]\n",
      "Generated 32 batches of size 32 in 2.552 sec\n",
      "Step 522941  [5.309 sec/step, loss=0.07546, avg_loss=0.07420]\n",
      "Step 522942  [5.288 sec/step, loss=0.07374, avg_loss=0.07419]\n",
      "Step 522943  [5.341 sec/step, loss=0.06558, avg_loss=0.07409]\n",
      "Step 522944  [5.352 sec/step, loss=0.07513, avg_loss=0.07410]\n",
      "Step 522945  [5.357 sec/step, loss=0.07141, avg_loss=0.07408]\n",
      "Step 522946  [5.358 sec/step, loss=0.07652, avg_loss=0.07409]\n",
      "Step 522947  [5.382 sec/step, loss=0.07334, avg_loss=0.07407]\n",
      "Step 522948  [5.377 sec/step, loss=0.07417, avg_loss=0.07406]\n",
      "Step 522949  [5.407 sec/step, loss=0.07413, avg_loss=0.07413]\n",
      "Step 522950  [5.424 sec/step, loss=0.07570, avg_loss=0.07413]\n",
      "Step 522951  [5.400 sec/step, loss=0.07200, avg_loss=0.07409]\n",
      "Step 522952  [5.405 sec/step, loss=0.07220, avg_loss=0.07407]\n",
      "Step 522953  [5.398 sec/step, loss=0.07152, avg_loss=0.07404]\n",
      "Step 522954  [5.383 sec/step, loss=0.07403, avg_loss=0.07402]\n",
      "Step 522955  [5.393 sec/step, loss=0.07356, avg_loss=0.07409]\n",
      "Step 522956  [5.392 sec/step, loss=0.07547, avg_loss=0.07411]\n",
      "Step 522957  [5.381 sec/step, loss=0.07504, avg_loss=0.07410]\n",
      "Step 522958  [5.434 sec/step, loss=0.06710, avg_loss=0.07403]\n",
      "Step 522959  [5.431 sec/step, loss=0.07600, avg_loss=0.07407]\n",
      "Step 522960  [5.426 sec/step, loss=0.07622, avg_loss=0.07407]\n",
      "Step 522961  [5.368 sec/step, loss=0.06519, avg_loss=0.07401]\n",
      "Step 522962  [5.368 sec/step, loss=0.07399, avg_loss=0.07402]\n",
      "Step 522963  [5.355 sec/step, loss=0.07432, avg_loss=0.07401]\n",
      "Step 522964  [5.374 sec/step, loss=0.07535, avg_loss=0.07404]\n",
      "Step 522965  [5.371 sec/step, loss=0.07596, avg_loss=0.07405]\n",
      "Step 522966  [5.386 sec/step, loss=0.07506, avg_loss=0.07409]\n",
      "Step 522967  [5.398 sec/step, loss=0.07666, avg_loss=0.07410]\n",
      "Step 522968  [5.408 sec/step, loss=0.07346, avg_loss=0.07409]\n",
      "Step 522969  [5.402 sec/step, loss=0.07407, avg_loss=0.07410]\n",
      "Step 522970  [5.401 sec/step, loss=0.07117, avg_loss=0.07408]\n",
      "Step 522971  [5.382 sec/step, loss=0.07486, avg_loss=0.07409]\n",
      "Step 522972  [5.400 sec/step, loss=0.07667, avg_loss=0.07416]\n",
      "Generated 32 batches of size 32 in 2.524 sec\n",
      "Step 522973  [5.386 sec/step, loss=0.07404, avg_loss=0.07413]\n",
      "Step 522974  [5.383 sec/step, loss=0.07535, avg_loss=0.07411]\n",
      "Step 522975  [5.381 sec/step, loss=0.07545, avg_loss=0.07411]\n",
      "Step 522976  [5.385 sec/step, loss=0.07448, avg_loss=0.07412]\n",
      "Step 522977  [5.397 sec/step, loss=0.07427, avg_loss=0.07409]\n",
      "Step 522978  [5.399 sec/step, loss=0.07636, avg_loss=0.07410]\n",
      "Step 522979  [5.389 sec/step, loss=0.07548, avg_loss=0.07409]\n",
      "Step 522980  [5.393 sec/step, loss=0.07544, avg_loss=0.07410]\n",
      "Step 522981  [5.403 sec/step, loss=0.07709, avg_loss=0.07414]\n",
      "Step 522982  [5.387 sec/step, loss=0.07385, avg_loss=0.07412]\n",
      "Step 522983  [5.404 sec/step, loss=0.07414, avg_loss=0.07411]\n",
      "Step 522984  [5.401 sec/step, loss=0.07149, avg_loss=0.07407]\n",
      "Step 522985  [5.404 sec/step, loss=0.07512, avg_loss=0.07407]\n",
      "Step 522986  [5.392 sec/step, loss=0.07475, avg_loss=0.07407]\n",
      "Step 522987  [5.395 sec/step, loss=0.07733, avg_loss=0.07408]\n",
      "Step 522988  [5.403 sec/step, loss=0.07653, avg_loss=0.07409]\n",
      "Step 522989  [5.405 sec/step, loss=0.07483, avg_loss=0.07409]\n",
      "Step 522990  [5.407 sec/step, loss=0.07463, avg_loss=0.07410]\n",
      "Step 522991  [5.409 sec/step, loss=0.07572, avg_loss=0.07411]\n",
      "Step 522992  [5.386 sec/step, loss=0.07250, avg_loss=0.07408]\n",
      "Step 522993  [5.398 sec/step, loss=0.07609, avg_loss=0.07414]\n",
      "Step 522994  [5.386 sec/step, loss=0.07684, avg_loss=0.07415]\n",
      "Step 522995  [5.372 sec/step, loss=0.07226, avg_loss=0.07414]\n",
      "Step 522996  [5.355 sec/step, loss=0.06676, avg_loss=0.07406]\n",
      "Step 522997  [5.356 sec/step, loss=0.07665, avg_loss=0.07409]\n",
      "Step 522998  [5.358 sec/step, loss=0.07586, avg_loss=0.07409]\n",
      "Step 522999  [5.410 sec/step, loss=0.06669, avg_loss=0.07404]\n",
      "Step 523000  [5.406 sec/step, loss=0.07243, avg_loss=0.07402]\n",
      "Writing summary at step: 523000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-523000\n",
      "Saving audio and alignment...\n",
      "Input: mazbuutd aytdaymaadd sae afjuun kay matdluubaa istdayhsaalii vufuudd koo xofzaddaa kajjaa dzaa saktdaa hae~___________________________\n",
      "Step 523001  [5.401 sec/step, loss=0.07564, avg_loss=0.07401]\n",
      "Step 523002  [5.401 sec/step, loss=0.07588, avg_loss=0.07404]\n",
      "Generated 32 batches of size 32 in 2.350 sec\n",
      "Step 523003  [5.395 sec/step, loss=0.07569, avg_loss=0.07403]\n",
      "Step 523004  [5.397 sec/step, loss=0.07437, avg_loss=0.07404]\n",
      "Step 523005  [5.406 sec/step, loss=0.07358, avg_loss=0.07402]\n",
      "Step 523006  [5.417 sec/step, loss=0.07433, avg_loss=0.07400]\n",
      "Step 523007  [5.409 sec/step, loss=0.07426, avg_loss=0.07398]\n",
      "Step 523008  [5.416 sec/step, loss=0.07623, avg_loss=0.07399]\n",
      "Step 523009  [5.419 sec/step, loss=0.07245, avg_loss=0.07398]\n",
      "Step 523010  [5.423 sec/step, loss=0.07455, avg_loss=0.07398]\n",
      "Step 523011  [5.430 sec/step, loss=0.07457, avg_loss=0.07401]\n",
      "Step 523012  [5.433 sec/step, loss=0.07425, avg_loss=0.07401]\n",
      "Step 523013  [5.501 sec/step, loss=0.06733, avg_loss=0.07403]\n",
      "Step 523014  [5.494 sec/step, loss=0.07723, avg_loss=0.07403]\n",
      "Step 523015  [5.449 sec/step, loss=0.07532, avg_loss=0.07412]\n",
      "Step 523016  [5.451 sec/step, loss=0.07437, avg_loss=0.07411]\n",
      "Step 523017  [5.443 sec/step, loss=0.07435, avg_loss=0.07409]\n",
      "Step 523018  [5.442 sec/step, loss=0.07721, avg_loss=0.07412]\n",
      "Step 523019  [5.456 sec/step, loss=0.07630, avg_loss=0.07414]\n",
      "Step 523020  [5.452 sec/step, loss=0.07475, avg_loss=0.07414]\n",
      "Step 523021  [5.452 sec/step, loss=0.07666, avg_loss=0.07414]\n",
      "Step 523022  [5.463 sec/step, loss=0.07709, avg_loss=0.07415]\n",
      "Step 523023  [5.463 sec/step, loss=0.07612, avg_loss=0.07418]\n",
      "Step 523024  [5.463 sec/step, loss=0.07518, avg_loss=0.07421]\n",
      "Step 523025  [5.465 sec/step, loss=0.07184, avg_loss=0.07419]\n",
      "Step 523026  [5.461 sec/step, loss=0.07519, avg_loss=0.07420]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 523027  [5.454 sec/step, loss=0.07638, avg_loss=0.07421]\n",
      "Step 523028  [5.461 sec/step, loss=0.07653, avg_loss=0.07426]\n",
      "Step 523029  [5.463 sec/step, loss=0.07432, avg_loss=0.07425]\n",
      "Step 523030  [5.445 sec/step, loss=0.07606, avg_loss=0.07424]\n",
      "Step 523031  [5.425 sec/step, loss=0.06637, avg_loss=0.07415]\n",
      "Step 523032  [5.410 sec/step, loss=0.07583, avg_loss=0.07415]\n",
      "Step 523033  [5.428 sec/step, loss=0.07621, avg_loss=0.07418]\n",
      "Step 523034  [5.428 sec/step, loss=0.07140, avg_loss=0.07415]\n",
      "Generated 32 batches of size 32 in 2.362 sec\n",
      "Step 523035  [5.438 sec/step, loss=0.07541, avg_loss=0.07415]\n",
      "Step 523036  [5.458 sec/step, loss=0.07471, avg_loss=0.07418]\n",
      "Step 523037  [5.457 sec/step, loss=0.07389, avg_loss=0.07417]\n",
      "Step 523038  [5.450 sec/step, loss=0.07605, avg_loss=0.07417]\n",
      "Step 523039  [5.456 sec/step, loss=0.07363, avg_loss=0.07416]\n",
      "Step 523040  [5.465 sec/step, loss=0.07496, avg_loss=0.07424]\n",
      "Step 523041  [5.492 sec/step, loss=0.07332, avg_loss=0.07422]\n",
      "Step 523042  [5.485 sec/step, loss=0.07236, avg_loss=0.07420]\n",
      "Step 523043  [5.437 sec/step, loss=0.07596, avg_loss=0.07431]\n",
      "Step 523044  [5.443 sec/step, loss=0.07657, avg_loss=0.07432]\n",
      "Step 523045  [5.446 sec/step, loss=0.07534, avg_loss=0.07436]\n",
      "Step 523046  [5.443 sec/step, loss=0.07565, avg_loss=0.07435]\n",
      "Step 523047  [5.421 sec/step, loss=0.07492, avg_loss=0.07437]\n",
      "Step 523048  [5.420 sec/step, loss=0.07567, avg_loss=0.07438]\n",
      "Step 523049  [5.399 sec/step, loss=0.07442, avg_loss=0.07439]\n",
      "Step 523050  [5.368 sec/step, loss=0.06729, avg_loss=0.07430]\n",
      "Step 523051  [5.378 sec/step, loss=0.07441, avg_loss=0.07433]\n",
      "Step 523052  [5.374 sec/step, loss=0.07426, avg_loss=0.07435]\n",
      "Step 523053  [5.378 sec/step, loss=0.07603, avg_loss=0.07439]\n",
      "Step 523054  [5.389 sec/step, loss=0.07692, avg_loss=0.07442]\n",
      "Step 523055  [5.448 sec/step, loss=0.06764, avg_loss=0.07436]\n",
      "Step 523056  [5.442 sec/step, loss=0.07285, avg_loss=0.07434]\n",
      "Step 523057  [5.449 sec/step, loss=0.07226, avg_loss=0.07431]\n",
      "Step 523058  [5.406 sec/step, loss=0.07242, avg_loss=0.07436]\n",
      "Step 523059  [5.430 sec/step, loss=0.07374, avg_loss=0.07434]\n",
      "Step 523060  [5.408 sec/step, loss=0.07223, avg_loss=0.07430]\n",
      "Step 523061  [5.443 sec/step, loss=0.07418, avg_loss=0.07439]\n",
      "Step 523062  [5.434 sec/step, loss=0.07622, avg_loss=0.07441]\n",
      "Step 523063  [5.433 sec/step, loss=0.07177, avg_loss=0.07439]\n",
      "Step 523064  [5.428 sec/step, loss=0.07647, avg_loss=0.07440]\n",
      "Step 523065  [5.439 sec/step, loss=0.07495, avg_loss=0.07439]\n",
      "Step 523066  [5.421 sec/step, loss=0.07183, avg_loss=0.07435]\n",
      "Generated 32 batches of size 32 in 2.336 sec\n",
      "Step 523067  [5.420 sec/step, loss=0.07553, avg_loss=0.07434]\n",
      "Step 523068  [5.399 sec/step, loss=0.07455, avg_loss=0.07435]\n",
      "Step 523069  [5.423 sec/step, loss=0.07669, avg_loss=0.07438]\n",
      "Step 523070  [5.448 sec/step, loss=0.07426, avg_loss=0.07441]\n",
      "Step 523071  [5.451 sec/step, loss=0.07514, avg_loss=0.07441]\n",
      "Step 523072  [5.436 sec/step, loss=0.07335, avg_loss=0.07438]\n",
      "Step 523073  [5.453 sec/step, loss=0.07730, avg_loss=0.07441]\n",
      "Step 523074  [5.449 sec/step, loss=0.07329, avg_loss=0.07439]\n",
      "Step 523075  [5.447 sec/step, loss=0.07388, avg_loss=0.07438]\n",
      "Step 523076  [5.459 sec/step, loss=0.07642, avg_loss=0.07440]\n",
      "Step 523077  [5.431 sec/step, loss=0.07552, avg_loss=0.07441]\n",
      "Step 523078  [5.411 sec/step, loss=0.07416, avg_loss=0.07439]\n",
      "Step 523079  [5.462 sec/step, loss=0.06630, avg_loss=0.07430]\n",
      "Step 523080  [5.455 sec/step, loss=0.07336, avg_loss=0.07428]\n",
      "Step 523081  [5.445 sec/step, loss=0.07541, avg_loss=0.07426]\n",
      "Step 523082  [5.467 sec/step, loss=0.07601, avg_loss=0.07428]\n",
      "Step 523083  [5.436 sec/step, loss=0.07435, avg_loss=0.07428]\n",
      "Step 523084  [5.431 sec/step, loss=0.06613, avg_loss=0.07423]\n",
      "Step 523085  [5.425 sec/step, loss=0.07321, avg_loss=0.07421]\n",
      "Step 523086  [5.439 sec/step, loss=0.07572, avg_loss=0.07422]\n",
      "Step 523087  [5.418 sec/step, loss=0.07450, avg_loss=0.07419]\n",
      "Step 523088  [5.412 sec/step, loss=0.07428, avg_loss=0.07417]\n",
      "Step 523089  [5.402 sec/step, loss=0.07469, avg_loss=0.07417]\n",
      "Step 523090  [5.405 sec/step, loss=0.07520, avg_loss=0.07417]\n",
      "Step 523091  [5.416 sec/step, loss=0.07594, avg_loss=0.07417]\n",
      "Step 523092  [5.411 sec/step, loss=0.07378, avg_loss=0.07419]\n",
      "Step 523093  [5.417 sec/step, loss=0.07503, avg_loss=0.07418]\n",
      "Step 523094  [5.413 sec/step, loss=0.07645, avg_loss=0.07417]\n",
      "Step 523095  [5.426 sec/step, loss=0.07499, avg_loss=0.07420]\n",
      "Step 523096  [5.455 sec/step, loss=0.07697, avg_loss=0.07430]\n",
      "Step 523097  [5.431 sec/step, loss=0.07304, avg_loss=0.07427]\n",
      "Step 523098  [5.436 sec/step, loss=0.07702, avg_loss=0.07428]\n",
      "Generated 32 batches of size 32 in 2.364 sec\n",
      "Step 523099  [5.399 sec/step, loss=0.07697, avg_loss=0.07438]\n",
      "Step 523100  [5.416 sec/step, loss=0.07525, avg_loss=0.07441]\n",
      "Writing summary at step: 523100\n",
      "Step 523101  [5.417 sec/step, loss=0.07562, avg_loss=0.07441]\n",
      "Step 523102  [5.424 sec/step, loss=0.07468, avg_loss=0.07440]\n",
      "Step 523103  [5.420 sec/step, loss=0.07563, avg_loss=0.07440]\n",
      "Step 523104  [5.437 sec/step, loss=0.07274, avg_loss=0.07438]\n",
      "Step 523105  [5.423 sec/step, loss=0.07639, avg_loss=0.07441]\n",
      "Step 523106  [5.400 sec/step, loss=0.07230, avg_loss=0.07439]\n",
      "Step 523107  [5.417 sec/step, loss=0.07604, avg_loss=0.07441]\n",
      "Step 523108  [5.405 sec/step, loss=0.07406, avg_loss=0.07438]\n",
      "Step 523109  [5.397 sec/step, loss=0.07242, avg_loss=0.07438]\n",
      "Step 523110  [5.411 sec/step, loss=0.07647, avg_loss=0.07440]\n",
      "Step 523111  [5.433 sec/step, loss=0.07672, avg_loss=0.07442]\n",
      "Step 523112  [5.491 sec/step, loss=0.06629, avg_loss=0.07434]\n",
      "Step 523113  [5.452 sec/step, loss=0.07381, avg_loss=0.07441]\n",
      "Step 523114  [5.455 sec/step, loss=0.07495, avg_loss=0.07439]\n",
      "Step 523115  [5.452 sec/step, loss=0.07599, avg_loss=0.07439]\n",
      "Step 523116  [5.455 sec/step, loss=0.07597, avg_loss=0.07441]\n",
      "Step 523117  [5.452 sec/step, loss=0.07405, avg_loss=0.07441]\n",
      "Step 523118  [5.436 sec/step, loss=0.07429, avg_loss=0.07438]\n",
      "Step 523119  [5.449 sec/step, loss=0.07386, avg_loss=0.07435]\n",
      "Step 523120  [5.460 sec/step, loss=0.07482, avg_loss=0.07435]\n",
      "Step 523121  [5.435 sec/step, loss=0.06554, avg_loss=0.07424]\n",
      "Step 523122  [5.424 sec/step, loss=0.07510, avg_loss=0.07422]\n",
      "Step 523123  [5.394 sec/step, loss=0.07157, avg_loss=0.07418]\n",
      "Step 523124  [5.380 sec/step, loss=0.07180, avg_loss=0.07414]\n",
      "Step 523125  [5.400 sec/step, loss=0.07676, avg_loss=0.07419]\n",
      "Step 523126  [5.398 sec/step, loss=0.07554, avg_loss=0.07420]\n",
      "Step 523127  [5.409 sec/step, loss=0.07437, avg_loss=0.07418]\n",
      "Step 523128  [5.395 sec/step, loss=0.07573, avg_loss=0.07417]\n",
      "Step 523129  [5.390 sec/step, loss=0.07516, avg_loss=0.07418]\n",
      "Generated 32 batches of size 32 in 2.432 sec\n",
      "Step 523130  [5.400 sec/step, loss=0.07531, avg_loss=0.07417]\n",
      "Step 523131  [5.424 sec/step, loss=0.07569, avg_loss=0.07426]\n",
      "Step 523132  [5.435 sec/step, loss=0.07628, avg_loss=0.07427]\n",
      "Step 523133  [5.429 sec/step, loss=0.07495, avg_loss=0.07425]\n",
      "Step 523134  [5.445 sec/step, loss=0.07659, avg_loss=0.07431]\n",
      "Step 523135  [5.443 sec/step, loss=0.07352, avg_loss=0.07429]\n",
      "Step 523136  [5.433 sec/step, loss=0.07093, avg_loss=0.07425]\n",
      "Step 523137  [5.418 sec/step, loss=0.07487, avg_loss=0.07426]\n",
      "Step 523138  [5.408 sec/step, loss=0.07384, avg_loss=0.07424]\n",
      "Step 523139  [5.407 sec/step, loss=0.07554, avg_loss=0.07426]\n",
      "Step 523140  [5.407 sec/step, loss=0.07483, avg_loss=0.07425]\n",
      "Step 523141  [5.406 sec/step, loss=0.07322, avg_loss=0.07425]\n",
      "Step 523142  [5.417 sec/step, loss=0.07496, avg_loss=0.07428]\n",
      "Step 523143  [5.425 sec/step, loss=0.07661, avg_loss=0.07429]\n",
      "Step 523144  [5.434 sec/step, loss=0.07603, avg_loss=0.07428]\n",
      "Step 523145  [5.435 sec/step, loss=0.07297, avg_loss=0.07426]\n",
      "Step 523146  [5.420 sec/step, loss=0.07183, avg_loss=0.07422]\n",
      "Step 523147  [5.426 sec/step, loss=0.07463, avg_loss=0.07422]\n",
      "Step 523148  [5.423 sec/step, loss=0.07042, avg_loss=0.07416]\n",
      "Step 523149  [5.416 sec/step, loss=0.07276, avg_loss=0.07415]\n",
      "Step 523150  [5.439 sec/step, loss=0.07520, avg_loss=0.07423]\n",
      "Step 523151  [5.422 sec/step, loss=0.06711, avg_loss=0.07415]\n",
      "Step 523152  [5.429 sec/step, loss=0.07478, avg_loss=0.07416]\n",
      "Step 523153  [5.426 sec/step, loss=0.07496, avg_loss=0.07415]\n",
      "Step 523154  [5.433 sec/step, loss=0.07570, avg_loss=0.07413]\n",
      "Step 523155  [5.380 sec/step, loss=0.07650, avg_loss=0.07422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 523156  [5.379 sec/step, loss=0.07442, avg_loss=0.07424]\n",
      "Step 523157  [5.426 sec/step, loss=0.06726, avg_loss=0.07419]\n",
      "Step 523158  [5.430 sec/step, loss=0.07651, avg_loss=0.07423]\n",
      "Step 523159  [5.409 sec/step, loss=0.07514, avg_loss=0.07424]\n",
      "Step 523160  [5.425 sec/step, loss=0.07640, avg_loss=0.07429]\n",
      "Step 523161  [5.413 sec/step, loss=0.07645, avg_loss=0.07431]\n",
      "Generated 32 batches of size 32 in 2.409 sec\n",
      "Step 523162  [5.412 sec/step, loss=0.07601, avg_loss=0.07431]\n",
      "Step 523163  [5.415 sec/step, loss=0.07618, avg_loss=0.07435]\n",
      "Step 523164  [5.402 sec/step, loss=0.07145, avg_loss=0.07430]\n",
      "Step 523165  [5.403 sec/step, loss=0.07611, avg_loss=0.07431]\n",
      "Step 523166  [5.418 sec/step, loss=0.07249, avg_loss=0.07432]\n",
      "Step 523167  [5.420 sec/step, loss=0.07671, avg_loss=0.07433]\n",
      "Step 523168  [5.435 sec/step, loss=0.07598, avg_loss=0.07434]\n",
      "Step 523169  [5.427 sec/step, loss=0.07460, avg_loss=0.07432]\n",
      "Step 523170  [5.414 sec/step, loss=0.07443, avg_loss=0.07433]\n",
      "Step 523171  [5.407 sec/step, loss=0.07489, avg_loss=0.07432]\n",
      "Step 523172  [5.418 sec/step, loss=0.07533, avg_loss=0.07434]\n",
      "Step 523173  [5.393 sec/step, loss=0.07437, avg_loss=0.07431]\n",
      "Step 523174  [5.393 sec/step, loss=0.07585, avg_loss=0.07434]\n",
      "Step 523175  [5.408 sec/step, loss=0.07599, avg_loss=0.07436]\n",
      "Step 523176  [5.398 sec/step, loss=0.07618, avg_loss=0.07436]\n",
      "Step 523177  [5.397 sec/step, loss=0.07528, avg_loss=0.07435]\n",
      "Step 523178  [5.389 sec/step, loss=0.06564, avg_loss=0.07427]\n",
      "Step 523179  [5.337 sec/step, loss=0.07358, avg_loss=0.07434]\n",
      "Step 523180  [5.345 sec/step, loss=0.07623, avg_loss=0.07437]\n",
      "Step 523181  [5.369 sec/step, loss=0.07343, avg_loss=0.07435]\n",
      "Step 523182  [5.347 sec/step, loss=0.07445, avg_loss=0.07434]\n",
      "Step 523183  [5.362 sec/step, loss=0.07397, avg_loss=0.07433]\n",
      "Step 523184  [5.427 sec/step, loss=0.06662, avg_loss=0.07434]\n",
      "Step 523185  [5.448 sec/step, loss=0.07356, avg_loss=0.07434]\n",
      "Step 523186  [5.446 sec/step, loss=0.07510, avg_loss=0.07433]\n",
      "Step 523187  [5.449 sec/step, loss=0.07549, avg_loss=0.07434]\n",
      "Step 523188  [5.451 sec/step, loss=0.07637, avg_loss=0.07436]\n",
      "Step 523189  [5.456 sec/step, loss=0.07204, avg_loss=0.07434]\n",
      "Step 523190  [5.473 sec/step, loss=0.07698, avg_loss=0.07436]\n",
      "Step 523191  [5.448 sec/step, loss=0.07412, avg_loss=0.07434]\n",
      "Step 523192  [5.464 sec/step, loss=0.07659, avg_loss=0.07437]\n",
      "Step 523193  [5.466 sec/step, loss=0.07609, avg_loss=0.07438]\n",
      "Generated 32 batches of size 32 in 2.400 sec\n",
      "Step 523194  [5.481 sec/step, loss=0.07457, avg_loss=0.07436]\n",
      "Step 523195  [5.487 sec/step, loss=0.07447, avg_loss=0.07435]\n",
      "Step 523196  [5.477 sec/step, loss=0.07441, avg_loss=0.07433]\n",
      "Step 523197  [5.489 sec/step, loss=0.07127, avg_loss=0.07431]\n",
      "Step 523198  [5.467 sec/step, loss=0.07192, avg_loss=0.07426]\n",
      "Step 523199  [5.443 sec/step, loss=0.07188, avg_loss=0.07421]\n",
      "Step 523200  [5.426 sec/step, loss=0.07389, avg_loss=0.07419]\n",
      "Writing summary at step: 523200\n",
      "Step 523201  [5.426 sec/step, loss=0.07500, avg_loss=0.07419]\n",
      "Step 523202  [5.419 sec/step, loss=0.07297, avg_loss=0.07417]\n",
      "Step 523203  [5.407 sec/step, loss=0.07440, avg_loss=0.07416]\n",
      "Step 523204  [5.383 sec/step, loss=0.07193, avg_loss=0.07415]\n",
      "Step 523205  [5.372 sec/step, loss=0.07525, avg_loss=0.07414]\n",
      "Step 523206  [5.409 sec/step, loss=0.07348, avg_loss=0.07415]\n",
      "Step 523207  [5.385 sec/step, loss=0.07379, avg_loss=0.07413]\n",
      "Step 523208  [5.373 sec/step, loss=0.07154, avg_loss=0.07410]\n",
      "Step 523209  [5.386 sec/step, loss=0.07419, avg_loss=0.07412]\n",
      "Step 523210  [5.366 sec/step, loss=0.07094, avg_loss=0.07407]\n",
      "Step 523211  [5.348 sec/step, loss=0.07145, avg_loss=0.07401]\n",
      "Step 523212  [5.281 sec/step, loss=0.06628, avg_loss=0.07401]\n",
      "Step 523213  [5.269 sec/step, loss=0.07521, avg_loss=0.07403]\n",
      "Step 523214  [5.308 sec/step, loss=0.06714, avg_loss=0.07395]\n",
      "Step 523215  [5.308 sec/step, loss=0.07521, avg_loss=0.07394]\n",
      "Step 523216  [5.313 sec/step, loss=0.07666, avg_loss=0.07395]\n",
      "Step 523217  [5.330 sec/step, loss=0.07663, avg_loss=0.07397]\n",
      "Step 523218  [5.338 sec/step, loss=0.07619, avg_loss=0.07399]\n",
      "Step 523219  [5.308 sec/step, loss=0.07546, avg_loss=0.07401]\n",
      "Step 523220  [5.316 sec/step, loss=0.07505, avg_loss=0.07401]\n",
      "Step 523221  [5.342 sec/step, loss=0.07385, avg_loss=0.07409]\n",
      "Step 523222  [5.346 sec/step, loss=0.07484, avg_loss=0.07409]\n",
      "Step 523223  [5.361 sec/step, loss=0.07484, avg_loss=0.07412]\n",
      "Step 523224  [5.374 sec/step, loss=0.07427, avg_loss=0.07415]\n",
      "Generated 32 batches of size 32 in 2.349 sec\n",
      "Step 523225  [5.375 sec/step, loss=0.07502, avg_loss=0.07413]\n",
      "Step 523226  [5.369 sec/step, loss=0.07411, avg_loss=0.07412]\n",
      "Step 523227  [5.375 sec/step, loss=0.07277, avg_loss=0.07410]\n",
      "Step 523228  [5.394 sec/step, loss=0.07596, avg_loss=0.07410]\n",
      "Step 523229  [5.401 sec/step, loss=0.07592, avg_loss=0.07411]\n",
      "Step 523230  [5.396 sec/step, loss=0.07549, avg_loss=0.07411]\n",
      "Step 523231  [5.404 sec/step, loss=0.07549, avg_loss=0.07411]\n",
      "Step 523232  [5.391 sec/step, loss=0.07379, avg_loss=0.07409]\n",
      "Step 523233  [5.392 sec/step, loss=0.07746, avg_loss=0.07411]\n",
      "Step 523234  [5.367 sec/step, loss=0.07153, avg_loss=0.07406]\n",
      "Step 523235  [5.362 sec/step, loss=0.07325, avg_loss=0.07406]\n",
      "Step 523236  [5.357 sec/step, loss=0.07397, avg_loss=0.07409]\n",
      "Step 523237  [5.417 sec/step, loss=0.06621, avg_loss=0.07400]\n",
      "Step 523238  [5.426 sec/step, loss=0.07488, avg_loss=0.07401]\n",
      "Step 523239  [5.430 sec/step, loss=0.07596, avg_loss=0.07402]\n",
      "Step 523240  [5.447 sec/step, loss=0.07676, avg_loss=0.07403]\n",
      "Step 523241  [5.433 sec/step, loss=0.07406, avg_loss=0.07404]\n",
      "Step 523242  [5.441 sec/step, loss=0.07549, avg_loss=0.07405]\n",
      "Step 523243  [5.439 sec/step, loss=0.07623, avg_loss=0.07404]\n",
      "Step 523244  [5.423 sec/step, loss=0.07497, avg_loss=0.07403]\n",
      "Step 523245  [5.408 sec/step, loss=0.07235, avg_loss=0.07403]\n",
      "Step 523246  [5.426 sec/step, loss=0.07482, avg_loss=0.07406]\n",
      "Step 523247  [5.408 sec/step, loss=0.07313, avg_loss=0.07404]\n",
      "Step 523248  [5.413 sec/step, loss=0.07487, avg_loss=0.07409]\n",
      "Step 523249  [5.424 sec/step, loss=0.07540, avg_loss=0.07411]\n",
      "Step 523250  [5.418 sec/step, loss=0.07570, avg_loss=0.07412]\n",
      "Step 523251  [5.433 sec/step, loss=0.07290, avg_loss=0.07418]\n",
      "Step 523252  [5.418 sec/step, loss=0.06645, avg_loss=0.07409]\n",
      "Step 523253  [5.428 sec/step, loss=0.07492, avg_loss=0.07409]\n",
      "Step 523254  [5.425 sec/step, loss=0.07340, avg_loss=0.07407]\n",
      "Step 523255  [5.423 sec/step, loss=0.07579, avg_loss=0.07406]\n",
      "Step 523256  [5.441 sec/step, loss=0.07662, avg_loss=0.07408]\n",
      "Generated 32 batches of size 32 in 2.411 sec\n",
      "Step 523257  [5.419 sec/step, loss=0.07589, avg_loss=0.07417]\n",
      "Step 523258  [5.421 sec/step, loss=0.07629, avg_loss=0.07417]\n",
      "Step 523259  [5.431 sec/step, loss=0.07591, avg_loss=0.07418]\n",
      "Step 523260  [5.426 sec/step, loss=0.07363, avg_loss=0.07415]\n",
      "Step 523261  [5.424 sec/step, loss=0.07485, avg_loss=0.07413]\n",
      "Step 523262  [5.433 sec/step, loss=0.07653, avg_loss=0.07414]\n",
      "Step 523263  [5.421 sec/step, loss=0.07447, avg_loss=0.07412]\n",
      "Step 523264  [5.427 sec/step, loss=0.07258, avg_loss=0.07413]\n",
      "Step 523265  [5.412 sec/step, loss=0.07485, avg_loss=0.07412]\n",
      "Step 523266  [5.408 sec/step, loss=0.07412, avg_loss=0.07414]\n",
      "Step 523267  [5.405 sec/step, loss=0.07293, avg_loss=0.07410]\n",
      "Step 523268  [5.412 sec/step, loss=0.07601, avg_loss=0.07410]\n",
      "Step 523269  [5.408 sec/step, loss=0.07503, avg_loss=0.07410]\n",
      "Step 523270  [5.407 sec/step, loss=0.07135, avg_loss=0.07407]\n",
      "Step 523271  [5.408 sec/step, loss=0.07559, avg_loss=0.07408]\n",
      "Step 523272  [5.417 sec/step, loss=0.07568, avg_loss=0.07408]\n",
      "Step 523273  [5.436 sec/step, loss=0.07595, avg_loss=0.07410]\n",
      "Step 523274  [5.447 sec/step, loss=0.07684, avg_loss=0.07411]\n",
      "Step 523275  [5.431 sec/step, loss=0.07296, avg_loss=0.07408]\n",
      "Step 523276  [5.430 sec/step, loss=0.07424, avg_loss=0.07406]\n",
      "Step 523277  [5.442 sec/step, loss=0.07700, avg_loss=0.07408]\n",
      "Step 523278  [5.463 sec/step, loss=0.07537, avg_loss=0.07417]\n",
      "Step 523279  [5.517 sec/step, loss=0.06754, avg_loss=0.07411]\n",
      "Step 523280  [5.501 sec/step, loss=0.07433, avg_loss=0.07409]\n",
      "Step 523281  [5.490 sec/step, loss=0.07552, avg_loss=0.07411]\n",
      "Step 523282  [5.500 sec/step, loss=0.07613, avg_loss=0.07413]\n",
      "Step 523283  [5.476 sec/step, loss=0.07158, avg_loss=0.07411]\n",
      "Step 523284  [5.435 sec/step, loss=0.07576, avg_loss=0.07420]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 523285  [5.416 sec/step, loss=0.07469, avg_loss=0.07421]\n",
      "Step 523286  [5.419 sec/step, loss=0.07472, avg_loss=0.07421]\n",
      "Step 523287  [5.433 sec/step, loss=0.07408, avg_loss=0.07419]\n",
      "Step 523288  [5.430 sec/step, loss=0.07633, avg_loss=0.07419]\n",
      "Generated 32 batches of size 32 in 2.556 sec\n",
      "Step 523289  [5.436 sec/step, loss=0.07357, avg_loss=0.07421]\n",
      "Step 523290  [5.419 sec/step, loss=0.07569, avg_loss=0.07419]\n",
      "Step 523291  [5.420 sec/step, loss=0.07357, avg_loss=0.07419]\n",
      "Step 523292  [5.395 sec/step, loss=0.07182, avg_loss=0.07414]\n",
      "Step 523293  [5.415 sec/step, loss=0.07341, avg_loss=0.07411]\n",
      "Step 523294  [5.381 sec/step, loss=0.06576, avg_loss=0.07403]\n",
      "Step 523295  [5.370 sec/step, loss=0.07398, avg_loss=0.07402]\n",
      "Step 523296  [5.381 sec/step, loss=0.07723, avg_loss=0.07405]\n",
      "Step 523297  [5.379 sec/step, loss=0.07559, avg_loss=0.07409]\n",
      "Step 523298  [5.399 sec/step, loss=0.07563, avg_loss=0.07413]\n",
      "Step 523299  [5.412 sec/step, loss=0.07551, avg_loss=0.07417]\n",
      "Step 523300  [5.408 sec/step, loss=0.07392, avg_loss=0.07417]\n",
      "Writing summary at step: 523300\n",
      "Step 523301  [5.393 sec/step, loss=0.07418, avg_loss=0.07416]\n",
      "Step 523302  [5.390 sec/step, loss=0.07419, avg_loss=0.07417]\n",
      "Step 523303  [5.404 sec/step, loss=0.07465, avg_loss=0.07417]\n",
      "Step 523304  [5.403 sec/step, loss=0.07581, avg_loss=0.07421]\n",
      "Step 523305  [5.407 sec/step, loss=0.07192, avg_loss=0.07418]\n",
      "Step 523306  [5.377 sec/step, loss=0.07591, avg_loss=0.07420]\n",
      "Step 523307  [5.368 sec/step, loss=0.06724, avg_loss=0.07414]\n",
      "Step 523308  [5.432 sec/step, loss=0.06628, avg_loss=0.07408]\n",
      "Step 523309  [5.418 sec/step, loss=0.07294, avg_loss=0.07407]\n",
      "Step 523310  [5.442 sec/step, loss=0.07674, avg_loss=0.07413]\n",
      "Step 523311  [5.459 sec/step, loss=0.07645, avg_loss=0.07418]\n",
      "Step 523312  [5.473 sec/step, loss=0.07122, avg_loss=0.07423]\n",
      "Step 523313  [5.485 sec/step, loss=0.07403, avg_loss=0.07422]\n",
      "Step 523314  [5.432 sec/step, loss=0.07561, avg_loss=0.07430]\n",
      "Step 523315  [5.443 sec/step, loss=0.07413, avg_loss=0.07429]\n",
      "Step 523316  [5.445 sec/step, loss=0.07638, avg_loss=0.07429]\n",
      "Step 523317  [5.433 sec/step, loss=0.07520, avg_loss=0.07427]\n",
      "Step 523318  [5.441 sec/step, loss=0.07691, avg_loss=0.07428]\n",
      "Step 523319  [5.445 sec/step, loss=0.07590, avg_loss=0.07429]\n",
      "Generated 32 batches of size 32 in 2.380 sec\n",
      "Step 523320  [5.445 sec/step, loss=0.07465, avg_loss=0.07428]\n",
      "Step 523321  [5.435 sec/step, loss=0.07396, avg_loss=0.07428]\n",
      "Step 523322  [5.421 sec/step, loss=0.07148, avg_loss=0.07425]\n",
      "Step 523323  [5.434 sec/step, loss=0.07552, avg_loss=0.07426]\n",
      "Step 523324  [5.440 sec/step, loss=0.07374, avg_loss=0.07425]\n",
      "Step 523325  [5.442 sec/step, loss=0.07359, avg_loss=0.07424]\n",
      "Step 523326  [5.476 sec/step, loss=0.07286, avg_loss=0.07422]\n",
      "Step 523327  [5.469 sec/step, loss=0.07619, avg_loss=0.07426]\n",
      "Step 523328  [5.451 sec/step, loss=0.07400, avg_loss=0.07424]\n",
      "Step 523329  [5.450 sec/step, loss=0.07473, avg_loss=0.07423]\n",
      "Step 523330  [5.456 sec/step, loss=0.07569, avg_loss=0.07423]\n",
      "Step 523331  [5.442 sec/step, loss=0.07424, avg_loss=0.07422]\n",
      "Step 523332  [5.454 sec/step, loss=0.07620, avg_loss=0.07424]\n",
      "Step 523333  [5.446 sec/step, loss=0.07363, avg_loss=0.07420]\n",
      "Step 523334  [5.460 sec/step, loss=0.07552, avg_loss=0.07424]\n",
      "Step 523335  [5.453 sec/step, loss=0.07445, avg_loss=0.07425]\n",
      "Step 523336  [5.453 sec/step, loss=0.07417, avg_loss=0.07426]\n",
      "Step 523337  [5.394 sec/step, loss=0.07068, avg_loss=0.07430]\n",
      "Step 523338  [5.401 sec/step, loss=0.07646, avg_loss=0.07432]\n",
      "Step 523339  [5.396 sec/step, loss=0.07191, avg_loss=0.07428]\n",
      "Step 523340  [5.395 sec/step, loss=0.07638, avg_loss=0.07427]\n",
      "Step 523341  [5.385 sec/step, loss=0.07543, avg_loss=0.07429]\n",
      "Step 523342  [5.378 sec/step, loss=0.07532, avg_loss=0.07428]\n",
      "Step 523343  [5.373 sec/step, loss=0.07486, avg_loss=0.07427]\n",
      "Step 523344  [5.361 sec/step, loss=0.07150, avg_loss=0.07424]\n",
      "Step 523345  [5.387 sec/step, loss=0.07694, avg_loss=0.07428]\n",
      "Step 523346  [5.376 sec/step, loss=0.07543, avg_loss=0.07429]\n",
      "Step 523347  [5.384 sec/step, loss=0.07562, avg_loss=0.07431]\n",
      "Step 523348  [5.398 sec/step, loss=0.07643, avg_loss=0.07433]\n",
      "Step 523349  [5.382 sec/step, loss=0.06665, avg_loss=0.07424]\n",
      "Step 523350  [5.368 sec/step, loss=0.07285, avg_loss=0.07421]\n",
      "Step 523351  [5.394 sec/step, loss=0.07590, avg_loss=0.07424]\n",
      "Generated 32 batches of size 32 in 2.433 sec\n",
      "Step 523352  [5.424 sec/step, loss=0.07536, avg_loss=0.07433]\n",
      "Step 523353  [5.433 sec/step, loss=0.07521, avg_loss=0.07433]\n",
      "Step 523354  [5.474 sec/step, loss=0.06681, avg_loss=0.07427]\n",
      "Step 523355  [5.484 sec/step, loss=0.07456, avg_loss=0.07426]\n",
      "Step 523356  [5.468 sec/step, loss=0.07332, avg_loss=0.07422]\n",
      "Step 523357  [5.451 sec/step, loss=0.07419, avg_loss=0.07421]\n",
      "Step 523358  [5.441 sec/step, loss=0.07505, avg_loss=0.07419]\n",
      "Step 523359  [5.423 sec/step, loss=0.07404, avg_loss=0.07418]\n",
      "Step 523360  [5.426 sec/step, loss=0.07534, avg_loss=0.07419]\n",
      "Step 523361  [5.420 sec/step, loss=0.07385, avg_loss=0.07418]\n",
      "Step 523362  [5.413 sec/step, loss=0.07621, avg_loss=0.07418]\n",
      "Step 523363  [5.427 sec/step, loss=0.07507, avg_loss=0.07419]\n",
      "Step 523364  [5.437 sec/step, loss=0.07400, avg_loss=0.07420]\n",
      "Step 523365  [5.453 sec/step, loss=0.07554, avg_loss=0.07421]\n",
      "Step 523366  [5.453 sec/step, loss=0.07523, avg_loss=0.07422]\n",
      "Step 523367  [5.474 sec/step, loss=0.07335, avg_loss=0.07422]\n",
      "Step 523368  [5.461 sec/step, loss=0.07419, avg_loss=0.07420]\n",
      "Step 523369  [5.456 sec/step, loss=0.07107, avg_loss=0.07416]\n",
      "Step 523370  [5.511 sec/step, loss=0.06669, avg_loss=0.07412]\n",
      "Step 523371  [5.517 sec/step, loss=0.07635, avg_loss=0.07413]\n",
      "Step 523372  [5.492 sec/step, loss=0.07399, avg_loss=0.07411]\n",
      "Step 523373  [5.481 sec/step, loss=0.07394, avg_loss=0.07409]\n",
      "Step 523374  [5.462 sec/step, loss=0.07257, avg_loss=0.07405]\n",
      "Step 523375  [5.453 sec/step, loss=0.07113, avg_loss=0.07403]\n",
      "Step 523376  [5.443 sec/step, loss=0.07103, avg_loss=0.07400]\n",
      "Step 523377  [5.445 sec/step, loss=0.07638, avg_loss=0.07399]\n",
      "Step 523378  [5.442 sec/step, loss=0.07495, avg_loss=0.07398]\n",
      "Step 523379  [5.393 sec/step, loss=0.07528, avg_loss=0.07406]\n",
      "Step 523380  [5.422 sec/step, loss=0.07330, avg_loss=0.07405]\n",
      "Step 523381  [5.419 sec/step, loss=0.07544, avg_loss=0.07405]\n",
      "Step 523382  [5.413 sec/step, loss=0.07271, avg_loss=0.07402]\n",
      "Step 523383  [5.435 sec/step, loss=0.07652, avg_loss=0.07407]\n",
      "Generated 32 batches of size 32 in 2.565 sec\n",
      "Step 523384  [5.427 sec/step, loss=0.07564, avg_loss=0.07407]\n",
      "Step 523385  [5.425 sec/step, loss=0.07578, avg_loss=0.07408]\n",
      "Step 523386  [5.424 sec/step, loss=0.07321, avg_loss=0.07406]\n",
      "Step 523387  [5.402 sec/step, loss=0.07478, avg_loss=0.07407]\n",
      "Step 523388  [5.404 sec/step, loss=0.07423, avg_loss=0.07405]\n",
      "Step 523389  [5.413 sec/step, loss=0.07272, avg_loss=0.07404]\n",
      "Step 523390  [5.420 sec/step, loss=0.07571, avg_loss=0.07404]\n",
      "Step 523391  [5.429 sec/step, loss=0.07663, avg_loss=0.07407]\n",
      "Step 523392  [5.424 sec/step, loss=0.06600, avg_loss=0.07401]\n",
      "Step 523393  [5.403 sec/step, loss=0.07611, avg_loss=0.07404]\n",
      "Step 523394  [5.416 sec/step, loss=0.07537, avg_loss=0.07413]\n",
      "Step 523395  [5.435 sec/step, loss=0.07436, avg_loss=0.07414]\n",
      "Step 523396  [5.416 sec/step, loss=0.07410, avg_loss=0.07411]\n",
      "Step 523397  [5.419 sec/step, loss=0.07264, avg_loss=0.07408]\n",
      "Step 523398  [5.433 sec/step, loss=0.07373, avg_loss=0.07406]\n",
      "Step 523399  [5.432 sec/step, loss=0.07516, avg_loss=0.07405]\n",
      "Step 523400  [5.445 sec/step, loss=0.07445, avg_loss=0.07406]\n",
      "Writing summary at step: 523400\n",
      "Step 523401  [5.450 sec/step, loss=0.07382, avg_loss=0.07406]\n",
      "Step 523402  [5.447 sec/step, loss=0.07444, avg_loss=0.07406]\n",
      "Step 523403  [5.440 sec/step, loss=0.07350, avg_loss=0.07405]\n",
      "Step 523404  [5.448 sec/step, loss=0.07610, avg_loss=0.07405]\n",
      "Step 523405  [5.460 sec/step, loss=0.07667, avg_loss=0.07410]\n",
      "Step 523406  [5.468 sec/step, loss=0.07622, avg_loss=0.07410]\n",
      "Step 523407  [5.495 sec/step, loss=0.07657, avg_loss=0.07419]\n",
      "Step 523408  [5.434 sec/step, loss=0.07099, avg_loss=0.07424]\n",
      "Step 523409  [5.438 sec/step, loss=0.07524, avg_loss=0.07426]\n",
      "Step 523410  [5.417 sec/step, loss=0.07463, avg_loss=0.07424]\n",
      "Step 523411  [5.411 sec/step, loss=0.07625, avg_loss=0.07424]\n",
      "Step 523412  [5.414 sec/step, loss=0.07581, avg_loss=0.07429]\n",
      "Step 523413  [5.412 sec/step, loss=0.07651, avg_loss=0.07431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 523414  [5.464 sec/step, loss=0.06655, avg_loss=0.07422]\n",
      "Generated 32 batches of size 32 in 2.527 sec\n",
      "Step 523415  [5.456 sec/step, loss=0.07403, avg_loss=0.07422]\n",
      "Step 523416  [5.452 sec/step, loss=0.07310, avg_loss=0.07419]\n",
      "Step 523417  [5.452 sec/step, loss=0.07192, avg_loss=0.07415]\n",
      "Step 523418  [5.470 sec/step, loss=0.07336, avg_loss=0.07412]\n",
      "Step 523419  [5.481 sec/step, loss=0.07429, avg_loss=0.07410]\n",
      "Step 523420  [5.471 sec/step, loss=0.07561, avg_loss=0.07411]\n",
      "Step 523421  [5.485 sec/step, loss=0.07648, avg_loss=0.07414]\n",
      "Step 523422  [5.482 sec/step, loss=0.07110, avg_loss=0.07413]\n",
      "Step 523423  [5.446 sec/step, loss=0.06551, avg_loss=0.07403]\n",
      "Step 523424  [5.432 sec/step, loss=0.07122, avg_loss=0.07401]\n",
      "Step 523425  [5.426 sec/step, loss=0.07488, avg_loss=0.07402]\n",
      "Step 523426  [5.414 sec/step, loss=0.07355, avg_loss=0.07403]\n",
      "Step 523427  [5.411 sec/step, loss=0.07645, avg_loss=0.07403]\n",
      "Step 523428  [5.400 sec/step, loss=0.07264, avg_loss=0.07402]\n",
      "Step 523429  [5.402 sec/step, loss=0.07746, avg_loss=0.07404]\n",
      "Step 523430  [5.391 sec/step, loss=0.07429, avg_loss=0.07403]\n",
      "Step 523431  [5.402 sec/step, loss=0.07685, avg_loss=0.07406]\n",
      "Step 523432  [5.397 sec/step, loss=0.07557, avg_loss=0.07405]\n",
      "Step 523433  [5.398 sec/step, loss=0.07545, avg_loss=0.07407]\n",
      "Step 523434  [5.421 sec/step, loss=0.07374, avg_loss=0.07405]\n",
      "Step 523435  [5.422 sec/step, loss=0.07422, avg_loss=0.07405]\n",
      "Step 523436  [5.448 sec/step, loss=0.07441, avg_loss=0.07405]\n",
      "Step 523437  [5.468 sec/step, loss=0.07421, avg_loss=0.07409]\n",
      "Step 523438  [5.507 sec/step, loss=0.06723, avg_loss=0.07399]\n",
      "Step 523439  [5.514 sec/step, loss=0.07290, avg_loss=0.07400]\n",
      "Step 523440  [5.500 sec/step, loss=0.07427, avg_loss=0.07398]\n",
      "Step 523441  [5.505 sec/step, loss=0.07624, avg_loss=0.07399]\n",
      "Step 523442  [5.509 sec/step, loss=0.07292, avg_loss=0.07397]\n",
      "Step 523443  [5.506 sec/step, loss=0.07500, avg_loss=0.07397]\n",
      "Step 523444  [5.530 sec/step, loss=0.07403, avg_loss=0.07399]\n",
      "Step 523445  [5.520 sec/step, loss=0.07560, avg_loss=0.07398]\n",
      "Step 523446  [5.507 sec/step, loss=0.06698, avg_loss=0.07390]\n",
      "Generated 32 batches of size 32 in 2.525 sec\n",
      "Step 523447  [5.504 sec/step, loss=0.07431, avg_loss=0.07388]\n",
      "Step 523448  [5.498 sec/step, loss=0.07651, avg_loss=0.07388]\n",
      "Step 523449  [5.525 sec/step, loss=0.07605, avg_loss=0.07398]\n",
      "Step 523450  [5.528 sec/step, loss=0.07203, avg_loss=0.07397]\n",
      "Step 523451  [5.503 sec/step, loss=0.07561, avg_loss=0.07397]\n",
      "Step 523452  [5.486 sec/step, loss=0.07450, avg_loss=0.07396]\n",
      "Step 523453  [5.474 sec/step, loss=0.07282, avg_loss=0.07393]\n",
      "Step 523454  [5.421 sec/step, loss=0.07399, avg_loss=0.07401]\n",
      "Step 523455  [5.415 sec/step, loss=0.07177, avg_loss=0.07398]\n",
      "Step 523456  [5.420 sec/step, loss=0.07412, avg_loss=0.07399]\n",
      "Step 523457  [5.409 sec/step, loss=0.07510, avg_loss=0.07399]\n",
      "Step 523458  [5.404 sec/step, loss=0.07376, avg_loss=0.07398]\n",
      "Step 523459  [5.413 sec/step, loss=0.07447, avg_loss=0.07399]\n",
      "Step 523460  [5.403 sec/step, loss=0.07445, avg_loss=0.07398]\n",
      "Step 523461  [5.429 sec/step, loss=0.07367, avg_loss=0.07398]\n",
      "Step 523462  [5.409 sec/step, loss=0.07350, avg_loss=0.07395]\n",
      "Step 523463  [5.401 sec/step, loss=0.07530, avg_loss=0.07395]\n",
      "Step 523464  [5.394 sec/step, loss=0.07515, avg_loss=0.07396]\n",
      "Step 523465  [5.387 sec/step, loss=0.07618, avg_loss=0.07397]\n",
      "Step 523466  [5.392 sec/step, loss=0.07625, avg_loss=0.07398]\n",
      "Step 523467  [5.380 sec/step, loss=0.07540, avg_loss=0.07400]\n",
      "Step 523468  [5.373 sec/step, loss=0.07393, avg_loss=0.07400]\n",
      "Step 523469  [5.386 sec/step, loss=0.07262, avg_loss=0.07401]\n",
      "Step 523470  [5.354 sec/step, loss=0.07512, avg_loss=0.07410]\n",
      "Step 523471  [5.361 sec/step, loss=0.07567, avg_loss=0.07409]\n",
      "Step 523472  [5.372 sec/step, loss=0.07512, avg_loss=0.07410]\n",
      "Step 523473  [5.371 sec/step, loss=0.07171, avg_loss=0.07408]\n",
      "Step 523474  [5.377 sec/step, loss=0.07093, avg_loss=0.07406]\n",
      "Step 523475  [5.393 sec/step, loss=0.07690, avg_loss=0.07412]\n",
      "Step 523476  [5.407 sec/step, loss=0.07445, avg_loss=0.07415]\n",
      "Step 523477  [5.386 sec/step, loss=0.07117, avg_loss=0.07410]\n",
      "Step 523478  [5.382 sec/step, loss=0.07548, avg_loss=0.07411]\n",
      "Generated 32 batches of size 32 in 2.486 sec\n",
      "Step 523479  [5.385 sec/step, loss=0.07343, avg_loss=0.07409]\n",
      "Step 523480  [5.378 sec/step, loss=0.07635, avg_loss=0.07412]\n",
      "Step 523481  [5.365 sec/step, loss=0.07466, avg_loss=0.07411]\n",
      "Step 523482  [5.380 sec/step, loss=0.07427, avg_loss=0.07413]\n",
      "Step 523483  [5.354 sec/step, loss=0.06573, avg_loss=0.07402]\n",
      "Step 523484  [5.339 sec/step, loss=0.07259, avg_loss=0.07399]\n",
      "Step 523485  [5.346 sec/step, loss=0.07501, avg_loss=0.07398]\n",
      "Step 523486  [5.354 sec/step, loss=0.07650, avg_loss=0.07401]\n",
      "Step 523487  [5.415 sec/step, loss=0.06684, avg_loss=0.07393]\n",
      "Step 523488  [5.415 sec/step, loss=0.07613, avg_loss=0.07395]\n",
      "Step 523489  [5.388 sec/step, loss=0.06617, avg_loss=0.07389]\n",
      "Step 523490  [5.392 sec/step, loss=0.07560, avg_loss=0.07389]\n",
      "Step 523491  [5.414 sec/step, loss=0.07287, avg_loss=0.07385]\n",
      "Step 523492  [5.431 sec/step, loss=0.07266, avg_loss=0.07392]\n",
      "Step 523493  [5.421 sec/step, loss=0.07501, avg_loss=0.07390]\n",
      "Step 523494  [5.434 sec/step, loss=0.07526, avg_loss=0.07390]\n",
      "Step 523495  [5.430 sec/step, loss=0.07685, avg_loss=0.07393]\n",
      "Step 523496  [5.442 sec/step, loss=0.07386, avg_loss=0.07393]\n",
      "Step 523497  [5.434 sec/step, loss=0.07447, avg_loss=0.07394]\n",
      "Step 523498  [5.416 sec/step, loss=0.07633, avg_loss=0.07397]\n",
      "Step 523499  [5.415 sec/step, loss=0.07514, avg_loss=0.07397]\n",
      "Step 523500  [5.408 sec/step, loss=0.07178, avg_loss=0.07394]\n",
      "Writing summary at step: 523500\n",
      "Step 523501  [5.404 sec/step, loss=0.07372, avg_loss=0.07394]\n",
      "Step 523502  [5.431 sec/step, loss=0.07571, avg_loss=0.07395]\n",
      "Step 523503  [5.431 sec/step, loss=0.07549, avg_loss=0.07397]\n",
      "Step 523504  [5.426 sec/step, loss=0.07508, avg_loss=0.07396]\n",
      "Step 523505  [5.425 sec/step, loss=0.07627, avg_loss=0.07396]\n",
      "Step 523506  [5.417 sec/step, loss=0.07268, avg_loss=0.07393]\n",
      "Step 523507  [5.406 sec/step, loss=0.07515, avg_loss=0.07391]\n",
      "Step 523508  [5.402 sec/step, loss=0.07173, avg_loss=0.07392]\n",
      "Step 523509  [5.416 sec/step, loss=0.07453, avg_loss=0.07391]\n",
      "Generated 32 batches of size 32 in 2.378 sec\n",
      "Step 523510  [5.437 sec/step, loss=0.07367, avg_loss=0.07390]\n",
      "Step 523511  [5.431 sec/step, loss=0.07493, avg_loss=0.07389]\n",
      "Step 523512  [5.482 sec/step, loss=0.06623, avg_loss=0.07379]\n",
      "Step 523513  [5.487 sec/step, loss=0.07613, avg_loss=0.07379]\n",
      "Step 523514  [5.428 sec/step, loss=0.07124, avg_loss=0.07384]\n",
      "Step 523515  [5.413 sec/step, loss=0.07460, avg_loss=0.07384]\n",
      "Step 523516  [5.409 sec/step, loss=0.07379, avg_loss=0.07385]\n",
      "Step 523517  [5.407 sec/step, loss=0.07376, avg_loss=0.07387]\n",
      "Step 523518  [5.390 sec/step, loss=0.07694, avg_loss=0.07390]\n",
      "Step 523519  [5.392 sec/step, loss=0.07552, avg_loss=0.07391]\n",
      "Step 523520  [5.397 sec/step, loss=0.07479, avg_loss=0.07391]\n",
      "Step 523521  [5.390 sec/step, loss=0.07520, avg_loss=0.07389]\n",
      "Step 523522  [5.397 sec/step, loss=0.07459, avg_loss=0.07393]\n",
      "Step 523523  [5.404 sec/step, loss=0.07404, avg_loss=0.07401]\n",
      "Step 523524  [5.411 sec/step, loss=0.07437, avg_loss=0.07405]\n",
      "Step 523525  [5.415 sec/step, loss=0.07475, avg_loss=0.07404]\n",
      "Step 523526  [5.404 sec/step, loss=0.07537, avg_loss=0.07406]\n",
      "Step 523527  [5.422 sec/step, loss=0.07497, avg_loss=0.07405]\n",
      "Step 523528  [5.438 sec/step, loss=0.07622, avg_loss=0.07408]\n",
      "Step 523529  [5.438 sec/step, loss=0.07634, avg_loss=0.07407]\n",
      "Step 523530  [5.425 sec/step, loss=0.07261, avg_loss=0.07406]\n",
      "Step 523531  [5.424 sec/step, loss=0.07661, avg_loss=0.07405]\n",
      "Step 523532  [5.417 sec/step, loss=0.07344, avg_loss=0.07403]\n",
      "Step 523533  [5.424 sec/step, loss=0.07586, avg_loss=0.07404]\n",
      "Step 523534  [5.400 sec/step, loss=0.07502, avg_loss=0.07405]\n",
      "Step 523535  [5.402 sec/step, loss=0.07302, avg_loss=0.07404]\n",
      "Step 523536  [5.375 sec/step, loss=0.07328, avg_loss=0.07403]\n",
      "Step 523537  [5.365 sec/step, loss=0.07504, avg_loss=0.07403]\n",
      "Step 523538  [5.306 sec/step, loss=0.07184, avg_loss=0.07408]\n",
      "Step 523539  [5.300 sec/step, loss=0.07601, avg_loss=0.07411]\n",
      "Step 523540  [5.313 sec/step, loss=0.07674, avg_loss=0.07414]\n",
      "Step 523541  [5.316 sec/step, loss=0.07577, avg_loss=0.07413]\n",
      "Generated 32 batches of size 32 in 2.934 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 523542  [5.301 sec/step, loss=0.06605, avg_loss=0.07406]\n",
      "Step 523543  [5.300 sec/step, loss=0.07239, avg_loss=0.07404]\n",
      "Step 523544  [5.304 sec/step, loss=0.07581, avg_loss=0.07405]\n",
      "Step 523545  [5.297 sec/step, loss=0.07539, avg_loss=0.07405]\n",
      "Step 523546  [5.363 sec/step, loss=0.06635, avg_loss=0.07405]\n",
      "Step 523547  [5.364 sec/step, loss=0.07184, avg_loss=0.07402]\n",
      "Step 523548  [5.371 sec/step, loss=0.07541, avg_loss=0.07401]\n",
      "Step 523549  [5.367 sec/step, loss=0.07480, avg_loss=0.07400]\n",
      "Step 523550  [5.378 sec/step, loss=0.07524, avg_loss=0.07403]\n",
      "Step 523551  [5.388 sec/step, loss=0.07629, avg_loss=0.07404]\n",
      "Step 523552  [5.386 sec/step, loss=0.07377, avg_loss=0.07403]\n",
      "Step 523553  [5.400 sec/step, loss=0.07326, avg_loss=0.07403]\n",
      "Step 523554  [5.406 sec/step, loss=0.07295, avg_loss=0.07402]\n",
      "Step 523555  [5.396 sec/step, loss=0.07445, avg_loss=0.07405]\n",
      "Step 523556  [5.395 sec/step, loss=0.07254, avg_loss=0.07403]\n",
      "Step 523557  [5.400 sec/step, loss=0.07590, avg_loss=0.07404]\n",
      "Step 523558  [5.418 sec/step, loss=0.07693, avg_loss=0.07407]\n",
      "Step 523559  [5.424 sec/step, loss=0.07613, avg_loss=0.07409]\n",
      "Step 523560  [5.444 sec/step, loss=0.07643, avg_loss=0.07411]\n",
      "Step 523561  [5.416 sec/step, loss=0.07520, avg_loss=0.07413]\n",
      "Step 523562  [5.423 sec/step, loss=0.07415, avg_loss=0.07413]\n",
      "Step 523563  [5.431 sec/step, loss=0.07478, avg_loss=0.07413]\n",
      "Step 523564  [5.442 sec/step, loss=0.07585, avg_loss=0.07413]\n",
      "Step 523565  [5.438 sec/step, loss=0.07572, avg_loss=0.07413]\n",
      "Step 523566  [5.418 sec/step, loss=0.07153, avg_loss=0.07408]\n",
      "Step 523567  [5.409 sec/step, loss=0.07198, avg_loss=0.07405]\n",
      "Step 523568  [5.444 sec/step, loss=0.07350, avg_loss=0.07404]\n",
      "Step 523569  [5.430 sec/step, loss=0.07594, avg_loss=0.07408]\n",
      "Step 523570  [5.408 sec/step, loss=0.07158, avg_loss=0.07404]\n",
      "Step 523571  [5.445 sec/step, loss=0.06635, avg_loss=0.07395]\n",
      "Step 523572  [5.425 sec/step, loss=0.06633, avg_loss=0.07386]\n",
      "Step 523573  [5.431 sec/step, loss=0.07561, avg_loss=0.07390]\n",
      "Generated 32 batches of size 32 in 2.528 sec\n",
      "Step 523574  [5.430 sec/step, loss=0.07449, avg_loss=0.07393]\n",
      "Step 523575  [5.430 sec/step, loss=0.07574, avg_loss=0.07392]\n",
      "Step 523576  [5.415 sec/step, loss=0.07087, avg_loss=0.07389]\n",
      "Step 523577  [5.421 sec/step, loss=0.07253, avg_loss=0.07390]\n",
      "Step 523578  [5.432 sec/step, loss=0.07655, avg_loss=0.07391]\n",
      "Step 523579  [5.440 sec/step, loss=0.07451, avg_loss=0.07392]\n",
      "Step 523580  [5.422 sec/step, loss=0.07373, avg_loss=0.07390]\n",
      "Step 523581  [5.429 sec/step, loss=0.07632, avg_loss=0.07391]\n",
      "Step 523582  [5.417 sec/step, loss=0.07473, avg_loss=0.07392]\n",
      "Step 523583  [5.443 sec/step, loss=0.07613, avg_loss=0.07402]\n",
      "Step 523584  [5.461 sec/step, loss=0.07571, avg_loss=0.07405]\n",
      "Step 523585  [5.468 sec/step, loss=0.07611, avg_loss=0.07406]\n",
      "Step 523586  [5.456 sec/step, loss=0.07461, avg_loss=0.07404]\n",
      "Step 523587  [5.414 sec/step, loss=0.07417, avg_loss=0.07412]\n",
      "Step 523588  [5.407 sec/step, loss=0.07556, avg_loss=0.07411]\n",
      "Step 523589  [5.430 sec/step, loss=0.07414, avg_loss=0.07419]\n",
      "Step 523590  [5.430 sec/step, loss=0.07635, avg_loss=0.07420]\n",
      "Step 523591  [5.433 sec/step, loss=0.07376, avg_loss=0.07421]\n",
      "Step 523592  [5.435 sec/step, loss=0.07532, avg_loss=0.07424]\n",
      "Step 523593  [5.427 sec/step, loss=0.07433, avg_loss=0.07423]\n",
      "Step 523594  [5.418 sec/step, loss=0.07549, avg_loss=0.07423]\n",
      "Step 523595  [5.407 sec/step, loss=0.07547, avg_loss=0.07422]\n",
      "Step 523596  [5.401 sec/step, loss=0.07550, avg_loss=0.07423]\n",
      "Step 523597  [5.394 sec/step, loss=0.07248, avg_loss=0.07421]\n",
      "Step 523598  [5.392 sec/step, loss=0.07475, avg_loss=0.07420]\n",
      "Step 523599  [5.440 sec/step, loss=0.06669, avg_loss=0.07411]\n",
      "Step 523600  [5.436 sec/step, loss=0.07404, avg_loss=0.07414]\n",
      "Writing summary at step: 523600\n",
      "Step 523601  [5.441 sec/step, loss=0.07220, avg_loss=0.07412]\n",
      "Step 523602  [5.435 sec/step, loss=0.07606, avg_loss=0.07412]\n",
      "Step 523603  [5.452 sec/step, loss=0.07646, avg_loss=0.07413]\n",
      "Step 523604  [5.437 sec/step, loss=0.07146, avg_loss=0.07410]\n",
      "Generated 32 batches of size 32 in 2.390 sec\n",
      "Step 523605  [5.447 sec/step, loss=0.07548, avg_loss=0.07409]\n",
      "Step 523606  [5.466 sec/step, loss=0.07607, avg_loss=0.07412]\n",
      "Step 523607  [5.459 sec/step, loss=0.07415, avg_loss=0.07411]\n",
      "Step 523608  [5.479 sec/step, loss=0.07448, avg_loss=0.07414]\n",
      "Step 523609  [5.476 sec/step, loss=0.07652, avg_loss=0.07416]\n",
      "Step 523610  [5.468 sec/step, loss=0.07584, avg_loss=0.07418]\n",
      "Step 523611  [5.467 sec/step, loss=0.07432, avg_loss=0.07418]\n",
      "Step 523612  [5.413 sec/step, loss=0.07428, avg_loss=0.07426]\n",
      "Step 523613  [5.394 sec/step, loss=0.07333, avg_loss=0.07423]\n",
      "Step 523614  [5.418 sec/step, loss=0.07548, avg_loss=0.07427]\n",
      "Step 523615  [5.418 sec/step, loss=0.07433, avg_loss=0.07427]\n",
      "Step 523616  [5.414 sec/step, loss=0.07393, avg_loss=0.07427]\n",
      "Step 523617  [5.405 sec/step, loss=0.07152, avg_loss=0.07425]\n",
      "Step 523618  [5.395 sec/step, loss=0.07465, avg_loss=0.07422]\n",
      "Step 523619  [5.396 sec/step, loss=0.07579, avg_loss=0.07423]\n",
      "Step 523620  [5.418 sec/step, loss=0.07254, avg_loss=0.07420]\n",
      "Step 523621  [5.410 sec/step, loss=0.07558, avg_loss=0.07421]\n",
      "Step 523622  [5.427 sec/step, loss=0.07412, avg_loss=0.07420]\n",
      "Step 523623  [5.434 sec/step, loss=0.07583, avg_loss=0.07422]\n",
      "Step 523624  [5.429 sec/step, loss=0.07466, avg_loss=0.07422]\n",
      "Step 523625  [5.427 sec/step, loss=0.07639, avg_loss=0.07424]\n",
      "Step 523626  [5.432 sec/step, loss=0.07385, avg_loss=0.07423]\n",
      "Step 523627  [5.399 sec/step, loss=0.07383, avg_loss=0.07421]\n",
      "Step 523628  [5.394 sec/step, loss=0.07334, avg_loss=0.07419]\n",
      "Step 523629  [5.383 sec/step, loss=0.07601, avg_loss=0.07418]\n",
      "Step 523630  [5.398 sec/step, loss=0.07569, avg_loss=0.07421]\n",
      "Step 523631  [5.384 sec/step, loss=0.07448, avg_loss=0.07419]\n",
      "Step 523632  [5.389 sec/step, loss=0.07207, avg_loss=0.07418]\n",
      "Step 523633  [5.385 sec/step, loss=0.07539, avg_loss=0.07417]\n",
      "Step 523634  [5.389 sec/step, loss=0.07560, avg_loss=0.07418]\n",
      "Step 523635  [5.395 sec/step, loss=0.07613, avg_loss=0.07421]\n",
      "Step 523636  [5.452 sec/step, loss=0.06740, avg_loss=0.07415]\n",
      "Generated 32 batches of size 32 in 2.407 sec\n",
      "Step 523637  [5.459 sec/step, loss=0.07468, avg_loss=0.07415]\n",
      "Step 523638  [5.469 sec/step, loss=0.07461, avg_loss=0.07418]\n",
      "Step 523639  [5.481 sec/step, loss=0.07667, avg_loss=0.07418]\n",
      "Step 523640  [5.464 sec/step, loss=0.07151, avg_loss=0.07413]\n",
      "Step 523641  [5.472 sec/step, loss=0.07504, avg_loss=0.07412]\n",
      "Step 523642  [5.492 sec/step, loss=0.07641, avg_loss=0.07423]\n",
      "Step 523643  [5.503 sec/step, loss=0.07645, avg_loss=0.07427]\n",
      "Step 523644  [5.469 sec/step, loss=0.06663, avg_loss=0.07418]\n",
      "Step 523645  [5.483 sec/step, loss=0.07612, avg_loss=0.07418]\n",
      "Step 523646  [5.434 sec/step, loss=0.07601, avg_loss=0.07428]\n",
      "Step 523647  [5.426 sec/step, loss=0.07390, avg_loss=0.07430]\n",
      "Step 523648  [5.396 sec/step, loss=0.07358, avg_loss=0.07428]\n",
      "Step 523649  [5.418 sec/step, loss=0.07442, avg_loss=0.07428]\n",
      "Step 523650  [5.408 sec/step, loss=0.07157, avg_loss=0.07424]\n",
      "Step 523651  [5.411 sec/step, loss=0.07528, avg_loss=0.07423]\n",
      "Step 523652  [5.420 sec/step, loss=0.07535, avg_loss=0.07425]\n",
      "Step 523653  [5.450 sec/step, loss=0.06649, avg_loss=0.07418]\n",
      "Step 523654  [5.449 sec/step, loss=0.07573, avg_loss=0.07421]\n",
      "Step 523655  [5.480 sec/step, loss=0.07436, avg_loss=0.07421]\n",
      "Step 523656  [5.478 sec/step, loss=0.07607, avg_loss=0.07424]\n",
      "Step 523657  [5.456 sec/step, loss=0.06737, avg_loss=0.07416]\n",
      "Step 523658  [5.435 sec/step, loss=0.07454, avg_loss=0.07413]\n",
      "Step 523659  [5.419 sec/step, loss=0.07570, avg_loss=0.07413]\n",
      "Step 523660  [5.417 sec/step, loss=0.07613, avg_loss=0.07412]\n",
      "Step 523661  [5.420 sec/step, loss=0.07604, avg_loss=0.07413]\n",
      "Step 523662  [5.421 sec/step, loss=0.07512, avg_loss=0.07414]\n",
      "Step 523663  [5.431 sec/step, loss=0.07685, avg_loss=0.07416]\n",
      "Step 523664  [5.418 sec/step, loss=0.07267, avg_loss=0.07413]\n",
      "Step 523665  [5.423 sec/step, loss=0.07661, avg_loss=0.07414]\n",
      "Step 523666  [5.448 sec/step, loss=0.07684, avg_loss=0.07419]\n",
      "Step 523667  [5.457 sec/step, loss=0.07689, avg_loss=0.07424]\n",
      "Step 523668  [5.437 sec/step, loss=0.07694, avg_loss=0.07428]\n",
      "Generated 32 batches of size 32 in 2.422 sec\n",
      "Step 523669  [5.459 sec/step, loss=0.07775, avg_loss=0.07430]\n",
      "Step 523670  [5.468 sec/step, loss=0.07573, avg_loss=0.07434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 523671  [5.418 sec/step, loss=0.07395, avg_loss=0.07441]\n",
      "Step 523672  [5.428 sec/step, loss=0.07431, avg_loss=0.07449]\n",
      "Step 523673  [5.425 sec/step, loss=0.07514, avg_loss=0.07449]\n",
      "Step 523674  [5.433 sec/step, loss=0.07509, avg_loss=0.07449]\n",
      "Step 523675  [5.428 sec/step, loss=0.07400, avg_loss=0.07448]\n",
      "Step 523676  [5.434 sec/step, loss=0.07256, avg_loss=0.07449]\n",
      "Step 523677  [5.450 sec/step, loss=0.07504, avg_loss=0.07452]\n",
      "Step 523678  [5.445 sec/step, loss=0.07466, avg_loss=0.07450]\n",
      "Step 523679  [5.428 sec/step, loss=0.07384, avg_loss=0.07449]\n",
      "Step 523680  [5.421 sec/step, loss=0.07156, avg_loss=0.07447]\n",
      "Step 523681  [5.418 sec/step, loss=0.07649, avg_loss=0.07447]\n",
      "Step 523682  [5.469 sec/step, loss=0.06646, avg_loss=0.07439]\n",
      "Step 523683  [5.488 sec/step, loss=0.07392, avg_loss=0.07437]\n",
      "Step 523684  [5.474 sec/step, loss=0.07498, avg_loss=0.07436]\n",
      "Step 523685  [5.473 sec/step, loss=0.07674, avg_loss=0.07437]\n",
      "Step 523686  [5.476 sec/step, loss=0.07621, avg_loss=0.07438]\n",
      "Step 523687  [5.480 sec/step, loss=0.07440, avg_loss=0.07439]\n",
      "Step 523688  [5.467 sec/step, loss=0.06977, avg_loss=0.07433]\n",
      "Step 523689  [5.469 sec/step, loss=0.07647, avg_loss=0.07435]\n",
      "Step 523690  [5.477 sec/step, loss=0.07661, avg_loss=0.07435]\n",
      "Step 523691  [5.452 sec/step, loss=0.07581, avg_loss=0.07437]\n",
      "Step 523692  [5.450 sec/step, loss=0.07534, avg_loss=0.07437]\n",
      "Step 523693  [5.444 sec/step, loss=0.06642, avg_loss=0.07430]\n",
      "Step 523694  [5.443 sec/step, loss=0.07428, avg_loss=0.07428]\n",
      "Step 523695  [5.443 sec/step, loss=0.07637, avg_loss=0.07429]\n",
      "Step 523696  [5.441 sec/step, loss=0.07136, avg_loss=0.07425]\n",
      "Step 523697  [5.459 sec/step, loss=0.07556, avg_loss=0.07428]\n",
      "Step 523698  [5.468 sec/step, loss=0.07643, avg_loss=0.07430]\n",
      "Step 523699  [5.423 sec/step, loss=0.07559, avg_loss=0.07439]\n",
      "Step 523700  [5.438 sec/step, loss=0.07569, avg_loss=0.07440]\n",
      "Writing summary at step: 523700\n",
      "Generated 32 batches of size 32 in 2.461 sec\n",
      "Step 523701  [5.443 sec/step, loss=0.07589, avg_loss=0.07444]\n",
      "Step 523702  [5.444 sec/step, loss=0.07637, avg_loss=0.07444]\n",
      "Step 523703  [5.455 sec/step, loss=0.07462, avg_loss=0.07443]\n",
      "Step 523704  [5.477 sec/step, loss=0.07394, avg_loss=0.07445]\n",
      "Step 523705  [5.451 sec/step, loss=0.07216, avg_loss=0.07442]\n",
      "Step 523706  [5.434 sec/step, loss=0.07508, avg_loss=0.07441]\n",
      "Step 523707  [5.433 sec/step, loss=0.07448, avg_loss=0.07441]\n",
      "Step 523708  [5.427 sec/step, loss=0.07507, avg_loss=0.07442]\n",
      "Step 523709  [5.429 sec/step, loss=0.07659, avg_loss=0.07442]\n",
      "Step 523710  [5.448 sec/step, loss=0.07475, avg_loss=0.07441]\n",
      "Step 523711  [5.434 sec/step, loss=0.07161, avg_loss=0.07438]\n",
      "Step 523712  [5.430 sec/step, loss=0.07428, avg_loss=0.07438]\n",
      "Step 523713  [5.445 sec/step, loss=0.07689, avg_loss=0.07441]\n",
      "Step 523714  [5.424 sec/step, loss=0.07366, avg_loss=0.07440]\n",
      "Step 523715  [5.434 sec/step, loss=0.07482, avg_loss=0.07440]\n",
      "Step 523716  [5.453 sec/step, loss=0.07533, avg_loss=0.07442]\n",
      "Step 523717  [5.456 sec/step, loss=0.07202, avg_loss=0.07442]\n",
      "Step 523718  [5.452 sec/step, loss=0.07532, avg_loss=0.07443]\n",
      "Step 523719  [5.447 sec/step, loss=0.07650, avg_loss=0.07443]\n",
      "Step 523720  [5.425 sec/step, loss=0.07512, avg_loss=0.07446]\n",
      "Step 523721  [5.411 sec/step, loss=0.06516, avg_loss=0.07436]\n",
      "Step 523722  [5.451 sec/step, loss=0.06606, avg_loss=0.07428]\n",
      "Step 523723  [5.462 sec/step, loss=0.07630, avg_loss=0.07428]\n",
      "Step 523724  [5.480 sec/step, loss=0.07641, avg_loss=0.07430]\n",
      "Step 523725  [5.474 sec/step, loss=0.07478, avg_loss=0.07428]\n",
      "Step 523726  [5.470 sec/step, loss=0.07441, avg_loss=0.07429]\n",
      "Step 523727  [5.476 sec/step, loss=0.07275, avg_loss=0.07428]\n",
      "Step 523728  [5.468 sec/step, loss=0.07442, avg_loss=0.07429]\n",
      "Step 523729  [5.480 sec/step, loss=0.07573, avg_loss=0.07428]\n",
      "Step 523730  [5.497 sec/step, loss=0.07523, avg_loss=0.07428]\n",
      "Step 523731  [5.507 sec/step, loss=0.07511, avg_loss=0.07429]\n",
      "Generated 32 batches of size 32 in 2.372 sec\n",
      "Step 523732  [5.514 sec/step, loss=0.07532, avg_loss=0.07432]\n",
      "Step 523733  [5.512 sec/step, loss=0.07618, avg_loss=0.07433]\n",
      "Step 523734  [5.514 sec/step, loss=0.07436, avg_loss=0.07431]\n",
      "Step 523735  [5.511 sec/step, loss=0.07556, avg_loss=0.07431]\n",
      "Step 523736  [5.457 sec/step, loss=0.07393, avg_loss=0.07437]\n",
      "Step 523737  [5.458 sec/step, loss=0.07621, avg_loss=0.07439]\n",
      "Step 523738  [5.455 sec/step, loss=0.07310, avg_loss=0.07437]\n",
      "Step 523739  [5.447 sec/step, loss=0.07470, avg_loss=0.07435]\n",
      "Step 523740  [5.453 sec/step, loss=0.07378, avg_loss=0.07438]\n",
      "Step 523741  [5.436 sec/step, loss=0.07277, avg_loss=0.07435]\n",
      "Step 523742  [5.441 sec/step, loss=0.07640, avg_loss=0.07435]\n",
      "Step 523743  [5.426 sec/step, loss=0.07568, avg_loss=0.07435]\n",
      "Step 523744  [5.449 sec/step, loss=0.07566, avg_loss=0.07444]\n",
      "Step 523745  [5.438 sec/step, loss=0.07182, avg_loss=0.07439]\n",
      "Step 523746  [5.445 sec/step, loss=0.07630, avg_loss=0.07440]\n",
      "Step 523747  [5.458 sec/step, loss=0.07585, avg_loss=0.07442]\n",
      "Step 523748  [5.482 sec/step, loss=0.07526, avg_loss=0.07443]\n",
      "Step 523749  [5.460 sec/step, loss=0.07309, avg_loss=0.07442]\n",
      "Step 523750  [5.480 sec/step, loss=0.07680, avg_loss=0.07447]\n",
      "Step 523751  [5.455 sec/step, loss=0.07188, avg_loss=0.07444]\n",
      "Step 523752  [5.462 sec/step, loss=0.07416, avg_loss=0.07443]\n",
      "Step 523753  [5.406 sec/step, loss=0.07435, avg_loss=0.07450]\n",
      "Step 523754  [5.388 sec/step, loss=0.06779, avg_loss=0.07442]\n",
      "Step 523755  [5.364 sec/step, loss=0.07409, avg_loss=0.07442]\n",
      "Step 523756  [5.368 sec/step, loss=0.07561, avg_loss=0.07442]\n",
      "Step 523757  [5.387 sec/step, loss=0.07482, avg_loss=0.07449]\n",
      "Step 523758  [5.406 sec/step, loss=0.07416, avg_loss=0.07449]\n",
      "Step 523759  [5.398 sec/step, loss=0.07123, avg_loss=0.07444]\n",
      "Step 523760  [5.399 sec/step, loss=0.07644, avg_loss=0.07445]\n",
      "Step 523761  [5.423 sec/step, loss=0.07389, avg_loss=0.07443]\n",
      "Step 523762  [5.417 sec/step, loss=0.07444, avg_loss=0.07442]\n",
      "Step 523763  [5.402 sec/step, loss=0.07506, avg_loss=0.07440]\n",
      "Generated 32 batches of size 32 in 2.709 sec\n",
      "Step 523764  [5.398 sec/step, loss=0.07452, avg_loss=0.07442]\n",
      "Step 523765  [5.439 sec/step, loss=0.06677, avg_loss=0.07432]\n",
      "Step 523766  [5.447 sec/step, loss=0.07341, avg_loss=0.07429]\n",
      "Step 523767  [5.436 sec/step, loss=0.07606, avg_loss=0.07428]\n",
      "Step 523768  [5.428 sec/step, loss=0.07478, avg_loss=0.07426]\n",
      "Step 523769  [5.405 sec/step, loss=0.07433, avg_loss=0.07422]\n",
      "Step 523770  [5.406 sec/step, loss=0.07675, avg_loss=0.07423]\n",
      "Step 523771  [5.420 sec/step, loss=0.07737, avg_loss=0.07427]\n",
      "Step 523772  [5.425 sec/step, loss=0.07345, avg_loss=0.07426]\n",
      "Step 523773  [5.423 sec/step, loss=0.07605, avg_loss=0.07427]\n",
      "Step 523774  [5.424 sec/step, loss=0.07680, avg_loss=0.07428]\n",
      "Step 523775  [5.432 sec/step, loss=0.07562, avg_loss=0.07430]\n",
      "Step 523776  [5.438 sec/step, loss=0.07501, avg_loss=0.07432]\n",
      "Step 523777  [5.424 sec/step, loss=0.07536, avg_loss=0.07433]\n",
      "Step 523778  [5.420 sec/step, loss=0.07447, avg_loss=0.07433]\n",
      "Step 523779  [5.424 sec/step, loss=0.07395, avg_loss=0.07433]\n",
      "Step 523780  [5.436 sec/step, loss=0.07153, avg_loss=0.07433]\n",
      "Step 523781  [5.445 sec/step, loss=0.07454, avg_loss=0.07431]\n",
      "Step 523782  [5.409 sec/step, loss=0.07673, avg_loss=0.07441]\n",
      "Step 523783  [5.383 sec/step, loss=0.07478, avg_loss=0.07442]\n",
      "Step 523784  [5.398 sec/step, loss=0.07483, avg_loss=0.07442]\n",
      "Step 523785  [5.383 sec/step, loss=0.07389, avg_loss=0.07439]\n",
      "Step 523786  [5.366 sec/step, loss=0.06572, avg_loss=0.07428]\n",
      "Step 523787  [5.361 sec/step, loss=0.07527, avg_loss=0.07429]\n",
      "Step 523788  [5.375 sec/step, loss=0.07627, avg_loss=0.07436]\n",
      "Step 523789  [5.356 sec/step, loss=0.07496, avg_loss=0.07434]\n",
      "Step 523790  [5.336 sec/step, loss=0.07419, avg_loss=0.07432]\n",
      "Step 523791  [5.353 sec/step, loss=0.07379, avg_loss=0.07430]\n",
      "Step 523792  [5.344 sec/step, loss=0.07347, avg_loss=0.07428]\n",
      "Step 523793  [5.371 sec/step, loss=0.07681, avg_loss=0.07438]\n",
      "Step 523794  [5.362 sec/step, loss=0.07167, avg_loss=0.07436]\n",
      "Step 523795  [5.375 sec/step, loss=0.07365, avg_loss=0.07433]\n",
      "Generated 32 batches of size 32 in 2.621 sec\n",
      "Step 523796  [5.434 sec/step, loss=0.06792, avg_loss=0.07430]\n",
      "Step 523797  [5.416 sec/step, loss=0.07185, avg_loss=0.07426]\n",
      "Step 523798  [5.422 sec/step, loss=0.07689, avg_loss=0.07426]\n",
      "Step 523799  [5.424 sec/step, loss=0.07662, avg_loss=0.07427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 523800  [5.420 sec/step, loss=0.07497, avg_loss=0.07427]\n",
      "Writing summary at step: 523800\n",
      "Step 523801  [5.446 sec/step, loss=0.07375, avg_loss=0.07424]\n",
      "Step 523802  [5.434 sec/step, loss=0.07564, avg_loss=0.07424]\n",
      "Step 523803  [5.403 sec/step, loss=0.07475, avg_loss=0.07424]\n",
      "Step 523804  [5.381 sec/step, loss=0.07449, avg_loss=0.07424]\n",
      "Step 523805  [5.385 sec/step, loss=0.07423, avg_loss=0.07427]\n",
      "Step 523806  [5.403 sec/step, loss=0.07563, avg_loss=0.07427]\n",
      "Step 523807  [5.414 sec/step, loss=0.07504, avg_loss=0.07428]\n",
      "Step 523808  [5.465 sec/step, loss=0.06569, avg_loss=0.07418]\n",
      "Step 523809  [5.468 sec/step, loss=0.07587, avg_loss=0.07417]\n",
      "Step 523810  [5.452 sec/step, loss=0.07496, avg_loss=0.07418]\n",
      "Step 523811  [5.471 sec/step, loss=0.07541, avg_loss=0.07421]\n",
      "Step 523812  [5.479 sec/step, loss=0.07568, avg_loss=0.07423]\n",
      "Step 523813  [5.477 sec/step, loss=0.07478, avg_loss=0.07421]\n",
      "Step 523814  [5.494 sec/step, loss=0.07340, avg_loss=0.07421]\n",
      "Step 523815  [5.480 sec/step, loss=0.07239, avg_loss=0.07418]\n",
      "Step 523816  [5.454 sec/step, loss=0.07084, avg_loss=0.07414]\n",
      "Step 523817  [5.467 sec/step, loss=0.07564, avg_loss=0.07417]\n",
      "Step 523818  [5.472 sec/step, loss=0.07638, avg_loss=0.07418]\n",
      "Step 523819  [5.475 sec/step, loss=0.07691, avg_loss=0.07419]\n",
      "Step 523820  [5.476 sec/step, loss=0.07521, avg_loss=0.07419]\n",
      "Step 523821  [5.492 sec/step, loss=0.07460, avg_loss=0.07428]\n",
      "Step 523822  [5.435 sec/step, loss=0.07469, avg_loss=0.07437]\n",
      "Step 523823  [5.424 sec/step, loss=0.07559, avg_loss=0.07436]\n",
      "Step 523824  [5.406 sec/step, loss=0.07354, avg_loss=0.07433]\n",
      "Step 523825  [5.415 sec/step, loss=0.07650, avg_loss=0.07435]\n",
      "Step 523826  [5.407 sec/step, loss=0.07513, avg_loss=0.07436]\n",
      "Generated 32 batches of size 32 in 2.566 sec\n",
      "Step 523827  [5.409 sec/step, loss=0.07095, avg_loss=0.07434]\n",
      "Step 523828  [5.432 sec/step, loss=0.07625, avg_loss=0.07436]\n",
      "Step 523829  [5.404 sec/step, loss=0.06542, avg_loss=0.07425]\n",
      "Step 523830  [5.410 sec/step, loss=0.07507, avg_loss=0.07425]\n",
      "Step 523831  [5.401 sec/step, loss=0.07394, avg_loss=0.07424]\n",
      "Step 523832  [5.401 sec/step, loss=0.07680, avg_loss=0.07426]\n",
      "Step 523833  [5.398 sec/step, loss=0.07103, avg_loss=0.07420]\n",
      "Step 523834  [5.391 sec/step, loss=0.07511, avg_loss=0.07421]\n",
      "Step 523835  [5.406 sec/step, loss=0.07412, avg_loss=0.07420]\n",
      "Step 523836  [5.395 sec/step, loss=0.06755, avg_loss=0.07413]\n",
      "Step 523837  [5.381 sec/step, loss=0.07253, avg_loss=0.07410]\n",
      "Step 523838  [5.380 sec/step, loss=0.07500, avg_loss=0.07412]\n",
      "Step 523839  [5.378 sec/step, loss=0.07410, avg_loss=0.07411]\n",
      "Step 523840  [5.385 sec/step, loss=0.07495, avg_loss=0.07412]\n",
      "Step 523841  [5.410 sec/step, loss=0.07267, avg_loss=0.07412]\n",
      "Step 523842  [5.404 sec/step, loss=0.07634, avg_loss=0.07412]\n",
      "Step 523843  [5.404 sec/step, loss=0.07555, avg_loss=0.07412]\n",
      "Step 523844  [5.403 sec/step, loss=0.07523, avg_loss=0.07411]\n",
      "Step 523845  [5.455 sec/step, loss=0.06664, avg_loss=0.07406]\n",
      "Step 523846  [5.464 sec/step, loss=0.07614, avg_loss=0.07406]\n",
      "Step 523847  [5.455 sec/step, loss=0.07405, avg_loss=0.07404]\n",
      "Step 523848  [5.438 sec/step, loss=0.07465, avg_loss=0.07404]\n",
      "Step 523849  [5.424 sec/step, loss=0.07444, avg_loss=0.07405]\n",
      "Step 523850  [5.409 sec/step, loss=0.07403, avg_loss=0.07402]\n",
      "Step 523851  [5.434 sec/step, loss=0.07393, avg_loss=0.07404]\n",
      "Step 523852  [5.434 sec/step, loss=0.07612, avg_loss=0.07406]\n",
      "Step 523853  [5.427 sec/step, loss=0.07186, avg_loss=0.07404]\n",
      "Step 523854  [5.439 sec/step, loss=0.07365, avg_loss=0.07410]\n",
      "Step 523855  [5.442 sec/step, loss=0.07432, avg_loss=0.07410]\n",
      "Step 523856  [5.448 sec/step, loss=0.07553, avg_loss=0.07410]\n",
      "Step 523857  [5.452 sec/step, loss=0.07580, avg_loss=0.07411]\n",
      "Step 523858  [5.451 sec/step, loss=0.07652, avg_loss=0.07413]\n",
      "Generated 32 batches of size 32 in 2.446 sec\n",
      "Step 523859  [5.469 sec/step, loss=0.07582, avg_loss=0.07418]\n",
      "Step 523860  [5.475 sec/step, loss=0.07546, avg_loss=0.07417]\n",
      "Step 523861  [5.458 sec/step, loss=0.07570, avg_loss=0.07419]\n",
      "Step 523862  [5.467 sec/step, loss=0.07617, avg_loss=0.07420]\n",
      "Step 523863  [5.471 sec/step, loss=0.07519, avg_loss=0.07420]\n",
      "Step 523864  [5.468 sec/step, loss=0.07261, avg_loss=0.07419]\n",
      "Step 523865  [5.418 sec/step, loss=0.07416, avg_loss=0.07426]\n",
      "Step 523866  [5.403 sec/step, loss=0.07552, avg_loss=0.07428]\n",
      "Step 523867  [5.415 sec/step, loss=0.07615, avg_loss=0.07428]\n",
      "Step 523868  [5.423 sec/step, loss=0.07649, avg_loss=0.07430]\n",
      "Step 523869  [5.445 sec/step, loss=0.07612, avg_loss=0.07432]\n",
      "Step 523870  [5.438 sec/step, loss=0.07510, avg_loss=0.07430]\n",
      "Step 523871  [5.424 sec/step, loss=0.07537, avg_loss=0.07428]\n",
      "Step 523872  [5.408 sec/step, loss=0.06620, avg_loss=0.07421]\n",
      "Step 523873  [5.423 sec/step, loss=0.07686, avg_loss=0.07422]\n",
      "Step 523874  [5.419 sec/step, loss=0.07677, avg_loss=0.07422]\n",
      "Step 523875  [5.429 sec/step, loss=0.07756, avg_loss=0.07423]\n",
      "Step 523876  [5.438 sec/step, loss=0.07691, avg_loss=0.07425]\n",
      "Step 523877  [5.440 sec/step, loss=0.07548, avg_loss=0.07425]\n",
      "Step 523878  [5.442 sec/step, loss=0.07515, avg_loss=0.07426]\n",
      "Step 523879  [5.447 sec/step, loss=0.07477, avg_loss=0.07427]\n",
      "Step 523880  [5.451 sec/step, loss=0.07559, avg_loss=0.07431]\n",
      "Step 523881  [5.435 sec/step, loss=0.07455, avg_loss=0.07431]\n",
      "Step 523882  [5.416 sec/step, loss=0.07346, avg_loss=0.07428]\n",
      "Step 523883  [5.426 sec/step, loss=0.07685, avg_loss=0.07430]\n",
      "Step 523884  [5.426 sec/step, loss=0.07624, avg_loss=0.07431]\n",
      "Step 523885  [5.416 sec/step, loss=0.07259, avg_loss=0.07430]\n",
      "Step 523886  [5.425 sec/step, loss=0.07472, avg_loss=0.07439]\n",
      "Step 523887  [5.406 sec/step, loss=0.07089, avg_loss=0.07435]\n",
      "Step 523888  [5.401 sec/step, loss=0.07301, avg_loss=0.07431]\n",
      "Step 523889  [5.419 sec/step, loss=0.07657, avg_loss=0.07433]\n",
      "Step 523890  [5.432 sec/step, loss=0.07554, avg_loss=0.07434]\n",
      "Generated 32 batches of size 32 in 2.361 sec\n",
      "Step 523891  [5.421 sec/step, loss=0.07555, avg_loss=0.07436]\n",
      "Step 523892  [5.422 sec/step, loss=0.07420, avg_loss=0.07437]\n",
      "Step 523893  [5.438 sec/step, loss=0.07442, avg_loss=0.07434]\n",
      "Step 523894  [5.446 sec/step, loss=0.07618, avg_loss=0.07439]\n",
      "Step 523895  [5.443 sec/step, loss=0.07566, avg_loss=0.07441]\n",
      "Step 523896  [5.398 sec/step, loss=0.07718, avg_loss=0.07450]\n",
      "Step 523897  [5.402 sec/step, loss=0.07586, avg_loss=0.07454]\n",
      "Step 523898  [5.386 sec/step, loss=0.07660, avg_loss=0.07454]\n",
      "Step 523899  [5.429 sec/step, loss=0.06805, avg_loss=0.07445]\n",
      "Step 523900  [5.430 sec/step, loss=0.07701, avg_loss=0.07447]\n",
      "Writing summary at step: 523900\n",
      "Step 523901  [5.406 sec/step, loss=0.07307, avg_loss=0.07447]\n",
      "Step 523902  [5.403 sec/step, loss=0.07611, avg_loss=0.07447]\n",
      "Step 523903  [5.424 sec/step, loss=0.07456, avg_loss=0.07447]\n",
      "Step 523904  [5.444 sec/step, loss=0.07684, avg_loss=0.07449]\n",
      "Step 523905  [5.434 sec/step, loss=0.07552, avg_loss=0.07451]\n",
      "Step 523906  [5.416 sec/step, loss=0.07308, avg_loss=0.07448]\n",
      "Step 523907  [5.421 sec/step, loss=0.07748, avg_loss=0.07450]\n",
      "Step 523908  [5.359 sec/step, loss=0.07183, avg_loss=0.07457]\n",
      "Step 523909  [5.356 sec/step, loss=0.07432, avg_loss=0.07455]\n",
      "Step 523910  [5.351 sec/step, loss=0.07579, avg_loss=0.07456]\n",
      "Step 523911  [5.341 sec/step, loss=0.07525, avg_loss=0.07456]\n",
      "Step 523912  [5.341 sec/step, loss=0.07582, avg_loss=0.07456]\n",
      "Step 523913  [5.325 sec/step, loss=0.07412, avg_loss=0.07455]\n",
      "Step 523914  [5.301 sec/step, loss=0.07264, avg_loss=0.07454]\n",
      "Step 523915  [5.310 sec/step, loss=0.07417, avg_loss=0.07456]\n",
      "Step 523916  [5.325 sec/step, loss=0.07563, avg_loss=0.07461]\n",
      "Step 523917  [5.338 sec/step, loss=0.07633, avg_loss=0.07462]\n",
      "Step 523918  [5.333 sec/step, loss=0.07429, avg_loss=0.07460]\n",
      "Step 523919  [5.330 sec/step, loss=0.07699, avg_loss=0.07460]\n",
      "Step 523920  [5.327 sec/step, loss=0.07328, avg_loss=0.07458]\n",
      "Step 523921  [5.337 sec/step, loss=0.07715, avg_loss=0.07460]\n",
      "Generated 32 batches of size 32 in 2.388 sec\n",
      "Step 523922  [5.348 sec/step, loss=0.07437, avg_loss=0.07460]\n",
      "Step 523923  [5.334 sec/step, loss=0.06579, avg_loss=0.07450]\n",
      "Step 523924  [5.391 sec/step, loss=0.06755, avg_loss=0.07444]\n",
      "Step 523925  [5.382 sec/step, loss=0.07583, avg_loss=0.07444]\n",
      "Step 523926  [5.384 sec/step, loss=0.07619, avg_loss=0.07445]\n",
      "Step 523927  [5.396 sec/step, loss=0.07689, avg_loss=0.07451]\n",
      "Step 523928  [5.411 sec/step, loss=0.07400, avg_loss=0.07448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 523929  [5.435 sec/step, loss=0.07569, avg_loss=0.07459]\n",
      "Step 523930  [5.427 sec/step, loss=0.07612, avg_loss=0.07460]\n",
      "Step 523931  [5.432 sec/step, loss=0.07633, avg_loss=0.07462]\n",
      "Step 523932  [5.435 sec/step, loss=0.07647, avg_loss=0.07462]\n",
      "Step 523933  [5.444 sec/step, loss=0.07640, avg_loss=0.07467]\n",
      "Step 523934  [5.451 sec/step, loss=0.07602, avg_loss=0.07468]\n",
      "Step 523935  [5.426 sec/step, loss=0.07256, avg_loss=0.07466]\n",
      "Step 523936  [5.455 sec/step, loss=0.07443, avg_loss=0.07473]\n",
      "Step 523937  [5.461 sec/step, loss=0.07552, avg_loss=0.07476]\n",
      "Step 523938  [5.465 sec/step, loss=0.07462, avg_loss=0.07476]\n",
      "Step 523939  [5.475 sec/step, loss=0.07515, avg_loss=0.07477]\n",
      "Step 523940  [5.461 sec/step, loss=0.07506, avg_loss=0.07477]\n",
      "Step 523941  [5.461 sec/step, loss=0.07379, avg_loss=0.07478]\n",
      "Step 523942  [5.441 sec/step, loss=0.07234, avg_loss=0.07474]\n",
      "Step 523943  [5.428 sec/step, loss=0.06756, avg_loss=0.07466]\n",
      "Step 523944  [5.419 sec/step, loss=0.07583, avg_loss=0.07467]\n",
      "Step 523945  [5.382 sec/step, loss=0.07708, avg_loss=0.07477]\n",
      "Step 523946  [5.384 sec/step, loss=0.07608, avg_loss=0.07477]\n",
      "Step 523947  [5.383 sec/step, loss=0.07435, avg_loss=0.07477]\n",
      "Step 523948  [5.395 sec/step, loss=0.07533, avg_loss=0.07478]\n",
      "Step 523949  [5.411 sec/step, loss=0.07462, avg_loss=0.07478]\n",
      "Step 523950  [5.430 sec/step, loss=0.07656, avg_loss=0.07481]\n",
      "Step 523951  [5.420 sec/step, loss=0.07523, avg_loss=0.07482]\n",
      "Step 523952  [5.404 sec/step, loss=0.07217, avg_loss=0.07478]\n",
      "Step 523953  [5.410 sec/step, loss=0.07414, avg_loss=0.07480]\n",
      "Generated 32 batches of size 32 in 2.400 sec\n",
      "Step 523954  [5.428 sec/step, loss=0.07667, avg_loss=0.07483]\n",
      "Step 523955  [5.427 sec/step, loss=0.07490, avg_loss=0.07484]\n",
      "Step 523956  [5.417 sec/step, loss=0.07267, avg_loss=0.07481]\n",
      "Step 523957  [5.407 sec/step, loss=0.07586, avg_loss=0.07481]\n",
      "Step 523958  [5.390 sec/step, loss=0.07450, avg_loss=0.07479]\n",
      "Step 523959  [5.394 sec/step, loss=0.07438, avg_loss=0.07478]\n",
      "Step 523960  [5.380 sec/step, loss=0.07611, avg_loss=0.07478]\n",
      "Step 523961  [5.422 sec/step, loss=0.06662, avg_loss=0.07469]\n",
      "Step 523962  [5.425 sec/step, loss=0.07537, avg_loss=0.07469]\n",
      "Step 523963  [5.433 sec/step, loss=0.07544, avg_loss=0.07469]\n",
      "Step 523964  [5.440 sec/step, loss=0.07546, avg_loss=0.07472]\n",
      "Step 523965  [5.435 sec/step, loss=0.07244, avg_loss=0.07470]\n",
      "Step 523966  [5.440 sec/step, loss=0.07396, avg_loss=0.07468]\n",
      "Step 523967  [5.412 sec/step, loss=0.07109, avg_loss=0.07463]\n",
      "Step 523968  [5.414 sec/step, loss=0.07620, avg_loss=0.07463]\n",
      "Step 523969  [5.402 sec/step, loss=0.07644, avg_loss=0.07463]\n",
      "Step 523970  [5.395 sec/step, loss=0.07441, avg_loss=0.07463]\n",
      "Step 523971  [5.418 sec/step, loss=0.07394, avg_loss=0.07461]\n",
      "Step 523972  [5.441 sec/step, loss=0.07515, avg_loss=0.07470]\n",
      "Step 523973  [5.447 sec/step, loss=0.07401, avg_loss=0.07467]\n",
      "Step 523974  [5.460 sec/step, loss=0.07609, avg_loss=0.07467]\n",
      "Step 523975  [5.450 sec/step, loss=0.07223, avg_loss=0.07461]\n",
      "Step 523976  [5.433 sec/step, loss=0.07394, avg_loss=0.07458]\n",
      "Step 523977  [5.423 sec/step, loss=0.07457, avg_loss=0.07457]\n",
      "Step 523978  [5.430 sec/step, loss=0.07624, avg_loss=0.07459]\n",
      "Step 523979  [5.426 sec/step, loss=0.07554, avg_loss=0.07459]\n",
      "Step 523980  [5.423 sec/step, loss=0.07457, avg_loss=0.07458]\n",
      "Step 523981  [5.422 sec/step, loss=0.07382, avg_loss=0.07458]\n",
      "Step 523982  [5.411 sec/step, loss=0.06680, avg_loss=0.07451]\n",
      "Step 523983  [5.409 sec/step, loss=0.07618, avg_loss=0.07450]\n",
      "Step 523984  [5.394 sec/step, loss=0.07185, avg_loss=0.07446]\n",
      "Step 523985  [5.403 sec/step, loss=0.07431, avg_loss=0.07448]\n",
      "Generated 32 batches of size 32 in 2.599 sec\n",
      "Step 523986  [5.464 sec/step, loss=0.06783, avg_loss=0.07441]\n",
      "Step 523987  [5.479 sec/step, loss=0.07594, avg_loss=0.07446]\n",
      "Step 523988  [5.483 sec/step, loss=0.07226, avg_loss=0.07445]\n",
      "Step 523989  [5.489 sec/step, loss=0.07687, avg_loss=0.07445]\n",
      "Step 523990  [5.484 sec/step, loss=0.07513, avg_loss=0.07445]\n",
      "Step 523991  [5.473 sec/step, loss=0.07597, avg_loss=0.07445]\n",
      "Step 523992  [5.487 sec/step, loss=0.07554, avg_loss=0.07447]\n",
      "Step 523993  [5.462 sec/step, loss=0.07578, avg_loss=0.07448]\n",
      "Step 523994  [5.463 sec/step, loss=0.07447, avg_loss=0.07446]\n",
      "Step 523995  [5.451 sec/step, loss=0.07411, avg_loss=0.07445]\n",
      "Step 523996  [5.452 sec/step, loss=0.07658, avg_loss=0.07444]\n",
      "Step 523997  [5.512 sec/step, loss=0.06546, avg_loss=0.07434]\n",
      "Step 523998  [5.537 sec/step, loss=0.07517, avg_loss=0.07432]\n",
      "Step 523999  [5.479 sec/step, loss=0.07388, avg_loss=0.07438]\n",
      "Step 524000  [5.474 sec/step, loss=0.07477, avg_loss=0.07436]\n",
      "Writing summary at step: 524000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-524000\n",
      "Saving audio and alignment...\n",
      "Input: odz or farxanddaa nay dats dzhaalar miin maggrabii tdashaxxus kaa haamil paehhlaa muhazzab baraahman biznis miin ddaykhaa~____________________\n",
      "Step 524001  [5.476 sec/step, loss=0.07370, avg_loss=0.07436]\n",
      "Step 524002  [5.473 sec/step, loss=0.07222, avg_loss=0.07433]\n",
      "Step 524003  [5.455 sec/step, loss=0.07548, avg_loss=0.07434]\n",
      "Step 524004  [5.456 sec/step, loss=0.07481, avg_loss=0.07431]\n",
      "Step 524005  [5.466 sec/step, loss=0.07308, avg_loss=0.07429]\n",
      "Step 524006  [5.469 sec/step, loss=0.07574, avg_loss=0.07432]\n",
      "Step 524007  [5.466 sec/step, loss=0.07413, avg_loss=0.07428]\n",
      "Step 524008  [5.467 sec/step, loss=0.07489, avg_loss=0.07431]\n",
      "Step 524009  [5.459 sec/step, loss=0.07481, avg_loss=0.07432]\n",
      "Step 524010  [5.453 sec/step, loss=0.07479, avg_loss=0.07431]\n",
      "Step 524011  [5.475 sec/step, loss=0.07618, avg_loss=0.07432]\n",
      "Step 524012  [5.474 sec/step, loss=0.07569, avg_loss=0.07432]\n",
      "Step 524013  [5.481 sec/step, loss=0.07500, avg_loss=0.07433]\n",
      "Step 524014  [5.503 sec/step, loss=0.07639, avg_loss=0.07436]\n",
      "Step 524015  [5.497 sec/step, loss=0.07134, avg_loss=0.07434]\n",
      "Generated 32 batches of size 32 in 2.534 sec\n",
      "Step 524016  [5.495 sec/step, loss=0.07527, avg_loss=0.07433]\n",
      "Step 524017  [5.497 sec/step, loss=0.07528, avg_loss=0.07432]\n",
      "Step 524018  [5.493 sec/step, loss=0.07395, avg_loss=0.07432]\n",
      "Step 524019  [5.495 sec/step, loss=0.07590, avg_loss=0.07431]\n",
      "Step 524020  [5.501 sec/step, loss=0.07734, avg_loss=0.07435]\n",
      "Step 524021  [5.496 sec/step, loss=0.07514, avg_loss=0.07433]\n",
      "Step 524022  [5.478 sec/step, loss=0.07177, avg_loss=0.07430]\n",
      "Step 524023  [5.505 sec/step, loss=0.07338, avg_loss=0.07438]\n",
      "Step 524024  [5.450 sec/step, loss=0.07414, avg_loss=0.07444]\n",
      "Step 524025  [5.433 sec/step, loss=0.06740, avg_loss=0.07436]\n",
      "Step 524026  [5.436 sec/step, loss=0.07297, avg_loss=0.07433]\n",
      "Step 524027  [5.417 sec/step, loss=0.07390, avg_loss=0.07430]\n",
      "Step 524028  [5.382 sec/step, loss=0.07444, avg_loss=0.07430]\n",
      "Step 524029  [5.393 sec/step, loss=0.07423, avg_loss=0.07429]\n",
      "Step 524030  [5.380 sec/step, loss=0.07492, avg_loss=0.07427]\n",
      "Step 524031  [5.377 sec/step, loss=0.07479, avg_loss=0.07426]\n",
      "Step 524032  [5.374 sec/step, loss=0.07580, avg_loss=0.07425]\n",
      "Step 524033  [5.368 sec/step, loss=0.07270, avg_loss=0.07422]\n",
      "Step 524034  [5.369 sec/step, loss=0.07597, avg_loss=0.07422]\n",
      "Step 524035  [5.375 sec/step, loss=0.07552, avg_loss=0.07424]\n",
      "Step 524036  [5.376 sec/step, loss=0.07657, avg_loss=0.07427]\n",
      "Step 524037  [5.368 sec/step, loss=0.07390, avg_loss=0.07425]\n",
      "Step 524038  [5.379 sec/step, loss=0.07638, avg_loss=0.07427]\n",
      "Step 524039  [5.375 sec/step, loss=0.07521, avg_loss=0.07427]\n",
      "Step 524040  [5.386 sec/step, loss=0.07525, avg_loss=0.07427]\n",
      "Step 524041  [5.364 sec/step, loss=0.07661, avg_loss=0.07430]\n",
      "Step 524042  [5.364 sec/step, loss=0.07278, avg_loss=0.07430]\n",
      "Step 524043  [5.404 sec/step, loss=0.07599, avg_loss=0.07439]\n",
      "Step 524044  [5.406 sec/step, loss=0.07457, avg_loss=0.07437]\n",
      "Step 524045  [5.389 sec/step, loss=0.07219, avg_loss=0.07433]\n",
      "Step 524046  [5.382 sec/step, loss=0.07506, avg_loss=0.07432]\n",
      "Step 524047  [5.395 sec/step, loss=0.07252, avg_loss=0.07430]\n",
      "Generated 32 batches of size 32 in 2.390 sec\n",
      "Step 524048  [5.403 sec/step, loss=0.07605, avg_loss=0.07430]\n",
      "Step 524049  [5.409 sec/step, loss=0.07605, avg_loss=0.07432]\n",
      "Step 524050  [5.444 sec/step, loss=0.06602, avg_loss=0.07421]\n",
      "Step 524051  [5.433 sec/step, loss=0.07298, avg_loss=0.07419]\n",
      "Step 524052  [5.439 sec/step, loss=0.07605, avg_loss=0.07423]\n",
      "Step 524053  [5.438 sec/step, loss=0.07046, avg_loss=0.07419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 524054  [5.418 sec/step, loss=0.07429, avg_loss=0.07417]\n",
      "Step 524055  [5.417 sec/step, loss=0.07553, avg_loss=0.07418]\n",
      "Step 524056  [5.430 sec/step, loss=0.07523, avg_loss=0.07420]\n",
      "Step 524057  [5.431 sec/step, loss=0.07397, avg_loss=0.07418]\n",
      "Step 524058  [5.437 sec/step, loss=0.07570, avg_loss=0.07419]\n",
      "Step 524059  [5.412 sec/step, loss=0.07159, avg_loss=0.07417]\n",
      "Step 524060  [5.425 sec/step, loss=0.07467, avg_loss=0.07415]\n",
      "Step 524061  [5.369 sec/step, loss=0.07205, avg_loss=0.07421]\n",
      "Step 524062  [5.368 sec/step, loss=0.07488, avg_loss=0.07420]\n",
      "Step 524063  [5.352 sec/step, loss=0.07527, avg_loss=0.07420]\n",
      "Step 524064  [5.350 sec/step, loss=0.07357, avg_loss=0.07418]\n",
      "Step 524065  [5.360 sec/step, loss=0.07583, avg_loss=0.07421]\n",
      "Step 524066  [5.363 sec/step, loss=0.07585, avg_loss=0.07423]\n",
      "Step 524067  [5.375 sec/step, loss=0.07176, avg_loss=0.07424]\n",
      "Step 524068  [5.365 sec/step, loss=0.07620, avg_loss=0.07424]\n",
      "Step 524069  [5.364 sec/step, loss=0.07452, avg_loss=0.07422]\n",
      "Step 524070  [5.383 sec/step, loss=0.07656, avg_loss=0.07424]\n",
      "Step 524071  [5.362 sec/step, loss=0.07532, avg_loss=0.07426]\n",
      "Step 524072  [5.350 sec/step, loss=0.07338, avg_loss=0.07424]\n",
      "Step 524073  [5.328 sec/step, loss=0.07486, avg_loss=0.07425]\n",
      "Step 524074  [5.302 sec/step, loss=0.07424, avg_loss=0.07423]\n",
      "Step 524075  [5.282 sec/step, loss=0.06541, avg_loss=0.07416]\n",
      "Step 524076  [5.299 sec/step, loss=0.07555, avg_loss=0.07418]\n",
      "Step 524077  [5.300 sec/step, loss=0.07371, avg_loss=0.07417]\n",
      "Step 524078  [5.281 sec/step, loss=0.07026, avg_loss=0.07411]\n",
      "Step 524079  [5.304 sec/step, loss=0.07389, avg_loss=0.07409]\n",
      "Generated 32 batches of size 32 in 2.443 sec\n",
      "Step 524080  [5.308 sec/step, loss=0.07502, avg_loss=0.07410]\n",
      "Step 524081  [5.312 sec/step, loss=0.07531, avg_loss=0.07411]\n",
      "Step 524082  [5.336 sec/step, loss=0.07657, avg_loss=0.07421]\n",
      "Step 524083  [5.337 sec/step, loss=0.07439, avg_loss=0.07419]\n",
      "Step 524084  [5.397 sec/step, loss=0.06736, avg_loss=0.07415]\n",
      "Step 524085  [5.401 sec/step, loss=0.07492, avg_loss=0.07415]\n",
      "Step 524086  [5.355 sec/step, loss=0.07538, avg_loss=0.07423]\n",
      "Step 524087  [5.368 sec/step, loss=0.07523, avg_loss=0.07422]\n",
      "Step 524088  [5.375 sec/step, loss=0.07642, avg_loss=0.07426]\n",
      "Step 524089  [5.358 sec/step, loss=0.07264, avg_loss=0.07422]\n",
      "Step 524090  [5.348 sec/step, loss=0.07342, avg_loss=0.07420]\n",
      "Step 524091  [5.352 sec/step, loss=0.07551, avg_loss=0.07420]\n",
      "Step 524092  [5.334 sec/step, loss=0.07274, avg_loss=0.07417]\n",
      "Step 524093  [5.340 sec/step, loss=0.07591, avg_loss=0.07417]\n",
      "Step 524094  [5.339 sec/step, loss=0.07398, avg_loss=0.07417]\n",
      "Step 524095  [5.349 sec/step, loss=0.07508, avg_loss=0.07418]\n",
      "Step 524096  [5.350 sec/step, loss=0.07681, avg_loss=0.07418]\n",
      "Step 524097  [5.301 sec/step, loss=0.07561, avg_loss=0.07428]\n",
      "Step 524098  [5.303 sec/step, loss=0.07564, avg_loss=0.07428]\n",
      "Step 524099  [5.327 sec/step, loss=0.07648, avg_loss=0.07431]\n",
      "Step 524100  [5.341 sec/step, loss=0.07554, avg_loss=0.07432]\n",
      "Writing summary at step: 524100\n",
      "Step 524101  [5.338 sec/step, loss=0.07553, avg_loss=0.07434]\n",
      "Step 524102  [5.335 sec/step, loss=0.07473, avg_loss=0.07436]\n",
      "Step 524103  [5.351 sec/step, loss=0.07355, avg_loss=0.07434]\n",
      "Step 524104  [5.336 sec/step, loss=0.07545, avg_loss=0.07435]\n",
      "Step 524105  [5.336 sec/step, loss=0.07418, avg_loss=0.07436]\n",
      "Step 524106  [5.339 sec/step, loss=0.07534, avg_loss=0.07436]\n",
      "Step 524107  [5.325 sec/step, loss=0.07083, avg_loss=0.07432]\n",
      "Step 524108  [5.331 sec/step, loss=0.07394, avg_loss=0.07431]\n",
      "Step 524109  [5.319 sec/step, loss=0.07380, avg_loss=0.07430]\n",
      "Step 524110  [5.326 sec/step, loss=0.07503, avg_loss=0.07431]\n",
      "Generated 32 batches of size 32 in 2.389 sec\n",
      "Step 524111  [5.319 sec/step, loss=0.07446, avg_loss=0.07429]\n",
      "Step 524112  [5.304 sec/step, loss=0.07189, avg_loss=0.07425]\n",
      "Step 524113  [5.301 sec/step, loss=0.07508, avg_loss=0.07425]\n",
      "Step 524114  [5.304 sec/step, loss=0.07574, avg_loss=0.07424]\n",
      "Step 524115  [5.296 sec/step, loss=0.06623, avg_loss=0.07419]\n",
      "Step 524116  [5.344 sec/step, loss=0.06578, avg_loss=0.07410]\n",
      "Step 524117  [5.327 sec/step, loss=0.07337, avg_loss=0.07408]\n",
      "Step 524118  [5.338 sec/step, loss=0.07500, avg_loss=0.07409]\n",
      "Step 524119  [5.337 sec/step, loss=0.07699, avg_loss=0.07410]\n",
      "Step 524120  [5.320 sec/step, loss=0.07379, avg_loss=0.07406]\n",
      "Step 524121  [5.315 sec/step, loss=0.07455, avg_loss=0.07406]\n",
      "Step 524122  [5.315 sec/step, loss=0.07268, avg_loss=0.07407]\n",
      "Step 524123  [5.319 sec/step, loss=0.07501, avg_loss=0.07408]\n",
      "Step 524124  [5.329 sec/step, loss=0.07434, avg_loss=0.07409]\n",
      "Step 524125  [5.362 sec/step, loss=0.07566, avg_loss=0.07417]\n",
      "Step 524126  [5.371 sec/step, loss=0.07594, avg_loss=0.07420]\n",
      "Step 524127  [5.378 sec/step, loss=0.07574, avg_loss=0.07422]\n",
      "Step 524128  [5.436 sec/step, loss=0.06631, avg_loss=0.07414]\n",
      "Step 524129  [5.413 sec/step, loss=0.07517, avg_loss=0.07414]\n",
      "Step 524130  [5.401 sec/step, loss=0.07399, avg_loss=0.07414]\n",
      "Step 524131  [5.403 sec/step, loss=0.07563, avg_loss=0.07414]\n",
      "Step 524132  [5.394 sec/step, loss=0.07170, avg_loss=0.07410]\n",
      "Step 524133  [5.402 sec/step, loss=0.07611, avg_loss=0.07414]\n",
      "Step 524134  [5.420 sec/step, loss=0.07269, avg_loss=0.07410]\n",
      "Step 524135  [5.416 sec/step, loss=0.07143, avg_loss=0.07406]\n",
      "Step 524136  [5.403 sec/step, loss=0.07556, avg_loss=0.07405]\n",
      "Step 524137  [5.415 sec/step, loss=0.07656, avg_loss=0.07408]\n",
      "Step 524138  [5.408 sec/step, loss=0.07515, avg_loss=0.07407]\n",
      "Step 524139  [5.401 sec/step, loss=0.07305, avg_loss=0.07405]\n",
      "Step 524140  [5.413 sec/step, loss=0.07640, avg_loss=0.07406]\n",
      "Step 524141  [5.406 sec/step, loss=0.07459, avg_loss=0.07404]\n",
      "Step 524142  [5.427 sec/step, loss=0.07640, avg_loss=0.07407]\n",
      "Generated 32 batches of size 32 in 2.831 sec\n",
      "Step 524143  [5.393 sec/step, loss=0.06779, avg_loss=0.07399]\n",
      "Step 524144  [5.403 sec/step, loss=0.07689, avg_loss=0.07401]\n",
      "Step 524145  [5.411 sec/step, loss=0.07522, avg_loss=0.07404]\n",
      "Step 524146  [5.387 sec/step, loss=0.07254, avg_loss=0.07402]\n",
      "Step 524147  [5.386 sec/step, loss=0.07523, avg_loss=0.07405]\n",
      "Step 524148  [5.370 sec/step, loss=0.07505, avg_loss=0.07404]\n",
      "Step 524149  [5.353 sec/step, loss=0.07368, avg_loss=0.07401]\n",
      "Step 524150  [5.312 sec/step, loss=0.07431, avg_loss=0.07410]\n",
      "Step 524151  [5.317 sec/step, loss=0.07393, avg_loss=0.07411]\n",
      "Step 524152  [5.328 sec/step, loss=0.07382, avg_loss=0.07408]\n",
      "Step 524153  [5.345 sec/step, loss=0.07604, avg_loss=0.07414]\n",
      "Step 524154  [5.357 sec/step, loss=0.07597, avg_loss=0.07416]\n",
      "Step 524155  [5.372 sec/step, loss=0.07621, avg_loss=0.07416]\n",
      "Step 524156  [5.350 sec/step, loss=0.06995, avg_loss=0.07411]\n",
      "Step 524157  [5.359 sec/step, loss=0.07644, avg_loss=0.07413]\n",
      "Step 524158  [5.359 sec/step, loss=0.07516, avg_loss=0.07413]\n",
      "Step 524159  [5.377 sec/step, loss=0.07466, avg_loss=0.07416]\n",
      "Step 524160  [5.362 sec/step, loss=0.07495, avg_loss=0.07416]\n",
      "Step 524161  [5.362 sec/step, loss=0.07272, avg_loss=0.07417]\n",
      "Step 524162  [5.354 sec/step, loss=0.07072, avg_loss=0.07413]\n",
      "Step 524163  [5.366 sec/step, loss=0.07474, avg_loss=0.07412]\n",
      "Step 524164  [5.390 sec/step, loss=0.07387, avg_loss=0.07413]\n",
      "Step 524165  [5.375 sec/step, loss=0.07411, avg_loss=0.07411]\n",
      "Step 524166  [5.389 sec/step, loss=0.07308, avg_loss=0.07408]\n",
      "Step 524167  [5.397 sec/step, loss=0.07511, avg_loss=0.07411]\n",
      "Step 524168  [5.394 sec/step, loss=0.07350, avg_loss=0.07409]\n",
      "Step 524169  [5.386 sec/step, loss=0.07541, avg_loss=0.07410]\n",
      "Step 524170  [5.379 sec/step, loss=0.07542, avg_loss=0.07408]\n",
      "Step 524171  [5.372 sec/step, loss=0.07441, avg_loss=0.07408]\n",
      "Step 524172  [5.365 sec/step, loss=0.07108, avg_loss=0.07405]\n",
      "Step 524173  [5.378 sec/step, loss=0.07649, avg_loss=0.07407]\n",
      "Step 524174  [5.399 sec/step, loss=0.07422, avg_loss=0.07407]\n",
      "Generated 32 batches of size 32 in 2.370 sec\n",
      "Step 524175  [5.420 sec/step, loss=0.07479, avg_loss=0.07416]\n",
      "Step 524176  [5.406 sec/step, loss=0.07512, avg_loss=0.07416]\n",
      "Step 524177  [5.408 sec/step, loss=0.07284, avg_loss=0.07415]\n",
      "Step 524178  [5.433 sec/step, loss=0.07649, avg_loss=0.07421]\n",
      "Step 524179  [5.407 sec/step, loss=0.07457, avg_loss=0.07422]\n",
      "Step 524180  [5.454 sec/step, loss=0.06721, avg_loss=0.07414]\n",
      "Step 524181  [5.446 sec/step, loss=0.07415, avg_loss=0.07413]\n",
      "Step 524182  [5.441 sec/step, loss=0.07591, avg_loss=0.07412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 524183  [5.415 sec/step, loss=0.06530, avg_loss=0.07403]\n",
      "Step 524184  [5.374 sec/step, loss=0.07608, avg_loss=0.07412]\n",
      "Step 524185  [5.376 sec/step, loss=0.07554, avg_loss=0.07412]\n",
      "Step 524186  [5.377 sec/step, loss=0.07468, avg_loss=0.07412]\n",
      "Step 524187  [5.372 sec/step, loss=0.07565, avg_loss=0.07412]\n",
      "Step 524188  [5.364 sec/step, loss=0.07505, avg_loss=0.07411]\n",
      "Step 524189  [5.367 sec/step, loss=0.07439, avg_loss=0.07413]\n",
      "Step 524190  [5.370 sec/step, loss=0.07547, avg_loss=0.07415]\n",
      "Step 524191  [5.367 sec/step, loss=0.07379, avg_loss=0.07413]\n",
      "Step 524192  [5.364 sec/step, loss=0.07153, avg_loss=0.07412]\n",
      "Step 524193  [5.354 sec/step, loss=0.07569, avg_loss=0.07411]\n",
      "Step 524194  [5.366 sec/step, loss=0.07625, avg_loss=0.07414]\n",
      "Step 524195  [5.355 sec/step, loss=0.07254, avg_loss=0.07411]\n",
      "Step 524196  [5.342 sec/step, loss=0.07513, avg_loss=0.07410]\n",
      "Step 524197  [5.348 sec/step, loss=0.07464, avg_loss=0.07409]\n",
      "Step 524198  [5.314 sec/step, loss=0.07183, avg_loss=0.07405]\n",
      "Step 524199  [5.304 sec/step, loss=0.07651, avg_loss=0.07405]\n",
      "Step 524200  [5.293 sec/step, loss=0.07278, avg_loss=0.07402]\n",
      "Writing summary at step: 524200\n",
      "Step 524201  [5.282 sec/step, loss=0.07440, avg_loss=0.07401]\n",
      "Step 524202  [5.285 sec/step, loss=0.07177, avg_loss=0.07398]\n",
      "Step 524203  [5.277 sec/step, loss=0.07557, avg_loss=0.07400]\n",
      "Step 524204  [5.276 sec/step, loss=0.07404, avg_loss=0.07399]\n",
      "Step 524205  [5.268 sec/step, loss=0.07481, avg_loss=0.07399]\n",
      "Generated 32 batches of size 32 in 2.599 sec\n",
      "Step 524206  [5.318 sec/step, loss=0.06549, avg_loss=0.07389]\n",
      "Step 524207  [5.352 sec/step, loss=0.07514, avg_loss=0.07394]\n",
      "Step 524208  [5.359 sec/step, loss=0.07682, avg_loss=0.07396]\n",
      "Step 524209  [5.359 sec/step, loss=0.07425, avg_loss=0.07397]\n",
      "Step 524210  [5.362 sec/step, loss=0.07711, avg_loss=0.07399]\n",
      "Step 524211  [5.368 sec/step, loss=0.07633, avg_loss=0.07401]\n",
      "Step 524212  [5.400 sec/step, loss=0.07593, avg_loss=0.07405]\n",
      "Step 524213  [5.408 sec/step, loss=0.07612, avg_loss=0.07406]\n",
      "Step 524214  [5.412 sec/step, loss=0.07595, avg_loss=0.07406]\n",
      "Step 524215  [5.427 sec/step, loss=0.07564, avg_loss=0.07416]\n",
      "Step 524216  [5.387 sec/step, loss=0.07686, avg_loss=0.07427]\n",
      "Step 524217  [5.392 sec/step, loss=0.07254, avg_loss=0.07426]\n",
      "Step 524218  [5.393 sec/step, loss=0.07595, avg_loss=0.07427]\n",
      "Step 524219  [5.383 sec/step, loss=0.07276, avg_loss=0.07423]\n",
      "Step 524220  [5.396 sec/step, loss=0.07655, avg_loss=0.07425]\n",
      "Step 524221  [5.408 sec/step, loss=0.07643, avg_loss=0.07427]\n",
      "Step 524222  [5.433 sec/step, loss=0.07502, avg_loss=0.07430]\n",
      "Step 524223  [5.409 sec/step, loss=0.07443, avg_loss=0.07429]\n",
      "Step 524224  [5.414 sec/step, loss=0.07496, avg_loss=0.07430]\n",
      "Step 524225  [5.384 sec/step, loss=0.07182, avg_loss=0.07426]\n",
      "Step 524226  [5.425 sec/step, loss=0.06750, avg_loss=0.07417]\n",
      "Step 524227  [5.431 sec/step, loss=0.07643, avg_loss=0.07418]\n",
      "Step 524228  [5.383 sec/step, loss=0.07586, avg_loss=0.07428]\n",
      "Step 524229  [5.380 sec/step, loss=0.07443, avg_loss=0.07427]\n",
      "Step 524230  [5.403 sec/step, loss=0.07670, avg_loss=0.07430]\n",
      "Step 524231  [5.430 sec/step, loss=0.07402, avg_loss=0.07428]\n",
      "Step 524232  [5.434 sec/step, loss=0.07307, avg_loss=0.07429]\n",
      "Step 524233  [5.429 sec/step, loss=0.07541, avg_loss=0.07429]\n",
      "Step 524234  [5.407 sec/step, loss=0.07561, avg_loss=0.07432]\n",
      "Step 524235  [5.400 sec/step, loss=0.06710, avg_loss=0.07427]\n",
      "Step 524236  [5.389 sec/step, loss=0.07164, avg_loss=0.07423]\n",
      "Step 524237  [5.382 sec/step, loss=0.07493, avg_loss=0.07422]\n",
      "Generated 32 batches of size 32 in 2.629 sec\n",
      "Step 524238  [5.382 sec/step, loss=0.07136, avg_loss=0.07418]\n",
      "Step 524239  [5.385 sec/step, loss=0.07557, avg_loss=0.07420]\n",
      "Step 524240  [5.392 sec/step, loss=0.07389, avg_loss=0.07418]\n",
      "Step 524241  [5.407 sec/step, loss=0.07635, avg_loss=0.07420]\n",
      "Step 524242  [5.393 sec/step, loss=0.07404, avg_loss=0.07417]\n",
      "Step 524243  [5.404 sec/step, loss=0.07299, avg_loss=0.07422]\n",
      "Step 524244  [5.406 sec/step, loss=0.07555, avg_loss=0.07421]\n",
      "Step 524245  [5.398 sec/step, loss=0.07324, avg_loss=0.07419]\n",
      "Step 524246  [5.405 sec/step, loss=0.07437, avg_loss=0.07421]\n",
      "Step 524247  [5.415 sec/step, loss=0.07644, avg_loss=0.07422]\n",
      "Step 524248  [5.415 sec/step, loss=0.07364, avg_loss=0.07421]\n",
      "Step 524249  [5.425 sec/step, loss=0.07557, avg_loss=0.07423]\n",
      "Step 524250  [5.399 sec/step, loss=0.06672, avg_loss=0.07415]\n",
      "Step 524251  [5.418 sec/step, loss=0.07465, avg_loss=0.07416]\n",
      "Step 524252  [5.407 sec/step, loss=0.07540, avg_loss=0.07417]\n",
      "Step 524253  [5.392 sec/step, loss=0.07367, avg_loss=0.07415]\n",
      "Step 524254  [5.403 sec/step, loss=0.07560, avg_loss=0.07415]\n",
      "Step 524255  [5.403 sec/step, loss=0.07655, avg_loss=0.07415]\n",
      "Step 524256  [5.422 sec/step, loss=0.07687, avg_loss=0.07422]\n",
      "Step 524257  [5.408 sec/step, loss=0.07422, avg_loss=0.07420]\n",
      "Step 524258  [5.406 sec/step, loss=0.07455, avg_loss=0.07419]\n",
      "Step 524259  [5.416 sec/step, loss=0.07650, avg_loss=0.07421]\n",
      "Step 524260  [5.417 sec/step, loss=0.07387, avg_loss=0.07420]\n",
      "Step 524261  [5.427 sec/step, loss=0.07493, avg_loss=0.07422]\n",
      "Step 524262  [5.481 sec/step, loss=0.06580, avg_loss=0.07417]\n",
      "Step 524263  [5.474 sec/step, loss=0.07214, avg_loss=0.07414]\n",
      "Step 524264  [5.452 sec/step, loss=0.07344, avg_loss=0.07414]\n",
      "Step 524265  [5.467 sec/step, loss=0.07587, avg_loss=0.07416]\n",
      "Step 524266  [5.439 sec/step, loss=0.07548, avg_loss=0.07418]\n",
      "Step 524267  [5.443 sec/step, loss=0.07514, avg_loss=0.07418]\n",
      "Step 524268  [5.440 sec/step, loss=0.07430, avg_loss=0.07419]\n",
      "Step 524269  [5.471 sec/step, loss=0.07291, avg_loss=0.07417]\n",
      "Generated 32 batches of size 32 in 2.640 sec\n",
      "Step 524270  [5.462 sec/step, loss=0.07481, avg_loss=0.07416]\n",
      "Step 524271  [5.468 sec/step, loss=0.07571, avg_loss=0.07417]\n",
      "Step 524272  [5.471 sec/step, loss=0.07224, avg_loss=0.07418]\n",
      "Step 524273  [5.470 sec/step, loss=0.07648, avg_loss=0.07418]\n",
      "Step 524274  [5.445 sec/step, loss=0.07192, avg_loss=0.07416]\n",
      "Step 524275  [5.447 sec/step, loss=0.07636, avg_loss=0.07418]\n",
      "Step 524276  [5.451 sec/step, loss=0.07488, avg_loss=0.07417]\n",
      "Step 524277  [5.463 sec/step, loss=0.07360, avg_loss=0.07418]\n",
      "Step 524278  [5.461 sec/step, loss=0.07430, avg_loss=0.07416]\n",
      "Step 524279  [5.475 sec/step, loss=0.07496, avg_loss=0.07416]\n",
      "Step 524280  [5.418 sec/step, loss=0.07427, avg_loss=0.07423]\n",
      "Step 524281  [5.419 sec/step, loss=0.07312, avg_loss=0.07422]\n",
      "Step 524282  [5.422 sec/step, loss=0.07529, avg_loss=0.07422]\n",
      "Step 524283  [5.437 sec/step, loss=0.07477, avg_loss=0.07431]\n",
      "Step 524284  [5.418 sec/step, loss=0.07431, avg_loss=0.07429]\n",
      "Step 524285  [5.425 sec/step, loss=0.07653, avg_loss=0.07430]\n",
      "Step 524286  [5.396 sec/step, loss=0.06599, avg_loss=0.07422]\n",
      "Step 524287  [5.382 sec/step, loss=0.07543, avg_loss=0.07422]\n",
      "Step 524288  [5.383 sec/step, loss=0.07501, avg_loss=0.07422]\n",
      "Step 524289  [5.395 sec/step, loss=0.07609, avg_loss=0.07423]\n",
      "Step 524290  [5.396 sec/step, loss=0.07441, avg_loss=0.07422]\n",
      "Step 524291  [5.448 sec/step, loss=0.06734, avg_loss=0.07416]\n",
      "Step 524292  [5.466 sec/step, loss=0.07326, avg_loss=0.07417]\n",
      "Step 524293  [5.466 sec/step, loss=0.07538, avg_loss=0.07417]\n",
      "Step 524294  [5.464 sec/step, loss=0.07622, avg_loss=0.07417]\n",
      "Step 524295  [5.457 sec/step, loss=0.07088, avg_loss=0.07415]\n",
      "Step 524296  [5.464 sec/step, loss=0.07386, avg_loss=0.07414]\n",
      "Step 524297  [5.464 sec/step, loss=0.07632, avg_loss=0.07416]\n",
      "Step 524298  [5.469 sec/step, loss=0.07195, avg_loss=0.07416]\n",
      "Step 524299  [5.468 sec/step, loss=0.07525, avg_loss=0.07415]\n",
      "Step 524300  [5.491 sec/step, loss=0.07321, avg_loss=0.07415]\n",
      "Writing summary at step: 524300\n",
      "Generated 32 batches of size 32 in 2.452 sec\n",
      "Step 524301  [5.520 sec/step, loss=0.07513, avg_loss=0.07416]\n",
      "Step 524302  [5.537 sec/step, loss=0.07672, avg_loss=0.07421]\n",
      "Step 524303  [5.530 sec/step, loss=0.07376, avg_loss=0.07419]\n",
      "Step 524304  [5.521 sec/step, loss=0.07132, avg_loss=0.07416]\n",
      "Step 524305  [5.549 sec/step, loss=0.07614, avg_loss=0.07418]\n",
      "Step 524306  [5.496 sec/step, loss=0.07425, avg_loss=0.07426]\n",
      "Step 524307  [5.474 sec/step, loss=0.07515, avg_loss=0.07426]\n",
      "Step 524308  [5.472 sec/step, loss=0.07281, avg_loss=0.07422]\n",
      "Step 524309  [5.476 sec/step, loss=0.07356, avg_loss=0.07422]\n",
      "Step 524310  [5.463 sec/step, loss=0.07373, avg_loss=0.07418]\n",
      "Step 524311  [5.451 sec/step, loss=0.07637, avg_loss=0.07418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 524312  [5.449 sec/step, loss=0.07362, avg_loss=0.07416]\n",
      "Step 524313  [5.440 sec/step, loss=0.07433, avg_loss=0.07414]\n",
      "Step 524314  [5.426 sec/step, loss=0.07549, avg_loss=0.07414]\n",
      "Step 524315  [5.438 sec/step, loss=0.07626, avg_loss=0.07414]\n",
      "Step 524316  [5.421 sec/step, loss=0.07093, avg_loss=0.07408]\n",
      "Step 524317  [5.419 sec/step, loss=0.07574, avg_loss=0.07412]\n",
      "Step 524318  [5.426 sec/step, loss=0.07599, avg_loss=0.07412]\n",
      "Step 524319  [5.430 sec/step, loss=0.07415, avg_loss=0.07413]\n",
      "Step 524320  [5.414 sec/step, loss=0.07139, avg_loss=0.07408]\n",
      "Step 524321  [5.412 sec/step, loss=0.07646, avg_loss=0.07408]\n",
      "Step 524322  [5.383 sec/step, loss=0.06566, avg_loss=0.07399]\n",
      "Step 524323  [5.406 sec/step, loss=0.07523, avg_loss=0.07399]\n",
      "Step 524324  [5.418 sec/step, loss=0.07455, avg_loss=0.07399]\n",
      "Step 524325  [5.434 sec/step, loss=0.07495, avg_loss=0.07402]\n",
      "Step 524326  [5.403 sec/step, loss=0.07429, avg_loss=0.07409]\n",
      "Step 524327  [5.394 sec/step, loss=0.07541, avg_loss=0.07408]\n",
      "Step 524328  [5.390 sec/step, loss=0.07200, avg_loss=0.07404]\n",
      "Step 524329  [5.406 sec/step, loss=0.07558, avg_loss=0.07405]\n",
      "Step 524330  [5.391 sec/step, loss=0.07434, avg_loss=0.07403]\n",
      "Step 524331  [5.414 sec/step, loss=0.06797, avg_loss=0.07397]\n",
      "Step 524332  [5.422 sec/step, loss=0.07457, avg_loss=0.07398]\n",
      "Generated 32 batches of size 32 in 2.446 sec\n",
      "Step 524333  [5.450 sec/step, loss=0.07409, avg_loss=0.07397]\n",
      "Step 524334  [5.439 sec/step, loss=0.07415, avg_loss=0.07396]\n",
      "Step 524335  [5.453 sec/step, loss=0.07534, avg_loss=0.07404]\n",
      "Step 524336  [5.466 sec/step, loss=0.07209, avg_loss=0.07404]\n",
      "Step 524337  [5.452 sec/step, loss=0.07221, avg_loss=0.07401]\n",
      "Step 524338  [5.456 sec/step, loss=0.07660, avg_loss=0.07407]\n",
      "Step 524339  [5.450 sec/step, loss=0.07438, avg_loss=0.07406]\n",
      "Step 524340  [5.434 sec/step, loss=0.07531, avg_loss=0.07407]\n",
      "Step 524341  [5.421 sec/step, loss=0.07530, avg_loss=0.07406]\n",
      "Step 524342  [5.453 sec/step, loss=0.07504, avg_loss=0.07407]\n",
      "Step 524343  [5.436 sec/step, loss=0.06769, avg_loss=0.07402]\n",
      "Step 524344  [5.436 sec/step, loss=0.07674, avg_loss=0.07403]\n",
      "Step 524345  [5.440 sec/step, loss=0.07407, avg_loss=0.07404]\n",
      "Step 524346  [5.449 sec/step, loss=0.07245, avg_loss=0.07402]\n",
      "Step 524347  [5.441 sec/step, loss=0.07605, avg_loss=0.07401]\n",
      "Step 524348  [5.448 sec/step, loss=0.07477, avg_loss=0.07402]\n",
      "Step 524349  [5.455 sec/step, loss=0.07502, avg_loss=0.07402]\n",
      "Step 524350  [5.467 sec/step, loss=0.07287, avg_loss=0.07408]\n",
      "Step 524351  [5.465 sec/step, loss=0.07537, avg_loss=0.07409]\n",
      "Step 524352  [5.453 sec/step, loss=0.07427, avg_loss=0.07408]\n",
      "Step 524353  [5.459 sec/step, loss=0.07170, avg_loss=0.07406]\n",
      "Step 524354  [5.431 sec/step, loss=0.07106, avg_loss=0.07401]\n",
      "Step 524355  [5.417 sec/step, loss=0.07556, avg_loss=0.07400]\n",
      "Step 524356  [5.417 sec/step, loss=0.07638, avg_loss=0.07400]\n",
      "Step 524357  [5.416 sec/step, loss=0.07287, avg_loss=0.07398]\n",
      "Step 524358  [5.417 sec/step, loss=0.07484, avg_loss=0.07399]\n",
      "Step 524359  [5.397 sec/step, loss=0.07428, avg_loss=0.07396]\n",
      "Step 524360  [5.400 sec/step, loss=0.07508, avg_loss=0.07398]\n",
      "Step 524361  [5.392 sec/step, loss=0.07416, avg_loss=0.07397]\n",
      "Step 524362  [5.352 sec/step, loss=0.07602, avg_loss=0.07407]\n",
      "Step 524363  [5.339 sec/step, loss=0.07220, avg_loss=0.07407]\n",
      "Step 524364  [5.341 sec/step, loss=0.07439, avg_loss=0.07408]\n",
      "Generated 32 batches of size 32 in 2.390 sec\n",
      "Step 524365  [5.344 sec/step, loss=0.07526, avg_loss=0.07407]\n",
      "Step 524366  [5.352 sec/step, loss=0.07279, avg_loss=0.07405]\n",
      "Step 524367  [5.357 sec/step, loss=0.07454, avg_loss=0.07404]\n",
      "Step 524368  [5.405 sec/step, loss=0.06806, avg_loss=0.07398]\n",
      "Step 524369  [5.375 sec/step, loss=0.07561, avg_loss=0.07401]\n",
      "Step 524370  [5.398 sec/step, loss=0.07480, avg_loss=0.07401]\n",
      "Step 524371  [5.407 sec/step, loss=0.07396, avg_loss=0.07399]\n",
      "Step 524372  [5.430 sec/step, loss=0.07652, avg_loss=0.07403]\n",
      "Step 524373  [5.423 sec/step, loss=0.07621, avg_loss=0.07403]\n",
      "Step 524374  [5.448 sec/step, loss=0.07425, avg_loss=0.07405]\n",
      "Step 524375  [5.430 sec/step, loss=0.07427, avg_loss=0.07403]\n",
      "Step 524376  [5.443 sec/step, loss=0.07631, avg_loss=0.07405]\n",
      "Step 524377  [5.453 sec/step, loss=0.07666, avg_loss=0.07408]\n",
      "Step 524378  [5.443 sec/step, loss=0.07525, avg_loss=0.07409]\n",
      "Step 524379  [5.429 sec/step, loss=0.07396, avg_loss=0.07408]\n",
      "Step 524380  [5.434 sec/step, loss=0.07504, avg_loss=0.07408]\n",
      "Step 524381  [5.447 sec/step, loss=0.07542, avg_loss=0.07411]\n",
      "Step 524382  [5.470 sec/step, loss=0.07413, avg_loss=0.07409]\n",
      "Step 524383  [5.473 sec/step, loss=0.07166, avg_loss=0.07406]\n",
      "Step 524384  [5.488 sec/step, loss=0.07589, avg_loss=0.07408]\n",
      "Step 524385  [5.488 sec/step, loss=0.07600, avg_loss=0.07407]\n",
      "Step 524386  [5.519 sec/step, loss=0.07620, avg_loss=0.07418]\n",
      "Step 524387  [5.521 sec/step, loss=0.07148, avg_loss=0.07414]\n",
      "Step 524388  [5.517 sec/step, loss=0.07586, avg_loss=0.07415]\n",
      "Step 524389  [5.492 sec/step, loss=0.07208, avg_loss=0.07411]\n",
      "Step 524390  [5.495 sec/step, loss=0.07578, avg_loss=0.07412]\n",
      "Step 524391  [5.446 sec/step, loss=0.07483, avg_loss=0.07419]\n",
      "Step 524392  [5.456 sec/step, loss=0.07438, avg_loss=0.07420]\n",
      "Step 524393  [5.455 sec/step, loss=0.07316, avg_loss=0.07418]\n",
      "Step 524394  [5.442 sec/step, loss=0.07418, avg_loss=0.07416]\n",
      "Step 524395  [5.458 sec/step, loss=0.07498, avg_loss=0.07420]\n",
      "Step 524396  [5.473 sec/step, loss=0.07392, avg_loss=0.07420]\n",
      "Generated 32 batches of size 32 in 2.488 sec\n",
      "Step 524397  [5.474 sec/step, loss=0.07539, avg_loss=0.07419]\n",
      "Step 524398  [5.472 sec/step, loss=0.07369, avg_loss=0.07421]\n",
      "Step 524399  [5.456 sec/step, loss=0.07264, avg_loss=0.07419]\n",
      "Step 524400  [5.438 sec/step, loss=0.07640, avg_loss=0.07422]\n",
      "Writing summary at step: 524400\n",
      "Step 524401  [5.430 sec/step, loss=0.07488, avg_loss=0.07422]\n",
      "Step 524402  [5.423 sec/step, loss=0.07418, avg_loss=0.07419]\n",
      "Step 524403  [5.407 sec/step, loss=0.06577, avg_loss=0.07411]\n",
      "Step 524404  [5.472 sec/step, loss=0.06684, avg_loss=0.07407]\n",
      "Step 524405  [5.447 sec/step, loss=0.07368, avg_loss=0.07404]\n",
      "Step 524406  [5.449 sec/step, loss=0.07451, avg_loss=0.07404]\n",
      "Step 524407  [5.443 sec/step, loss=0.07553, avg_loss=0.07405]\n",
      "Step 524408  [5.493 sec/step, loss=0.06680, avg_loss=0.07399]\n",
      "Step 524409  [5.480 sec/step, loss=0.06676, avg_loss=0.07392]\n",
      "Step 524410  [5.487 sec/step, loss=0.07497, avg_loss=0.07393]\n",
      "Step 524411  [5.485 sec/step, loss=0.07542, avg_loss=0.07392]\n",
      "Step 524412  [5.458 sec/step, loss=0.07149, avg_loss=0.07390]\n",
      "Step 524413  [5.467 sec/step, loss=0.07440, avg_loss=0.07390]\n",
      "Step 524414  [5.491 sec/step, loss=0.07526, avg_loss=0.07390]\n",
      "Step 524415  [5.495 sec/step, loss=0.07572, avg_loss=0.07389]\n",
      "Step 524416  [5.511 sec/step, loss=0.07632, avg_loss=0.07395]\n",
      "Step 524417  [5.502 sec/step, loss=0.07324, avg_loss=0.07392]\n",
      "Step 524418  [5.491 sec/step, loss=0.07463, avg_loss=0.07391]\n",
      "Step 524419  [5.485 sec/step, loss=0.07543, avg_loss=0.07392]\n",
      "Step 524420  [5.509 sec/step, loss=0.07542, avg_loss=0.07396]\n",
      "Step 524421  [5.509 sec/step, loss=0.07645, avg_loss=0.07396]\n",
      "Step 524422  [5.527 sec/step, loss=0.07370, avg_loss=0.07404]\n",
      "Step 524423  [5.521 sec/step, loss=0.07620, avg_loss=0.07405]\n",
      "Step 524424  [5.491 sec/step, loss=0.07485, avg_loss=0.07406]\n",
      "Step 524425  [5.507 sec/step, loss=0.07533, avg_loss=0.07406]\n",
      "Step 524426  [5.486 sec/step, loss=0.07258, avg_loss=0.07404]\n",
      "Step 524427  [5.485 sec/step, loss=0.07339, avg_loss=0.07402]\n",
      "Generated 32 batches of size 32 in 2.807 sec\n",
      "Step 524428  [5.478 sec/step, loss=0.07238, avg_loss=0.07403]\n",
      "Step 524429  [5.474 sec/step, loss=0.07520, avg_loss=0.07402]\n",
      "Step 524430  [5.479 sec/step, loss=0.07507, avg_loss=0.07403]\n",
      "Step 524431  [5.418 sec/step, loss=0.07425, avg_loss=0.07409]\n",
      "Step 524432  [5.409 sec/step, loss=0.07494, avg_loss=0.07410]\n",
      "Step 524433  [5.384 sec/step, loss=0.07477, avg_loss=0.07410]\n",
      "Step 524434  [5.402 sec/step, loss=0.07598, avg_loss=0.07412]\n",
      "Step 524435  [5.407 sec/step, loss=0.07607, avg_loss=0.07413]\n",
      "Step 524436  [5.415 sec/step, loss=0.07639, avg_loss=0.07417]\n",
      "Step 524437  [5.442 sec/step, loss=0.07671, avg_loss=0.07422]\n",
      "Step 524438  [5.439 sec/step, loss=0.07459, avg_loss=0.07420]\n",
      "Step 524439  [5.448 sec/step, loss=0.07572, avg_loss=0.07421]\n",
      "Step 524440  [5.453 sec/step, loss=0.07627, avg_loss=0.07422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 524441  [5.439 sec/step, loss=0.07139, avg_loss=0.07418]\n",
      "Step 524442  [5.433 sec/step, loss=0.07377, avg_loss=0.07417]\n",
      "Step 524443  [5.463 sec/step, loss=0.07399, avg_loss=0.07423]\n",
      "Step 524444  [5.452 sec/step, loss=0.07176, avg_loss=0.07418]\n",
      "Step 524445  [5.453 sec/step, loss=0.07601, avg_loss=0.07420]\n",
      "Step 524446  [5.475 sec/step, loss=0.07397, avg_loss=0.07421]\n",
      "Step 524447  [5.477 sec/step, loss=0.07583, avg_loss=0.07421]\n",
      "Step 524448  [5.474 sec/step, loss=0.07405, avg_loss=0.07421]\n",
      "Step 524449  [5.463 sec/step, loss=0.07427, avg_loss=0.07420]\n",
      "Step 524450  [5.476 sec/step, loss=0.07519, avg_loss=0.07422]\n",
      "Step 524451  [5.463 sec/step, loss=0.07493, avg_loss=0.07422]\n",
      "Step 524452  [5.484 sec/step, loss=0.07458, avg_loss=0.07422]\n",
      "Step 524453  [5.490 sec/step, loss=0.07498, avg_loss=0.07425]\n",
      "Step 524454  [5.490 sec/step, loss=0.07165, avg_loss=0.07426]\n",
      "Step 524455  [5.501 sec/step, loss=0.07583, avg_loss=0.07426]\n",
      "Step 524456  [5.503 sec/step, loss=0.07675, avg_loss=0.07426]\n",
      "Step 524457  [5.507 sec/step, loss=0.07539, avg_loss=0.07429]\n",
      "Step 524458  [5.509 sec/step, loss=0.07525, avg_loss=0.07429]\n",
      "Step 524459  [5.506 sec/step, loss=0.07408, avg_loss=0.07429]\n",
      "Generated 32 batches of size 32 in 2.461 sec\n",
      "Step 524460  [5.510 sec/step, loss=0.07271, avg_loss=0.07427]\n",
      "Step 524461  [5.506 sec/step, loss=0.07459, avg_loss=0.07427]\n",
      "Step 524462  [5.491 sec/step, loss=0.07257, avg_loss=0.07424]\n",
      "Step 524463  [5.518 sec/step, loss=0.07591, avg_loss=0.07427]\n",
      "Step 524464  [5.514 sec/step, loss=0.07397, avg_loss=0.07427]\n",
      "Step 524465  [5.488 sec/step, loss=0.06506, avg_loss=0.07417]\n",
      "Step 524466  [5.532 sec/step, loss=0.06612, avg_loss=0.07410]\n",
      "Step 524467  [5.523 sec/step, loss=0.07540, avg_loss=0.07411]\n",
      "Step 524468  [5.472 sec/step, loss=0.07379, avg_loss=0.07417]\n",
      "Step 524469  [5.470 sec/step, loss=0.07387, avg_loss=0.07415]\n",
      "Step 524470  [5.465 sec/step, loss=0.07528, avg_loss=0.07416]\n",
      "Step 524471  [5.470 sec/step, loss=0.07477, avg_loss=0.07416]\n",
      "Step 524472  [5.448 sec/step, loss=0.07181, avg_loss=0.07412]\n",
      "Step 524473  [5.439 sec/step, loss=0.07379, avg_loss=0.07409]\n",
      "Step 524474  [5.455 sec/step, loss=0.07304, avg_loss=0.07408]\n",
      "Step 524475  [5.477 sec/step, loss=0.07655, avg_loss=0.07410]\n",
      "Step 524476  [5.460 sec/step, loss=0.07569, avg_loss=0.07410]\n",
      "Step 524477  [5.453 sec/step, loss=0.07625, avg_loss=0.07409]\n",
      "Step 524478  [5.441 sec/step, loss=0.07405, avg_loss=0.07408]\n",
      "Step 524479  [5.451 sec/step, loss=0.07511, avg_loss=0.07409]\n",
      "Step 524480  [5.457 sec/step, loss=0.07505, avg_loss=0.07409]\n",
      "Step 524481  [5.500 sec/step, loss=0.06659, avg_loss=0.07400]\n",
      "Step 524482  [5.477 sec/step, loss=0.07442, avg_loss=0.07401]\n",
      "Step 524483  [5.479 sec/step, loss=0.07525, avg_loss=0.07404]\n",
      "Step 524484  [5.483 sec/step, loss=0.07625, avg_loss=0.07405]\n",
      "Step 524485  [5.492 sec/step, loss=0.07527, avg_loss=0.07404]\n",
      "Step 524486  [5.492 sec/step, loss=0.07584, avg_loss=0.07404]\n",
      "Step 524487  [5.494 sec/step, loss=0.07463, avg_loss=0.07407]\n",
      "Step 524488  [5.499 sec/step, loss=0.07541, avg_loss=0.07406]\n",
      "Step 524489  [5.514 sec/step, loss=0.07371, avg_loss=0.07408]\n",
      "Step 524490  [5.495 sec/step, loss=0.06651, avg_loss=0.07399]\n",
      "Step 524491  [5.497 sec/step, loss=0.07571, avg_loss=0.07399]\n",
      "Generated 32 batches of size 32 in 2.495 sec\n",
      "Step 524492  [5.493 sec/step, loss=0.07575, avg_loss=0.07401]\n",
      "Step 524493  [5.505 sec/step, loss=0.07584, avg_loss=0.07404]\n",
      "Step 524494  [5.510 sec/step, loss=0.07470, avg_loss=0.07404]\n",
      "Step 524495  [5.502 sec/step, loss=0.07545, avg_loss=0.07405]\n",
      "Step 524496  [5.481 sec/step, loss=0.07153, avg_loss=0.07402]\n",
      "Step 524497  [5.486 sec/step, loss=0.07397, avg_loss=0.07401]\n",
      "Step 524498  [5.490 sec/step, loss=0.07286, avg_loss=0.07400]\n",
      "Step 524499  [5.494 sec/step, loss=0.07431, avg_loss=0.07402]\n",
      "Step 524500  [5.472 sec/step, loss=0.07211, avg_loss=0.07397]\n",
      "Writing summary at step: 524500\n",
      "Step 524501  [5.458 sec/step, loss=0.07529, avg_loss=0.07398]\n",
      "Step 524502  [5.454 sec/step, loss=0.07415, avg_loss=0.07398]\n",
      "Step 524503  [5.456 sec/step, loss=0.06626, avg_loss=0.07398]\n",
      "Step 524504  [5.409 sec/step, loss=0.07441, avg_loss=0.07406]\n",
      "Step 524505  [5.419 sec/step, loss=0.07565, avg_loss=0.07408]\n",
      "Step 524506  [5.405 sec/step, loss=0.07186, avg_loss=0.07405]\n",
      "Step 524507  [5.435 sec/step, loss=0.07334, avg_loss=0.07403]\n",
      "Step 524508  [5.435 sec/step, loss=0.06627, avg_loss=0.07402]\n",
      "Step 524509  [5.471 sec/step, loss=0.07328, avg_loss=0.07409]\n",
      "Step 524510  [5.465 sec/step, loss=0.07346, avg_loss=0.07407]\n",
      "Step 524511  [5.467 sec/step, loss=0.07610, avg_loss=0.07408]\n",
      "Step 524512  [5.489 sec/step, loss=0.07624, avg_loss=0.07413]\n",
      "Step 524513  [5.501 sec/step, loss=0.07697, avg_loss=0.07415]\n",
      "Step 524514  [5.469 sec/step, loss=0.07381, avg_loss=0.07414]\n",
      "Step 524515  [5.449 sec/step, loss=0.07402, avg_loss=0.07412]\n",
      "Step 524516  [5.427 sec/step, loss=0.07212, avg_loss=0.07408]\n",
      "Step 524517  [5.436 sec/step, loss=0.07193, avg_loss=0.07407]\n",
      "Step 524518  [5.446 sec/step, loss=0.07650, avg_loss=0.07408]\n",
      "Step 524519  [5.461 sec/step, loss=0.07682, avg_loss=0.07410]\n",
      "Step 524520  [5.460 sec/step, loss=0.07502, avg_loss=0.07409]\n",
      "Step 524521  [5.457 sec/step, loss=0.07491, avg_loss=0.07408]\n",
      "Step 524522  [5.446 sec/step, loss=0.07408, avg_loss=0.07408]\n",
      "Generated 32 batches of size 32 in 2.579 sec\n",
      "Step 524523  [5.439 sec/step, loss=0.07277, avg_loss=0.07405]\n",
      "Step 524524  [5.446 sec/step, loss=0.07537, avg_loss=0.07405]\n",
      "Step 524525  [5.426 sec/step, loss=0.07295, avg_loss=0.07403]\n",
      "Step 524526  [5.428 sec/step, loss=0.07464, avg_loss=0.07405]\n",
      "Step 524527  [5.440 sec/step, loss=0.07530, avg_loss=0.07407]\n",
      "Step 524528  [5.460 sec/step, loss=0.07499, avg_loss=0.07410]\n",
      "Step 524529  [5.458 sec/step, loss=0.07537, avg_loss=0.07410]\n",
      "Step 524530  [5.455 sec/step, loss=0.07155, avg_loss=0.07406]\n",
      "Step 524531  [5.472 sec/step, loss=0.07663, avg_loss=0.07409]\n",
      "Step 524532  [5.488 sec/step, loss=0.07630, avg_loss=0.07410]\n",
      "Step 524533  [5.475 sec/step, loss=0.07324, avg_loss=0.07408]\n",
      "Step 524534  [5.461 sec/step, loss=0.07289, avg_loss=0.07405]\n",
      "Step 524535  [5.459 sec/step, loss=0.07446, avg_loss=0.07404]\n",
      "Step 524536  [5.441 sec/step, loss=0.07406, avg_loss=0.07401]\n",
      "Step 524537  [5.455 sec/step, loss=0.07357, avg_loss=0.07398]\n",
      "Step 524538  [5.441 sec/step, loss=0.07120, avg_loss=0.07395]\n",
      "Step 524539  [5.446 sec/step, loss=0.07528, avg_loss=0.07394]\n",
      "Step 524540  [5.452 sec/step, loss=0.07491, avg_loss=0.07393]\n",
      "Step 524541  [5.464 sec/step, loss=0.07560, avg_loss=0.07397]\n",
      "Step 524542  [5.447 sec/step, loss=0.07527, avg_loss=0.07399]\n",
      "Step 524543  [5.419 sec/step, loss=0.07223, avg_loss=0.07397]\n",
      "Step 524544  [5.413 sec/step, loss=0.07398, avg_loss=0.07399]\n",
      "Step 524545  [5.416 sec/step, loss=0.07466, avg_loss=0.07398]\n",
      "Step 524546  [5.390 sec/step, loss=0.07570, avg_loss=0.07400]\n",
      "Step 524547  [5.389 sec/step, loss=0.07625, avg_loss=0.07400]\n",
      "Step 524548  [5.388 sec/step, loss=0.07493, avg_loss=0.07401]\n",
      "Step 524549  [5.391 sec/step, loss=0.07462, avg_loss=0.07401]\n",
      "Step 524550  [5.432 sec/step, loss=0.06673, avg_loss=0.07393]\n",
      "Step 524551  [5.447 sec/step, loss=0.07636, avg_loss=0.07394]\n",
      "Step 524552  [5.448 sec/step, loss=0.07554, avg_loss=0.07395]\n",
      "Step 524553  [5.443 sec/step, loss=0.07504, avg_loss=0.07395]\n",
      "Step 524554  [5.459 sec/step, loss=0.07524, avg_loss=0.07399]\n",
      "Generated 32 batches of size 32 in 2.465 sec\n",
      "Step 524555  [5.463 sec/step, loss=0.07571, avg_loss=0.07399]\n",
      "Step 524556  [5.453 sec/step, loss=0.07371, avg_loss=0.07396]\n",
      "Step 524557  [5.446 sec/step, loss=0.07453, avg_loss=0.07395]\n",
      "Step 524558  [5.455 sec/step, loss=0.07415, avg_loss=0.07394]\n",
      "Step 524559  [5.465 sec/step, loss=0.07149, avg_loss=0.07391]\n",
      "Step 524560  [5.467 sec/step, loss=0.07607, avg_loss=0.07395]\n",
      "Step 524561  [5.489 sec/step, loss=0.07596, avg_loss=0.07396]\n",
      "Step 524562  [5.476 sec/step, loss=0.06611, avg_loss=0.07389]\n",
      "Step 524563  [5.460 sec/step, loss=0.07547, avg_loss=0.07389]\n",
      "Step 524564  [5.468 sec/step, loss=0.07480, avg_loss=0.07390]\n",
      "Step 524565  [5.482 sec/step, loss=0.07306, avg_loss=0.07398]\n",
      "Step 524566  [5.422 sec/step, loss=0.07457, avg_loss=0.07406]\n",
      "Step 524567  [5.426 sec/step, loss=0.07503, avg_loss=0.07406]\n",
      "Step 524568  [5.434 sec/step, loss=0.07409, avg_loss=0.07406]\n",
      "Step 524569  [5.447 sec/step, loss=0.07565, avg_loss=0.07408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 524570  [5.421 sec/step, loss=0.07074, avg_loss=0.07403]\n",
      "Step 524571  [5.402 sec/step, loss=0.07160, avg_loss=0.07400]\n",
      "Step 524572  [5.416 sec/step, loss=0.07455, avg_loss=0.07403]\n",
      "Step 524573  [5.417 sec/step, loss=0.07372, avg_loss=0.07403]\n",
      "Step 524574  [5.374 sec/step, loss=0.06467, avg_loss=0.07395]\n",
      "Step 524575  [5.370 sec/step, loss=0.07425, avg_loss=0.07392]\n",
      "Step 524576  [5.372 sec/step, loss=0.07552, avg_loss=0.07392]\n",
      "Step 524577  [5.365 sec/step, loss=0.07586, avg_loss=0.07392]\n",
      "Step 524578  [5.377 sec/step, loss=0.07199, avg_loss=0.07390]\n",
      "Step 524579  [5.418 sec/step, loss=0.06606, avg_loss=0.07381]\n",
      "Step 524580  [5.416 sec/step, loss=0.07585, avg_loss=0.07381]\n",
      "Step 524581  [5.383 sec/step, loss=0.07514, avg_loss=0.07390]\n",
      "Step 524582  [5.374 sec/step, loss=0.07536, avg_loss=0.07391]\n",
      "Step 524583  [5.371 sec/step, loss=0.07567, avg_loss=0.07391]\n",
      "Step 524584  [5.375 sec/step, loss=0.07639, avg_loss=0.07391]\n",
      "Step 524585  [5.362 sec/step, loss=0.07492, avg_loss=0.07391]\n",
      "Step 524586  [5.347 sec/step, loss=0.07398, avg_loss=0.07389]\n",
      "Generated 32 batches of size 32 in 2.425 sec\n",
      "Step 524587  [5.365 sec/step, loss=0.07582, avg_loss=0.07390]\n",
      "Step 524588  [5.376 sec/step, loss=0.07638, avg_loss=0.07391]\n",
      "Step 524589  [5.385 sec/step, loss=0.07649, avg_loss=0.07394]\n",
      "Step 524590  [5.411 sec/step, loss=0.07644, avg_loss=0.07404]\n",
      "Step 524591  [5.398 sec/step, loss=0.07075, avg_loss=0.07399]\n",
      "Step 524592  [5.389 sec/step, loss=0.07451, avg_loss=0.07398]\n",
      "Step 524593  [5.374 sec/step, loss=0.07419, avg_loss=0.07396]\n",
      "Step 524594  [5.398 sec/step, loss=0.07521, avg_loss=0.07397]\n",
      "Step 524595  [5.411 sec/step, loss=0.07596, avg_loss=0.07397]\n",
      "Step 524596  [5.425 sec/step, loss=0.07610, avg_loss=0.07402]\n",
      "Step 524597  [5.415 sec/step, loss=0.07493, avg_loss=0.07403]\n",
      "Step 524598  [5.423 sec/step, loss=0.07624, avg_loss=0.07406]\n",
      "Step 524599  [5.414 sec/step, loss=0.06815, avg_loss=0.07400]\n",
      "Step 524600  [5.442 sec/step, loss=0.07596, avg_loss=0.07404]\n",
      "Writing summary at step: 524600\n",
      "Step 524601  [5.452 sec/step, loss=0.07522, avg_loss=0.07404]\n",
      "Step 524602  [5.462 sec/step, loss=0.07528, avg_loss=0.07405]\n",
      "Step 524603  [5.467 sec/step, loss=0.07110, avg_loss=0.07410]\n",
      "Step 524604  [5.466 sec/step, loss=0.07567, avg_loss=0.07411]\n",
      "Step 524605  [5.457 sec/step, loss=0.07405, avg_loss=0.07409]\n",
      "Step 524606  [5.470 sec/step, loss=0.07526, avg_loss=0.07413]\n",
      "Step 524607  [5.443 sec/step, loss=0.07481, avg_loss=0.07414]\n",
      "Step 524608  [5.390 sec/step, loss=0.07576, avg_loss=0.07424]\n",
      "Step 524609  [5.375 sec/step, loss=0.07477, avg_loss=0.07425]\n",
      "Step 524610  [5.377 sec/step, loss=0.07068, avg_loss=0.07423]\n",
      "Step 524611  [5.388 sec/step, loss=0.07344, avg_loss=0.07420]\n",
      "Step 524612  [5.388 sec/step, loss=0.07608, avg_loss=0.07420]\n",
      "Step 524613  [5.361 sec/step, loss=0.07419, avg_loss=0.07417]\n",
      "Step 524614  [5.379 sec/step, loss=0.07438, avg_loss=0.07417]\n",
      "Step 524615  [5.406 sec/step, loss=0.07302, avg_loss=0.07416]\n",
      "Step 524616  [5.405 sec/step, loss=0.07194, avg_loss=0.07416]\n",
      "Step 524617  [5.428 sec/step, loss=0.07355, avg_loss=0.07418]\n",
      "Generated 32 batches of size 32 in 2.475 sec\n",
      "Step 524618  [5.429 sec/step, loss=0.07527, avg_loss=0.07417]\n",
      "Step 524619  [5.409 sec/step, loss=0.07426, avg_loss=0.07414]\n",
      "Step 524620  [5.400 sec/step, loss=0.07354, avg_loss=0.07413]\n",
      "Step 524621  [5.395 sec/step, loss=0.07394, avg_loss=0.07412]\n",
      "Step 524622  [5.398 sec/step, loss=0.07393, avg_loss=0.07412]\n",
      "Step 524623  [5.450 sec/step, loss=0.06608, avg_loss=0.07405]\n",
      "Step 524624  [5.447 sec/step, loss=0.07242, avg_loss=0.07402]\n",
      "Step 524625  [5.445 sec/step, loss=0.07503, avg_loss=0.07404]\n",
      "Step 524626  [5.446 sec/step, loss=0.07356, avg_loss=0.07403]\n",
      "Step 524627  [5.442 sec/step, loss=0.07554, avg_loss=0.07403]\n",
      "Step 524628  [5.438 sec/step, loss=0.07666, avg_loss=0.07405]\n",
      "Step 524629  [5.433 sec/step, loss=0.07555, avg_loss=0.07405]\n",
      "Step 524630  [5.421 sec/step, loss=0.07160, avg_loss=0.07405]\n",
      "Step 524631  [5.404 sec/step, loss=0.07401, avg_loss=0.07402]\n",
      "Step 524632  [5.380 sec/step, loss=0.07404, avg_loss=0.07400]\n",
      "Step 524633  [5.389 sec/step, loss=0.07489, avg_loss=0.07402]\n",
      "Step 524634  [5.376 sec/step, loss=0.06637, avg_loss=0.07395]\n",
      "Step 524635  [5.384 sec/step, loss=0.07642, avg_loss=0.07397]\n",
      "Step 524636  [5.443 sec/step, loss=0.06724, avg_loss=0.07390]\n",
      "Step 524637  [5.410 sec/step, loss=0.07296, avg_loss=0.07390]\n",
      "Step 524638  [5.423 sec/step, loss=0.07333, avg_loss=0.07392]\n",
      "Step 524639  [5.409 sec/step, loss=0.07338, avg_loss=0.07390]\n",
      "Step 524640  [5.391 sec/step, loss=0.07581, avg_loss=0.07391]\n",
      "Step 524641  [5.401 sec/step, loss=0.07561, avg_loss=0.07391]\n",
      "Step 524642  [5.395 sec/step, loss=0.07397, avg_loss=0.07390]\n",
      "Step 524643  [5.408 sec/step, loss=0.07428, avg_loss=0.07392]\n",
      "Step 524644  [5.416 sec/step, loss=0.07570, avg_loss=0.07393]\n",
      "Step 524645  [5.439 sec/step, loss=0.07374, avg_loss=0.07393]\n",
      "Step 524646  [5.438 sec/step, loss=0.07410, avg_loss=0.07391]\n",
      "Step 524647  [5.417 sec/step, loss=0.07259, avg_loss=0.07387]\n",
      "Step 524648  [5.431 sec/step, loss=0.07629, avg_loss=0.07389]\n",
      "Step 524649  [5.440 sec/step, loss=0.07644, avg_loss=0.07390]\n",
      "Generated 32 batches of size 32 in 2.412 sec\n",
      "Step 524650  [5.407 sec/step, loss=0.07425, avg_loss=0.07398]\n",
      "Step 524651  [5.406 sec/step, loss=0.07445, avg_loss=0.07396]\n",
      "Step 524652  [5.403 sec/step, loss=0.07711, avg_loss=0.07398]\n",
      "Step 524653  [5.406 sec/step, loss=0.07564, avg_loss=0.07398]\n",
      "Step 524654  [5.400 sec/step, loss=0.07083, avg_loss=0.07394]\n",
      "Step 524655  [5.399 sec/step, loss=0.07617, avg_loss=0.07394]\n",
      "Step 524656  [5.400 sec/step, loss=0.07321, avg_loss=0.07394]\n",
      "Step 524657  [5.416 sec/step, loss=0.07511, avg_loss=0.07394]\n",
      "Step 524658  [5.425 sec/step, loss=0.07395, avg_loss=0.07394]\n",
      "Step 524659  [5.435 sec/step, loss=0.07427, avg_loss=0.07397]\n",
      "Step 524660  [5.426 sec/step, loss=0.07465, avg_loss=0.07396]\n",
      "Step 524661  [5.411 sec/step, loss=0.07439, avg_loss=0.07394]\n",
      "Step 524662  [5.439 sec/step, loss=0.07291, avg_loss=0.07401]\n",
      "Step 524663  [5.438 sec/step, loss=0.07394, avg_loss=0.07399]\n",
      "Step 524664  [5.445 sec/step, loss=0.07583, avg_loss=0.07400]\n",
      "Step 524665  [5.439 sec/step, loss=0.07436, avg_loss=0.07402]\n",
      "Step 524666  [5.476 sec/step, loss=0.07316, avg_loss=0.07400]\n",
      "Step 524667  [5.456 sec/step, loss=0.07133, avg_loss=0.07396]\n",
      "Step 524668  [5.506 sec/step, loss=0.06599, avg_loss=0.07388]\n",
      "Step 524669  [5.487 sec/step, loss=0.07226, avg_loss=0.07385]\n",
      "Step 524670  [5.487 sec/step, loss=0.07096, avg_loss=0.07385]\n",
      "Step 524671  [5.503 sec/step, loss=0.07581, avg_loss=0.07389]\n",
      "Step 524672  [5.519 sec/step, loss=0.07296, avg_loss=0.07388]\n",
      "Step 524673  [5.532 sec/step, loss=0.07588, avg_loss=0.07390]\n",
      "Step 524674  [5.546 sec/step, loss=0.07535, avg_loss=0.07401]\n",
      "Step 524675  [5.536 sec/step, loss=0.07515, avg_loss=0.07402]\n",
      "Step 524676  [5.531 sec/step, loss=0.07464, avg_loss=0.07401]\n",
      "Step 524677  [5.531 sec/step, loss=0.07521, avg_loss=0.07400]\n",
      "Step 524678  [5.547 sec/step, loss=0.07599, avg_loss=0.07404]\n",
      "Step 524679  [5.498 sec/step, loss=0.07561, avg_loss=0.07414]\n",
      "Step 524680  [5.491 sec/step, loss=0.07258, avg_loss=0.07410]\n",
      "Step 524681  [5.479 sec/step, loss=0.07418, avg_loss=0.07409]\n",
      "Generated 32 batches of size 32 in 2.424 sec\n",
      "Step 524682  [5.499 sec/step, loss=0.07619, avg_loss=0.07410]\n",
      "Step 524683  [5.503 sec/step, loss=0.07373, avg_loss=0.07408]\n",
      "Step 524684  [5.488 sec/step, loss=0.07241, avg_loss=0.07404]\n",
      "Step 524685  [5.485 sec/step, loss=0.07460, avg_loss=0.07404]\n",
      "Step 524686  [5.495 sec/step, loss=0.07634, avg_loss=0.07406]\n",
      "Step 524687  [5.479 sec/step, loss=0.07549, avg_loss=0.07406]\n",
      "Step 524688  [5.476 sec/step, loss=0.07375, avg_loss=0.07403]\n",
      "Step 524689  [5.452 sec/step, loss=0.06502, avg_loss=0.07392]\n",
      "Step 524690  [5.450 sec/step, loss=0.07677, avg_loss=0.07392]\n",
      "Step 524691  [5.472 sec/step, loss=0.07456, avg_loss=0.07396]\n",
      "Step 524692  [5.473 sec/step, loss=0.07488, avg_loss=0.07396]\n",
      "Step 524693  [5.473 sec/step, loss=0.07381, avg_loss=0.07396]\n",
      "Step 524694  [5.463 sec/step, loss=0.07631, avg_loss=0.07397]\n",
      "Step 524695  [5.506 sec/step, loss=0.06666, avg_loss=0.07388]\n",
      "Step 524696  [5.477 sec/step, loss=0.06643, avg_loss=0.07378]\n",
      "Step 524697  [5.479 sec/step, loss=0.07518, avg_loss=0.07378]\n",
      "Step 524698  [5.459 sec/step, loss=0.07179, avg_loss=0.07374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 524699  [5.466 sec/step, loss=0.07105, avg_loss=0.07377]\n",
      "Step 524700  [5.448 sec/step, loss=0.07423, avg_loss=0.07375]\n",
      "Writing summary at step: 524700\n",
      "Step 524701  [5.446 sec/step, loss=0.07496, avg_loss=0.07375]\n",
      "Step 524702  [5.437 sec/step, loss=0.07533, avg_loss=0.07375]\n",
      "Step 524703  [5.450 sec/step, loss=0.07573, avg_loss=0.07379]\n",
      "Step 524704  [5.452 sec/step, loss=0.07549, avg_loss=0.07379]\n",
      "Step 524705  [5.484 sec/step, loss=0.07317, avg_loss=0.07378]\n",
      "Step 524706  [5.479 sec/step, loss=0.07531, avg_loss=0.07378]\n",
      "Step 524707  [5.471 sec/step, loss=0.07458, avg_loss=0.07378]\n",
      "Step 524708  [5.468 sec/step, loss=0.07315, avg_loss=0.07376]\n",
      "Step 524709  [5.474 sec/step, loss=0.07644, avg_loss=0.07377]\n",
      "Step 524710  [5.481 sec/step, loss=0.07688, avg_loss=0.07384]\n",
      "Step 524711  [5.477 sec/step, loss=0.07651, avg_loss=0.07387]\n",
      "Step 524712  [5.468 sec/step, loss=0.07299, avg_loss=0.07384]\n",
      "Generated 32 batches of size 32 in 2.518 sec\n",
      "Step 524713  [5.486 sec/step, loss=0.07551, avg_loss=0.07385]\n",
      "Step 524714  [5.488 sec/step, loss=0.07622, avg_loss=0.07387]\n",
      "Step 524715  [5.482 sec/step, loss=0.07395, avg_loss=0.07388]\n",
      "Step 524716  [5.495 sec/step, loss=0.07497, avg_loss=0.07391]\n",
      "Step 524717  [5.469 sec/step, loss=0.07419, avg_loss=0.07391]\n",
      "Step 524718  [5.456 sec/step, loss=0.07253, avg_loss=0.07389]\n",
      "Step 524719  [5.475 sec/step, loss=0.07622, avg_loss=0.07390]\n",
      "Step 524720  [5.459 sec/step, loss=0.07204, avg_loss=0.07389]\n",
      "Step 524721  [5.478 sec/step, loss=0.07320, avg_loss=0.07388]\n",
      "Step 524722  [5.497 sec/step, loss=0.07645, avg_loss=0.07391]\n",
      "Step 524723  [5.450 sec/step, loss=0.07470, avg_loss=0.07399]\n",
      "Step 524724  [5.451 sec/step, loss=0.07523, avg_loss=0.07402]\n",
      "Step 524725  [5.503 sec/step, loss=0.06686, avg_loss=0.07394]\n",
      "Step 524726  [5.499 sec/step, loss=0.07341, avg_loss=0.07394]\n",
      "Step 524727  [5.493 sec/step, loss=0.07442, avg_loss=0.07393]\n",
      "Step 524728  [5.496 sec/step, loss=0.07507, avg_loss=0.07391]\n",
      "Step 524729  [5.503 sec/step, loss=0.07404, avg_loss=0.07390]\n",
      "Step 524730  [5.530 sec/step, loss=0.07623, avg_loss=0.07394]\n",
      "Step 524731  [5.558 sec/step, loss=0.07486, avg_loss=0.07395]\n",
      "Step 524732  [5.574 sec/step, loss=0.07635, avg_loss=0.07397]\n",
      "Step 524733  [5.580 sec/step, loss=0.07643, avg_loss=0.07399]\n",
      "Step 524734  [5.599 sec/step, loss=0.07451, avg_loss=0.07407]\n",
      "Step 524735  [5.601 sec/step, loss=0.07646, avg_loss=0.07407]\n",
      "Step 524736  [5.536 sec/step, loss=0.07163, avg_loss=0.07412]\n",
      "Step 524737  [5.538 sec/step, loss=0.07132, avg_loss=0.07410]\n",
      "Step 524738  [5.529 sec/step, loss=0.07371, avg_loss=0.07410]\n",
      "Step 524739  [5.546 sec/step, loss=0.07369, avg_loss=0.07411]\n",
      "Step 524740  [5.547 sec/step, loss=0.07524, avg_loss=0.07410]\n",
      "Step 524741  [5.541 sec/step, loss=0.07568, avg_loss=0.07410]\n",
      "Step 524742  [5.550 sec/step, loss=0.07535, avg_loss=0.07411]\n",
      "Step 524743  [5.540 sec/step, loss=0.07119, avg_loss=0.07408]\n",
      "Step 524744  [5.540 sec/step, loss=0.07463, avg_loss=0.07407]\n",
      "Generated 32 batches of size 32 in 2.590 sec\n",
      "Step 524745  [5.518 sec/step, loss=0.07366, avg_loss=0.07407]\n",
      "Step 524746  [5.527 sec/step, loss=0.07446, avg_loss=0.07408]\n",
      "Step 524747  [5.531 sec/step, loss=0.07446, avg_loss=0.07409]\n",
      "Step 524748  [5.519 sec/step, loss=0.07535, avg_loss=0.07409]\n",
      "Step 524749  [5.533 sec/step, loss=0.07292, avg_loss=0.07405]\n",
      "Step 524750  [5.513 sec/step, loss=0.07274, avg_loss=0.07403]\n",
      "Step 524751  [5.495 sec/step, loss=0.07414, avg_loss=0.07403]\n",
      "Step 524752  [5.494 sec/step, loss=0.07560, avg_loss=0.07402]\n",
      "Step 524753  [5.473 sec/step, loss=0.06606, avg_loss=0.07392]\n",
      "Step 524754  [5.488 sec/step, loss=0.07449, avg_loss=0.07396]\n",
      "Step 524755  [5.474 sec/step, loss=0.07189, avg_loss=0.07391]\n",
      "Step 524756  [5.521 sec/step, loss=0.06642, avg_loss=0.07385]\n",
      "Step 524757  [5.524 sec/step, loss=0.07522, avg_loss=0.07385]\n",
      "Step 524758  [5.505 sec/step, loss=0.07533, avg_loss=0.07386]\n",
      "Step 524759  [5.505 sec/step, loss=0.07540, avg_loss=0.07387]\n",
      "Step 524760  [5.494 sec/step, loss=0.07432, avg_loss=0.07387]\n",
      "Step 524761  [5.495 sec/step, loss=0.07543, avg_loss=0.07388]\n",
      "Step 524762  [5.494 sec/step, loss=0.07602, avg_loss=0.07391]\n",
      "Step 524763  [5.508 sec/step, loss=0.07612, avg_loss=0.07393]\n",
      "Step 524764  [5.499 sec/step, loss=0.07579, avg_loss=0.07393]\n",
      "Step 524765  [5.493 sec/step, loss=0.06702, avg_loss=0.07386]\n",
      "Step 524766  [5.470 sec/step, loss=0.07513, avg_loss=0.07388]\n",
      "Step 524767  [5.474 sec/step, loss=0.07363, avg_loss=0.07390]\n",
      "Step 524768  [5.428 sec/step, loss=0.07517, avg_loss=0.07399]\n",
      "Step 524769  [5.431 sec/step, loss=0.07435, avg_loss=0.07401]\n",
      "Step 524770  [5.471 sec/step, loss=0.07345, avg_loss=0.07404]\n",
      "Step 524771  [5.456 sec/step, loss=0.07516, avg_loss=0.07403]\n",
      "Step 524772  [5.448 sec/step, loss=0.07477, avg_loss=0.07405]\n",
      "Step 524773  [5.439 sec/step, loss=0.07561, avg_loss=0.07405]\n",
      "Step 524774  [5.438 sec/step, loss=0.07328, avg_loss=0.07403]\n",
      "Step 524775  [5.443 sec/step, loss=0.07463, avg_loss=0.07402]\n",
      "Step 524776  [5.450 sec/step, loss=0.07404, avg_loss=0.07402]\n",
      "Generated 32 batches of size 32 in 2.422 sec\n",
      "Step 524777  [5.465 sec/step, loss=0.07617, avg_loss=0.07403]\n",
      "Step 524778  [5.444 sec/step, loss=0.07350, avg_loss=0.07400]\n",
      "Step 524779  [5.448 sec/step, loss=0.07444, avg_loss=0.07399]\n",
      "Step 524780  [5.469 sec/step, loss=0.07682, avg_loss=0.07403]\n",
      "Step 524781  [5.450 sec/step, loss=0.07318, avg_loss=0.07402]\n",
      "Step 524782  [5.424 sec/step, loss=0.07150, avg_loss=0.07397]\n",
      "Step 524783  [5.425 sec/step, loss=0.07646, avg_loss=0.07400]\n",
      "Step 524784  [5.448 sec/step, loss=0.07291, avg_loss=0.07401]\n",
      "Step 524785  [5.447 sec/step, loss=0.07308, avg_loss=0.07399]\n",
      "Step 524786  [5.438 sec/step, loss=0.07288, avg_loss=0.07396]\n",
      "Step 524787  [5.454 sec/step, loss=0.07546, avg_loss=0.07396]\n",
      "Step 524788  [5.454 sec/step, loss=0.07584, avg_loss=0.07398]\n",
      "Step 524789  [5.475 sec/step, loss=0.07453, avg_loss=0.07407]\n",
      "Step 524790  [5.463 sec/step, loss=0.07405, avg_loss=0.07405]\n",
      "Step 524791  [5.438 sec/step, loss=0.07038, avg_loss=0.07400]\n",
      "Step 524792  [5.438 sec/step, loss=0.07472, avg_loss=0.07400]\n",
      "Step 524793  [5.449 sec/step, loss=0.07565, avg_loss=0.07402]\n",
      "Step 524794  [5.432 sec/step, loss=0.07509, avg_loss=0.07401]\n",
      "Step 524795  [5.372 sec/step, loss=0.07351, avg_loss=0.07408]\n",
      "Step 524796  [5.373 sec/step, loss=0.06465, avg_loss=0.07406]\n",
      "Step 524797  [5.376 sec/step, loss=0.07629, avg_loss=0.07407]\n",
      "Step 524798  [5.379 sec/step, loss=0.07053, avg_loss=0.07406]\n",
      "Step 524799  [5.378 sec/step, loss=0.07435, avg_loss=0.07409]\n",
      "Step 524800  [5.393 sec/step, loss=0.07590, avg_loss=0.07411]\n",
      "Writing summary at step: 524800\n",
      "Step 524801  [5.389 sec/step, loss=0.07568, avg_loss=0.07411]\n",
      "Step 524802  [5.385 sec/step, loss=0.07056, avg_loss=0.07407]\n",
      "Step 524803  [5.383 sec/step, loss=0.07216, avg_loss=0.07403]\n",
      "Step 524804  [5.385 sec/step, loss=0.07615, avg_loss=0.07404]\n",
      "Step 524805  [5.373 sec/step, loss=0.07574, avg_loss=0.07406]\n",
      "Step 524806  [5.385 sec/step, loss=0.07641, avg_loss=0.07407]\n",
      "Step 524807  [5.398 sec/step, loss=0.07484, avg_loss=0.07408]\n",
      "Generated 32 batches of size 32 in 2.425 sec\n",
      "Step 524808  [5.433 sec/step, loss=0.07533, avg_loss=0.07410]\n",
      "Step 524809  [5.436 sec/step, loss=0.07580, avg_loss=0.07409]\n",
      "Step 524810  [5.429 sec/step, loss=0.07462, avg_loss=0.07407]\n",
      "Step 524811  [5.422 sec/step, loss=0.07590, avg_loss=0.07406]\n",
      "Step 524812  [5.472 sec/step, loss=0.06631, avg_loss=0.07400]\n",
      "Step 524813  [5.457 sec/step, loss=0.07410, avg_loss=0.07398]\n",
      "Step 524814  [5.441 sec/step, loss=0.07576, avg_loss=0.07398]\n",
      "Step 524815  [5.431 sec/step, loss=0.07429, avg_loss=0.07398]\n",
      "Step 524816  [5.444 sec/step, loss=0.07449, avg_loss=0.07398]\n",
      "Step 524817  [5.456 sec/step, loss=0.07591, avg_loss=0.07399]\n",
      "Step 524818  [5.465 sec/step, loss=0.07485, avg_loss=0.07402]\n",
      "Step 524819  [5.455 sec/step, loss=0.07505, avg_loss=0.07401]\n",
      "Step 524820  [5.467 sec/step, loss=0.07514, avg_loss=0.07404]\n",
      "Step 524821  [5.458 sec/step, loss=0.07428, avg_loss=0.07405]\n",
      "Step 524822  [5.446 sec/step, loss=0.07526, avg_loss=0.07404]\n",
      "Step 524823  [5.443 sec/step, loss=0.07494, avg_loss=0.07404]\n",
      "Step 524824  [5.445 sec/step, loss=0.07185, avg_loss=0.07400]\n",
      "Step 524825  [5.393 sec/step, loss=0.07528, avg_loss=0.07409]\n",
      "Step 524826  [5.421 sec/step, loss=0.07442, avg_loss=0.07410]\n",
      "Step 524827  [5.433 sec/step, loss=0.07680, avg_loss=0.07412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 524828  [5.416 sec/step, loss=0.07413, avg_loss=0.07411]\n",
      "Step 524829  [5.411 sec/step, loss=0.07300, avg_loss=0.07410]\n",
      "Step 524830  [5.400 sec/step, loss=0.07542, avg_loss=0.07409]\n",
      "Step 524831  [5.378 sec/step, loss=0.07393, avg_loss=0.07408]\n",
      "Step 524832  [5.373 sec/step, loss=0.07484, avg_loss=0.07407]\n",
      "Step 524833  [5.373 sec/step, loss=0.07584, avg_loss=0.07406]\n",
      "Step 524834  [5.360 sec/step, loss=0.07433, avg_loss=0.07406]\n",
      "Step 524835  [5.359 sec/step, loss=0.07352, avg_loss=0.07403]\n",
      "Step 524836  [5.375 sec/step, loss=0.07589, avg_loss=0.07408]\n",
      "Step 524837  [5.391 sec/step, loss=0.07562, avg_loss=0.07412]\n",
      "Step 524838  [5.383 sec/step, loss=0.06693, avg_loss=0.07405]\n",
      "Step 524839  [5.380 sec/step, loss=0.07596, avg_loss=0.07407]\n",
      "Generated 32 batches of size 32 in 2.773 sec\n",
      "Step 524840  [5.372 sec/step, loss=0.07299, avg_loss=0.07405]\n",
      "Step 524841  [5.370 sec/step, loss=0.07430, avg_loss=0.07404]\n",
      "Step 524842  [5.416 sec/step, loss=0.06576, avg_loss=0.07394]\n",
      "Step 524843  [5.431 sec/step, loss=0.07511, avg_loss=0.07398]\n",
      "Step 524844  [5.424 sec/step, loss=0.07369, avg_loss=0.07397]\n",
      "Step 524845  [5.416 sec/step, loss=0.07278, avg_loss=0.07396]\n",
      "Step 524846  [5.426 sec/step, loss=0.07548, avg_loss=0.07397]\n",
      "Step 524847  [5.425 sec/step, loss=0.07113, avg_loss=0.07394]\n",
      "Step 524848  [5.432 sec/step, loss=0.07648, avg_loss=0.07395]\n",
      "Step 524849  [5.402 sec/step, loss=0.07299, avg_loss=0.07395]\n",
      "Step 524850  [5.420 sec/step, loss=0.07443, avg_loss=0.07397]\n",
      "Step 524851  [5.419 sec/step, loss=0.07416, avg_loss=0.07397]\n",
      "Step 524852  [5.434 sec/step, loss=0.07458, avg_loss=0.07396]\n",
      "Step 524853  [5.459 sec/step, loss=0.07604, avg_loss=0.07406]\n",
      "Step 524854  [5.453 sec/step, loss=0.07647, avg_loss=0.07408]\n",
      "Step 524855  [5.454 sec/step, loss=0.07547, avg_loss=0.07411]\n",
      "Step 524856  [5.399 sec/step, loss=0.07366, avg_loss=0.07419]\n",
      "Step 524857  [5.383 sec/step, loss=0.07065, avg_loss=0.07414]\n",
      "Step 524858  [5.408 sec/step, loss=0.07393, avg_loss=0.07413]\n",
      "Step 524859  [5.403 sec/step, loss=0.07194, avg_loss=0.07409]\n",
      "Step 524860  [5.404 sec/step, loss=0.07190, avg_loss=0.07407]\n",
      "Step 524861  [5.414 sec/step, loss=0.07640, avg_loss=0.07408]\n",
      "Step 524862  [5.416 sec/step, loss=0.07507, avg_loss=0.07407]\n",
      "Step 524863  [5.411 sec/step, loss=0.07537, avg_loss=0.07406]\n",
      "Step 524864  [5.393 sec/step, loss=0.06533, avg_loss=0.07396]\n",
      "Step 524865  [5.424 sec/step, loss=0.07667, avg_loss=0.07405]\n",
      "Step 524866  [5.420 sec/step, loss=0.07468, avg_loss=0.07405]\n",
      "Step 524867  [5.420 sec/step, loss=0.07433, avg_loss=0.07405]\n",
      "Step 524868  [5.416 sec/step, loss=0.07479, avg_loss=0.07405]\n",
      "Step 524869  [5.432 sec/step, loss=0.07612, avg_loss=0.07407]\n",
      "Step 524870  [5.404 sec/step, loss=0.07180, avg_loss=0.07405]\n",
      "Step 524871  [5.392 sec/step, loss=0.07195, avg_loss=0.07402]\n",
      "Generated 32 batches of size 32 in 2.427 sec\n",
      "Step 524872  [5.390 sec/step, loss=0.07602, avg_loss=0.07403]\n",
      "Step 524873  [5.445 sec/step, loss=0.06712, avg_loss=0.07395]\n",
      "Step 524874  [5.462 sec/step, loss=0.07637, avg_loss=0.07398]\n",
      "Step 524875  [5.466 sec/step, loss=0.07491, avg_loss=0.07398]\n",
      "Step 524876  [5.468 sec/step, loss=0.07578, avg_loss=0.07400]\n",
      "Step 524877  [5.447 sec/step, loss=0.07251, avg_loss=0.07396]\n",
      "Step 524878  [5.461 sec/step, loss=0.07658, avg_loss=0.07399]\n",
      "Step 524879  [5.458 sec/step, loss=0.07560, avg_loss=0.07400]\n",
      "Step 524880  [5.444 sec/step, loss=0.07224, avg_loss=0.07396]\n",
      "Step 524881  [5.473 sec/step, loss=0.07467, avg_loss=0.07397]\n",
      "Step 524882  [5.480 sec/step, loss=0.07527, avg_loss=0.07401]\n",
      "Step 524883  [5.475 sec/step, loss=0.07442, avg_loss=0.07399]\n",
      "Step 524884  [5.452 sec/step, loss=0.07273, avg_loss=0.07399]\n",
      "Step 524885  [5.456 sec/step, loss=0.07467, avg_loss=0.07400]\n",
      "Step 524886  [5.445 sec/step, loss=0.07231, avg_loss=0.07400]\n",
      "Step 524887  [5.435 sec/step, loss=0.07636, avg_loss=0.07401]\n",
      "Step 524888  [5.450 sec/step, loss=0.07244, avg_loss=0.07397]\n",
      "Step 524889  [5.443 sec/step, loss=0.07550, avg_loss=0.07398]\n",
      "Step 524890  [5.435 sec/step, loss=0.07114, avg_loss=0.07395]\n",
      "Step 524891  [5.454 sec/step, loss=0.07406, avg_loss=0.07399]\n",
      "Step 524892  [5.446 sec/step, loss=0.07244, avg_loss=0.07397]\n",
      "Step 524893  [5.445 sec/step, loss=0.07625, avg_loss=0.07397]\n",
      "Step 524894  [5.446 sec/step, loss=0.07177, avg_loss=0.07394]\n",
      "Step 524895  [5.455 sec/step, loss=0.07556, avg_loss=0.07396]\n",
      "Step 524896  [5.472 sec/step, loss=0.07514, avg_loss=0.07407]\n",
      "Step 524897  [5.514 sec/step, loss=0.06588, avg_loss=0.07396]\n",
      "Step 524898  [5.534 sec/step, loss=0.07617, avg_loss=0.07402]\n",
      "Step 524899  [5.526 sec/step, loss=0.06597, avg_loss=0.07394]\n",
      "Step 524900  [5.528 sec/step, loss=0.07593, avg_loss=0.07394]\n",
      "Writing summary at step: 524900\n",
      "Step 524901  [5.538 sec/step, loss=0.07418, avg_loss=0.07392]\n",
      "Step 524902  [5.545 sec/step, loss=0.07538, avg_loss=0.07397]\n",
      "Generated 32 batches of size 32 in 2.611 sec\n",
      "Step 524903  [5.545 sec/step, loss=0.07396, avg_loss=0.07399]\n",
      "Step 524904  [5.532 sec/step, loss=0.07414, avg_loss=0.07397]\n",
      "Step 524905  [5.526 sec/step, loss=0.07632, avg_loss=0.07397]\n",
      "Step 524906  [5.537 sec/step, loss=0.07593, avg_loss=0.07397]\n",
      "Step 524907  [5.542 sec/step, loss=0.07556, avg_loss=0.07398]\n",
      "Step 524908  [5.525 sec/step, loss=0.07620, avg_loss=0.07398]\n",
      "Step 524909  [5.518 sec/step, loss=0.07359, avg_loss=0.07396]\n",
      "Step 524910  [5.508 sec/step, loss=0.07429, avg_loss=0.07396]\n",
      "Step 524911  [5.507 sec/step, loss=0.07309, avg_loss=0.07393]\n",
      "Step 524912  [5.451 sec/step, loss=0.07315, avg_loss=0.07400]\n",
      "Step 524913  [5.463 sec/step, loss=0.07511, avg_loss=0.07401]\n",
      "Step 524914  [5.459 sec/step, loss=0.07341, avg_loss=0.07399]\n",
      "Step 524915  [5.455 sec/step, loss=0.07494, avg_loss=0.07399]\n",
      "Step 524916  [5.445 sec/step, loss=0.07508, avg_loss=0.07400]\n",
      "Step 524917  [5.435 sec/step, loss=0.07229, avg_loss=0.07396]\n",
      "Step 524918  [5.432 sec/step, loss=0.07462, avg_loss=0.07396]\n",
      "Step 524919  [5.441 sec/step, loss=0.07438, avg_loss=0.07395]\n",
      "Step 524920  [5.453 sec/step, loss=0.07572, avg_loss=0.07396]\n",
      "Step 524921  [5.439 sec/step, loss=0.07311, avg_loss=0.07395]\n",
      "Step 524922  [5.428 sec/step, loss=0.07154, avg_loss=0.07391]\n",
      "Step 524923  [5.434 sec/step, loss=0.07627, avg_loss=0.07392]\n",
      "Step 524924  [5.448 sec/step, loss=0.07588, avg_loss=0.07396]\n",
      "Step 524925  [5.458 sec/step, loss=0.07633, avg_loss=0.07397]\n",
      "Step 524926  [5.441 sec/step, loss=0.07597, avg_loss=0.07399]\n",
      "Step 524927  [5.449 sec/step, loss=0.07308, avg_loss=0.07395]\n",
      "Step 524928  [5.456 sec/step, loss=0.07427, avg_loss=0.07395]\n",
      "Step 524929  [5.454 sec/step, loss=0.07529, avg_loss=0.07398]\n",
      "Step 524930  [5.444 sec/step, loss=0.07439, avg_loss=0.07397]\n",
      "Step 524931  [5.476 sec/step, loss=0.07337, avg_loss=0.07396]\n",
      "Step 524932  [5.473 sec/step, loss=0.07474, avg_loss=0.07396]\n",
      "Step 524933  [5.468 sec/step, loss=0.07550, avg_loss=0.07396]\n",
      "Step 524934  [5.484 sec/step, loss=0.07543, avg_loss=0.07397]\n",
      "Generated 32 batches of size 32 in 2.889 sec\n",
      "Step 524935  [5.463 sec/step, loss=0.06510, avg_loss=0.07388]\n",
      "Step 524936  [5.477 sec/step, loss=0.07725, avg_loss=0.07390]\n",
      "Step 524937  [5.514 sec/step, loss=0.06725, avg_loss=0.07381]\n",
      "Step 524938  [5.529 sec/step, loss=0.07511, avg_loss=0.07389]\n",
      "Step 524939  [5.510 sec/step, loss=0.07092, avg_loss=0.07384]\n",
      "Step 524940  [5.513 sec/step, loss=0.07420, avg_loss=0.07386]\n",
      "Step 524941  [5.503 sec/step, loss=0.07397, avg_loss=0.07385]\n",
      "Step 524942  [5.465 sec/step, loss=0.07466, avg_loss=0.07394]\n",
      "Step 524943  [5.461 sec/step, loss=0.07434, avg_loss=0.07393]\n",
      "Step 524944  [5.465 sec/step, loss=0.07102, avg_loss=0.07391]\n",
      "Step 524945  [5.457 sec/step, loss=0.07414, avg_loss=0.07392]\n",
      "Step 524946  [5.445 sec/step, loss=0.07581, avg_loss=0.07392]\n",
      "Step 524947  [5.472 sec/step, loss=0.07552, avg_loss=0.07397]\n",
      "Step 524948  [5.462 sec/step, loss=0.07377, avg_loss=0.07394]\n",
      "Step 524949  [5.462 sec/step, loss=0.07499, avg_loss=0.07396]\n",
      "Step 524950  [5.474 sec/step, loss=0.07275, avg_loss=0.07394]\n",
      "Step 524951  [5.492 sec/step, loss=0.07633, avg_loss=0.07397]\n",
      "Step 524952  [5.464 sec/step, loss=0.07297, avg_loss=0.07395]\n",
      "Step 524953  [5.462 sec/step, loss=0.07404, avg_loss=0.07393]\n",
      "Step 524954  [5.451 sec/step, loss=0.07372, avg_loss=0.07390]\n",
      "Step 524955  [5.451 sec/step, loss=0.07554, avg_loss=0.07390]\n",
      "Step 524956  [5.448 sec/step, loss=0.07438, avg_loss=0.07391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 524957  [5.507 sec/step, loss=0.06612, avg_loss=0.07386]\n",
      "Step 524958  [5.492 sec/step, loss=0.07599, avg_loss=0.07389]\n",
      "Step 524959  [5.481 sec/step, loss=0.07367, avg_loss=0.07390]\n",
      "Step 524960  [5.498 sec/step, loss=0.07615, avg_loss=0.07394]\n",
      "Step 524961  [5.491 sec/step, loss=0.07486, avg_loss=0.07393]\n",
      "Step 524962  [5.481 sec/step, loss=0.07553, avg_loss=0.07393]\n",
      "Step 524963  [5.466 sec/step, loss=0.07218, avg_loss=0.07390]\n",
      "Step 524964  [5.468 sec/step, loss=0.07290, avg_loss=0.07398]\n",
      "Step 524965  [5.454 sec/step, loss=0.07503, avg_loss=0.07396]\n",
      "Step 524966  [5.472 sec/step, loss=0.07586, avg_loss=0.07397]\n",
      "Generated 32 batches of size 32 in 2.448 sec\n",
      "Step 524967  [5.492 sec/step, loss=0.07657, avg_loss=0.07400]\n",
      "Step 524968  [5.492 sec/step, loss=0.07485, avg_loss=0.07400]\n",
      "Step 524969  [5.483 sec/step, loss=0.07457, avg_loss=0.07398]\n",
      "Step 524970  [5.496 sec/step, loss=0.07516, avg_loss=0.07401]\n",
      "Step 524971  [5.521 sec/step, loss=0.07314, avg_loss=0.07403]\n",
      "Step 524972  [5.515 sec/step, loss=0.07602, avg_loss=0.07403]\n",
      "Step 524973  [5.475 sec/step, loss=0.07671, avg_loss=0.07412]\n",
      "Step 524974  [5.445 sec/step, loss=0.06745, avg_loss=0.07403]\n",
      "Step 524975  [5.443 sec/step, loss=0.07591, avg_loss=0.07404]\n",
      "Step 524976  [5.424 sec/step, loss=0.06542, avg_loss=0.07394]\n",
      "Step 524977  [5.430 sec/step, loss=0.07539, avg_loss=0.07397]\n",
      "Step 524978  [5.429 sec/step, loss=0.07241, avg_loss=0.07393]\n",
      "Step 524979  [5.420 sec/step, loss=0.07439, avg_loss=0.07391]\n",
      "Step 524980  [5.430 sec/step, loss=0.07453, avg_loss=0.07394]\n",
      "Step 524981  [5.424 sec/step, loss=0.07734, avg_loss=0.07396]\n",
      "Step 524982  [5.443 sec/step, loss=0.07654, avg_loss=0.07398]\n",
      "Step 524983  [5.494 sec/step, loss=0.06746, avg_loss=0.07391]\n",
      "Step 524984  [5.502 sec/step, loss=0.07543, avg_loss=0.07393]\n",
      "Step 524985  [5.522 sec/step, loss=0.07420, avg_loss=0.07393]\n",
      "Step 524986  [5.537 sec/step, loss=0.07534, avg_loss=0.07396]\n",
      "Step 524987  [5.518 sec/step, loss=0.07481, avg_loss=0.07394]\n",
      "Step 524988  [5.490 sec/step, loss=0.07316, avg_loss=0.07395]\n",
      "Step 524989  [5.491 sec/step, loss=0.07243, avg_loss=0.07392]\n",
      "Step 524990  [5.494 sec/step, loss=0.07444, avg_loss=0.07395]\n",
      "Step 524991  [5.495 sec/step, loss=0.07664, avg_loss=0.07398]\n",
      "Step 524992  [5.507 sec/step, loss=0.07583, avg_loss=0.07401]\n",
      "Step 524993  [5.501 sec/step, loss=0.07578, avg_loss=0.07401]\n",
      "Step 524994  [5.501 sec/step, loss=0.07431, avg_loss=0.07403]\n",
      "Step 524995  [5.511 sec/step, loss=0.07437, avg_loss=0.07402]\n",
      "Step 524996  [5.507 sec/step, loss=0.07316, avg_loss=0.07400]\n",
      "Step 524997  [5.464 sec/step, loss=0.07680, avg_loss=0.07411]\n",
      "Step 524998  [5.460 sec/step, loss=0.07532, avg_loss=0.07410]\n",
      "Generated 32 batches of size 32 in 2.893 sec\n",
      "Step 524999  [5.469 sec/step, loss=0.07267, avg_loss=0.07417]\n",
      "Step 525000  [5.458 sec/step, loss=0.07457, avg_loss=0.07416]\n",
      "Writing summary at step: 525000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-525000\n",
      "Saving audio and alignment...\n",
      "Input: apnii rahaaish kii siidz kaa natdiidzaa khooay zavaar nay anggashtd dzabrray pir rakh ddii~___________________\n",
      "Step 525001  [5.436 sec/step, loss=0.07082, avg_loss=0.07412]\n",
      "Step 525002  [5.432 sec/step, loss=0.07631, avg_loss=0.07413]\n",
      "Step 525003  [5.441 sec/step, loss=0.07613, avg_loss=0.07415]\n",
      "Step 525004  [5.457 sec/step, loss=0.07535, avg_loss=0.07417]\n",
      "Step 525005  [5.460 sec/step, loss=0.07646, avg_loss=0.07417]\n",
      "Step 525006  [5.443 sec/step, loss=0.07532, avg_loss=0.07416]\n",
      "Step 525007  [5.457 sec/step, loss=0.07371, avg_loss=0.07414]\n",
      "Step 525008  [5.442 sec/step, loss=0.07563, avg_loss=0.07414]\n",
      "Step 525009  [5.441 sec/step, loss=0.07494, avg_loss=0.07415]\n",
      "Step 525010  [5.437 sec/step, loss=0.07194, avg_loss=0.07413]\n",
      "Step 525011  [5.487 sec/step, loss=0.06678, avg_loss=0.07406]\n",
      "Step 525012  [5.481 sec/step, loss=0.07457, avg_loss=0.07408]\n",
      "Step 525013  [5.478 sec/step, loss=0.07602, avg_loss=0.07409]\n",
      "Step 525014  [5.489 sec/step, loss=0.07568, avg_loss=0.07411]\n",
      "Step 525015  [5.485 sec/step, loss=0.07436, avg_loss=0.07410]\n",
      "Step 525016  [5.477 sec/step, loss=0.07437, avg_loss=0.07410]\n",
      "Step 525017  [5.486 sec/step, loss=0.07605, avg_loss=0.07414]\n",
      "Step 525018  [5.492 sec/step, loss=0.07643, avg_loss=0.07415]\n",
      "Step 525019  [5.502 sec/step, loss=0.07402, avg_loss=0.07415]\n",
      "Step 525020  [5.474 sec/step, loss=0.06690, avg_loss=0.07406]\n",
      "Step 525021  [5.479 sec/step, loss=0.07497, avg_loss=0.07408]\n",
      "Step 525022  [5.489 sec/step, loss=0.07339, avg_loss=0.07410]\n",
      "Step 525023  [5.477 sec/step, loss=0.07178, avg_loss=0.07405]\n",
      "Step 525024  [5.473 sec/step, loss=0.07642, avg_loss=0.07406]\n",
      "Step 525025  [5.470 sec/step, loss=0.07481, avg_loss=0.07404]\n",
      "Step 525026  [5.459 sec/step, loss=0.07589, avg_loss=0.07404]\n",
      "Step 525027  [5.428 sec/step, loss=0.07152, avg_loss=0.07403]\n",
      "Step 525028  [5.428 sec/step, loss=0.07525, avg_loss=0.07404]\n",
      "Generated 32 batches of size 32 in 2.444 sec\n",
      "Step 525029  [5.449 sec/step, loss=0.07667, avg_loss=0.07405]\n",
      "Step 525030  [5.446 sec/step, loss=0.07428, avg_loss=0.07405]\n",
      "Step 525031  [5.418 sec/step, loss=0.07472, avg_loss=0.07406]\n",
      "Step 525032  [5.426 sec/step, loss=0.07501, avg_loss=0.07407]\n",
      "Step 525033  [5.418 sec/step, loss=0.07367, avg_loss=0.07405]\n",
      "Step 525034  [5.424 sec/step, loss=0.07426, avg_loss=0.07404]\n",
      "Step 525035  [5.442 sec/step, loss=0.07535, avg_loss=0.07414]\n",
      "Step 525036  [5.437 sec/step, loss=0.07387, avg_loss=0.07410]\n",
      "Step 525037  [5.391 sec/step, loss=0.07614, avg_loss=0.07419]\n",
      "Step 525038  [5.403 sec/step, loss=0.07632, avg_loss=0.07421]\n",
      "Step 525039  [5.426 sec/step, loss=0.07619, avg_loss=0.07426]\n",
      "Step 525040  [5.428 sec/step, loss=0.07391, avg_loss=0.07426]\n",
      "Step 525041  [5.446 sec/step, loss=0.07465, avg_loss=0.07426]\n",
      "Step 525042  [5.442 sec/step, loss=0.07408, avg_loss=0.07426]\n",
      "Step 525043  [5.445 sec/step, loss=0.07492, avg_loss=0.07426]\n",
      "Step 525044  [5.432 sec/step, loss=0.06505, avg_loss=0.07420]\n",
      "Step 525045  [5.443 sec/step, loss=0.07320, avg_loss=0.07419]\n",
      "Step 525046  [5.432 sec/step, loss=0.07292, avg_loss=0.07416]\n",
      "Step 525047  [5.443 sec/step, loss=0.07273, avg_loss=0.07414]\n",
      "Step 525048  [5.462 sec/step, loss=0.07492, avg_loss=0.07415]\n",
      "Step 525049  [5.461 sec/step, loss=0.07322, avg_loss=0.07413]\n",
      "Step 525050  [5.443 sec/step, loss=0.07650, avg_loss=0.07417]\n",
      "Step 525051  [5.426 sec/step, loss=0.07421, avg_loss=0.07415]\n",
      "Step 525052  [5.435 sec/step, loss=0.07552, avg_loss=0.07417]\n",
      "Step 525053  [5.432 sec/step, loss=0.07545, avg_loss=0.07419]\n",
      "Step 525054  [5.438 sec/step, loss=0.07189, avg_loss=0.07417]\n",
      "Step 525055  [5.439 sec/step, loss=0.07514, avg_loss=0.07416]\n",
      "Step 525056  [5.459 sec/step, loss=0.07351, avg_loss=0.07416]\n",
      "Step 525057  [5.395 sec/step, loss=0.07175, avg_loss=0.07421]\n",
      "Step 525058  [5.387 sec/step, loss=0.07448, avg_loss=0.07420]\n",
      "Step 525059  [5.386 sec/step, loss=0.07335, avg_loss=0.07419]\n",
      "Step 525060  [5.369 sec/step, loss=0.07463, avg_loss=0.07418]\n",
      "Generated 32 batches of size 32 in 2.428 sec\n",
      "Step 525061  [5.384 sec/step, loss=0.07508, avg_loss=0.07418]\n",
      "Step 525062  [5.433 sec/step, loss=0.06647, avg_loss=0.07409]\n",
      "Step 525063  [5.448 sec/step, loss=0.07632, avg_loss=0.07413]\n",
      "Step 525064  [5.471 sec/step, loss=0.07680, avg_loss=0.07417]\n",
      "Step 525065  [5.469 sec/step, loss=0.07475, avg_loss=0.07417]\n",
      "Step 525066  [5.449 sec/step, loss=0.07524, avg_loss=0.07416]\n",
      "Step 525067  [5.439 sec/step, loss=0.07619, avg_loss=0.07416]\n",
      "Step 525068  [5.436 sec/step, loss=0.07532, avg_loss=0.07416]\n",
      "Step 525069  [5.424 sec/step, loss=0.07085, avg_loss=0.07412]\n",
      "Step 525070  [5.401 sec/step, loss=0.07191, avg_loss=0.07409]\n",
      "Step 525071  [5.415 sec/step, loss=0.07313, avg_loss=0.07409]\n",
      "Step 525072  [5.431 sec/step, loss=0.07357, avg_loss=0.07407]\n",
      "Step 525073  [5.416 sec/step, loss=0.07317, avg_loss=0.07403]\n",
      "Step 525074  [5.424 sec/step, loss=0.07449, avg_loss=0.07410]\n",
      "Step 525075  [5.414 sec/step, loss=0.07378, avg_loss=0.07408]\n",
      "Step 525076  [5.441 sec/step, loss=0.07657, avg_loss=0.07419]\n",
      "Step 525077  [5.424 sec/step, loss=0.06755, avg_loss=0.07411]\n",
      "Step 525078  [5.422 sec/step, loss=0.07488, avg_loss=0.07414]\n",
      "Step 525079  [5.430 sec/step, loss=0.07574, avg_loss=0.07415]\n",
      "Step 525080  [5.467 sec/step, loss=0.06667, avg_loss=0.07407]\n",
      "Step 525081  [5.463 sec/step, loss=0.07362, avg_loss=0.07404]\n",
      "Step 525082  [5.455 sec/step, loss=0.07582, avg_loss=0.07403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 525083  [5.408 sec/step, loss=0.07490, avg_loss=0.07410]\n",
      "Step 525084  [5.414 sec/step, loss=0.07599, avg_loss=0.07411]\n",
      "Step 525085  [5.389 sec/step, loss=0.07302, avg_loss=0.07410]\n",
      "Step 525086  [5.385 sec/step, loss=0.07513, avg_loss=0.07410]\n",
      "Step 525087  [5.394 sec/step, loss=0.07123, avg_loss=0.07406]\n",
      "Step 525088  [5.405 sec/step, loss=0.07608, avg_loss=0.07409]\n",
      "Step 525089  [5.406 sec/step, loss=0.07501, avg_loss=0.07411]\n",
      "Step 525090  [5.405 sec/step, loss=0.07062, avg_loss=0.07408]\n",
      "Step 525091  [5.397 sec/step, loss=0.07434, avg_loss=0.07405]\n",
      "Step 525092  [5.379 sec/step, loss=0.07288, avg_loss=0.07402]\n",
      "Generated 32 batches of size 32 in 2.329 sec\n",
      "Step 525093  [5.388 sec/step, loss=0.07549, avg_loss=0.07402]\n",
      "Step 525094  [5.400 sec/step, loss=0.07629, avg_loss=0.07404]\n",
      "Step 525095  [5.384 sec/step, loss=0.07548, avg_loss=0.07405]\n",
      "Step 525096  [5.389 sec/step, loss=0.07523, avg_loss=0.07407]\n",
      "Step 525097  [5.386 sec/step, loss=0.07564, avg_loss=0.07406]\n",
      "Step 525098  [5.388 sec/step, loss=0.07441, avg_loss=0.07405]\n",
      "Step 525099  [5.390 sec/step, loss=0.07392, avg_loss=0.07406]\n",
      "Step 525100  [5.398 sec/step, loss=0.07383, avg_loss=0.07406]\n",
      "Writing summary at step: 525100\n",
      "Step 525101  [5.409 sec/step, loss=0.07444, avg_loss=0.07409]\n",
      "Step 525102  [5.458 sec/step, loss=0.06599, avg_loss=0.07399]\n",
      "Step 525103  [5.442 sec/step, loss=0.07381, avg_loss=0.07397]\n",
      "Step 525104  [5.425 sec/step, loss=0.07333, avg_loss=0.07395]\n",
      "Step 525105  [5.409 sec/step, loss=0.07486, avg_loss=0.07393]\n",
      "Step 525106  [5.412 sec/step, loss=0.07551, avg_loss=0.07393]\n",
      "Step 525107  [5.383 sec/step, loss=0.07552, avg_loss=0.07395]\n",
      "Step 525108  [5.392 sec/step, loss=0.07549, avg_loss=0.07395]\n",
      "Step 525109  [5.391 sec/step, loss=0.07186, avg_loss=0.07392]\n",
      "Step 525110  [5.412 sec/step, loss=0.07644, avg_loss=0.07396]\n",
      "Step 525111  [5.366 sec/step, loss=0.07459, avg_loss=0.07404]\n",
      "Step 525112  [5.377 sec/step, loss=0.07546, avg_loss=0.07405]\n",
      "Step 525113  [5.365 sec/step, loss=0.07023, avg_loss=0.07399]\n",
      "Step 525114  [5.347 sec/step, loss=0.07022, avg_loss=0.07394]\n",
      "Step 525115  [5.354 sec/step, loss=0.07503, avg_loss=0.07394]\n",
      "Step 525116  [5.369 sec/step, loss=0.07575, avg_loss=0.07396]\n",
      "Step 525117  [5.383 sec/step, loss=0.07329, avg_loss=0.07393]\n",
      "Step 525118  [5.385 sec/step, loss=0.07399, avg_loss=0.07391]\n",
      "Step 525119  [5.382 sec/step, loss=0.07570, avg_loss=0.07392]\n",
      "Step 525120  [5.408 sec/step, loss=0.07412, avg_loss=0.07400]\n",
      "Step 525121  [5.405 sec/step, loss=0.07463, avg_loss=0.07399]\n",
      "Step 525122  [5.423 sec/step, loss=0.07620, avg_loss=0.07402]\n",
      "Step 525123  [5.440 sec/step, loss=0.07639, avg_loss=0.07407]\n",
      "Generated 32 batches of size 32 in 2.585 sec\n",
      "Step 525124  [5.434 sec/step, loss=0.07393, avg_loss=0.07404]\n",
      "Step 525125  [5.425 sec/step, loss=0.07287, avg_loss=0.07402]\n",
      "Step 525126  [5.426 sec/step, loss=0.07133, avg_loss=0.07398]\n",
      "Step 525127  [5.427 sec/step, loss=0.07419, avg_loss=0.07400]\n",
      "Step 525128  [5.436 sec/step, loss=0.07544, avg_loss=0.07401]\n",
      "Step 525129  [5.418 sec/step, loss=0.07470, avg_loss=0.07399]\n",
      "Step 525130  [5.407 sec/step, loss=0.06490, avg_loss=0.07389]\n",
      "Step 525131  [5.410 sec/step, loss=0.07595, avg_loss=0.07390]\n",
      "Step 525132  [5.411 sec/step, loss=0.07657, avg_loss=0.07392]\n",
      "Step 525133  [5.413 sec/step, loss=0.07227, avg_loss=0.07391]\n",
      "Step 525134  [5.428 sec/step, loss=0.07316, avg_loss=0.07389]\n",
      "Step 525135  [5.430 sec/step, loss=0.07632, avg_loss=0.07390]\n",
      "Step 525136  [5.434 sec/step, loss=0.07641, avg_loss=0.07393]\n",
      "Step 525137  [5.420 sec/step, loss=0.07292, avg_loss=0.07390]\n",
      "Step 525138  [5.399 sec/step, loss=0.07221, avg_loss=0.07386]\n",
      "Step 525139  [5.382 sec/step, loss=0.07331, avg_loss=0.07383]\n",
      "Step 525140  [5.384 sec/step, loss=0.07545, avg_loss=0.07384]\n",
      "Step 525141  [5.389 sec/step, loss=0.07628, avg_loss=0.07386]\n",
      "Step 525142  [5.384 sec/step, loss=0.07440, avg_loss=0.07386]\n",
      "Step 525143  [5.380 sec/step, loss=0.07529, avg_loss=0.07387]\n",
      "Step 525144  [5.395 sec/step, loss=0.07440, avg_loss=0.07396]\n",
      "Step 525145  [5.389 sec/step, loss=0.07286, avg_loss=0.07396]\n",
      "Step 525146  [5.402 sec/step, loss=0.07639, avg_loss=0.07399]\n",
      "Step 525147  [5.372 sec/step, loss=0.07280, avg_loss=0.07399]\n",
      "Step 525148  [5.362 sec/step, loss=0.07566, avg_loss=0.07400]\n",
      "Step 525149  [5.372 sec/step, loss=0.07611, avg_loss=0.07403]\n",
      "Step 525150  [5.383 sec/step, loss=0.07352, avg_loss=0.07400]\n",
      "Step 525151  [5.393 sec/step, loss=0.07495, avg_loss=0.07401]\n",
      "Step 525152  [5.378 sec/step, loss=0.07439, avg_loss=0.07399]\n",
      "Step 525153  [5.386 sec/step, loss=0.07407, avg_loss=0.07398]\n",
      "Step 525154  [5.369 sec/step, loss=0.07249, avg_loss=0.07399]\n",
      "Step 525155  [5.367 sec/step, loss=0.07548, avg_loss=0.07399]\n",
      "Generated 32 batches of size 32 in 2.331 sec\n",
      "Step 525156  [5.360 sec/step, loss=0.07557, avg_loss=0.07401]\n",
      "Step 525157  [5.378 sec/step, loss=0.07577, avg_loss=0.07405]\n",
      "Step 525158  [5.358 sec/step, loss=0.06669, avg_loss=0.07397]\n",
      "Step 525159  [5.378 sec/step, loss=0.07417, avg_loss=0.07398]\n",
      "Step 525160  [5.438 sec/step, loss=0.06678, avg_loss=0.07390]\n",
      "Step 525161  [5.419 sec/step, loss=0.07590, avg_loss=0.07391]\n",
      "Step 525162  [5.371 sec/step, loss=0.07579, avg_loss=0.07400]\n",
      "Step 525163  [5.359 sec/step, loss=0.07417, avg_loss=0.07398]\n",
      "Step 525164  [5.360 sec/step, loss=0.07610, avg_loss=0.07398]\n",
      "Step 525165  [5.411 sec/step, loss=0.06798, avg_loss=0.07391]\n",
      "Step 525166  [5.422 sec/step, loss=0.07434, avg_loss=0.07390]\n",
      "Step 525167  [5.422 sec/step, loss=0.07522, avg_loss=0.07389]\n",
      "Step 525168  [5.440 sec/step, loss=0.07538, avg_loss=0.07389]\n",
      "Step 525169  [5.443 sec/step, loss=0.07381, avg_loss=0.07392]\n",
      "Step 525170  [5.451 sec/step, loss=0.07431, avg_loss=0.07394]\n",
      "Step 525171  [5.433 sec/step, loss=0.07461, avg_loss=0.07396]\n",
      "Step 525172  [5.422 sec/step, loss=0.07634, avg_loss=0.07399]\n",
      "Step 525173  [5.434 sec/step, loss=0.07659, avg_loss=0.07402]\n",
      "Step 525174  [5.426 sec/step, loss=0.06805, avg_loss=0.07396]\n",
      "Step 525175  [5.418 sec/step, loss=0.07343, avg_loss=0.07395]\n",
      "Step 525176  [5.407 sec/step, loss=0.07555, avg_loss=0.07394]\n",
      "Step 525177  [5.425 sec/step, loss=0.07531, avg_loss=0.07402]\n",
      "Step 525178  [5.415 sec/step, loss=0.07539, avg_loss=0.07402]\n",
      "Step 525179  [5.411 sec/step, loss=0.07129, avg_loss=0.07398]\n",
      "Step 525180  [5.351 sec/step, loss=0.07193, avg_loss=0.07403]\n",
      "Step 525181  [5.358 sec/step, loss=0.07501, avg_loss=0.07405]\n",
      "Step 525182  [5.354 sec/step, loss=0.07523, avg_loss=0.07404]\n",
      "Step 525183  [5.350 sec/step, loss=0.07572, avg_loss=0.07405]\n",
      "Step 525184  [5.364 sec/step, loss=0.07353, avg_loss=0.07402]\n",
      "Step 525185  [5.361 sec/step, loss=0.07564, avg_loss=0.07405]\n",
      "Step 525186  [5.357 sec/step, loss=0.07512, avg_loss=0.07405]\n",
      "Step 525187  [5.366 sec/step, loss=0.07696, avg_loss=0.07411]\n",
      "Generated 32 batches of size 32 in 2.411 sec\n",
      "Step 525188  [5.362 sec/step, loss=0.07297, avg_loss=0.07408]\n",
      "Step 525189  [5.361 sec/step, loss=0.07420, avg_loss=0.07407]\n",
      "Step 525190  [5.381 sec/step, loss=0.07595, avg_loss=0.07412]\n",
      "Step 525191  [5.400 sec/step, loss=0.07558, avg_loss=0.07413]\n",
      "Step 525192  [5.419 sec/step, loss=0.07494, avg_loss=0.07415]\n",
      "Step 525193  [5.417 sec/step, loss=0.07459, avg_loss=0.07415]\n",
      "Step 525194  [5.394 sec/step, loss=0.07499, avg_loss=0.07413]\n",
      "Step 525195  [5.395 sec/step, loss=0.07331, avg_loss=0.07411]\n",
      "Step 525196  [5.404 sec/step, loss=0.07630, avg_loss=0.07412]\n",
      "Step 525197  [5.395 sec/step, loss=0.07382, avg_loss=0.07410]\n",
      "Step 525198  [5.391 sec/step, loss=0.07576, avg_loss=0.07412]\n",
      "Step 525199  [5.392 sec/step, loss=0.07253, avg_loss=0.07410]\n",
      "Step 525200  [5.399 sec/step, loss=0.07510, avg_loss=0.07412]\n",
      "Writing summary at step: 525200\n",
      "Step 525201  [5.427 sec/step, loss=0.07318, avg_loss=0.07410]\n",
      "Step 525202  [5.373 sec/step, loss=0.07568, avg_loss=0.07420]\n",
      "Step 525203  [5.391 sec/step, loss=0.07492, avg_loss=0.07421]\n",
      "Step 525204  [5.388 sec/step, loss=0.07035, avg_loss=0.07418]\n",
      "Step 525205  [5.383 sec/step, loss=0.07333, avg_loss=0.07417]\n",
      "Step 525206  [5.429 sec/step, loss=0.06496, avg_loss=0.07406]\n",
      "Step 525207  [5.429 sec/step, loss=0.07520, avg_loss=0.07406]\n",
      "Step 525208  [5.415 sec/step, loss=0.07457, avg_loss=0.07405]\n",
      "Step 525209  [5.413 sec/step, loss=0.07295, avg_loss=0.07406]\n",
      "Step 525210  [5.394 sec/step, loss=0.07431, avg_loss=0.07404]\n",
      "Step 525211  [5.391 sec/step, loss=0.07565, avg_loss=0.07405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 525212  [5.390 sec/step, loss=0.07428, avg_loss=0.07404]\n",
      "Step 525213  [5.408 sec/step, loss=0.07644, avg_loss=0.07410]\n",
      "Step 525214  [5.436 sec/step, loss=0.07682, avg_loss=0.07416]\n",
      "Step 525215  [5.429 sec/step, loss=0.07190, avg_loss=0.07413]\n",
      "Step 525216  [5.435 sec/step, loss=0.07652, avg_loss=0.07414]\n",
      "Step 525217  [5.409 sec/step, loss=0.07575, avg_loss=0.07417]\n",
      "Step 525218  [5.402 sec/step, loss=0.07627, avg_loss=0.07419]\n",
      "Generated 32 batches of size 32 in 2.355 sec\n",
      "Step 525219  [5.400 sec/step, loss=0.07649, avg_loss=0.07420]\n",
      "Step 525220  [5.400 sec/step, loss=0.07603, avg_loss=0.07422]\n",
      "Step 525221  [5.409 sec/step, loss=0.07635, avg_loss=0.07423]\n",
      "Step 525222  [5.408 sec/step, loss=0.07425, avg_loss=0.07421]\n",
      "Step 525223  [5.399 sec/step, loss=0.07570, avg_loss=0.07421]\n",
      "Step 525224  [5.384 sec/step, loss=0.07021, avg_loss=0.07417]\n",
      "Step 525225  [5.394 sec/step, loss=0.07482, avg_loss=0.07419]\n",
      "Step 525226  [5.401 sec/step, loss=0.07250, avg_loss=0.07420]\n",
      "Step 525227  [5.395 sec/step, loss=0.06468, avg_loss=0.07410]\n",
      "Step 525228  [5.380 sec/step, loss=0.07422, avg_loss=0.07409]\n",
      "Step 525229  [5.380 sec/step, loss=0.07470, avg_loss=0.07409]\n",
      "Step 525230  [5.398 sec/step, loss=0.07434, avg_loss=0.07419]\n",
      "Step 525231  [5.409 sec/step, loss=0.07616, avg_loss=0.07419]\n",
      "Step 525232  [5.408 sec/step, loss=0.07309, avg_loss=0.07415]\n",
      "Step 525233  [5.413 sec/step, loss=0.07473, avg_loss=0.07418]\n",
      "Step 525234  [5.392 sec/step, loss=0.07504, avg_loss=0.07420]\n",
      "Step 525235  [5.383 sec/step, loss=0.07519, avg_loss=0.07419]\n",
      "Step 525236  [5.365 sec/step, loss=0.07574, avg_loss=0.07418]\n",
      "Step 525237  [5.362 sec/step, loss=0.07463, avg_loss=0.07420]\n",
      "Step 525238  [5.369 sec/step, loss=0.07178, avg_loss=0.07419]\n",
      "Step 525239  [5.378 sec/step, loss=0.07466, avg_loss=0.07421]\n",
      "Step 525240  [5.376 sec/step, loss=0.07389, avg_loss=0.07419]\n",
      "Step 525241  [5.360 sec/step, loss=0.07520, avg_loss=0.07418]\n",
      "Step 525242  [5.355 sec/step, loss=0.07519, avg_loss=0.07419]\n",
      "Step 525243  [5.366 sec/step, loss=0.07675, avg_loss=0.07420]\n",
      "Step 525244  [5.373 sec/step, loss=0.07367, avg_loss=0.07419]\n",
      "Step 525245  [5.370 sec/step, loss=0.07406, avg_loss=0.07421]\n",
      "Step 525246  [5.411 sec/step, loss=0.06697, avg_loss=0.07411]\n",
      "Step 525247  [5.423 sec/step, loss=0.07628, avg_loss=0.07415]\n",
      "Step 525248  [5.430 sec/step, loss=0.07627, avg_loss=0.07415]\n",
      "Step 525249  [5.413 sec/step, loss=0.07179, avg_loss=0.07411]\n",
      "Step 525250  [5.380 sec/step, loss=0.07165, avg_loss=0.07409]\n",
      "Generated 32 batches of size 32 in 2.354 sec\n",
      "Step 525251  [5.399 sec/step, loss=0.07444, avg_loss=0.07409]\n",
      "Step 525252  [5.437 sec/step, loss=0.07357, avg_loss=0.07408]\n",
      "Step 525253  [5.423 sec/step, loss=0.07450, avg_loss=0.07408]\n",
      "Step 525254  [5.441 sec/step, loss=0.07501, avg_loss=0.07411]\n",
      "Step 525255  [5.457 sec/step, loss=0.07604, avg_loss=0.07411]\n",
      "Step 525256  [5.434 sec/step, loss=0.06609, avg_loss=0.07402]\n",
      "Step 525257  [5.430 sec/step, loss=0.07458, avg_loss=0.07401]\n",
      "Step 525258  [5.455 sec/step, loss=0.07645, avg_loss=0.07410]\n",
      "Step 525259  [5.443 sec/step, loss=0.07561, avg_loss=0.07412]\n",
      "Step 525260  [5.391 sec/step, loss=0.07151, avg_loss=0.07417]\n",
      "Step 525261  [5.395 sec/step, loss=0.07506, avg_loss=0.07416]\n",
      "Step 525262  [5.410 sec/step, loss=0.07512, avg_loss=0.07415]\n",
      "Step 525263  [5.430 sec/step, loss=0.07664, avg_loss=0.07418]\n",
      "Step 525264  [5.419 sec/step, loss=0.07438, avg_loss=0.07416]\n",
      "Step 525265  [5.351 sec/step, loss=0.06725, avg_loss=0.07415]\n",
      "Step 525266  [5.328 sec/step, loss=0.07205, avg_loss=0.07413]\n",
      "Step 525267  [5.334 sec/step, loss=0.07677, avg_loss=0.07414]\n",
      "Step 525268  [5.326 sec/step, loss=0.07663, avg_loss=0.07416]\n",
      "Step 525269  [5.338 sec/step, loss=0.07482, avg_loss=0.07417]\n",
      "Step 525270  [5.346 sec/step, loss=0.07514, avg_loss=0.07417]\n",
      "Step 525271  [5.340 sec/step, loss=0.07574, avg_loss=0.07419]\n",
      "Step 525272  [5.343 sec/step, loss=0.07609, avg_loss=0.07418]\n",
      "Step 525273  [5.324 sec/step, loss=0.07132, avg_loss=0.07413]\n",
      "Step 525274  [5.355 sec/step, loss=0.07479, avg_loss=0.07420]\n",
      "Step 525275  [5.370 sec/step, loss=0.07499, avg_loss=0.07421]\n",
      "Step 525276  [5.380 sec/step, loss=0.07330, avg_loss=0.07419]\n",
      "Step 525277  [5.369 sec/step, loss=0.07413, avg_loss=0.07418]\n",
      "Step 525278  [5.372 sec/step, loss=0.07333, avg_loss=0.07416]\n",
      "Step 525279  [5.369 sec/step, loss=0.07345, avg_loss=0.07418]\n",
      "Step 525280  [5.387 sec/step, loss=0.07421, avg_loss=0.07420]\n",
      "Step 525281  [5.377 sec/step, loss=0.07517, avg_loss=0.07421]\n",
      "Step 525282  [5.371 sec/step, loss=0.07462, avg_loss=0.07420]\n",
      "Generated 32 batches of size 32 in 2.533 sec\n",
      "Step 525283  [5.372 sec/step, loss=0.07556, avg_loss=0.07420]\n",
      "Step 525284  [5.337 sec/step, loss=0.07340, avg_loss=0.07420]\n",
      "Step 525285  [5.352 sec/step, loss=0.07625, avg_loss=0.07420]\n",
      "Step 525286  [5.352 sec/step, loss=0.07497, avg_loss=0.07420]\n",
      "Step 525287  [5.352 sec/step, loss=0.07472, avg_loss=0.07418]\n",
      "Step 525288  [5.398 sec/step, loss=0.06602, avg_loss=0.07411]\n",
      "Step 525289  [5.425 sec/step, loss=0.07173, avg_loss=0.07408]\n",
      "Step 525290  [5.412 sec/step, loss=0.07524, avg_loss=0.07408]\n",
      "Step 525291  [5.390 sec/step, loss=0.07342, avg_loss=0.07406]\n",
      "Step 525292  [5.395 sec/step, loss=0.07477, avg_loss=0.07405]\n",
      "Step 525293  [5.380 sec/step, loss=0.07173, avg_loss=0.07403]\n",
      "Step 525294  [5.374 sec/step, loss=0.06657, avg_loss=0.07394]\n",
      "Step 525295  [5.377 sec/step, loss=0.07273, avg_loss=0.07394]\n",
      "Step 525296  [5.364 sec/step, loss=0.07510, avg_loss=0.07392]\n",
      "Step 525297  [5.391 sec/step, loss=0.07401, avg_loss=0.07393]\n",
      "Step 525298  [5.398 sec/step, loss=0.07568, avg_loss=0.07392]\n",
      "Step 525299  [5.430 sec/step, loss=0.07337, avg_loss=0.07393]\n",
      "Step 525300  [5.429 sec/step, loss=0.07697, avg_loss=0.07395]\n",
      "Writing summary at step: 525300\n",
      "Step 525301  [5.392 sec/step, loss=0.07413, avg_loss=0.07396]\n",
      "Step 525302  [5.390 sec/step, loss=0.07369, avg_loss=0.07394]\n",
      "Step 525303  [5.388 sec/step, loss=0.07587, avg_loss=0.07395]\n",
      "Step 525304  [5.400 sec/step, loss=0.07552, avg_loss=0.07400]\n",
      "Step 525305  [5.400 sec/step, loss=0.07426, avg_loss=0.07401]\n",
      "Step 525306  [5.336 sec/step, loss=0.07191, avg_loss=0.07408]\n",
      "Step 525307  [5.350 sec/step, loss=0.07612, avg_loss=0.07409]\n",
      "Step 525308  [5.353 sec/step, loss=0.07532, avg_loss=0.07410]\n",
      "Step 525309  [5.353 sec/step, loss=0.07549, avg_loss=0.07412]\n",
      "Step 525310  [5.413 sec/step, loss=0.06648, avg_loss=0.07405]\n",
      "Step 525311  [5.423 sec/step, loss=0.07456, avg_loss=0.07403]\n",
      "Step 525312  [5.425 sec/step, loss=0.07623, avg_loss=0.07405]\n",
      "Step 525313  [5.410 sec/step, loss=0.07383, avg_loss=0.07403]\n",
      "Generated 32 batches of size 32 in 2.417 sec\n",
      "Step 525314  [5.405 sec/step, loss=0.07510, avg_loss=0.07401]\n",
      "Step 525315  [5.415 sec/step, loss=0.07575, avg_loss=0.07405]\n",
      "Step 525316  [5.400 sec/step, loss=0.07561, avg_loss=0.07404]\n",
      "Step 525317  [5.394 sec/step, loss=0.07306, avg_loss=0.07401]\n",
      "Step 525318  [5.393 sec/step, loss=0.07200, avg_loss=0.07397]\n",
      "Step 525319  [5.381 sec/step, loss=0.07550, avg_loss=0.07396]\n",
      "Step 525320  [5.374 sec/step, loss=0.07516, avg_loss=0.07395]\n",
      "Step 525321  [5.379 sec/step, loss=0.07637, avg_loss=0.07395]\n",
      "Step 525322  [5.371 sec/step, loss=0.07634, avg_loss=0.07397]\n",
      "Step 525323  [5.361 sec/step, loss=0.07377, avg_loss=0.07395]\n",
      "Step 525324  [5.382 sec/step, loss=0.07691, avg_loss=0.07402]\n",
      "Step 525325  [5.385 sec/step, loss=0.07524, avg_loss=0.07402]\n",
      "Step 525326  [5.381 sec/step, loss=0.07557, avg_loss=0.07406]\n",
      "Step 525327  [5.393 sec/step, loss=0.07569, avg_loss=0.07417]\n",
      "Step 525328  [5.449 sec/step, loss=0.06723, avg_loss=0.07410]\n",
      "Step 525329  [5.438 sec/step, loss=0.07200, avg_loss=0.07407]\n",
      "Step 525330  [5.447 sec/step, loss=0.07674, avg_loss=0.07409]\n",
      "Step 525331  [5.427 sec/step, loss=0.07379, avg_loss=0.07407]\n",
      "Step 525332  [5.440 sec/step, loss=0.07381, avg_loss=0.07408]\n",
      "Step 525333  [5.434 sec/step, loss=0.07205, avg_loss=0.07405]\n",
      "Step 525334  [5.426 sec/step, loss=0.07556, avg_loss=0.07405]\n",
      "Step 525335  [5.426 sec/step, loss=0.07410, avg_loss=0.07404]\n",
      "Step 525336  [5.431 sec/step, loss=0.07594, avg_loss=0.07405]\n",
      "Step 525337  [5.447 sec/step, loss=0.07523, avg_loss=0.07405]\n",
      "Step 525338  [5.462 sec/step, loss=0.07656, avg_loss=0.07410]\n",
      "Step 525339  [5.472 sec/step, loss=0.07607, avg_loss=0.07411]\n",
      "Step 525340  [5.482 sec/step, loss=0.07525, avg_loss=0.07413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 525341  [5.472 sec/step, loss=0.07229, avg_loss=0.07410]\n",
      "Step 525342  [5.471 sec/step, loss=0.07425, avg_loss=0.07409]\n",
      "Step 525343  [5.445 sec/step, loss=0.06591, avg_loss=0.07398]\n",
      "Step 525344  [5.433 sec/step, loss=0.07035, avg_loss=0.07395]\n",
      "Step 525345  [5.437 sec/step, loss=0.07430, avg_loss=0.07395]\n",
      "Generated 32 batches of size 32 in 2.322 sec\n",
      "Step 525346  [5.393 sec/step, loss=0.07564, avg_loss=0.07404]\n",
      "Step 525347  [5.397 sec/step, loss=0.07411, avg_loss=0.07401]\n",
      "Step 525348  [5.388 sec/step, loss=0.07580, avg_loss=0.07401]\n",
      "Step 525349  [5.402 sec/step, loss=0.07471, avg_loss=0.07404]\n",
      "Step 525350  [5.439 sec/step, loss=0.07366, avg_loss=0.07406]\n",
      "Step 525351  [5.429 sec/step, loss=0.07641, avg_loss=0.07408]\n",
      "Step 525352  [5.413 sec/step, loss=0.07413, avg_loss=0.07408]\n",
      "Step 525353  [5.407 sec/step, loss=0.07457, avg_loss=0.07408]\n",
      "Step 525354  [5.400 sec/step, loss=0.07178, avg_loss=0.07405]\n",
      "Step 525355  [5.397 sec/step, loss=0.07703, avg_loss=0.07406]\n",
      "Step 525356  [5.419 sec/step, loss=0.07581, avg_loss=0.07416]\n",
      "Step 525357  [5.420 sec/step, loss=0.07532, avg_loss=0.07417]\n",
      "Step 525358  [5.438 sec/step, loss=0.07323, avg_loss=0.07413]\n",
      "Step 525359  [5.443 sec/step, loss=0.07372, avg_loss=0.07412]\n",
      "Step 525360  [5.435 sec/step, loss=0.07063, avg_loss=0.07411]\n",
      "Step 525361  [5.424 sec/step, loss=0.07430, avg_loss=0.07410]\n",
      "Step 525362  [5.408 sec/step, loss=0.07454, avg_loss=0.07409]\n",
      "Step 525363  [5.394 sec/step, loss=0.07381, avg_loss=0.07407]\n",
      "Step 525364  [5.399 sec/step, loss=0.07499, avg_loss=0.07407]\n",
      "Step 525365  [5.415 sec/step, loss=0.07532, avg_loss=0.07415]\n",
      "Step 525366  [5.419 sec/step, loss=0.07427, avg_loss=0.07417]\n",
      "Step 525367  [5.413 sec/step, loss=0.07558, avg_loss=0.07416]\n",
      "Step 525368  [5.402 sec/step, loss=0.07526, avg_loss=0.07415]\n",
      "Step 525369  [5.418 sec/step, loss=0.07329, avg_loss=0.07413]\n",
      "Step 525370  [5.405 sec/step, loss=0.07242, avg_loss=0.07411]\n",
      "Step 525371  [5.399 sec/step, loss=0.07409, avg_loss=0.07409]\n",
      "Step 525372  [5.374 sec/step, loss=0.07169, avg_loss=0.07405]\n",
      "Step 525373  [5.394 sec/step, loss=0.07418, avg_loss=0.07407]\n",
      "Step 525374  [5.393 sec/step, loss=0.07617, avg_loss=0.07409]\n",
      "Step 525375  [5.401 sec/step, loss=0.07316, avg_loss=0.07407]\n",
      "Step 525376  [5.440 sec/step, loss=0.06607, avg_loss=0.07400]\n",
      "Step 525377  [5.451 sec/step, loss=0.07581, avg_loss=0.07401]\n",
      "Generated 32 batches of size 32 in 2.424 sec\n",
      "Step 525378  [5.467 sec/step, loss=0.07616, avg_loss=0.07404]\n",
      "Step 525379  [5.470 sec/step, loss=0.07059, avg_loss=0.07401]\n",
      "Step 525380  [5.462 sec/step, loss=0.07448, avg_loss=0.07402]\n",
      "Step 525381  [5.473 sec/step, loss=0.07651, avg_loss=0.07403]\n",
      "Step 525382  [5.456 sec/step, loss=0.06590, avg_loss=0.07394]\n",
      "Step 525383  [5.451 sec/step, loss=0.07247, avg_loss=0.07391]\n",
      "Step 525384  [5.460 sec/step, loss=0.07529, avg_loss=0.07393]\n",
      "Step 525385  [5.462 sec/step, loss=0.07610, avg_loss=0.07393]\n",
      "Step 525386  [5.474 sec/step, loss=0.07558, avg_loss=0.07394]\n",
      "Step 525387  [5.454 sec/step, loss=0.07127, avg_loss=0.07390]\n",
      "Step 525388  [5.406 sec/step, loss=0.07554, avg_loss=0.07400]\n",
      "Step 525389  [5.376 sec/step, loss=0.07541, avg_loss=0.07403]\n",
      "Step 525390  [5.380 sec/step, loss=0.07216, avg_loss=0.07400]\n",
      "Step 525391  [5.368 sec/step, loss=0.06510, avg_loss=0.07392]\n",
      "Step 525392  [5.355 sec/step, loss=0.07456, avg_loss=0.07392]\n",
      "Step 525393  [5.358 sec/step, loss=0.07414, avg_loss=0.07394]\n",
      "Step 525394  [5.378 sec/step, loss=0.07513, avg_loss=0.07403]\n",
      "Step 525395  [5.382 sec/step, loss=0.07439, avg_loss=0.07404]\n",
      "Step 525396  [5.433 sec/step, loss=0.06602, avg_loss=0.07395]\n",
      "Step 525397  [5.415 sec/step, loss=0.07491, avg_loss=0.07396]\n",
      "Step 525398  [5.432 sec/step, loss=0.07350, avg_loss=0.07394]\n",
      "Step 525399  [5.415 sec/step, loss=0.07357, avg_loss=0.07394]\n",
      "Step 525400  [5.414 sec/step, loss=0.07615, avg_loss=0.07393]\n",
      "Writing summary at step: 525400\n",
      "Step 525401  [5.421 sec/step, loss=0.07540, avg_loss=0.07395]\n",
      "Step 525402  [5.418 sec/step, loss=0.07331, avg_loss=0.07394]\n",
      "Step 525403  [5.410 sec/step, loss=0.07575, avg_loss=0.07394]\n",
      "Step 525404  [5.417 sec/step, loss=0.07621, avg_loss=0.07395]\n",
      "Step 525405  [5.432 sec/step, loss=0.07616, avg_loss=0.07397]\n",
      "Step 525406  [5.459 sec/step, loss=0.07355, avg_loss=0.07398]\n",
      "Step 525407  [5.439 sec/step, loss=0.07206, avg_loss=0.07394]\n",
      "Step 525408  [5.438 sec/step, loss=0.07408, avg_loss=0.07393]\n",
      "Generated 32 batches of size 32 in 2.331 sec\n",
      "Step 525409  [5.448 sec/step, loss=0.07643, avg_loss=0.07394]\n",
      "Step 525410  [5.419 sec/step, loss=0.07349, avg_loss=0.07401]\n",
      "Step 525411  [5.406 sec/step, loss=0.07211, avg_loss=0.07399]\n",
      "Step 525412  [5.415 sec/step, loss=0.07557, avg_loss=0.07398]\n",
      "Step 525413  [5.412 sec/step, loss=0.07429, avg_loss=0.07398]\n",
      "Step 525414  [5.414 sec/step, loss=0.07680, avg_loss=0.07400]\n",
      "Step 525415  [5.401 sec/step, loss=0.07278, avg_loss=0.07397]\n",
      "Step 525416  [5.405 sec/step, loss=0.07159, avg_loss=0.07393]\n",
      "Step 525417  [5.424 sec/step, loss=0.07638, avg_loss=0.07396]\n",
      "Step 525418  [5.405 sec/step, loss=0.07161, avg_loss=0.07396]\n",
      "Step 525419  [5.401 sec/step, loss=0.07359, avg_loss=0.07394]\n",
      "Step 525420  [5.387 sec/step, loss=0.07113, avg_loss=0.07390]\n",
      "Step 525421  [5.370 sec/step, loss=0.07343, avg_loss=0.07387]\n",
      "Step 525422  [5.360 sec/step, loss=0.07503, avg_loss=0.07386]\n",
      "Step 525423  [5.364 sec/step, loss=0.07408, avg_loss=0.07386]\n",
      "Step 525424  [5.351 sec/step, loss=0.07269, avg_loss=0.07382]\n",
      "Step 525425  [5.370 sec/step, loss=0.07367, avg_loss=0.07380]\n",
      "Step 525426  [5.372 sec/step, loss=0.07490, avg_loss=0.07380]\n",
      "Step 525427  [5.376 sec/step, loss=0.07494, avg_loss=0.07379]\n",
      "Step 525428  [5.318 sec/step, loss=0.07465, avg_loss=0.07386]\n",
      "Step 525429  [5.333 sec/step, loss=0.07351, avg_loss=0.07388]\n",
      "Step 525430  [5.332 sec/step, loss=0.07632, avg_loss=0.07387]\n",
      "Step 525431  [5.390 sec/step, loss=0.06705, avg_loss=0.07381]\n",
      "Step 525432  [5.366 sec/step, loss=0.07474, avg_loss=0.07382]\n",
      "Step 525433  [5.376 sec/step, loss=0.07370, avg_loss=0.07383]\n",
      "Step 525434  [5.368 sec/step, loss=0.07422, avg_loss=0.07382]\n",
      "Step 525435  [5.380 sec/step, loss=0.07435, avg_loss=0.07382]\n",
      "Step 525436  [5.395 sec/step, loss=0.07540, avg_loss=0.07382]\n",
      "Step 525437  [5.394 sec/step, loss=0.07530, avg_loss=0.07382]\n",
      "Step 525438  [5.365 sec/step, loss=0.06615, avg_loss=0.07371]\n",
      "Step 525439  [5.362 sec/step, loss=0.07644, avg_loss=0.07372]\n",
      "Step 525440  [5.363 sec/step, loss=0.07630, avg_loss=0.07373]\n",
      "Generated 32 batches of size 32 in 2.580 sec\n",
      "Step 525441  [5.378 sec/step, loss=0.07529, avg_loss=0.07376]\n",
      "Step 525442  [5.384 sec/step, loss=0.07483, avg_loss=0.07376]\n",
      "Step 525443  [5.393 sec/step, loss=0.07354, avg_loss=0.07384]\n",
      "Step 525444  [5.400 sec/step, loss=0.07604, avg_loss=0.07390]\n",
      "Step 525445  [5.416 sec/step, loss=0.07622, avg_loss=0.07391]\n",
      "Step 525446  [5.423 sec/step, loss=0.07580, avg_loss=0.07392]\n",
      "Step 525447  [5.425 sec/step, loss=0.07566, avg_loss=0.07393]\n",
      "Step 525448  [5.418 sec/step, loss=0.07158, avg_loss=0.07389]\n",
      "Step 525449  [5.415 sec/step, loss=0.07585, avg_loss=0.07390]\n",
      "Step 525450  [5.385 sec/step, loss=0.07385, avg_loss=0.07390]\n",
      "Step 525451  [5.382 sec/step, loss=0.07638, avg_loss=0.07390]\n",
      "Step 525452  [5.368 sec/step, loss=0.07260, avg_loss=0.07389]\n",
      "Step 525453  [5.391 sec/step, loss=0.07592, avg_loss=0.07390]\n",
      "Step 525454  [5.394 sec/step, loss=0.07444, avg_loss=0.07393]\n",
      "Step 525455  [5.390 sec/step, loss=0.07525, avg_loss=0.07391]\n",
      "Step 525456  [5.374 sec/step, loss=0.07411, avg_loss=0.07389]\n",
      "Step 525457  [5.372 sec/step, loss=0.07291, avg_loss=0.07387]\n",
      "Step 525458  [5.329 sec/step, loss=0.06739, avg_loss=0.07381]\n",
      "Step 525459  [5.319 sec/step, loss=0.07491, avg_loss=0.07382]\n",
      "Step 525460  [5.331 sec/step, loss=0.07195, avg_loss=0.07384]\n",
      "Step 525461  [5.339 sec/step, loss=0.07572, avg_loss=0.07385]\n",
      "Step 525462  [5.341 sec/step, loss=0.07496, avg_loss=0.07385]\n",
      "Step 525463  [5.339 sec/step, loss=0.07402, avg_loss=0.07386]\n",
      "Step 525464  [5.338 sec/step, loss=0.07537, avg_loss=0.07386]\n",
      "Step 525465  [5.340 sec/step, loss=0.07536, avg_loss=0.07386]\n",
      "Step 525466  [5.361 sec/step, loss=0.07587, avg_loss=0.07388]\n",
      "Step 525467  [5.370 sec/step, loss=0.07409, avg_loss=0.07386]\n",
      "Step 525468  [5.381 sec/step, loss=0.07601, avg_loss=0.07387]\n",
      "Step 525469  [5.374 sec/step, loss=0.07560, avg_loss=0.07389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 525470  [5.434 sec/step, loss=0.06607, avg_loss=0.07383]\n",
      "Step 525471  [5.431 sec/step, loss=0.07406, avg_loss=0.07383]\n",
      "Step 525472  [5.472 sec/step, loss=0.07296, avg_loss=0.07384]\n",
      "Generated 32 batches of size 32 in 2.336 sec\n",
      "Step 525473  [5.470 sec/step, loss=0.07261, avg_loss=0.07383]\n",
      "Step 525474  [5.442 sec/step, loss=0.07298, avg_loss=0.07379]\n",
      "Step 525475  [5.422 sec/step, loss=0.07114, avg_loss=0.07377]\n",
      "Step 525476  [5.369 sec/step, loss=0.07392, avg_loss=0.07385]\n",
      "Step 525477  [5.376 sec/step, loss=0.07526, avg_loss=0.07385]\n",
      "Step 525478  [5.368 sec/step, loss=0.07654, avg_loss=0.07385]\n",
      "Step 525479  [5.376 sec/step, loss=0.07544, avg_loss=0.07390]\n",
      "Step 525480  [5.373 sec/step, loss=0.07568, avg_loss=0.07391]\n",
      "Step 525481  [5.379 sec/step, loss=0.07583, avg_loss=0.07390]\n",
      "Step 525482  [5.398 sec/step, loss=0.07370, avg_loss=0.07398]\n",
      "Step 525483  [5.386 sec/step, loss=0.06509, avg_loss=0.07391]\n",
      "Step 525484  [5.398 sec/step, loss=0.07617, avg_loss=0.07392]\n",
      "Step 525485  [5.379 sec/step, loss=0.07353, avg_loss=0.07389]\n",
      "Step 525486  [5.373 sec/step, loss=0.07373, avg_loss=0.07387]\n",
      "Step 525487  [5.435 sec/step, loss=0.06594, avg_loss=0.07382]\n",
      "Step 525488  [5.445 sec/step, loss=0.07630, avg_loss=0.07383]\n",
      "Step 525489  [5.444 sec/step, loss=0.07278, avg_loss=0.07380]\n",
      "Step 525490  [5.434 sec/step, loss=0.07243, avg_loss=0.07380]\n",
      "Step 525491  [5.454 sec/step, loss=0.07511, avg_loss=0.07390]\n",
      "Step 525492  [5.465 sec/step, loss=0.07617, avg_loss=0.07392]\n",
      "Step 525493  [5.485 sec/step, loss=0.07394, avg_loss=0.07392]\n",
      "Step 525494  [5.471 sec/step, loss=0.07103, avg_loss=0.07388]\n",
      "Step 525495  [5.477 sec/step, loss=0.07353, avg_loss=0.07387]\n",
      "Step 525496  [5.427 sec/step, loss=0.07498, avg_loss=0.07396]\n",
      "Step 525497  [5.422 sec/step, loss=0.07454, avg_loss=0.07395]\n",
      "Step 525498  [5.382 sec/step, loss=0.07085, avg_loss=0.07393]\n",
      "Step 525499  [5.396 sec/step, loss=0.07549, avg_loss=0.07395]\n",
      "Step 525500  [5.378 sec/step, loss=0.07531, avg_loss=0.07394]\n",
      "Writing summary at step: 525500\n",
      "Step 525501  [5.390 sec/step, loss=0.07612, avg_loss=0.07395]\n",
      "Step 525502  [5.399 sec/step, loss=0.07638, avg_loss=0.07398]\n",
      "Step 525503  [5.401 sec/step, loss=0.07570, avg_loss=0.07398]\n",
      "Generated 32 batches of size 32 in 2.377 sec\n",
      "Step 525504  [5.402 sec/step, loss=0.07487, avg_loss=0.07396]\n",
      "Step 525505  [5.405 sec/step, loss=0.07617, avg_loss=0.07396]\n",
      "Step 525506  [5.381 sec/step, loss=0.07494, avg_loss=0.07398]\n",
      "Step 525507  [5.399 sec/step, loss=0.07509, avg_loss=0.07401]\n",
      "Step 525508  [5.401 sec/step, loss=0.07520, avg_loss=0.07402]\n",
      "Step 525509  [5.396 sec/step, loss=0.07451, avg_loss=0.07400]\n",
      "Step 525510  [5.385 sec/step, loss=0.07578, avg_loss=0.07402]\n",
      "Step 525511  [5.380 sec/step, loss=0.07441, avg_loss=0.07404]\n",
      "Step 525512  [5.368 sec/step, loss=0.07231, avg_loss=0.07401]\n",
      "Step 525513  [5.392 sec/step, loss=0.07341, avg_loss=0.07400]\n",
      "Step 525514  [5.393 sec/step, loss=0.07622, avg_loss=0.07400]\n",
      "Step 525515  [5.399 sec/step, loss=0.07303, avg_loss=0.07400]\n",
      "Step 525516  [5.388 sec/step, loss=0.07400, avg_loss=0.07402]\n",
      "Step 525517  [5.375 sec/step, loss=0.07563, avg_loss=0.07402]\n",
      "Step 525518  [5.382 sec/step, loss=0.07264, avg_loss=0.07403]\n",
      "Step 525519  [5.396 sec/step, loss=0.07608, avg_loss=0.07405]\n",
      "Step 525520  [5.393 sec/step, loss=0.07143, avg_loss=0.07405]\n",
      "Step 525521  [5.381 sec/step, loss=0.06504, avg_loss=0.07397]\n",
      "Step 525522  [5.378 sec/step, loss=0.07427, avg_loss=0.07396]\n",
      "Step 525523  [5.373 sec/step, loss=0.07235, avg_loss=0.07395]\n",
      "Step 525524  [5.386 sec/step, loss=0.07606, avg_loss=0.07398]\n",
      "Step 525525  [5.357 sec/step, loss=0.07544, avg_loss=0.07400]\n",
      "Step 525526  [5.381 sec/step, loss=0.07408, avg_loss=0.07399]\n",
      "Step 525527  [5.382 sec/step, loss=0.07534, avg_loss=0.07399]\n",
      "Step 525528  [5.396 sec/step, loss=0.07398, avg_loss=0.07399]\n",
      "Step 525529  [5.395 sec/step, loss=0.07521, avg_loss=0.07400]\n",
      "Step 525530  [5.388 sec/step, loss=0.07426, avg_loss=0.07398]\n",
      "Step 525531  [5.337 sec/step, loss=0.07487, avg_loss=0.07406]\n",
      "Step 525532  [5.341 sec/step, loss=0.07385, avg_loss=0.07405]\n",
      "Step 525533  [5.339 sec/step, loss=0.07600, avg_loss=0.07407]\n",
      "Step 525534  [5.344 sec/step, loss=0.07383, avg_loss=0.07407]\n",
      "Step 525535  [5.322 sec/step, loss=0.07116, avg_loss=0.07404]\n",
      "Generated 32 batches of size 32 in 2.425 sec\n",
      "Step 525536  [5.307 sec/step, loss=0.07464, avg_loss=0.07403]\n",
      "Step 525537  [5.323 sec/step, loss=0.07368, avg_loss=0.07402]\n",
      "Step 525538  [5.343 sec/step, loss=0.07509, avg_loss=0.07410]\n",
      "Step 525539  [5.331 sec/step, loss=0.07335, avg_loss=0.07407]\n",
      "Step 525540  [5.335 sec/step, loss=0.07356, avg_loss=0.07405]\n",
      "Step 525541  [5.330 sec/step, loss=0.07383, avg_loss=0.07403]\n",
      "Step 525542  [5.375 sec/step, loss=0.06703, avg_loss=0.07395]\n",
      "Step 525543  [5.396 sec/step, loss=0.07596, avg_loss=0.07398]\n",
      "Step 525544  [5.403 sec/step, loss=0.07507, avg_loss=0.07397]\n",
      "Step 525545  [5.403 sec/step, loss=0.07389, avg_loss=0.07395]\n",
      "Step 525546  [5.386 sec/step, loss=0.07517, avg_loss=0.07394]\n",
      "Step 525547  [5.374 sec/step, loss=0.07392, avg_loss=0.07392]\n",
      "Step 525548  [5.388 sec/step, loss=0.07620, avg_loss=0.07397]\n",
      "Step 525549  [5.384 sec/step, loss=0.07476, avg_loss=0.07396]\n",
      "Step 525550  [5.384 sec/step, loss=0.07394, avg_loss=0.07396]\n",
      "Step 525551  [5.373 sec/step, loss=0.07549, avg_loss=0.07395]\n",
      "Step 525552  [5.387 sec/step, loss=0.07645, avg_loss=0.07399]\n",
      "Step 525553  [5.357 sec/step, loss=0.06652, avg_loss=0.07389]\n",
      "Step 525554  [5.380 sec/step, loss=0.07493, avg_loss=0.07390]\n",
      "Step 525555  [5.419 sec/step, loss=0.06919, avg_loss=0.07384]\n",
      "Step 525556  [5.436 sec/step, loss=0.07429, avg_loss=0.07384]\n",
      "Step 525557  [5.442 sec/step, loss=0.07517, avg_loss=0.07386]\n",
      "Step 525558  [5.462 sec/step, loss=0.07528, avg_loss=0.07394]\n",
      "Step 525559  [5.461 sec/step, loss=0.07397, avg_loss=0.07393]\n",
      "Step 525560  [5.458 sec/step, loss=0.07415, avg_loss=0.07395]\n",
      "Step 525561  [5.456 sec/step, loss=0.07421, avg_loss=0.07394]\n",
      "Step 525562  [5.461 sec/step, loss=0.07444, avg_loss=0.07393]\n",
      "Step 525563  [5.463 sec/step, loss=0.07519, avg_loss=0.07394]\n",
      "Step 525564  [5.466 sec/step, loss=0.07600, avg_loss=0.07395]\n",
      "Step 525565  [5.456 sec/step, loss=0.07185, avg_loss=0.07392]\n",
      "Step 525566  [5.431 sec/step, loss=0.07265, avg_loss=0.07388]\n",
      "Step 525567  [5.435 sec/step, loss=0.07598, avg_loss=0.07390]\n",
      "Generated 32 batches of size 32 in 2.379 sec\n",
      "Step 525568  [5.440 sec/step, loss=0.07671, avg_loss=0.07391]\n",
      "Step 525569  [5.422 sec/step, loss=0.07340, avg_loss=0.07389]\n",
      "Step 525570  [5.366 sec/step, loss=0.07033, avg_loss=0.07393]\n",
      "Step 525571  [5.376 sec/step, loss=0.07530, avg_loss=0.07394]\n",
      "Step 525572  [5.339 sec/step, loss=0.07448, avg_loss=0.07396]\n",
      "Step 525573  [5.333 sec/step, loss=0.07542, avg_loss=0.07399]\n",
      "Step 525574  [5.347 sec/step, loss=0.07461, avg_loss=0.07400]\n",
      "Step 525575  [5.374 sec/step, loss=0.07590, avg_loss=0.07405]\n",
      "Step 525576  [5.387 sec/step, loss=0.07636, avg_loss=0.07407]\n",
      "Step 525577  [5.375 sec/step, loss=0.07348, avg_loss=0.07406]\n",
      "Step 525578  [5.382 sec/step, loss=0.07568, avg_loss=0.07405]\n",
      "Step 525579  [5.360 sec/step, loss=0.06544, avg_loss=0.07395]\n",
      "Step 525580  [5.364 sec/step, loss=0.07146, avg_loss=0.07391]\n",
      "Step 525581  [5.339 sec/step, loss=0.07439, avg_loss=0.07389]\n",
      "Step 525582  [5.339 sec/step, loss=0.07555, avg_loss=0.07391]\n",
      "Step 525583  [5.356 sec/step, loss=0.07559, avg_loss=0.07401]\n",
      "Step 525584  [5.339 sec/step, loss=0.07170, avg_loss=0.07397]\n",
      "Step 525585  [5.347 sec/step, loss=0.07533, avg_loss=0.07399]\n",
      "Step 525586  [5.352 sec/step, loss=0.07562, avg_loss=0.07401]\n",
      "Step 525587  [5.311 sec/step, loss=0.07650, avg_loss=0.07411]\n",
      "Step 525588  [5.297 sec/step, loss=0.07496, avg_loss=0.07410]\n",
      "Step 525589  [5.314 sec/step, loss=0.07583, avg_loss=0.07413]\n",
      "Step 525590  [5.330 sec/step, loss=0.07555, avg_loss=0.07416]\n",
      "Step 525591  [5.321 sec/step, loss=0.07390, avg_loss=0.07415]\n",
      "Step 525592  [5.362 sec/step, loss=0.06655, avg_loss=0.07405]\n",
      "Step 525593  [5.352 sec/step, loss=0.07191, avg_loss=0.07403]\n",
      "Step 525594  [5.349 sec/step, loss=0.07104, avg_loss=0.07403]\n",
      "Step 525595  [5.338 sec/step, loss=0.07313, avg_loss=0.07403]\n",
      "Step 525596  [5.343 sec/step, loss=0.07627, avg_loss=0.07404]\n",
      "Step 525597  [5.333 sec/step, loss=0.07044, avg_loss=0.07400]\n",
      "Step 525598  [5.365 sec/step, loss=0.07585, avg_loss=0.07405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 525599  [5.331 sec/step, loss=0.07410, avg_loss=0.07404]\n",
      "Generated 32 batches of size 32 in 2.336 sec\n",
      "Step 525600  [5.342 sec/step, loss=0.07447, avg_loss=0.07403]\n",
      "Writing summary at step: 525600\n",
      "Step 525601  [5.344 sec/step, loss=0.07348, avg_loss=0.07400]\n",
      "Step 525602  [5.354 sec/step, loss=0.07616, avg_loss=0.07400]\n",
      "Step 525603  [5.345 sec/step, loss=0.07367, avg_loss=0.07398]\n",
      "Step 525604  [5.344 sec/step, loss=0.07517, avg_loss=0.07398]\n",
      "Step 525605  [5.330 sec/step, loss=0.07483, avg_loss=0.07397]\n",
      "Step 525606  [5.337 sec/step, loss=0.07568, avg_loss=0.07398]\n",
      "Step 525607  [5.355 sec/step, loss=0.07315, avg_loss=0.07396]\n",
      "Step 525608  [5.369 sec/step, loss=0.07619, avg_loss=0.07397]\n",
      "Step 525609  [5.358 sec/step, loss=0.07430, avg_loss=0.07396]\n",
      "Step 525610  [5.350 sec/step, loss=0.07603, avg_loss=0.07397]\n",
      "Step 525611  [5.357 sec/step, loss=0.07208, avg_loss=0.07394]\n",
      "Step 525612  [5.409 sec/step, loss=0.06682, avg_loss=0.07389]\n",
      "Step 525613  [5.388 sec/step, loss=0.07420, avg_loss=0.07390]\n",
      "Step 525614  [5.390 sec/step, loss=0.07571, avg_loss=0.07389]\n",
      "Step 525615  [5.381 sec/step, loss=0.07304, avg_loss=0.07389]\n",
      "Step 525616  [5.381 sec/step, loss=0.07413, avg_loss=0.07389]\n",
      "Step 525617  [5.376 sec/step, loss=0.07276, avg_loss=0.07386]\n",
      "Step 525618  [5.395 sec/step, loss=0.07600, avg_loss=0.07390]\n",
      "Step 525619  [5.407 sec/step, loss=0.07375, avg_loss=0.07387]\n",
      "Step 525620  [5.421 sec/step, loss=0.07402, avg_loss=0.07390]\n",
      "Step 525621  [5.441 sec/step, loss=0.07242, avg_loss=0.07397]\n",
      "Step 525622  [5.455 sec/step, loss=0.07597, avg_loss=0.07399]\n",
      "Step 525623  [5.465 sec/step, loss=0.07426, avg_loss=0.07401]\n",
      "Step 525624  [5.439 sec/step, loss=0.06544, avg_loss=0.07390]\n",
      "Step 525625  [5.446 sec/step, loss=0.07454, avg_loss=0.07389]\n",
      "Step 525626  [5.416 sec/step, loss=0.07514, avg_loss=0.07391]\n",
      "Step 525627  [5.421 sec/step, loss=0.07592, avg_loss=0.07391]\n",
      "Step 525628  [5.414 sec/step, loss=0.07502, avg_loss=0.07392]\n",
      "Step 525629  [5.406 sec/step, loss=0.07320, avg_loss=0.07390]\n",
      "Step 525630  [5.395 sec/step, loss=0.07144, avg_loss=0.07387]\n",
      "Generated 32 batches of size 32 in 2.370 sec\n",
      "Step 525631  [5.411 sec/step, loss=0.07352, avg_loss=0.07386]\n",
      "Step 525632  [5.408 sec/step, loss=0.07508, avg_loss=0.07387]\n",
      "Step 525633  [5.420 sec/step, loss=0.07316, avg_loss=0.07384]\n",
      "Step 525634  [5.439 sec/step, loss=0.07618, avg_loss=0.07387]\n",
      "Step 525635  [5.454 sec/step, loss=0.07440, avg_loss=0.07390]\n",
      "Step 525636  [5.459 sec/step, loss=0.07572, avg_loss=0.07391]\n",
      "Step 525637  [5.426 sec/step, loss=0.07141, avg_loss=0.07389]\n",
      "Step 525638  [5.432 sec/step, loss=0.07644, avg_loss=0.07390]\n",
      "Step 525639  [5.443 sec/step, loss=0.07646, avg_loss=0.07393]\n",
      "Step 525640  [5.430 sec/step, loss=0.07354, avg_loss=0.07393]\n",
      "Step 525641  [5.435 sec/step, loss=0.07595, avg_loss=0.07395]\n",
      "Step 525642  [5.396 sec/step, loss=0.07624, avg_loss=0.07405]\n",
      "Step 525643  [5.408 sec/step, loss=0.07542, avg_loss=0.07404]\n",
      "Step 525644  [5.405 sec/step, loss=0.07499, avg_loss=0.07404]\n",
      "Step 525645  [5.403 sec/step, loss=0.07313, avg_loss=0.07403]\n",
      "Step 525646  [5.394 sec/step, loss=0.07331, avg_loss=0.07401]\n",
      "Step 525647  [5.391 sec/step, loss=0.07560, avg_loss=0.07403]\n",
      "Step 525648  [5.370 sec/step, loss=0.07232, avg_loss=0.07399]\n",
      "Step 525649  [5.370 sec/step, loss=0.07573, avg_loss=0.07400]\n",
      "Step 525650  [5.386 sec/step, loss=0.07590, avg_loss=0.07402]\n",
      "Step 525651  [5.401 sec/step, loss=0.07559, avg_loss=0.07402]\n",
      "Step 525652  [5.385 sec/step, loss=0.07363, avg_loss=0.07399]\n",
      "Step 525653  [5.386 sec/step, loss=0.06784, avg_loss=0.07401]\n",
      "Step 525654  [5.361 sec/step, loss=0.07474, avg_loss=0.07400]\n",
      "Step 525655  [5.369 sec/step, loss=0.06639, avg_loss=0.07398]\n",
      "Step 525656  [5.354 sec/step, loss=0.07143, avg_loss=0.07395]\n",
      "Step 525657  [5.366 sec/step, loss=0.07594, avg_loss=0.07396]\n",
      "Step 525658  [5.366 sec/step, loss=0.07485, avg_loss=0.07395]\n",
      "Step 525659  [5.379 sec/step, loss=0.07430, avg_loss=0.07395]\n",
      "Step 525660  [5.390 sec/step, loss=0.07686, avg_loss=0.07398]\n",
      "Step 525661  [5.383 sec/step, loss=0.07436, avg_loss=0.07398]\n",
      "Step 525662  [5.389 sec/step, loss=0.07597, avg_loss=0.07400]\n",
      "Generated 32 batches of size 32 in 2.588 sec\n",
      "Step 525663  [5.392 sec/step, loss=0.07395, avg_loss=0.07399]\n",
      "Step 525664  [5.385 sec/step, loss=0.07445, avg_loss=0.07397]\n",
      "Step 525665  [5.397 sec/step, loss=0.07244, avg_loss=0.07398]\n",
      "Step 525666  [5.416 sec/step, loss=0.07575, avg_loss=0.07401]\n",
      "Step 525667  [5.411 sec/step, loss=0.07525, avg_loss=0.07400]\n",
      "Step 525668  [5.391 sec/step, loss=0.07378, avg_loss=0.07397]\n",
      "Step 525669  [5.407 sec/step, loss=0.07408, avg_loss=0.07398]\n",
      "Step 525670  [5.410 sec/step, loss=0.07496, avg_loss=0.07402]\n",
      "Step 525671  [5.409 sec/step, loss=0.07499, avg_loss=0.07402]\n",
      "Step 525672  [5.424 sec/step, loss=0.07429, avg_loss=0.07402]\n",
      "Step 525673  [5.449 sec/step, loss=0.07381, avg_loss=0.07400]\n",
      "Step 525674  [5.439 sec/step, loss=0.07043, avg_loss=0.07396]\n",
      "Step 525675  [5.417 sec/step, loss=0.07501, avg_loss=0.07395]\n",
      "Step 525676  [5.421 sec/step, loss=0.07428, avg_loss=0.07393]\n",
      "Step 525677  [5.423 sec/step, loss=0.07145, avg_loss=0.07391]\n",
      "Step 525678  [5.416 sec/step, loss=0.07589, avg_loss=0.07391]\n",
      "Step 525679  [5.433 sec/step, loss=0.07540, avg_loss=0.07401]\n",
      "Step 525680  [5.445 sec/step, loss=0.07593, avg_loss=0.07406]\n",
      "Step 525681  [5.473 sec/step, loss=0.07310, avg_loss=0.07404]\n",
      "Step 525682  [5.480 sec/step, loss=0.07586, avg_loss=0.07405]\n",
      "Step 525683  [5.485 sec/step, loss=0.07593, avg_loss=0.07405]\n",
      "Step 525684  [5.504 sec/step, loss=0.07642, avg_loss=0.07410]\n",
      "Step 525685  [5.510 sec/step, loss=0.07629, avg_loss=0.07411]\n",
      "Step 525686  [5.510 sec/step, loss=0.07564, avg_loss=0.07411]\n",
      "Step 525687  [5.514 sec/step, loss=0.07637, avg_loss=0.07411]\n",
      "Step 525688  [5.505 sec/step, loss=0.07446, avg_loss=0.07410]\n",
      "Step 525689  [5.505 sec/step, loss=0.07468, avg_loss=0.07409]\n",
      "Step 525690  [5.482 sec/step, loss=0.07171, avg_loss=0.07405]\n",
      "Step 525691  [5.489 sec/step, loss=0.07482, avg_loss=0.07406]\n",
      "Step 525692  [5.439 sec/step, loss=0.07441, avg_loss=0.07414]\n",
      "Step 525693  [5.429 sec/step, loss=0.07377, avg_loss=0.07416]\n",
      "Step 525694  [5.451 sec/step, loss=0.07340, avg_loss=0.07418]\n",
      "Generated 32 batches of size 32 in 2.621 sec\n",
      "Step 525695  [5.454 sec/step, loss=0.07192, avg_loss=0.07417]\n",
      "Step 525696  [5.446 sec/step, loss=0.07299, avg_loss=0.07414]\n",
      "Step 525697  [5.453 sec/step, loss=0.07385, avg_loss=0.07417]\n",
      "Step 525698  [5.417 sec/step, loss=0.06621, avg_loss=0.07407]\n",
      "Step 525699  [5.432 sec/step, loss=0.07259, avg_loss=0.07406]\n",
      "Step 525700  [5.417 sec/step, loss=0.07322, avg_loss=0.07405]\n",
      "Writing summary at step: 525700\n",
      "Step 525701  [5.409 sec/step, loss=0.07264, avg_loss=0.07404]\n",
      "Step 525702  [5.399 sec/step, loss=0.07591, avg_loss=0.07404]\n",
      "Step 525703  [5.418 sec/step, loss=0.07554, avg_loss=0.07405]\n",
      "Step 525704  [5.422 sec/step, loss=0.07610, avg_loss=0.07406]\n",
      "Step 525705  [5.426 sec/step, loss=0.07523, avg_loss=0.07407]\n",
      "Step 525706  [5.424 sec/step, loss=0.07266, avg_loss=0.07404]\n",
      "Step 525707  [5.397 sec/step, loss=0.07482, avg_loss=0.07405]\n",
      "Step 525708  [5.405 sec/step, loss=0.07555, avg_loss=0.07405]\n",
      "Step 525709  [5.409 sec/step, loss=0.07539, avg_loss=0.07406]\n",
      "Step 525710  [5.416 sec/step, loss=0.07662, avg_loss=0.07407]\n",
      "Step 525711  [5.421 sec/step, loss=0.07226, avg_loss=0.07407]\n",
      "Step 525712  [5.356 sec/step, loss=0.06546, avg_loss=0.07405]\n",
      "Step 525713  [5.360 sec/step, loss=0.07410, avg_loss=0.07405]\n",
      "Step 525714  [5.337 sec/step, loss=0.07200, avg_loss=0.07402]\n",
      "Step 525715  [5.345 sec/step, loss=0.07490, avg_loss=0.07403]\n",
      "Step 525716  [5.366 sec/step, loss=0.07396, avg_loss=0.07403]\n",
      "Step 525717  [5.355 sec/step, loss=0.07191, avg_loss=0.07402]\n",
      "Step 525718  [5.355 sec/step, loss=0.07617, avg_loss=0.07403]\n",
      "Step 525719  [5.326 sec/step, loss=0.07425, avg_loss=0.07403]\n",
      "Step 525720  [5.318 sec/step, loss=0.07039, avg_loss=0.07399]\n",
      "Step 525721  [5.365 sec/step, loss=0.06622, avg_loss=0.07393]\n",
      "Step 525722  [5.364 sec/step, loss=0.07310, avg_loss=0.07390]\n",
      "Step 525723  [5.369 sec/step, loss=0.07668, avg_loss=0.07393]\n",
      "Step 525724  [5.389 sec/step, loss=0.07577, avg_loss=0.07403]\n",
      "Step 525725  [5.390 sec/step, loss=0.07518, avg_loss=0.07404]\n",
      "Generated 32 batches of size 32 in 2.561 sec\n",
      "Step 525726  [5.390 sec/step, loss=0.07417, avg_loss=0.07403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 525727  [5.384 sec/step, loss=0.07502, avg_loss=0.07402]\n",
      "Step 525728  [5.387 sec/step, loss=0.07444, avg_loss=0.07401]\n",
      "Step 525729  [5.384 sec/step, loss=0.07421, avg_loss=0.07402]\n",
      "Step 525730  [5.399 sec/step, loss=0.07511, avg_loss=0.07406]\n",
      "Step 525731  [5.384 sec/step, loss=0.07257, avg_loss=0.07405]\n",
      "Step 525732  [5.398 sec/step, loss=0.07632, avg_loss=0.07406]\n",
      "Step 525733  [5.387 sec/step, loss=0.07683, avg_loss=0.07410]\n",
      "Step 525734  [5.400 sec/step, loss=0.07333, avg_loss=0.07407]\n",
      "Step 525735  [5.388 sec/step, loss=0.07072, avg_loss=0.07403]\n",
      "Step 525736  [5.374 sec/step, loss=0.07377, avg_loss=0.07401]\n",
      "Step 525737  [5.398 sec/step, loss=0.07654, avg_loss=0.07407]\n",
      "Step 525738  [5.375 sec/step, loss=0.07183, avg_loss=0.07402]\n",
      "Step 525739  [5.367 sec/step, loss=0.07263, avg_loss=0.07398]\n",
      "Step 525740  [5.354 sec/step, loss=0.06660, avg_loss=0.07391]\n",
      "Step 525741  [5.356 sec/step, loss=0.07266, avg_loss=0.07388]\n",
      "Step 525742  [5.355 sec/step, loss=0.07513, avg_loss=0.07387]\n",
      "Step 525743  [5.346 sec/step, loss=0.07501, avg_loss=0.07386]\n",
      "Step 525744  [5.341 sec/step, loss=0.07517, avg_loss=0.07387]\n",
      "Step 525745  [5.358 sec/step, loss=0.07288, avg_loss=0.07386]\n",
      "Step 525746  [5.373 sec/step, loss=0.07624, avg_loss=0.07389]\n",
      "Step 525747  [5.423 sec/step, loss=0.06724, avg_loss=0.07381]\n",
      "Step 525748  [5.433 sec/step, loss=0.07499, avg_loss=0.07384]\n",
      "Step 525749  [5.449 sec/step, loss=0.07628, avg_loss=0.07384]\n",
      "Step 525750  [5.456 sec/step, loss=0.07531, avg_loss=0.07384]\n",
      "Step 525751  [5.438 sec/step, loss=0.07300, avg_loss=0.07381]\n",
      "Step 525752  [5.438 sec/step, loss=0.07385, avg_loss=0.07381]\n",
      "Step 525753  [5.462 sec/step, loss=0.07684, avg_loss=0.07390]\n",
      "Step 525754  [5.452 sec/step, loss=0.07152, avg_loss=0.07387]\n",
      "Step 525755  [5.413 sec/step, loss=0.07610, avg_loss=0.07397]\n",
      "Step 525756  [5.418 sec/step, loss=0.07543, avg_loss=0.07401]\n",
      "Step 525757  [5.409 sec/step, loss=0.07622, avg_loss=0.07401]\n",
      "Generated 32 batches of size 32 in 2.445 sec\n",
      "Step 525758  [5.414 sec/step, loss=0.07576, avg_loss=0.07402]\n",
      "Step 525759  [5.403 sec/step, loss=0.07534, avg_loss=0.07403]\n",
      "Step 525760  [5.389 sec/step, loss=0.07463, avg_loss=0.07401]\n",
      "Step 525761  [5.396 sec/step, loss=0.07466, avg_loss=0.07401]\n",
      "Step 525762  [5.392 sec/step, loss=0.07582, avg_loss=0.07401]\n",
      "Step 525763  [5.395 sec/step, loss=0.07478, avg_loss=0.07402]\n",
      "Step 525764  [5.391 sec/step, loss=0.07389, avg_loss=0.07401]\n",
      "Step 525765  [5.397 sec/step, loss=0.07648, avg_loss=0.07405]\n",
      "Step 525766  [5.398 sec/step, loss=0.07540, avg_loss=0.07405]\n",
      "Step 525767  [5.378 sec/step, loss=0.07137, avg_loss=0.07401]\n",
      "Step 525768  [5.411 sec/step, loss=0.07536, avg_loss=0.07403]\n",
      "Step 525769  [5.403 sec/step, loss=0.07498, avg_loss=0.07403]\n",
      "Step 525770  [5.409 sec/step, loss=0.07253, avg_loss=0.07401]\n",
      "Step 525771  [5.405 sec/step, loss=0.07457, avg_loss=0.07401]\n",
      "Step 525772  [5.412 sec/step, loss=0.07600, avg_loss=0.07402]\n",
      "Step 525773  [5.398 sec/step, loss=0.07631, avg_loss=0.07405]\n",
      "Step 525774  [5.404 sec/step, loss=0.07270, avg_loss=0.07407]\n",
      "Step 525775  [5.393 sec/step, loss=0.06572, avg_loss=0.07398]\n",
      "Step 525776  [5.383 sec/step, loss=0.07371, avg_loss=0.07397]\n",
      "Step 525777  [5.385 sec/step, loss=0.07262, avg_loss=0.07398]\n",
      "Step 525778  [5.363 sec/step, loss=0.07216, avg_loss=0.07395]\n",
      "Step 525779  [5.360 sec/step, loss=0.07543, avg_loss=0.07395]\n",
      "Step 525780  [5.338 sec/step, loss=0.07438, avg_loss=0.07393]\n",
      "Step 525781  [5.331 sec/step, loss=0.07414, avg_loss=0.07394]\n",
      "Step 525782  [5.330 sec/step, loss=0.07617, avg_loss=0.07394]\n",
      "Step 525783  [5.328 sec/step, loss=0.07639, avg_loss=0.07395]\n",
      "Step 525784  [5.325 sec/step, loss=0.07447, avg_loss=0.07393]\n",
      "Step 525785  [5.317 sec/step, loss=0.07547, avg_loss=0.07392]\n",
      "Step 525786  [5.312 sec/step, loss=0.07551, avg_loss=0.07392]\n",
      "Step 525787  [5.308 sec/step, loss=0.07647, avg_loss=0.07392]\n",
      "Step 525788  [5.311 sec/step, loss=0.07383, avg_loss=0.07391]\n",
      "Step 525789  [5.319 sec/step, loss=0.07509, avg_loss=0.07392]\n",
      "Generated 32 batches of size 32 in 2.501 sec\n",
      "Step 525790  [5.337 sec/step, loss=0.07514, avg_loss=0.07395]\n",
      "Step 525791  [5.328 sec/step, loss=0.07405, avg_loss=0.07395]\n",
      "Step 525792  [5.324 sec/step, loss=0.07334, avg_loss=0.07393]\n",
      "Step 525793  [5.329 sec/step, loss=0.07543, avg_loss=0.07395]\n",
      "Step 525794  [5.338 sec/step, loss=0.07413, avg_loss=0.07396]\n",
      "Step 525795  [5.386 sec/step, loss=0.06590, avg_loss=0.07390]\n",
      "Step 525796  [5.392 sec/step, loss=0.07523, avg_loss=0.07392]\n",
      "Step 525797  [5.402 sec/step, loss=0.07324, avg_loss=0.07391]\n",
      "Step 525798  [5.428 sec/step, loss=0.07436, avg_loss=0.07400]\n",
      "Step 525799  [5.420 sec/step, loss=0.07144, avg_loss=0.07398]\n",
      "Step 525800  [5.410 sec/step, loss=0.06737, avg_loss=0.07393]\n",
      "Writing summary at step: 525800\n",
      "Step 525801  [5.418 sec/step, loss=0.07652, avg_loss=0.07396]\n",
      "Step 525802  [5.412 sec/step, loss=0.07501, avg_loss=0.07396]\n",
      "Step 525803  [5.450 sec/step, loss=0.06732, avg_loss=0.07387]\n",
      "Step 525804  [5.454 sec/step, loss=0.07660, avg_loss=0.07388]\n",
      "Step 525805  [5.463 sec/step, loss=0.07632, avg_loss=0.07389]\n",
      "Step 525806  [5.481 sec/step, loss=0.07641, avg_loss=0.07393]\n",
      "Step 525807  [5.493 sec/step, loss=0.07609, avg_loss=0.07394]\n",
      "Step 525808  [5.477 sec/step, loss=0.07580, avg_loss=0.07394]\n",
      "Step 525809  [5.471 sec/step, loss=0.07386, avg_loss=0.07393]\n",
      "Step 525810  [5.461 sec/step, loss=0.07546, avg_loss=0.07392]\n",
      "Step 525811  [5.461 sec/step, loss=0.07184, avg_loss=0.07391]\n",
      "Step 525812  [5.467 sec/step, loss=0.07161, avg_loss=0.07397]\n",
      "Step 525813  [5.465 sec/step, loss=0.07223, avg_loss=0.07395]\n",
      "Step 525814  [5.476 sec/step, loss=0.07238, avg_loss=0.07396]\n",
      "Step 525815  [5.485 sec/step, loss=0.07652, avg_loss=0.07397]\n",
      "Step 525816  [5.467 sec/step, loss=0.07354, avg_loss=0.07397]\n",
      "Step 525817  [5.470 sec/step, loss=0.07412, avg_loss=0.07399]\n",
      "Step 525818  [5.445 sec/step, loss=0.07220, avg_loss=0.07395]\n",
      "Step 525819  [5.453 sec/step, loss=0.07541, avg_loss=0.07396]\n",
      "Step 525820  [5.461 sec/step, loss=0.07355, avg_loss=0.07399]\n",
      "Generated 32 batches of size 32 in 2.383 sec\n",
      "Step 525821  [5.423 sec/step, loss=0.07576, avg_loss=0.07409]\n",
      "Step 525822  [5.444 sec/step, loss=0.07351, avg_loss=0.07409]\n",
      "Step 525823  [5.435 sec/step, loss=0.07491, avg_loss=0.07408]\n",
      "Step 525824  [5.451 sec/step, loss=0.07528, avg_loss=0.07407]\n",
      "Step 525825  [5.451 sec/step, loss=0.07519, avg_loss=0.07407]\n",
      "Step 525826  [5.451 sec/step, loss=0.07419, avg_loss=0.07407]\n",
      "Step 525827  [5.460 sec/step, loss=0.07644, avg_loss=0.07409]\n",
      "Step 525828  [5.466 sec/step, loss=0.07477, avg_loss=0.07409]\n",
      "Step 525829  [5.483 sec/step, loss=0.07463, avg_loss=0.07409]\n",
      "Step 525830  [5.506 sec/step, loss=0.07325, avg_loss=0.07407]\n",
      "Step 525831  [5.505 sec/step, loss=0.07086, avg_loss=0.07406]\n",
      "Step 525832  [5.486 sec/step, loss=0.07537, avg_loss=0.07405]\n",
      "Step 525833  [5.473 sec/step, loss=0.07306, avg_loss=0.07401]\n",
      "Step 525834  [5.448 sec/step, loss=0.07581, avg_loss=0.07404]\n",
      "Step 525835  [5.468 sec/step, loss=0.07620, avg_loss=0.07409]\n",
      "Step 525836  [5.482 sec/step, loss=0.07636, avg_loss=0.07412]\n",
      "Step 525837  [5.453 sec/step, loss=0.06475, avg_loss=0.07400]\n",
      "Step 525838  [5.459 sec/step, loss=0.07401, avg_loss=0.07402]\n",
      "Step 525839  [5.460 sec/step, loss=0.07558, avg_loss=0.07405]\n",
      "Step 525840  [5.484 sec/step, loss=0.07502, avg_loss=0.07413]\n",
      "Step 525841  [5.492 sec/step, loss=0.07578, avg_loss=0.07417]\n",
      "Step 525842  [5.503 sec/step, loss=0.07312, avg_loss=0.07414]\n",
      "Step 525843  [5.493 sec/step, loss=0.07613, avg_loss=0.07416]\n",
      "Step 525844  [5.494 sec/step, loss=0.07175, avg_loss=0.07412]\n",
      "Step 525845  [5.470 sec/step, loss=0.07561, avg_loss=0.07415]\n",
      "Step 525846  [5.455 sec/step, loss=0.07166, avg_loss=0.07410]\n",
      "Step 525847  [5.417 sec/step, loss=0.07535, avg_loss=0.07418]\n",
      "Step 525848  [5.414 sec/step, loss=0.07364, avg_loss=0.07417]\n",
      "Step 525849  [5.413 sec/step, loss=0.07598, avg_loss=0.07417]\n",
      "Step 525850  [5.388 sec/step, loss=0.07139, avg_loss=0.07413]\n",
      "Step 525851  [5.387 sec/step, loss=0.07542, avg_loss=0.07415]\n",
      "Step 525852  [5.391 sec/step, loss=0.07467, avg_loss=0.07416]\n",
      "Generated 32 batches of size 32 in 2.419 sec\n",
      "Step 525853  [5.389 sec/step, loss=0.07447, avg_loss=0.07414]\n",
      "Step 525854  [5.405 sec/step, loss=0.07481, avg_loss=0.07417]\n",
      "Step 525855  [5.437 sec/step, loss=0.06927, avg_loss=0.07410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 525856  [5.446 sec/step, loss=0.07574, avg_loss=0.07410]\n",
      "Step 525857  [5.449 sec/step, loss=0.07339, avg_loss=0.07408]\n",
      "Step 525858  [5.432 sec/step, loss=0.07353, avg_loss=0.07405]\n",
      "Step 525859  [5.424 sec/step, loss=0.07477, avg_loss=0.07405]\n",
      "Step 525860  [5.438 sec/step, loss=0.07651, avg_loss=0.07407]\n",
      "Step 525861  [5.443 sec/step, loss=0.07366, avg_loss=0.07406]\n",
      "Step 525862  [5.450 sec/step, loss=0.07595, avg_loss=0.07406]\n",
      "Step 525863  [5.443 sec/step, loss=0.07437, avg_loss=0.07405]\n",
      "Step 525864  [5.450 sec/step, loss=0.07157, avg_loss=0.07403]\n",
      "Step 525865  [5.452 sec/step, loss=0.07379, avg_loss=0.07400]\n",
      "Step 525866  [5.451 sec/step, loss=0.07455, avg_loss=0.07400]\n",
      "Step 525867  [5.470 sec/step, loss=0.07566, avg_loss=0.07404]\n",
      "Step 525868  [5.457 sec/step, loss=0.07591, avg_loss=0.07404]\n",
      "Step 525869  [5.461 sec/step, loss=0.07589, avg_loss=0.07405]\n",
      "Step 525870  [5.508 sec/step, loss=0.06629, avg_loss=0.07399]\n",
      "Step 525871  [5.496 sec/step, loss=0.07212, avg_loss=0.07397]\n",
      "Step 525872  [5.498 sec/step, loss=0.07423, avg_loss=0.07395]\n",
      "Step 525873  [5.485 sec/step, loss=0.07393, avg_loss=0.07393]\n",
      "Step 525874  [5.484 sec/step, loss=0.07366, avg_loss=0.07393]\n",
      "Step 525875  [5.526 sec/step, loss=0.07525, avg_loss=0.07403]\n",
      "Step 525876  [5.528 sec/step, loss=0.07536, avg_loss=0.07405]\n",
      "Step 525877  [5.525 sec/step, loss=0.07526, avg_loss=0.07407]\n",
      "Step 525878  [5.542 sec/step, loss=0.07581, avg_loss=0.07411]\n",
      "Step 525879  [5.549 sec/step, loss=0.07523, avg_loss=0.07411]\n",
      "Step 525880  [5.574 sec/step, loss=0.07627, avg_loss=0.07413]\n",
      "Step 525881  [5.557 sec/step, loss=0.07363, avg_loss=0.07412]\n",
      "Step 525882  [5.558 sec/step, loss=0.07658, avg_loss=0.07413]\n",
      "Step 525883  [5.548 sec/step, loss=0.07406, avg_loss=0.07410]\n",
      "Step 525884  [5.539 sec/step, loss=0.07275, avg_loss=0.07408]\n",
      "Generated 32 batches of size 32 in 2.462 sec\n",
      "Step 525885  [5.555 sec/step, loss=0.07663, avg_loss=0.07410]\n",
      "Step 525886  [5.539 sec/step, loss=0.06759, avg_loss=0.07402]\n",
      "Step 525887  [5.521 sec/step, loss=0.07443, avg_loss=0.07400]\n",
      "Step 525888  [5.532 sec/step, loss=0.07562, avg_loss=0.07401]\n",
      "Step 525889  [5.511 sec/step, loss=0.07368, avg_loss=0.07400]\n",
      "Step 525890  [5.504 sec/step, loss=0.07538, avg_loss=0.07400]\n",
      "Step 525891  [5.522 sec/step, loss=0.07540, avg_loss=0.07402]\n",
      "Step 525892  [5.516 sec/step, loss=0.07170, avg_loss=0.07400]\n",
      "Step 525893  [5.512 sec/step, loss=0.07412, avg_loss=0.07399]\n",
      "Step 525894  [5.487 sec/step, loss=0.07327, avg_loss=0.07398]\n",
      "Step 525895  [5.440 sec/step, loss=0.07486, avg_loss=0.07407]\n",
      "Step 525896  [5.419 sec/step, loss=0.06634, avg_loss=0.07398]\n",
      "Step 525897  [5.429 sec/step, loss=0.07479, avg_loss=0.07399]\n",
      "Step 525898  [5.421 sec/step, loss=0.07515, avg_loss=0.07400]\n",
      "Step 525899  [5.443 sec/step, loss=0.07554, avg_loss=0.07404]\n",
      "Step 525900  [5.462 sec/step, loss=0.07525, avg_loss=0.07412]\n",
      "Writing summary at step: 525900\n",
      "Step 525901  [5.460 sec/step, loss=0.07637, avg_loss=0.07412]\n",
      "Step 525902  [5.461 sec/step, loss=0.07259, avg_loss=0.07410]\n",
      "Step 525903  [5.421 sec/step, loss=0.07363, avg_loss=0.07416]\n",
      "Step 525904  [5.401 sec/step, loss=0.07265, avg_loss=0.07412]\n",
      "Step 525905  [5.400 sec/step, loss=0.07652, avg_loss=0.07412]\n",
      "Step 525906  [5.392 sec/step, loss=0.07615, avg_loss=0.07412]\n",
      "Step 525907  [5.368 sec/step, loss=0.07182, avg_loss=0.07408]\n",
      "Step 525908  [5.366 sec/step, loss=0.07489, avg_loss=0.07407]\n",
      "Step 525909  [5.424 sec/step, loss=0.06699, avg_loss=0.07400]\n",
      "Step 525910  [5.429 sec/step, loss=0.07490, avg_loss=0.07399]\n",
      "Step 525911  [5.451 sec/step, loss=0.07281, avg_loss=0.07400]\n",
      "Step 525912  [5.473 sec/step, loss=0.07642, avg_loss=0.07405]\n",
      "Step 525913  [5.472 sec/step, loss=0.07486, avg_loss=0.07408]\n",
      "Step 525914  [5.469 sec/step, loss=0.07549, avg_loss=0.07411]\n",
      "Step 525915  [5.456 sec/step, loss=0.07424, avg_loss=0.07409]\n",
      "Generated 32 batches of size 32 in 2.373 sec\n",
      "Step 525916  [5.474 sec/step, loss=0.07329, avg_loss=0.07408]\n",
      "Step 525917  [5.474 sec/step, loss=0.07443, avg_loss=0.07409]\n",
      "Step 525918  [5.500 sec/step, loss=0.07594, avg_loss=0.07412]\n",
      "Step 525919  [5.495 sec/step, loss=0.07459, avg_loss=0.07412]\n",
      "Step 525920  [5.493 sec/step, loss=0.07515, avg_loss=0.07413]\n",
      "Step 525921  [5.468 sec/step, loss=0.07093, avg_loss=0.07408]\n",
      "Step 525922  [5.454 sec/step, loss=0.07523, avg_loss=0.07410]\n",
      "Step 525923  [5.459 sec/step, loss=0.07438, avg_loss=0.07410]\n",
      "Step 525924  [5.441 sec/step, loss=0.07330, avg_loss=0.07408]\n",
      "Step 525925  [5.433 sec/step, loss=0.07533, avg_loss=0.07408]\n",
      "Step 525926  [5.449 sec/step, loss=0.07682, avg_loss=0.07410]\n",
      "Step 525927  [5.431 sec/step, loss=0.07270, avg_loss=0.07407]\n",
      "Step 525928  [5.476 sec/step, loss=0.06595, avg_loss=0.07398]\n",
      "Step 525929  [5.477 sec/step, loss=0.07632, avg_loss=0.07399]\n",
      "Step 525930  [5.450 sec/step, loss=0.07587, avg_loss=0.07402]\n",
      "Step 525931  [5.458 sec/step, loss=0.07607, avg_loss=0.07407]\n",
      "Step 525932  [5.474 sec/step, loss=0.07470, avg_loss=0.07407]\n",
      "Step 525933  [5.485 sec/step, loss=0.07292, avg_loss=0.07406]\n",
      "Step 525934  [5.475 sec/step, loss=0.07396, avg_loss=0.07405]\n",
      "Step 525935  [5.462 sec/step, loss=0.07523, avg_loss=0.07404]\n",
      "Step 525936  [5.457 sec/step, loss=0.07530, avg_loss=0.07403]\n",
      "Step 525937  [5.489 sec/step, loss=0.07435, avg_loss=0.07412]\n",
      "Step 525938  [5.502 sec/step, loss=0.07496, avg_loss=0.07413]\n",
      "Step 525939  [5.502 sec/step, loss=0.07337, avg_loss=0.07411]\n",
      "Step 525940  [5.490 sec/step, loss=0.07365, avg_loss=0.07410]\n",
      "Step 525941  [5.487 sec/step, loss=0.07697, avg_loss=0.07411]\n",
      "Step 525942  [5.450 sec/step, loss=0.06511, avg_loss=0.07403]\n",
      "Step 525943  [5.433 sec/step, loss=0.07417, avg_loss=0.07401]\n",
      "Step 525944  [5.433 sec/step, loss=0.07263, avg_loss=0.07402]\n",
      "Step 525945  [5.435 sec/step, loss=0.07259, avg_loss=0.07399]\n",
      "Step 525946  [5.445 sec/step, loss=0.07118, avg_loss=0.07398]\n",
      "Step 525947  [5.418 sec/step, loss=0.07129, avg_loss=0.07394]\n",
      "Generated 32 batches of size 32 in 2.474 sec\n",
      "Step 525948  [5.442 sec/step, loss=0.07553, avg_loss=0.07396]\n",
      "Step 525949  [5.443 sec/step, loss=0.07641, avg_loss=0.07396]\n",
      "Step 525950  [5.458 sec/step, loss=0.07455, avg_loss=0.07400]\n",
      "Step 525951  [5.451 sec/step, loss=0.07144, avg_loss=0.07396]\n",
      "Step 525952  [5.460 sec/step, loss=0.07507, avg_loss=0.07396]\n",
      "Step 525953  [5.455 sec/step, loss=0.07499, avg_loss=0.07397]\n",
      "Step 525954  [5.445 sec/step, loss=0.07183, avg_loss=0.07394]\n",
      "Step 525955  [5.410 sec/step, loss=0.07631, avg_loss=0.07401]\n",
      "Step 525956  [5.432 sec/step, loss=0.07344, avg_loss=0.07398]\n",
      "Step 525957  [5.435 sec/step, loss=0.07573, avg_loss=0.07401]\n",
      "Step 525958  [5.438 sec/step, loss=0.07251, avg_loss=0.07400]\n",
      "Step 525959  [5.448 sec/step, loss=0.07404, avg_loss=0.07399]\n",
      "Step 525960  [5.450 sec/step, loss=0.07392, avg_loss=0.07396]\n",
      "Step 525961  [5.452 sec/step, loss=0.07480, avg_loss=0.07397]\n",
      "Step 525962  [5.440 sec/step, loss=0.07514, avg_loss=0.07397]\n",
      "Step 525963  [5.454 sec/step, loss=0.07595, avg_loss=0.07398]\n",
      "Step 525964  [5.453 sec/step, loss=0.07572, avg_loss=0.07402]\n",
      "Step 525965  [5.471 sec/step, loss=0.07350, avg_loss=0.07402]\n",
      "Step 525966  [5.456 sec/step, loss=0.07081, avg_loss=0.07398]\n",
      "Step 525967  [5.441 sec/step, loss=0.07371, avg_loss=0.07396]\n",
      "Step 525968  [5.446 sec/step, loss=0.07534, avg_loss=0.07396]\n",
      "Step 525969  [5.449 sec/step, loss=0.07651, avg_loss=0.07396]\n",
      "Step 525970  [5.384 sec/step, loss=0.07146, avg_loss=0.07402]\n",
      "Step 525971  [5.383 sec/step, loss=0.06575, avg_loss=0.07395]\n",
      "Step 525972  [5.366 sec/step, loss=0.07412, avg_loss=0.07395]\n",
      "Step 525973  [5.375 sec/step, loss=0.07501, avg_loss=0.07396]\n",
      "Step 525974  [5.376 sec/step, loss=0.07254, avg_loss=0.07395]\n",
      "Step 525975  [5.347 sec/step, loss=0.07583, avg_loss=0.07396]\n",
      "Step 525976  [5.332 sec/step, loss=0.07446, avg_loss=0.07395]\n",
      "Step 525977  [5.343 sec/step, loss=0.07637, avg_loss=0.07396]\n",
      "Step 525978  [5.344 sec/step, loss=0.07418, avg_loss=0.07394]\n",
      "Step 525979  [5.389 sec/step, loss=0.06611, avg_loss=0.07385]\n",
      "Generated 32 batches of size 32 in 2.447 sec\n",
      "Step 525980  [5.384 sec/step, loss=0.07530, avg_loss=0.07384]\n",
      "Step 525981  [5.387 sec/step, loss=0.07428, avg_loss=0.07385]\n",
      "Step 525982  [5.375 sec/step, loss=0.07542, avg_loss=0.07384]\n",
      "Step 525983  [5.391 sec/step, loss=0.07645, avg_loss=0.07386]\n",
      "Step 525984  [5.389 sec/step, loss=0.07461, avg_loss=0.07388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 525985  [5.374 sec/step, loss=0.07515, avg_loss=0.07386]\n",
      "Step 525986  [5.393 sec/step, loss=0.07580, avg_loss=0.07395]\n",
      "Step 525987  [5.414 sec/step, loss=0.07581, avg_loss=0.07396]\n",
      "Step 525988  [5.403 sec/step, loss=0.07392, avg_loss=0.07394]\n",
      "Step 525989  [5.409 sec/step, loss=0.07500, avg_loss=0.07396]\n",
      "Step 525990  [5.412 sec/step, loss=0.07189, avg_loss=0.07392]\n",
      "Step 525991  [5.410 sec/step, loss=0.07587, avg_loss=0.07393]\n",
      "Step 525992  [5.417 sec/step, loss=0.07531, avg_loss=0.07396]\n",
      "Step 525993  [5.432 sec/step, loss=0.07511, avg_loss=0.07397]\n",
      "Step 525994  [5.430 sec/step, loss=0.07220, avg_loss=0.07396]\n",
      "Step 525995  [5.425 sec/step, loss=0.07131, avg_loss=0.07393]\n",
      "Step 525996  [5.455 sec/step, loss=0.07616, avg_loss=0.07402]\n",
      "Step 525997  [5.430 sec/step, loss=0.07431, avg_loss=0.07402]\n",
      "Step 525998  [5.442 sec/step, loss=0.07632, avg_loss=0.07403]\n",
      "Step 525999  [5.431 sec/step, loss=0.07641, avg_loss=0.07404]\n",
      "Step 526000  [5.430 sec/step, loss=0.07534, avg_loss=0.07404]\n",
      "Writing summary at step: 526000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-526000\n",
      "Saving audio and alignment...\n",
      "Input: maaddhuu nay laedz or kaanddhlavii dzuuniiar naashtdaa vaaloon kay xallaaf parddhaan koo zayrayanvaan ghuust qaraarddaadd ddii~_______\n",
      "Step 526001  [5.408 sec/step, loss=0.07230, avg_loss=0.07400]\n",
      "Step 526002  [5.429 sec/step, loss=0.07320, avg_loss=0.07401]\n",
      "Step 526003  [5.420 sec/step, loss=0.07560, avg_loss=0.07403]\n",
      "Step 526004  [5.419 sec/step, loss=0.07398, avg_loss=0.07404]\n",
      "Step 526005  [5.404 sec/step, loss=0.07383, avg_loss=0.07401]\n",
      "Step 526006  [5.411 sec/step, loss=0.07668, avg_loss=0.07402]\n",
      "Step 526007  [5.415 sec/step, loss=0.07424, avg_loss=0.07404]\n",
      "Step 526008  [5.425 sec/step, loss=0.07399, avg_loss=0.07403]\n",
      "Step 526009  [5.359 sec/step, loss=0.06648, avg_loss=0.07403]\n",
      "Generated 32 batches of size 32 in 2.315 sec\n",
      "Step 526010  [5.360 sec/step, loss=0.07540, avg_loss=0.07403]\n",
      "Step 526011  [5.385 sec/step, loss=0.06679, avg_loss=0.07397]\n",
      "Step 526012  [5.377 sec/step, loss=0.07558, avg_loss=0.07396]\n",
      "Step 526013  [5.380 sec/step, loss=0.07560, avg_loss=0.07397]\n",
      "Step 526014  [5.386 sec/step, loss=0.07452, avg_loss=0.07396]\n",
      "Step 526015  [5.385 sec/step, loss=0.07341, avg_loss=0.07395]\n",
      "Step 526016  [5.374 sec/step, loss=0.07584, avg_loss=0.07398]\n",
      "Step 526017  [5.385 sec/step, loss=0.07388, avg_loss=0.07397]\n",
      "Step 526018  [5.383 sec/step, loss=0.07395, avg_loss=0.07395]\n",
      "Step 526019  [5.373 sec/step, loss=0.07030, avg_loss=0.07391]\n",
      "Step 526020  [5.373 sec/step, loss=0.07436, avg_loss=0.07390]\n",
      "Step 526021  [5.394 sec/step, loss=0.07426, avg_loss=0.07394]\n",
      "Step 526022  [5.394 sec/step, loss=0.07606, avg_loss=0.07394]\n",
      "Step 526023  [5.393 sec/step, loss=0.07445, avg_loss=0.07395]\n",
      "Step 526024  [5.402 sec/step, loss=0.07630, avg_loss=0.07398]\n",
      "Step 526025  [5.416 sec/step, loss=0.07462, avg_loss=0.07397]\n",
      "Step 526026  [5.393 sec/step, loss=0.07429, avg_loss=0.07394]\n",
      "Step 526027  [5.449 sec/step, loss=0.06562, avg_loss=0.07387]\n",
      "Step 526028  [5.391 sec/step, loss=0.07377, avg_loss=0.07395]\n",
      "Step 526029  [5.381 sec/step, loss=0.07532, avg_loss=0.07394]\n",
      "Step 526030  [5.380 sec/step, loss=0.07649, avg_loss=0.07395]\n",
      "Step 526031  [5.369 sec/step, loss=0.07403, avg_loss=0.07393]\n",
      "Step 526032  [5.365 sec/step, loss=0.07605, avg_loss=0.07394]\n",
      "Step 526033  [5.369 sec/step, loss=0.07707, avg_loss=0.07398]\n",
      "Step 526034  [5.374 sec/step, loss=0.07481, avg_loss=0.07399]\n",
      "Step 526035  [5.372 sec/step, loss=0.07547, avg_loss=0.07399]\n",
      "Step 526036  [5.365 sec/step, loss=0.07425, avg_loss=0.07398]\n",
      "Step 526037  [5.359 sec/step, loss=0.07716, avg_loss=0.07401]\n",
      "Step 526038  [5.337 sec/step, loss=0.06604, avg_loss=0.07392]\n",
      "Step 526039  [5.350 sec/step, loss=0.07655, avg_loss=0.07395]\n",
      "Step 526040  [5.369 sec/step, loss=0.07678, avg_loss=0.07398]\n",
      "Step 526041  [5.365 sec/step, loss=0.07720, avg_loss=0.07399]\n",
      "Generated 32 batches of size 32 in 2.446 sec\n",
      "Step 526042  [5.386 sec/step, loss=0.07490, avg_loss=0.07408]\n",
      "Step 526043  [5.393 sec/step, loss=0.07237, avg_loss=0.07407]\n",
      "Step 526044  [5.419 sec/step, loss=0.07316, avg_loss=0.07407]\n",
      "Step 526045  [5.413 sec/step, loss=0.07568, avg_loss=0.07410]\n",
      "Step 526046  [5.421 sec/step, loss=0.07553, avg_loss=0.07415]\n",
      "Step 526047  [5.428 sec/step, loss=0.07480, avg_loss=0.07418]\n",
      "Step 526048  [5.408 sec/step, loss=0.07562, avg_loss=0.07418]\n",
      "Step 526049  [5.380 sec/step, loss=0.07263, avg_loss=0.07414]\n",
      "Step 526050  [5.380 sec/step, loss=0.07502, avg_loss=0.07415]\n",
      "Step 526051  [5.413 sec/step, loss=0.07482, avg_loss=0.07418]\n",
      "Step 526052  [5.399 sec/step, loss=0.07380, avg_loss=0.07417]\n",
      "Step 526053  [5.411 sec/step, loss=0.07617, avg_loss=0.07418]\n",
      "Step 526054  [5.416 sec/step, loss=0.07180, avg_loss=0.07418]\n",
      "Step 526055  [5.422 sec/step, loss=0.07538, avg_loss=0.07417]\n",
      "Step 526056  [5.392 sec/step, loss=0.07283, avg_loss=0.07417]\n",
      "Step 526057  [5.370 sec/step, loss=0.07441, avg_loss=0.07415]\n",
      "Step 526058  [5.358 sec/step, loss=0.06592, avg_loss=0.07409]\n",
      "Step 526059  [5.366 sec/step, loss=0.07669, avg_loss=0.07411]\n",
      "Step 526060  [5.360 sec/step, loss=0.07501, avg_loss=0.07412]\n",
      "Step 526061  [5.366 sec/step, loss=0.07398, avg_loss=0.07412]\n",
      "Step 526062  [5.410 sec/step, loss=0.06818, avg_loss=0.07405]\n",
      "Step 526063  [5.396 sec/step, loss=0.07504, avg_loss=0.07404]\n",
      "Step 526064  [5.394 sec/step, loss=0.07538, avg_loss=0.07403]\n",
      "Step 526065  [5.367 sec/step, loss=0.07565, avg_loss=0.07406]\n",
      "Step 526066  [5.369 sec/step, loss=0.07355, avg_loss=0.07408]\n",
      "Step 526067  [5.362 sec/step, loss=0.07172, avg_loss=0.07406]\n",
      "Step 526068  [5.335 sec/step, loss=0.07113, avg_loss=0.07402]\n",
      "Step 526069  [5.320 sec/step, loss=0.07539, avg_loss=0.07401]\n",
      "Step 526070  [5.341 sec/step, loss=0.07469, avg_loss=0.07404]\n",
      "Step 526071  [5.383 sec/step, loss=0.07386, avg_loss=0.07412]\n",
      "Step 526072  [5.389 sec/step, loss=0.07545, avg_loss=0.07414]\n",
      "Step 526073  [5.390 sec/step, loss=0.07692, avg_loss=0.07416]\n",
      "Generated 32 batches of size 32 in 2.585 sec\n",
      "Step 526074  [5.394 sec/step, loss=0.07404, avg_loss=0.07417]\n",
      "Step 526075  [5.399 sec/step, loss=0.07590, avg_loss=0.07417]\n",
      "Step 526076  [5.421 sec/step, loss=0.07613, avg_loss=0.07419]\n",
      "Step 526077  [5.402 sec/step, loss=0.07209, avg_loss=0.07414]\n",
      "Step 526078  [5.400 sec/step, loss=0.07525, avg_loss=0.07416]\n",
      "Step 526079  [5.345 sec/step, loss=0.07474, avg_loss=0.07424]\n",
      "Step 526080  [5.344 sec/step, loss=0.07569, avg_loss=0.07425]\n",
      "Step 526081  [5.353 sec/step, loss=0.07638, avg_loss=0.07427]\n",
      "Step 526082  [5.360 sec/step, loss=0.07483, avg_loss=0.07426]\n",
      "Step 526083  [5.342 sec/step, loss=0.07199, avg_loss=0.07422]\n",
      "Step 526084  [5.333 sec/step, loss=0.07453, avg_loss=0.07422]\n",
      "Step 526085  [5.326 sec/step, loss=0.07101, avg_loss=0.07417]\n",
      "Step 526086  [5.309 sec/step, loss=0.07134, avg_loss=0.07413]\n",
      "Step 526087  [5.349 sec/step, loss=0.06553, avg_loss=0.07403]\n",
      "Step 526088  [5.371 sec/step, loss=0.07583, avg_loss=0.07405]\n",
      "Step 526089  [5.355 sec/step, loss=0.07049, avg_loss=0.07400]\n",
      "Step 526090  [5.357 sec/step, loss=0.07506, avg_loss=0.07403]\n",
      "Step 526091  [5.349 sec/step, loss=0.07474, avg_loss=0.07402]\n",
      "Step 526092  [5.353 sec/step, loss=0.07633, avg_loss=0.07403]\n",
      "Step 526093  [5.354 sec/step, loss=0.07630, avg_loss=0.07404]\n",
      "Step 526094  [5.376 sec/step, loss=0.07577, avg_loss=0.07408]\n",
      "Step 526095  [5.396 sec/step, loss=0.07514, avg_loss=0.07412]\n",
      "Step 526096  [5.382 sec/step, loss=0.07400, avg_loss=0.07410]\n",
      "Step 526097  [5.386 sec/step, loss=0.07384, avg_loss=0.07409]\n",
      "Step 526098  [5.382 sec/step, loss=0.07619, avg_loss=0.07409]\n",
      "Step 526099  [5.383 sec/step, loss=0.07435, avg_loss=0.07407]\n",
      "Step 526100  [5.387 sec/step, loss=0.07481, avg_loss=0.07406]\n",
      "Writing summary at step: 526100\n",
      "Step 526101  [5.396 sec/step, loss=0.07400, avg_loss=0.07408]\n",
      "Step 526102  [5.389 sec/step, loss=0.07386, avg_loss=0.07409]\n",
      "Step 526103  [5.402 sec/step, loss=0.07613, avg_loss=0.07409]\n",
      "Step 526104  [5.407 sec/step, loss=0.07180, avg_loss=0.07407]\n",
      "Generated 32 batches of size 32 in 2.569 sec\n",
      "Step 526105  [5.417 sec/step, loss=0.07533, avg_loss=0.07409]\n",
      "Step 526106  [5.409 sec/step, loss=0.07575, avg_loss=0.07408]\n",
      "Step 526107  [5.403 sec/step, loss=0.06840, avg_loss=0.07402]\n",
      "Step 526108  [5.381 sec/step, loss=0.07426, avg_loss=0.07402]\n",
      "Step 526109  [5.423 sec/step, loss=0.07457, avg_loss=0.07410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 526110  [5.418 sec/step, loss=0.07453, avg_loss=0.07409]\n",
      "Step 526111  [5.379 sec/step, loss=0.07659, avg_loss=0.07419]\n",
      "Step 526112  [5.380 sec/step, loss=0.07488, avg_loss=0.07418]\n",
      "Step 526113  [5.388 sec/step, loss=0.07476, avg_loss=0.07417]\n",
      "Step 526114  [5.394 sec/step, loss=0.07592, avg_loss=0.07419]\n",
      "Step 526115  [5.420 sec/step, loss=0.07326, avg_loss=0.07419]\n",
      "Step 526116  [5.424 sec/step, loss=0.07336, avg_loss=0.07416]\n",
      "Step 526117  [5.413 sec/step, loss=0.07361, avg_loss=0.07416]\n",
      "Step 526118  [5.405 sec/step, loss=0.07576, avg_loss=0.07418]\n",
      "Step 526119  [5.411 sec/step, loss=0.07383, avg_loss=0.07421]\n",
      "Step 526120  [5.415 sec/step, loss=0.07521, avg_loss=0.07422]\n",
      "Step 526121  [5.392 sec/step, loss=0.07182, avg_loss=0.07420]\n",
      "Step 526122  [5.388 sec/step, loss=0.07602, avg_loss=0.07420]\n",
      "Step 526123  [5.366 sec/step, loss=0.06574, avg_loss=0.07411]\n",
      "Step 526124  [5.361 sec/step, loss=0.07502, avg_loss=0.07410]\n",
      "Step 526125  [5.355 sec/step, loss=0.07557, avg_loss=0.07411]\n",
      "Step 526126  [5.359 sec/step, loss=0.07422, avg_loss=0.07411]\n",
      "Step 526127  [5.306 sec/step, loss=0.07272, avg_loss=0.07418]\n",
      "Step 526128  [5.309 sec/step, loss=0.07374, avg_loss=0.07418]\n",
      "Step 526129  [5.359 sec/step, loss=0.06751, avg_loss=0.07410]\n",
      "Step 526130  [5.369 sec/step, loss=0.07447, avg_loss=0.07408]\n",
      "Step 526131  [5.403 sec/step, loss=0.07343, avg_loss=0.07407]\n",
      "Step 526132  [5.394 sec/step, loss=0.07472, avg_loss=0.07406]\n",
      "Step 526133  [5.392 sec/step, loss=0.07590, avg_loss=0.07405]\n",
      "Step 526134  [5.391 sec/step, loss=0.07584, avg_loss=0.07406]\n",
      "Step 526135  [5.395 sec/step, loss=0.07559, avg_loss=0.07406]\n",
      "Step 526136  [5.414 sec/step, loss=0.07645, avg_loss=0.07408]\n",
      "Generated 32 batches of size 32 in 2.526 sec\n",
      "Step 526137  [5.411 sec/step, loss=0.07375, avg_loss=0.07405]\n",
      "Step 526138  [5.416 sec/step, loss=0.07149, avg_loss=0.07410]\n",
      "Step 526139  [5.403 sec/step, loss=0.07426, avg_loss=0.07408]\n",
      "Step 526140  [5.383 sec/step, loss=0.07566, avg_loss=0.07407]\n",
      "Step 526141  [5.371 sec/step, loss=0.07384, avg_loss=0.07403]\n",
      "Step 526142  [5.377 sec/step, loss=0.07671, avg_loss=0.07405]\n",
      "Step 526143  [5.388 sec/step, loss=0.07540, avg_loss=0.07408]\n",
      "Step 526144  [5.366 sec/step, loss=0.07576, avg_loss=0.07411]\n",
      "Step 526145  [5.382 sec/step, loss=0.07570, avg_loss=0.07411]\n",
      "Step 526146  [5.395 sec/step, loss=0.07544, avg_loss=0.07411]\n",
      "Step 526147  [5.410 sec/step, loss=0.07627, avg_loss=0.07412]\n",
      "Step 526148  [5.409 sec/step, loss=0.07447, avg_loss=0.07411]\n",
      "Step 526149  [5.427 sec/step, loss=0.07520, avg_loss=0.07414]\n",
      "Step 526150  [5.424 sec/step, loss=0.07565, avg_loss=0.07414]\n",
      "Step 526151  [5.415 sec/step, loss=0.07551, avg_loss=0.07415]\n",
      "Step 526152  [5.421 sec/step, loss=0.07158, avg_loss=0.07413]\n",
      "Step 526153  [5.410 sec/step, loss=0.07536, avg_loss=0.07412]\n",
      "Step 526154  [5.421 sec/step, loss=0.07453, avg_loss=0.07415]\n",
      "Step 526155  [5.434 sec/step, loss=0.07301, avg_loss=0.07412]\n",
      "Step 526156  [5.429 sec/step, loss=0.07381, avg_loss=0.07413]\n",
      "Step 526157  [5.428 sec/step, loss=0.07058, avg_loss=0.07409]\n",
      "Step 526158  [5.452 sec/step, loss=0.07595, avg_loss=0.07419]\n",
      "Step 526159  [5.440 sec/step, loss=0.07369, avg_loss=0.07416]\n",
      "Step 526160  [5.447 sec/step, loss=0.07651, avg_loss=0.07418]\n",
      "Step 526161  [5.439 sec/step, loss=0.07503, avg_loss=0.07419]\n",
      "Step 526162  [5.379 sec/step, loss=0.07230, avg_loss=0.07423]\n",
      "Step 526163  [5.381 sec/step, loss=0.07397, avg_loss=0.07422]\n",
      "Step 526164  [5.431 sec/step, loss=0.06602, avg_loss=0.07413]\n",
      "Step 526165  [5.429 sec/step, loss=0.07510, avg_loss=0.07412]\n",
      "Step 526166  [5.450 sec/step, loss=0.07378, avg_loss=0.07412]\n",
      "Step 526167  [5.461 sec/step, loss=0.07526, avg_loss=0.07416]\n",
      "Step 526168  [5.483 sec/step, loss=0.07607, avg_loss=0.07421]\n",
      "Generated 32 batches of size 32 in 3.050 sec\n",
      "Step 526169  [5.475 sec/step, loss=0.06690, avg_loss=0.07412]\n",
      "Step 526170  [5.461 sec/step, loss=0.07432, avg_loss=0.07412]\n",
      "Step 526171  [5.432 sec/step, loss=0.07178, avg_loss=0.07410]\n",
      "Step 526172  [5.438 sec/step, loss=0.07595, avg_loss=0.07410]\n",
      "Step 526173  [5.415 sec/step, loss=0.07176, avg_loss=0.07405]\n",
      "Step 526174  [5.411 sec/step, loss=0.07288, avg_loss=0.07404]\n",
      "Step 526175  [5.415 sec/step, loss=0.07461, avg_loss=0.07403]\n",
      "Step 526176  [5.405 sec/step, loss=0.07540, avg_loss=0.07402]\n",
      "Step 526177  [5.421 sec/step, loss=0.07594, avg_loss=0.07406]\n",
      "Step 526178  [5.418 sec/step, loss=0.07124, avg_loss=0.07402]\n",
      "Step 526179  [5.418 sec/step, loss=0.07527, avg_loss=0.07402]\n",
      "Step 526180  [5.404 sec/step, loss=0.07327, avg_loss=0.07400]\n",
      "Step 526181  [5.390 sec/step, loss=0.07267, avg_loss=0.07396]\n",
      "Step 526182  [5.399 sec/step, loss=0.07707, avg_loss=0.07399]\n",
      "Step 526183  [5.402 sec/step, loss=0.07418, avg_loss=0.07401]\n",
      "Step 526184  [5.400 sec/step, loss=0.07405, avg_loss=0.07400]\n",
      "Step 526185  [5.426 sec/step, loss=0.07410, avg_loss=0.07403]\n",
      "Step 526186  [5.441 sec/step, loss=0.07276, avg_loss=0.07405]\n",
      "Step 526187  [5.391 sec/step, loss=0.07604, avg_loss=0.07415]\n",
      "Step 526188  [5.426 sec/step, loss=0.06641, avg_loss=0.07406]\n",
      "Step 526189  [5.444 sec/step, loss=0.07627, avg_loss=0.07412]\n",
      "Step 526190  [5.443 sec/step, loss=0.07506, avg_loss=0.07412]\n",
      "Step 526191  [5.445 sec/step, loss=0.07238, avg_loss=0.07409]\n",
      "Step 526192  [5.434 sec/step, loss=0.07083, avg_loss=0.07404]\n",
      "Step 526193  [5.435 sec/step, loss=0.07451, avg_loss=0.07402]\n",
      "Step 526194  [5.433 sec/step, loss=0.07376, avg_loss=0.07400]\n",
      "Step 526195  [5.420 sec/step, loss=0.07423, avg_loss=0.07399]\n",
      "Step 526196  [5.448 sec/step, loss=0.07360, avg_loss=0.07399]\n",
      "Step 526197  [5.445 sec/step, loss=0.07314, avg_loss=0.07398]\n",
      "Step 526198  [5.450 sec/step, loss=0.07373, avg_loss=0.07396]\n",
      "Step 526199  [5.427 sec/step, loss=0.07161, avg_loss=0.07393]\n",
      "Step 526200  [5.425 sec/step, loss=0.07537, avg_loss=0.07393]\n",
      "Writing summary at step: 526200\n",
      "Generated 32 batches of size 32 in 2.337 sec\n",
      "Step 526201  [5.441 sec/step, loss=0.07639, avg_loss=0.07396]\n",
      "Step 526202  [5.412 sec/step, loss=0.06574, avg_loss=0.07388]\n",
      "Step 526203  [5.407 sec/step, loss=0.07603, avg_loss=0.07388]\n",
      "Step 526204  [5.416 sec/step, loss=0.07535, avg_loss=0.07391]\n",
      "Step 526205  [5.413 sec/step, loss=0.07519, avg_loss=0.07391]\n",
      "Step 526206  [5.408 sec/step, loss=0.07465, avg_loss=0.07390]\n",
      "Step 526207  [5.426 sec/step, loss=0.07423, avg_loss=0.07396]\n",
      "Step 526208  [5.448 sec/step, loss=0.07660, avg_loss=0.07398]\n",
      "Step 526209  [5.412 sec/step, loss=0.07138, avg_loss=0.07395]\n",
      "Step 526210  [5.403 sec/step, loss=0.07425, avg_loss=0.07395]\n",
      "Step 526211  [5.389 sec/step, loss=0.07557, avg_loss=0.07394]\n",
      "Step 526212  [5.377 sec/step, loss=0.07351, avg_loss=0.07392]\n",
      "Step 526213  [5.377 sec/step, loss=0.07544, avg_loss=0.07393]\n",
      "Step 526214  [5.370 sec/step, loss=0.07470, avg_loss=0.07392]\n",
      "Step 526215  [5.339 sec/step, loss=0.07400, avg_loss=0.07392]\n",
      "Step 526216  [5.346 sec/step, loss=0.07378, avg_loss=0.07393]\n",
      "Step 526217  [5.358 sec/step, loss=0.07590, avg_loss=0.07395]\n",
      "Step 526218  [5.352 sec/step, loss=0.07175, avg_loss=0.07391]\n",
      "Step 526219  [5.352 sec/step, loss=0.07362, avg_loss=0.07391]\n",
      "Step 526220  [5.377 sec/step, loss=0.07276, avg_loss=0.07388]\n",
      "Step 526221  [5.390 sec/step, loss=0.07372, avg_loss=0.07390]\n",
      "Step 526222  [5.392 sec/step, loss=0.07648, avg_loss=0.07391]\n",
      "Step 526223  [5.423 sec/step, loss=0.07574, avg_loss=0.07401]\n",
      "Step 526224  [5.427 sec/step, loss=0.07471, avg_loss=0.07400]\n",
      "Step 526225  [5.427 sec/step, loss=0.07461, avg_loss=0.07399]\n",
      "Step 526226  [5.435 sec/step, loss=0.07519, avg_loss=0.07400]\n",
      "Step 526227  [5.435 sec/step, loss=0.07522, avg_loss=0.07403]\n",
      "Step 526228  [5.439 sec/step, loss=0.07487, avg_loss=0.07404]\n",
      "Step 526229  [5.387 sec/step, loss=0.07354, avg_loss=0.07410]\n",
      "Step 526230  [5.393 sec/step, loss=0.07500, avg_loss=0.07411]\n",
      "Step 526231  [5.349 sec/step, loss=0.06674, avg_loss=0.07404]\n",
      "Generated 32 batches of size 32 in 2.741 sec\n",
      "Step 526232  [5.339 sec/step, loss=0.07179, avg_loss=0.07401]\n",
      "Step 526233  [5.339 sec/step, loss=0.07634, avg_loss=0.07401]\n",
      "Step 526234  [5.356 sec/step, loss=0.07558, avg_loss=0.07401]\n",
      "Step 526235  [5.358 sec/step, loss=0.07518, avg_loss=0.07401]\n",
      "Step 526236  [5.394 sec/step, loss=0.06603, avg_loss=0.07390]\n",
      "Step 526237  [5.390 sec/step, loss=0.07471, avg_loss=0.07391]\n",
      "Step 526238  [5.409 sec/step, loss=0.07631, avg_loss=0.07396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 526239  [5.404 sec/step, loss=0.07342, avg_loss=0.07395]\n",
      "Step 526240  [5.419 sec/step, loss=0.07599, avg_loss=0.07396]\n",
      "Step 526241  [5.410 sec/step, loss=0.06618, avg_loss=0.07388]\n",
      "Step 526242  [5.410 sec/step, loss=0.07352, avg_loss=0.07385]\n",
      "Step 526243  [5.453 sec/step, loss=0.06535, avg_loss=0.07375]\n",
      "Step 526244  [5.448 sec/step, loss=0.07501, avg_loss=0.07374]\n",
      "Step 526245  [5.431 sec/step, loss=0.07380, avg_loss=0.07372]\n",
      "Step 526246  [5.419 sec/step, loss=0.07433, avg_loss=0.07371]\n",
      "Step 526247  [5.416 sec/step, loss=0.07480, avg_loss=0.07369]\n",
      "Step 526248  [5.413 sec/step, loss=0.07355, avg_loss=0.07369]\n",
      "Step 526249  [5.401 sec/step, loss=0.07448, avg_loss=0.07368]\n",
      "Step 526250  [5.425 sec/step, loss=0.07542, avg_loss=0.07368]\n",
      "Step 526251  [5.407 sec/step, loss=0.07365, avg_loss=0.07366]\n",
      "Step 526252  [5.410 sec/step, loss=0.07461, avg_loss=0.07369]\n",
      "Step 526253  [5.410 sec/step, loss=0.07262, avg_loss=0.07366]\n",
      "Step 526254  [5.398 sec/step, loss=0.07417, avg_loss=0.07366]\n",
      "Step 526255  [5.388 sec/step, loss=0.07537, avg_loss=0.07368]\n",
      "Step 526256  [5.407 sec/step, loss=0.07607, avg_loss=0.07370]\n",
      "Step 526257  [5.430 sec/step, loss=0.07531, avg_loss=0.07375]\n",
      "Step 526258  [5.416 sec/step, loss=0.07068, avg_loss=0.07370]\n",
      "Step 526259  [5.410 sec/step, loss=0.07249, avg_loss=0.07369]\n",
      "Step 526260  [5.407 sec/step, loss=0.07484, avg_loss=0.07367]\n",
      "Step 526261  [5.414 sec/step, loss=0.07657, avg_loss=0.07368]\n",
      "Step 526262  [5.425 sec/step, loss=0.07611, avg_loss=0.07372]\n",
      "Step 526263  [5.422 sec/step, loss=0.07044, avg_loss=0.07369]\n",
      "Generated 32 batches of size 32 in 2.392 sec\n",
      "Step 526264  [5.380 sec/step, loss=0.07499, avg_loss=0.07378]\n",
      "Step 526265  [5.390 sec/step, loss=0.07629, avg_loss=0.07379]\n",
      "Step 526266  [5.363 sec/step, loss=0.07210, avg_loss=0.07377]\n",
      "Step 526267  [5.367 sec/step, loss=0.07466, avg_loss=0.07377]\n",
      "Step 526268  [5.350 sec/step, loss=0.07530, avg_loss=0.07376]\n",
      "Step 526269  [5.360 sec/step, loss=0.07554, avg_loss=0.07384]\n",
      "Step 526270  [5.380 sec/step, loss=0.07643, avg_loss=0.07387]\n",
      "Step 526271  [5.390 sec/step, loss=0.07585, avg_loss=0.07391]\n",
      "Step 526272  [5.384 sec/step, loss=0.07559, avg_loss=0.07390]\n",
      "Step 526273  [5.412 sec/step, loss=0.07627, avg_loss=0.07395]\n",
      "Step 526274  [5.405 sec/step, loss=0.07400, avg_loss=0.07396]\n",
      "Step 526275  [5.398 sec/step, loss=0.07376, avg_loss=0.07395]\n",
      "Step 526276  [5.382 sec/step, loss=0.07122, avg_loss=0.07391]\n",
      "Step 526277  [5.387 sec/step, loss=0.07626, avg_loss=0.07391]\n",
      "Step 526278  [5.378 sec/step, loss=0.07002, avg_loss=0.07390]\n",
      "Step 526279  [5.377 sec/step, loss=0.07400, avg_loss=0.07389]\n",
      "Step 526280  [5.431 sec/step, loss=0.06739, avg_loss=0.07383]\n",
      "Step 526281  [5.448 sec/step, loss=0.07629, avg_loss=0.07386]\n",
      "Step 526282  [5.429 sec/step, loss=0.07148, avg_loss=0.07381]\n",
      "Step 526283  [5.438 sec/step, loss=0.07540, avg_loss=0.07382]\n",
      "Step 526284  [5.458 sec/step, loss=0.07404, avg_loss=0.07382]\n",
      "Step 526285  [5.447 sec/step, loss=0.07612, avg_loss=0.07384]\n",
      "Step 526286  [5.450 sec/step, loss=0.07517, avg_loss=0.07386]\n",
      "Step 526287  [5.467 sec/step, loss=0.07571, avg_loss=0.07386]\n",
      "Step 526288  [5.428 sec/step, loss=0.07606, avg_loss=0.07396]\n",
      "Step 526289  [5.415 sec/step, loss=0.07306, avg_loss=0.07393]\n",
      "Step 526290  [5.420 sec/step, loss=0.07551, avg_loss=0.07393]\n",
      "Step 526291  [5.413 sec/step, loss=0.07445, avg_loss=0.07395]\n",
      "Step 526292  [5.436 sec/step, loss=0.07505, avg_loss=0.07399]\n",
      "Step 526293  [5.427 sec/step, loss=0.07148, avg_loss=0.07396]\n",
      "Step 526294  [5.418 sec/step, loss=0.07593, avg_loss=0.07398]\n",
      "Step 526295  [5.413 sec/step, loss=0.07479, avg_loss=0.07399]\n",
      "Generated 32 batches of size 32 in 2.502 sec\n",
      "Step 526296  [5.389 sec/step, loss=0.07551, avg_loss=0.07401]\n",
      "Step 526297  [5.394 sec/step, loss=0.07537, avg_loss=0.07403]\n",
      "Step 526298  [5.379 sec/step, loss=0.07142, avg_loss=0.07401]\n",
      "Step 526299  [5.402 sec/step, loss=0.07664, avg_loss=0.07406]\n",
      "Step 526300  [5.401 sec/step, loss=0.07519, avg_loss=0.07406]\n",
      "Writing summary at step: 526300\n",
      "Step 526301  [5.401 sec/step, loss=0.07634, avg_loss=0.07406]\n",
      "Step 526302  [5.445 sec/step, loss=0.07355, avg_loss=0.07413]\n",
      "Step 526303  [5.419 sec/step, loss=0.06575, avg_loss=0.07403]\n",
      "Step 526304  [5.417 sec/step, loss=0.07640, avg_loss=0.07404]\n",
      "Step 526305  [5.405 sec/step, loss=0.07226, avg_loss=0.07401]\n",
      "Step 526306  [5.414 sec/step, loss=0.07387, avg_loss=0.07401]\n",
      "Step 526307  [5.425 sec/step, loss=0.07614, avg_loss=0.07402]\n",
      "Step 526308  [5.423 sec/step, loss=0.07629, avg_loss=0.07402]\n",
      "Step 526309  [5.431 sec/step, loss=0.07063, avg_loss=0.07401]\n",
      "Step 526310  [5.488 sec/step, loss=0.06651, avg_loss=0.07394]\n",
      "Step 526311  [5.484 sec/step, loss=0.07395, avg_loss=0.07392]\n",
      "Step 526312  [5.492 sec/step, loss=0.07248, avg_loss=0.07391]\n",
      "Step 526313  [5.490 sec/step, loss=0.07453, avg_loss=0.07390]\n",
      "Step 526314  [5.488 sec/step, loss=0.07479, avg_loss=0.07390]\n",
      "Step 526315  [5.491 sec/step, loss=0.07362, avg_loss=0.07390]\n",
      "Step 526316  [5.481 sec/step, loss=0.07568, avg_loss=0.07392]\n",
      "Step 526317  [5.493 sec/step, loss=0.07394, avg_loss=0.07390]\n",
      "Step 526318  [5.485 sec/step, loss=0.07146, avg_loss=0.07389]\n",
      "Step 526319  [5.516 sec/step, loss=0.07318, avg_loss=0.07389]\n",
      "Step 526320  [5.508 sec/step, loss=0.07565, avg_loss=0.07392]\n",
      "Step 526321  [5.504 sec/step, loss=0.07415, avg_loss=0.07392]\n",
      "Step 526322  [5.494 sec/step, loss=0.07404, avg_loss=0.07390]\n",
      "Step 526323  [5.478 sec/step, loss=0.07538, avg_loss=0.07390]\n",
      "Step 526324  [5.466 sec/step, loss=0.07195, avg_loss=0.07387]\n",
      "Step 526325  [5.465 sec/step, loss=0.07549, avg_loss=0.07388]\n",
      "Step 526326  [5.467 sec/step, loss=0.07269, avg_loss=0.07385]\n",
      "Generated 32 batches of size 32 in 2.372 sec\n",
      "Step 526327  [5.480 sec/step, loss=0.07513, avg_loss=0.07385]\n",
      "Step 526328  [5.488 sec/step, loss=0.07549, avg_loss=0.07386]\n",
      "Step 526329  [5.479 sec/step, loss=0.07428, avg_loss=0.07386]\n",
      "Step 526330  [5.461 sec/step, loss=0.07490, avg_loss=0.07386]\n",
      "Step 526331  [5.486 sec/step, loss=0.07430, avg_loss=0.07394]\n",
      "Step 526332  [5.491 sec/step, loss=0.07506, avg_loss=0.07397]\n",
      "Step 526333  [5.497 sec/step, loss=0.07569, avg_loss=0.07396]\n",
      "Step 526334  [5.492 sec/step, loss=0.07594, avg_loss=0.07397]\n",
      "Step 526335  [5.472 sec/step, loss=0.06670, avg_loss=0.07388]\n",
      "Step 526336  [5.430 sec/step, loss=0.07613, avg_loss=0.07398]\n",
      "Step 526337  [5.424 sec/step, loss=0.07351, avg_loss=0.07397]\n",
      "Step 526338  [5.443 sec/step, loss=0.07365, avg_loss=0.07395]\n",
      "Step 526339  [5.489 sec/step, loss=0.06901, avg_loss=0.07390]\n",
      "Step 526340  [5.490 sec/step, loss=0.07451, avg_loss=0.07389]\n",
      "Step 526341  [5.506 sec/step, loss=0.07459, avg_loss=0.07397]\n",
      "Step 526342  [5.494 sec/step, loss=0.07329, avg_loss=0.07397]\n",
      "Step 526343  [5.460 sec/step, loss=0.07714, avg_loss=0.07409]\n",
      "Step 526344  [5.463 sec/step, loss=0.07540, avg_loss=0.07409]\n",
      "Step 526345  [5.451 sec/step, loss=0.06647, avg_loss=0.07402]\n",
      "Step 526346  [5.435 sec/step, loss=0.07429, avg_loss=0.07402]\n",
      "Step 526347  [5.439 sec/step, loss=0.07594, avg_loss=0.07403]\n",
      "Step 526348  [5.444 sec/step, loss=0.07547, avg_loss=0.07405]\n",
      "Step 526349  [5.447 sec/step, loss=0.07460, avg_loss=0.07405]\n",
      "Step 526350  [5.423 sec/step, loss=0.07576, avg_loss=0.07405]\n",
      "Step 526351  [5.437 sec/step, loss=0.07444, avg_loss=0.07406]\n",
      "Step 526352  [5.419 sec/step, loss=0.07205, avg_loss=0.07403]\n",
      "Step 526353  [5.415 sec/step, loss=0.07146, avg_loss=0.07402]\n",
      "Step 526354  [5.427 sec/step, loss=0.07613, avg_loss=0.07404]\n",
      "Step 526355  [5.414 sec/step, loss=0.07519, avg_loss=0.07404]\n",
      "Step 526356  [5.401 sec/step, loss=0.07468, avg_loss=0.07403]\n",
      "Step 526357  [5.394 sec/step, loss=0.07608, avg_loss=0.07403]\n",
      "Step 526358  [5.393 sec/step, loss=0.07410, avg_loss=0.07407]\n",
      "Generated 32 batches of size 32 in 2.577 sec\n",
      "Step 526359  [5.402 sec/step, loss=0.07375, avg_loss=0.07408]\n",
      "Step 526360  [5.406 sec/step, loss=0.07637, avg_loss=0.07410]\n",
      "Step 526361  [5.399 sec/step, loss=0.07493, avg_loss=0.07408]\n",
      "Step 526362  [5.394 sec/step, loss=0.07525, avg_loss=0.07407]\n",
      "Step 526363  [5.408 sec/step, loss=0.07590, avg_loss=0.07413]\n",
      "Step 526364  [5.402 sec/step, loss=0.07505, avg_loss=0.07413]\n",
      "Step 526365  [5.390 sec/step, loss=0.07523, avg_loss=0.07412]\n",
      "Step 526366  [5.394 sec/step, loss=0.07246, avg_loss=0.07412]\n",
      "Step 526367  [5.416 sec/step, loss=0.07371, avg_loss=0.07411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 526368  [5.435 sec/step, loss=0.07542, avg_loss=0.07411]\n",
      "Step 526369  [5.443 sec/step, loss=0.07393, avg_loss=0.07410]\n",
      "Step 526370  [5.425 sec/step, loss=0.07344, avg_loss=0.07407]\n",
      "Step 526371  [5.418 sec/step, loss=0.07577, avg_loss=0.07407]\n",
      "Step 526372  [5.418 sec/step, loss=0.07462, avg_loss=0.07406]\n",
      "Step 526373  [5.386 sec/step, loss=0.06602, avg_loss=0.07395]\n",
      "Step 526374  [5.393 sec/step, loss=0.07518, avg_loss=0.07396]\n",
      "Step 526375  [5.387 sec/step, loss=0.07335, avg_loss=0.07396]\n",
      "Step 526376  [5.404 sec/step, loss=0.07515, avg_loss=0.07400]\n",
      "Step 526377  [5.399 sec/step, loss=0.07491, avg_loss=0.07399]\n",
      "Step 526378  [5.426 sec/step, loss=0.07504, avg_loss=0.07404]\n",
      "Step 526379  [5.481 sec/step, loss=0.06698, avg_loss=0.07397]\n",
      "Step 526380  [5.430 sec/step, loss=0.07315, avg_loss=0.07402]\n",
      "Step 526381  [5.419 sec/step, loss=0.07436, avg_loss=0.07400]\n",
      "Step 526382  [5.410 sec/step, loss=0.07151, avg_loss=0.07401]\n",
      "Step 526383  [5.408 sec/step, loss=0.07480, avg_loss=0.07400]\n",
      "Step 526384  [5.406 sec/step, loss=0.07370, avg_loss=0.07400]\n",
      "Step 526385  [5.408 sec/step, loss=0.07584, avg_loss=0.07399]\n",
      "Step 526386  [5.411 sec/step, loss=0.07627, avg_loss=0.07400]\n",
      "Step 526387  [5.385 sec/step, loss=0.07441, avg_loss=0.07399]\n",
      "Step 526388  [5.387 sec/step, loss=0.07573, avg_loss=0.07399]\n",
      "Step 526389  [5.395 sec/step, loss=0.07531, avg_loss=0.07401]\n",
      "Step 526390  [5.399 sec/step, loss=0.07632, avg_loss=0.07402]\n",
      "Generated 32 batches of size 32 in 2.490 sec\n",
      "Step 526391  [5.397 sec/step, loss=0.07046, avg_loss=0.07398]\n",
      "Step 526392  [5.382 sec/step, loss=0.07205, avg_loss=0.07395]\n",
      "Step 526393  [5.389 sec/step, loss=0.07627, avg_loss=0.07400]\n",
      "Step 526394  [5.386 sec/step, loss=0.07589, avg_loss=0.07400]\n",
      "Step 526395  [5.383 sec/step, loss=0.07356, avg_loss=0.07398]\n",
      "Step 526396  [5.371 sec/step, loss=0.07428, avg_loss=0.07397]\n",
      "Step 526397  [5.375 sec/step, loss=0.07330, avg_loss=0.07395]\n",
      "Step 526398  [5.388 sec/step, loss=0.07648, avg_loss=0.07400]\n",
      "Step 526399  [5.404 sec/step, loss=0.07229, avg_loss=0.07396]\n",
      "Step 526400  [5.404 sec/step, loss=0.07502, avg_loss=0.07396]\n",
      "Writing summary at step: 526400\n",
      "Step 526401  [5.377 sec/step, loss=0.07208, avg_loss=0.07391]\n",
      "Step 526402  [5.341 sec/step, loss=0.07313, avg_loss=0.07391]\n",
      "Step 526403  [5.362 sec/step, loss=0.07463, avg_loss=0.07400]\n",
      "Step 526404  [5.364 sec/step, loss=0.07658, avg_loss=0.07400]\n",
      "Step 526405  [5.391 sec/step, loss=0.07628, avg_loss=0.07404]\n",
      "Step 526406  [5.387 sec/step, loss=0.07644, avg_loss=0.07407]\n",
      "Step 526407  [5.387 sec/step, loss=0.07482, avg_loss=0.07405]\n",
      "Step 526408  [5.384 sec/step, loss=0.07449, avg_loss=0.07403]\n",
      "Step 526409  [5.413 sec/step, loss=0.07385, avg_loss=0.07407]\n",
      "Step 526410  [5.360 sec/step, loss=0.07533, avg_loss=0.07415]\n",
      "Step 526411  [5.418 sec/step, loss=0.06797, avg_loss=0.07410]\n",
      "Step 526412  [5.413 sec/step, loss=0.07375, avg_loss=0.07411]\n",
      "Step 526413  [5.405 sec/step, loss=0.07234, avg_loss=0.07409]\n",
      "Step 526414  [5.397 sec/step, loss=0.07491, avg_loss=0.07409]\n",
      "Step 526415  [5.402 sec/step, loss=0.07256, avg_loss=0.07408]\n",
      "Step 526416  [5.390 sec/step, loss=0.07453, avg_loss=0.07406]\n",
      "Step 526417  [5.379 sec/step, loss=0.07555, avg_loss=0.07408]\n",
      "Step 526418  [5.391 sec/step, loss=0.07259, avg_loss=0.07409]\n",
      "Step 526419  [5.387 sec/step, loss=0.07430, avg_loss=0.07410]\n",
      "Step 526420  [5.366 sec/step, loss=0.07538, avg_loss=0.07410]\n",
      "Step 526421  [5.369 sec/step, loss=0.07512, avg_loss=0.07411]\n",
      "Generated 32 batches of size 32 in 2.366 sec\n",
      "Step 526422  [5.382 sec/step, loss=0.07511, avg_loss=0.07412]\n",
      "Step 526423  [5.374 sec/step, loss=0.07143, avg_loss=0.07408]\n",
      "Step 526424  [5.379 sec/step, loss=0.07590, avg_loss=0.07412]\n",
      "Step 526425  [5.388 sec/step, loss=0.07590, avg_loss=0.07413]\n",
      "Step 526426  [5.384 sec/step, loss=0.07346, avg_loss=0.07413]\n",
      "Step 526427  [5.379 sec/step, loss=0.07524, avg_loss=0.07413]\n",
      "Step 526428  [5.379 sec/step, loss=0.07688, avg_loss=0.07415]\n",
      "Step 526429  [5.373 sec/step, loss=0.06563, avg_loss=0.07406]\n",
      "Step 526430  [5.385 sec/step, loss=0.07403, avg_loss=0.07405]\n",
      "Step 526431  [5.403 sec/step, loss=0.07260, avg_loss=0.07404]\n",
      "Step 526432  [5.400 sec/step, loss=0.07359, avg_loss=0.07402]\n",
      "Step 526433  [5.402 sec/step, loss=0.07552, avg_loss=0.07402]\n",
      "Step 526434  [5.445 sec/step, loss=0.06608, avg_loss=0.07392]\n",
      "Step 526435  [5.445 sec/step, loss=0.06658, avg_loss=0.07392]\n",
      "Step 526436  [5.436 sec/step, loss=0.07545, avg_loss=0.07391]\n",
      "Step 526437  [5.440 sec/step, loss=0.07443, avg_loss=0.07392]\n",
      "Step 526438  [5.400 sec/step, loss=0.07148, avg_loss=0.07390]\n",
      "Step 526439  [5.359 sec/step, loss=0.07500, avg_loss=0.07396]\n",
      "Step 526440  [5.344 sec/step, loss=0.07510, avg_loss=0.07397]\n",
      "Step 526441  [5.346 sec/step, loss=0.07626, avg_loss=0.07398]\n",
      "Step 526442  [5.351 sec/step, loss=0.07524, avg_loss=0.07400]\n",
      "Step 526443  [5.340 sec/step, loss=0.07571, avg_loss=0.07399]\n",
      "Step 526444  [5.327 sec/step, loss=0.07400, avg_loss=0.07397]\n",
      "Step 526445  [5.342 sec/step, loss=0.07323, avg_loss=0.07404]\n",
      "Step 526446  [5.350 sec/step, loss=0.07192, avg_loss=0.07402]\n",
      "Step 526447  [5.330 sec/step, loss=0.07101, avg_loss=0.07397]\n",
      "Step 526448  [5.342 sec/step, loss=0.07659, avg_loss=0.07398]\n",
      "Step 526449  [5.352 sec/step, loss=0.07235, avg_loss=0.07396]\n",
      "Step 526450  [5.345 sec/step, loss=0.07299, avg_loss=0.07393]\n",
      "Step 526451  [5.342 sec/step, loss=0.07561, avg_loss=0.07394]\n",
      "Step 526452  [5.368 sec/step, loss=0.07625, avg_loss=0.07398]\n",
      "Step 526453  [5.376 sec/step, loss=0.07642, avg_loss=0.07403]\n",
      "Generated 32 batches of size 32 in 2.582 sec\n",
      "Step 526454  [5.359 sec/step, loss=0.07443, avg_loss=0.07402]\n",
      "Step 526455  [5.365 sec/step, loss=0.07652, avg_loss=0.07403]\n",
      "Step 526456  [5.360 sec/step, loss=0.07346, avg_loss=0.07402]\n",
      "Step 526457  [5.366 sec/step, loss=0.07674, avg_loss=0.07402]\n",
      "Step 526458  [5.383 sec/step, loss=0.07606, avg_loss=0.07404]\n",
      "Step 526459  [5.386 sec/step, loss=0.07578, avg_loss=0.07406]\n",
      "Step 526460  [5.375 sec/step, loss=0.07523, avg_loss=0.07405]\n",
      "Step 526461  [5.381 sec/step, loss=0.07363, avg_loss=0.07404]\n",
      "Step 526462  [5.393 sec/step, loss=0.07674, avg_loss=0.07405]\n",
      "Step 526463  [5.383 sec/step, loss=0.07551, avg_loss=0.07405]\n",
      "Step 526464  [5.394 sec/step, loss=0.07544, avg_loss=0.07405]\n",
      "Step 526465  [5.385 sec/step, loss=0.07099, avg_loss=0.07401]\n",
      "Step 526466  [5.390 sec/step, loss=0.07246, avg_loss=0.07401]\n",
      "Step 526467  [5.360 sec/step, loss=0.07413, avg_loss=0.07402]\n",
      "Step 526468  [5.352 sec/step, loss=0.07564, avg_loss=0.07402]\n",
      "Step 526469  [5.358 sec/step, loss=0.07601, avg_loss=0.07404]\n",
      "Step 526470  [5.361 sec/step, loss=0.07352, avg_loss=0.07404]\n",
      "Step 526471  [5.352 sec/step, loss=0.07354, avg_loss=0.07402]\n",
      "Step 526472  [5.400 sec/step, loss=0.06657, avg_loss=0.07394]\n",
      "Step 526473  [5.414 sec/step, loss=0.07514, avg_loss=0.07403]\n",
      "Step 526474  [5.415 sec/step, loss=0.07529, avg_loss=0.07403]\n",
      "Step 526475  [5.412 sec/step, loss=0.07425, avg_loss=0.07404]\n",
      "Step 526476  [5.409 sec/step, loss=0.07504, avg_loss=0.07404]\n",
      "Step 526477  [5.421 sec/step, loss=0.07552, avg_loss=0.07404]\n",
      "Step 526478  [5.412 sec/step, loss=0.07674, avg_loss=0.07406]\n",
      "Step 526479  [5.361 sec/step, loss=0.07458, avg_loss=0.07414]\n",
      "Step 526480  [5.369 sec/step, loss=0.07376, avg_loss=0.07414]\n",
      "Step 526481  [5.380 sec/step, loss=0.07365, avg_loss=0.07413]\n",
      "Step 526482  [5.405 sec/step, loss=0.07616, avg_loss=0.07418]\n",
      "Step 526483  [5.417 sec/step, loss=0.07611, avg_loss=0.07419]\n",
      "Step 526484  [5.413 sec/step, loss=0.07427, avg_loss=0.07420]\n",
      "Step 526485  [5.403 sec/step, loss=0.07489, avg_loss=0.07419]\n",
      "Generated 32 batches of size 32 in 2.408 sec\n",
      "Step 526486  [5.426 sec/step, loss=0.07278, avg_loss=0.07416]\n",
      "Step 526487  [5.439 sec/step, loss=0.07450, avg_loss=0.07416]\n",
      "Step 526488  [5.434 sec/step, loss=0.07602, avg_loss=0.07416]\n",
      "Step 526489  [5.420 sec/step, loss=0.07110, avg_loss=0.07412]\n",
      "Step 526490  [5.407 sec/step, loss=0.07328, avg_loss=0.07409]\n",
      "Step 526491  [5.397 sec/step, loss=0.06448, avg_loss=0.07403]\n",
      "Step 526492  [5.399 sec/step, loss=0.07439, avg_loss=0.07405]\n",
      "Step 526493  [5.389 sec/step, loss=0.07125, avg_loss=0.07400]\n",
      "Step 526494  [5.396 sec/step, loss=0.07502, avg_loss=0.07399]\n",
      "Step 526495  [5.396 sec/step, loss=0.07527, avg_loss=0.07401]\n",
      "Step 526496  [5.407 sec/step, loss=0.07468, avg_loss=0.07401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 526497  [5.413 sec/step, loss=0.07583, avg_loss=0.07404]\n",
      "Step 526498  [5.398 sec/step, loss=0.07330, avg_loss=0.07401]\n",
      "Step 526499  [5.367 sec/step, loss=0.07265, avg_loss=0.07401]\n",
      "Step 526500  [5.358 sec/step, loss=0.07384, avg_loss=0.07400]\n",
      "Writing summary at step: 526500\n",
      "Step 526501  [5.367 sec/step, loss=0.07372, avg_loss=0.07401]\n",
      "Step 526502  [5.387 sec/step, loss=0.07620, avg_loss=0.07405]\n",
      "Step 526503  [5.398 sec/step, loss=0.07536, avg_loss=0.07405]\n",
      "Step 526504  [5.372 sec/step, loss=0.06728, avg_loss=0.07396]\n",
      "Step 526505  [5.371 sec/step, loss=0.07687, avg_loss=0.07397]\n",
      "Step 526506  [5.414 sec/step, loss=0.06547, avg_loss=0.07386]\n",
      "Step 526507  [5.410 sec/step, loss=0.07535, avg_loss=0.07386]\n",
      "Step 526508  [5.403 sec/step, loss=0.07536, avg_loss=0.07387]\n",
      "Step 526509  [5.381 sec/step, loss=0.07205, avg_loss=0.07385]\n",
      "Step 526510  [5.370 sec/step, loss=0.07241, avg_loss=0.07382]\n",
      "Step 526511  [5.325 sec/step, loss=0.07226, avg_loss=0.07387]\n",
      "Step 526512  [5.327 sec/step, loss=0.07500, avg_loss=0.07388]\n",
      "Step 526513  [5.357 sec/step, loss=0.07281, avg_loss=0.07388]\n",
      "Step 526514  [5.371 sec/step, loss=0.07597, avg_loss=0.07389]\n",
      "Step 526515  [5.371 sec/step, loss=0.07473, avg_loss=0.07391]\n",
      "Step 526516  [5.382 sec/step, loss=0.07246, avg_loss=0.07389]\n",
      "Generated 32 batches of size 32 in 2.326 sec\n",
      "Step 526517  [5.388 sec/step, loss=0.07527, avg_loss=0.07389]\n",
      "Step 526518  [5.377 sec/step, loss=0.07418, avg_loss=0.07391]\n",
      "Step 526519  [5.347 sec/step, loss=0.07402, avg_loss=0.07390]\n",
      "Step 526520  [5.354 sec/step, loss=0.07537, avg_loss=0.07390]\n",
      "Step 526521  [5.368 sec/step, loss=0.07423, avg_loss=0.07390]\n",
      "Step 526522  [5.365 sec/step, loss=0.07457, avg_loss=0.07389]\n",
      "Step 526523  [5.395 sec/step, loss=0.07357, avg_loss=0.07391]\n",
      "Step 526524  [5.403 sec/step, loss=0.07616, avg_loss=0.07391]\n",
      "Step 526525  [5.379 sec/step, loss=0.07194, avg_loss=0.07387]\n",
      "Step 526526  [5.373 sec/step, loss=0.07196, avg_loss=0.07386]\n",
      "Step 526527  [5.358 sec/step, loss=0.07387, avg_loss=0.07385]\n",
      "Step 526528  [5.400 sec/step, loss=0.06614, avg_loss=0.07374]\n",
      "Step 526529  [5.425 sec/step, loss=0.07496, avg_loss=0.07383]\n",
      "Step 526530  [5.420 sec/step, loss=0.07606, avg_loss=0.07385]\n",
      "Step 526531  [5.407 sec/step, loss=0.07617, avg_loss=0.07389]\n",
      "Step 526532  [5.419 sec/step, loss=0.07493, avg_loss=0.07390]\n",
      "Step 526533  [5.407 sec/step, loss=0.07487, avg_loss=0.07389]\n",
      "Step 526534  [5.355 sec/step, loss=0.07458, avg_loss=0.07398]\n",
      "Step 526535  [5.386 sec/step, loss=0.07566, avg_loss=0.07407]\n",
      "Step 526536  [5.372 sec/step, loss=0.06448, avg_loss=0.07396]\n",
      "Step 526537  [5.368 sec/step, loss=0.07538, avg_loss=0.07397]\n",
      "Step 526538  [5.377 sec/step, loss=0.07325, avg_loss=0.07399]\n",
      "Step 526539  [5.372 sec/step, loss=0.07395, avg_loss=0.07398]\n",
      "Step 526540  [5.376 sec/step, loss=0.07605, avg_loss=0.07399]\n",
      "Step 526541  [5.382 sec/step, loss=0.07612, avg_loss=0.07399]\n",
      "Step 526542  [5.404 sec/step, loss=0.07370, avg_loss=0.07397]\n",
      "Step 526543  [5.412 sec/step, loss=0.07641, avg_loss=0.07398]\n",
      "Step 526544  [5.412 sec/step, loss=0.07291, avg_loss=0.07397]\n",
      "Step 526545  [5.411 sec/step, loss=0.07552, avg_loss=0.07399]\n",
      "Step 526546  [5.413 sec/step, loss=0.07471, avg_loss=0.07402]\n",
      "Step 526547  [5.433 sec/step, loss=0.07407, avg_loss=0.07405]\n",
      "Step 526548  [5.421 sec/step, loss=0.07523, avg_loss=0.07403]\n",
      "Generated 32 batches of size 32 in 2.402 sec\n",
      "Step 526549  [5.432 sec/step, loss=0.07605, avg_loss=0.07407]\n",
      "Step 526550  [5.449 sec/step, loss=0.07527, avg_loss=0.07409]\n",
      "Step 526551  [5.427 sec/step, loss=0.07121, avg_loss=0.07405]\n",
      "Step 526552  [5.417 sec/step, loss=0.07447, avg_loss=0.07403]\n",
      "Step 526553  [5.429 sec/step, loss=0.07503, avg_loss=0.07402]\n",
      "Step 526554  [5.428 sec/step, loss=0.07475, avg_loss=0.07402]\n",
      "Step 526555  [5.407 sec/step, loss=0.07110, avg_loss=0.07397]\n",
      "Step 526556  [5.412 sec/step, loss=0.07331, avg_loss=0.07397]\n",
      "Step 526557  [5.404 sec/step, loss=0.07455, avg_loss=0.07394]\n",
      "Step 526558  [5.387 sec/step, loss=0.07413, avg_loss=0.07392]\n",
      "Step 526559  [5.389 sec/step, loss=0.07508, avg_loss=0.07392]\n",
      "Step 526560  [5.398 sec/step, loss=0.07372, avg_loss=0.07390]\n",
      "Step 526561  [5.400 sec/step, loss=0.07677, avg_loss=0.07393]\n",
      "Step 526562  [5.386 sec/step, loss=0.07415, avg_loss=0.07391]\n",
      "Step 526563  [5.377 sec/step, loss=0.07331, avg_loss=0.07389]\n",
      "Step 526564  [5.370 sec/step, loss=0.07417, avg_loss=0.07387]\n",
      "Step 526565  [5.384 sec/step, loss=0.07500, avg_loss=0.07391]\n",
      "Step 526566  [5.406 sec/step, loss=0.07592, avg_loss=0.07395]\n",
      "Step 526567  [5.424 sec/step, loss=0.07330, avg_loss=0.07394]\n",
      "Step 526568  [5.417 sec/step, loss=0.07590, avg_loss=0.07394]\n",
      "Step 526569  [5.410 sec/step, loss=0.07510, avg_loss=0.07393]\n",
      "Step 526570  [5.414 sec/step, loss=0.07434, avg_loss=0.07394]\n",
      "Step 526571  [5.448 sec/step, loss=0.07303, avg_loss=0.07394]\n",
      "Step 526572  [5.408 sec/step, loss=0.07652, avg_loss=0.07404]\n",
      "Step 526573  [5.426 sec/step, loss=0.07592, avg_loss=0.07404]\n",
      "Step 526574  [5.479 sec/step, loss=0.06666, avg_loss=0.07396]\n",
      "Step 526575  [5.491 sec/step, loss=0.07534, avg_loss=0.07397]\n",
      "Step 526576  [5.492 sec/step, loss=0.07239, avg_loss=0.07394]\n",
      "Step 526577  [5.483 sec/step, loss=0.07686, avg_loss=0.07395]\n",
      "Step 526578  [5.474 sec/step, loss=0.07537, avg_loss=0.07394]\n",
      "Step 526579  [5.464 sec/step, loss=0.07424, avg_loss=0.07394]\n",
      "Step 526580  [5.443 sec/step, loss=0.07222, avg_loss=0.07392]\n",
      "Generated 32 batches of size 32 in 2.504 sec\n",
      "Step 526581  [5.426 sec/step, loss=0.07155, avg_loss=0.07390]\n",
      "Step 526582  [5.416 sec/step, loss=0.07371, avg_loss=0.07388]\n",
      "Step 526583  [5.400 sec/step, loss=0.07378, avg_loss=0.07385]\n",
      "Step 526584  [5.396 sec/step, loss=0.07428, avg_loss=0.07385]\n",
      "Step 526585  [5.398 sec/step, loss=0.07488, avg_loss=0.07385]\n",
      "Step 526586  [5.365 sec/step, loss=0.07232, avg_loss=0.07385]\n",
      "Step 526587  [5.369 sec/step, loss=0.07640, avg_loss=0.07387]\n",
      "Step 526588  [5.359 sec/step, loss=0.07523, avg_loss=0.07386]\n",
      "Step 526589  [5.357 sec/step, loss=0.06745, avg_loss=0.07382]\n",
      "Step 526590  [5.357 sec/step, loss=0.07084, avg_loss=0.07380]\n",
      "Step 526591  [5.362 sec/step, loss=0.07088, avg_loss=0.07386]\n",
      "Step 526592  [5.370 sec/step, loss=0.07626, avg_loss=0.07388]\n",
      "Step 526593  [5.384 sec/step, loss=0.07628, avg_loss=0.07393]\n",
      "Step 526594  [5.368 sec/step, loss=0.07432, avg_loss=0.07392]\n",
      "Step 526595  [5.370 sec/step, loss=0.07516, avg_loss=0.07392]\n",
      "Step 526596  [5.366 sec/step, loss=0.07535, avg_loss=0.07393]\n",
      "Step 526597  [5.358 sec/step, loss=0.07497, avg_loss=0.07392]\n",
      "Step 526598  [5.413 sec/step, loss=0.06591, avg_loss=0.07385]\n",
      "Step 526599  [5.432 sec/step, loss=0.07629, avg_loss=0.07388]\n",
      "Step 526600  [5.424 sec/step, loss=0.06486, avg_loss=0.07379]\n",
      "Writing summary at step: 526600\n",
      "Step 526601  [5.422 sec/step, loss=0.07375, avg_loss=0.07380]\n",
      "Step 526602  [5.416 sec/step, loss=0.07547, avg_loss=0.07379]\n",
      "Step 526603  [5.393 sec/step, loss=0.07380, avg_loss=0.07377]\n",
      "Step 526604  [5.409 sec/step, loss=0.07424, avg_loss=0.07384]\n",
      "Step 526605  [5.396 sec/step, loss=0.07484, avg_loss=0.07382]\n",
      "Step 526606  [5.358 sec/step, loss=0.07527, avg_loss=0.07392]\n",
      "Step 526607  [5.355 sec/step, loss=0.07446, avg_loss=0.07391]\n",
      "Step 526608  [5.356 sec/step, loss=0.07500, avg_loss=0.07391]\n",
      "Step 526609  [5.338 sec/step, loss=0.07175, avg_loss=0.07390]\n",
      "Step 526610  [5.359 sec/step, loss=0.07671, avg_loss=0.07395]\n",
      "Step 526611  [5.362 sec/step, loss=0.07360, avg_loss=0.07396]\n",
      "Generated 32 batches of size 32 in 2.578 sec\n",
      "Step 526612  [5.365 sec/step, loss=0.07219, avg_loss=0.07393]\n",
      "Step 526613  [5.349 sec/step, loss=0.07616, avg_loss=0.07397]\n",
      "Step 526614  [5.351 sec/step, loss=0.07470, avg_loss=0.07395]\n",
      "Step 526615  [5.358 sec/step, loss=0.07622, avg_loss=0.07397]\n",
      "Step 526616  [5.351 sec/step, loss=0.07325, avg_loss=0.07398]\n",
      "Step 526617  [5.369 sec/step, loss=0.07329, avg_loss=0.07396]\n",
      "Step 526618  [5.377 sec/step, loss=0.07319, avg_loss=0.07395]\n",
      "Step 526619  [5.384 sec/step, loss=0.07604, avg_loss=0.07397]\n",
      "Step 526620  [5.398 sec/step, loss=0.07275, avg_loss=0.07394]\n",
      "Step 526621  [5.394 sec/step, loss=0.07582, avg_loss=0.07396]\n",
      "Step 526622  [5.384 sec/step, loss=0.07619, avg_loss=0.07397]\n",
      "Step 526623  [5.376 sec/step, loss=0.07461, avg_loss=0.07398]\n",
      "Step 526624  [5.366 sec/step, loss=0.07478, avg_loss=0.07397]\n",
      "Step 526625  [5.396 sec/step, loss=0.07344, avg_loss=0.07398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 526626  [5.410 sec/step, loss=0.07589, avg_loss=0.07402]\n",
      "Step 526627  [5.427 sec/step, loss=0.07268, avg_loss=0.07401]\n",
      "Step 526628  [5.389 sec/step, loss=0.07471, avg_loss=0.07410]\n",
      "Step 526629  [5.375 sec/step, loss=0.07060, avg_loss=0.07405]\n",
      "Step 526630  [5.396 sec/step, loss=0.07334, avg_loss=0.07403]\n",
      "Step 526631  [5.380 sec/step, loss=0.07517, avg_loss=0.07402]\n",
      "Step 526632  [5.367 sec/step, loss=0.07130, avg_loss=0.07398]\n",
      "Step 526633  [5.358 sec/step, loss=0.07402, avg_loss=0.07397]\n",
      "Step 526634  [5.357 sec/step, loss=0.07294, avg_loss=0.07395]\n",
      "Step 526635  [5.343 sec/step, loss=0.07562, avg_loss=0.07395]\n",
      "Step 526636  [5.368 sec/step, loss=0.07574, avg_loss=0.07407]\n",
      "Step 526637  [5.373 sec/step, loss=0.07478, avg_loss=0.07406]\n",
      "Step 526638  [5.379 sec/step, loss=0.07219, avg_loss=0.07405]\n",
      "Step 526639  [5.395 sec/step, loss=0.07606, avg_loss=0.07407]\n",
      "Step 526640  [5.379 sec/step, loss=0.07209, avg_loss=0.07403]\n",
      "Step 526641  [5.377 sec/step, loss=0.07458, avg_loss=0.07402]\n",
      "Step 526642  [5.363 sec/step, loss=0.07589, avg_loss=0.07404]\n",
      "Step 526643  [5.356 sec/step, loss=0.07485, avg_loss=0.07402]\n",
      "Generated 32 batches of size 32 in 2.396 sec\n",
      "Step 526644  [5.375 sec/step, loss=0.07627, avg_loss=0.07406]\n",
      "Step 526645  [5.371 sec/step, loss=0.07361, avg_loss=0.07404]\n",
      "Step 526646  [5.386 sec/step, loss=0.07672, avg_loss=0.07406]\n",
      "Step 526647  [5.360 sec/step, loss=0.06872, avg_loss=0.07400]\n",
      "Step 526648  [5.354 sec/step, loss=0.07394, avg_loss=0.07399]\n",
      "Step 526649  [5.335 sec/step, loss=0.07535, avg_loss=0.07398]\n",
      "Step 526650  [5.314 sec/step, loss=0.07238, avg_loss=0.07396]\n",
      "Step 526651  [5.327 sec/step, loss=0.07472, avg_loss=0.07399]\n",
      "Step 526652  [5.377 sec/step, loss=0.06675, avg_loss=0.07391]\n",
      "Step 526653  [5.367 sec/step, loss=0.07555, avg_loss=0.07392]\n",
      "Step 526654  [5.370 sec/step, loss=0.07350, avg_loss=0.07391]\n",
      "Step 526655  [5.386 sec/step, loss=0.07569, avg_loss=0.07395]\n",
      "Step 526656  [5.399 sec/step, loss=0.07383, avg_loss=0.07396]\n",
      "Step 526657  [5.376 sec/step, loss=0.06539, avg_loss=0.07387]\n",
      "Step 526658  [5.388 sec/step, loss=0.07559, avg_loss=0.07388]\n",
      "Step 526659  [5.392 sec/step, loss=0.07622, avg_loss=0.07389]\n",
      "Step 526660  [5.404 sec/step, loss=0.07378, avg_loss=0.07389]\n",
      "Step 526661  [5.442 sec/step, loss=0.06717, avg_loss=0.07380]\n",
      "Step 526662  [5.461 sec/step, loss=0.07626, avg_loss=0.07382]\n",
      "Step 526663  [5.472 sec/step, loss=0.07281, avg_loss=0.07381]\n",
      "Step 526664  [5.467 sec/step, loss=0.07488, avg_loss=0.07382]\n",
      "Step 526665  [5.465 sec/step, loss=0.07281, avg_loss=0.07380]\n",
      "Step 526666  [5.477 sec/step, loss=0.07332, avg_loss=0.07377]\n",
      "Step 526667  [5.480 sec/step, loss=0.07599, avg_loss=0.07380]\n",
      "Step 526668  [5.485 sec/step, loss=0.07429, avg_loss=0.07378]\n",
      "Step 526669  [5.469 sec/step, loss=0.06996, avg_loss=0.07373]\n",
      "Step 526670  [5.477 sec/step, loss=0.07445, avg_loss=0.07373]\n",
      "Step 526671  [5.447 sec/step, loss=0.07378, avg_loss=0.07374]\n",
      "Step 526672  [5.431 sec/step, loss=0.07372, avg_loss=0.07371]\n",
      "Step 526673  [5.414 sec/step, loss=0.07338, avg_loss=0.07369]\n",
      "Step 526674  [5.367 sec/step, loss=0.07543, avg_loss=0.07377]\n",
      "Step 526675  [5.351 sec/step, loss=0.07165, avg_loss=0.07374]\n",
      "Generated 32 batches of size 32 in 2.566 sec\n",
      "Step 526676  [5.343 sec/step, loss=0.07399, avg_loss=0.07375]\n",
      "Step 526677  [5.336 sec/step, loss=0.07433, avg_loss=0.07373]\n",
      "Step 526678  [5.347 sec/step, loss=0.07522, avg_loss=0.07373]\n",
      "Step 526679  [5.363 sec/step, loss=0.07547, avg_loss=0.07374]\n",
      "Step 526680  [5.373 sec/step, loss=0.07520, avg_loss=0.07377]\n",
      "Step 526681  [5.380 sec/step, loss=0.07147, avg_loss=0.07377]\n",
      "Step 526682  [5.390 sec/step, loss=0.07614, avg_loss=0.07379]\n",
      "Step 526683  [5.387 sec/step, loss=0.07341, avg_loss=0.07379]\n",
      "Step 526684  [5.387 sec/step, loss=0.07554, avg_loss=0.07380]\n",
      "Step 526685  [5.386 sec/step, loss=0.07267, avg_loss=0.07378]\n",
      "Step 526686  [5.396 sec/step, loss=0.07492, avg_loss=0.07380]\n",
      "Step 526687  [5.398 sec/step, loss=0.07559, avg_loss=0.07380]\n",
      "Step 526688  [5.425 sec/step, loss=0.07271, avg_loss=0.07377]\n",
      "Step 526689  [5.434 sec/step, loss=0.07426, avg_loss=0.07384]\n",
      "Step 526690  [5.439 sec/step, loss=0.07575, avg_loss=0.07389]\n",
      "Step 526691  [5.458 sec/step, loss=0.07629, avg_loss=0.07394]\n",
      "Step 526692  [5.440 sec/step, loss=0.07076, avg_loss=0.07389]\n",
      "Step 526693  [5.424 sec/step, loss=0.07156, avg_loss=0.07384]\n",
      "Step 526694  [5.437 sec/step, loss=0.07547, avg_loss=0.07385]\n",
      "Step 526695  [5.437 sec/step, loss=0.07509, avg_loss=0.07385]\n",
      "Step 526696  [5.441 sec/step, loss=0.07346, avg_loss=0.07383]\n",
      "Step 526697  [5.441 sec/step, loss=0.07518, avg_loss=0.07383]\n",
      "Step 526698  [5.377 sec/step, loss=0.07161, avg_loss=0.07389]\n",
      "Step 526699  [5.362 sec/step, loss=0.07416, avg_loss=0.07387]\n",
      "Step 526700  [5.389 sec/step, loss=0.07643, avg_loss=0.07398]\n",
      "Writing summary at step: 526700\n",
      "Step 526701  [5.404 sec/step, loss=0.07643, avg_loss=0.07401]\n",
      "Step 526702  [5.413 sec/step, loss=0.07563, avg_loss=0.07401]\n",
      "Step 526703  [5.473 sec/step, loss=0.06583, avg_loss=0.07393]\n",
      "Step 526704  [5.489 sec/step, loss=0.07493, avg_loss=0.07394]\n",
      "Step 526705  [5.496 sec/step, loss=0.07545, avg_loss=0.07395]\n",
      "Step 526706  [5.489 sec/step, loss=0.07538, avg_loss=0.07395]\n",
      "Generated 32 batches of size 32 in 2.418 sec\n",
      "Step 526707  [5.494 sec/step, loss=0.07581, avg_loss=0.07396]\n",
      "Step 526708  [5.491 sec/step, loss=0.07386, avg_loss=0.07395]\n",
      "Step 526709  [5.502 sec/step, loss=0.07359, avg_loss=0.07397]\n",
      "Step 526710  [5.506 sec/step, loss=0.07446, avg_loss=0.07395]\n",
      "Step 526711  [5.496 sec/step, loss=0.07410, avg_loss=0.07395]\n",
      "Step 526712  [5.486 sec/step, loss=0.07434, avg_loss=0.07397]\n",
      "Step 526713  [5.495 sec/step, loss=0.07518, avg_loss=0.07396]\n",
      "Step 526714  [5.477 sec/step, loss=0.07336, avg_loss=0.07395]\n",
      "Step 526715  [5.478 sec/step, loss=0.07282, avg_loss=0.07392]\n",
      "Step 526716  [5.490 sec/step, loss=0.07500, avg_loss=0.07393]\n",
      "Step 526717  [5.469 sec/step, loss=0.07452, avg_loss=0.07394]\n",
      "Step 526718  [5.474 sec/step, loss=0.07571, avg_loss=0.07397]\n",
      "Step 526719  [5.473 sec/step, loss=0.07435, avg_loss=0.07395]\n",
      "Step 526720  [5.465 sec/step, loss=0.07500, avg_loss=0.07398]\n",
      "Step 526721  [5.449 sec/step, loss=0.07351, avg_loss=0.07395]\n",
      "Step 526722  [5.430 sec/step, loss=0.06760, avg_loss=0.07387]\n",
      "Step 526723  [5.415 sec/step, loss=0.07414, avg_loss=0.07386]\n",
      "Step 526724  [5.409 sec/step, loss=0.07429, avg_loss=0.07386]\n",
      "Step 526725  [5.393 sec/step, loss=0.07236, avg_loss=0.07385]\n",
      "Step 526726  [5.383 sec/step, loss=0.07546, avg_loss=0.07384]\n",
      "Step 526727  [5.390 sec/step, loss=0.07594, avg_loss=0.07387]\n",
      "Step 526728  [5.384 sec/step, loss=0.07551, avg_loss=0.07388]\n",
      "Step 526729  [5.381 sec/step, loss=0.07106, avg_loss=0.07389]\n",
      "Step 526730  [5.363 sec/step, loss=0.07619, avg_loss=0.07392]\n",
      "Step 526731  [5.363 sec/step, loss=0.07351, avg_loss=0.07390]\n",
      "Step 526732  [5.358 sec/step, loss=0.07230, avg_loss=0.07391]\n",
      "Step 526733  [5.352 sec/step, loss=0.07224, avg_loss=0.07389]\n",
      "Step 526734  [5.368 sec/step, loss=0.07647, avg_loss=0.07393]\n",
      "Step 526735  [5.378 sec/step, loss=0.07614, avg_loss=0.07393]\n",
      "Step 526736  [5.371 sec/step, loss=0.07281, avg_loss=0.07390]\n",
      "Step 526737  [5.370 sec/step, loss=0.07522, avg_loss=0.07391]\n",
      "Step 526738  [5.393 sec/step, loss=0.07563, avg_loss=0.07394]\n",
      "Generated 32 batches of size 32 in 2.479 sec\n",
      "Step 526739  [5.387 sec/step, loss=0.07443, avg_loss=0.07393]\n",
      "Step 526740  [5.412 sec/step, loss=0.07354, avg_loss=0.07394]\n",
      "Step 526741  [5.423 sec/step, loss=0.07565, avg_loss=0.07395]\n",
      "Step 526742  [5.459 sec/step, loss=0.06517, avg_loss=0.07384]\n",
      "Step 526743  [5.460 sec/step, loss=0.07485, avg_loss=0.07384]\n",
      "Step 526744  [5.455 sec/step, loss=0.07658, avg_loss=0.07385]\n",
      "Step 526745  [5.475 sec/step, loss=0.07437, avg_loss=0.07385]\n",
      "Step 526746  [5.458 sec/step, loss=0.07349, avg_loss=0.07382]\n",
      "Step 526747  [5.472 sec/step, loss=0.07494, avg_loss=0.07388]\n",
      "Step 526748  [5.480 sec/step, loss=0.07524, avg_loss=0.07390]\n",
      "Step 526749  [5.534 sec/step, loss=0.06648, avg_loss=0.07381]\n",
      "Step 526750  [5.553 sec/step, loss=0.07460, avg_loss=0.07383]\n",
      "Step 526751  [5.567 sec/step, loss=0.07380, avg_loss=0.07382]\n",
      "Step 526752  [5.503 sec/step, loss=0.07153, avg_loss=0.07387]\n",
      "Step 526753  [5.492 sec/step, loss=0.07208, avg_loss=0.07383]\n",
      "Step 526754  [5.501 sec/step, loss=0.07424, avg_loss=0.07384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 526755  [5.513 sec/step, loss=0.07661, avg_loss=0.07385]\n",
      "Step 526756  [5.513 sec/step, loss=0.07602, avg_loss=0.07387]\n",
      "Step 526757  [5.540 sec/step, loss=0.07655, avg_loss=0.07398]\n",
      "Step 526758  [5.551 sec/step, loss=0.07545, avg_loss=0.07398]\n",
      "Step 526759  [5.540 sec/step, loss=0.07514, avg_loss=0.07397]\n",
      "Step 526760  [5.511 sec/step, loss=0.07345, avg_loss=0.07397]\n",
      "Step 526761  [5.457 sec/step, loss=0.07319, avg_loss=0.07403]\n",
      "Step 526762  [5.446 sec/step, loss=0.07538, avg_loss=0.07402]\n",
      "Step 526763  [5.443 sec/step, loss=0.07407, avg_loss=0.07403]\n",
      "Step 526764  [5.468 sec/step, loss=0.07286, avg_loss=0.07401]\n",
      "Step 526765  [5.465 sec/step, loss=0.07303, avg_loss=0.07401]\n",
      "Step 526766  [5.426 sec/step, loss=0.07389, avg_loss=0.07402]\n",
      "Step 526767  [5.410 sec/step, loss=0.07505, avg_loss=0.07401]\n",
      "Step 526768  [5.419 sec/step, loss=0.07623, avg_loss=0.07403]\n",
      "Step 526769  [5.433 sec/step, loss=0.07510, avg_loss=0.07408]\n",
      "Step 526770  [5.433 sec/step, loss=0.07460, avg_loss=0.07408]\n",
      "Generated 32 batches of size 32 in 2.547 sec\n",
      "Step 526771  [5.444 sec/step, loss=0.07453, avg_loss=0.07409]\n",
      "Step 526772  [5.458 sec/step, loss=0.07411, avg_loss=0.07409]\n",
      "Step 526773  [5.467 sec/step, loss=0.07511, avg_loss=0.07411]\n",
      "Step 526774  [5.449 sec/step, loss=0.06670, avg_loss=0.07403]\n",
      "Step 526775  [5.466 sec/step, loss=0.07537, avg_loss=0.07406]\n",
      "Step 526776  [5.465 sec/step, loss=0.07408, avg_loss=0.07406]\n",
      "Step 526777  [5.454 sec/step, loss=0.07096, avg_loss=0.07403]\n",
      "Step 526778  [5.446 sec/step, loss=0.07430, avg_loss=0.07402]\n",
      "Step 526779  [5.449 sec/step, loss=0.07622, avg_loss=0.07403]\n",
      "Step 526780  [5.458 sec/step, loss=0.07589, avg_loss=0.07403]\n",
      "Step 526781  [5.463 sec/step, loss=0.07479, avg_loss=0.07407]\n",
      "Step 526782  [5.479 sec/step, loss=0.07366, avg_loss=0.07404]\n",
      "Step 526783  [5.501 sec/step, loss=0.07572, avg_loss=0.07407]\n",
      "Step 526784  [5.498 sec/step, loss=0.07303, avg_loss=0.07404]\n",
      "Step 526785  [5.500 sec/step, loss=0.07531, avg_loss=0.07407]\n",
      "Step 526786  [5.481 sec/step, loss=0.07414, avg_loss=0.07406]\n",
      "Step 526787  [5.470 sec/step, loss=0.07434, avg_loss=0.07405]\n",
      "Step 526788  [5.454 sec/step, loss=0.07583, avg_loss=0.07408]\n",
      "Step 526789  [5.480 sec/step, loss=0.07519, avg_loss=0.07409]\n",
      "Step 526790  [5.489 sec/step, loss=0.07396, avg_loss=0.07407]\n",
      "Step 526791  [5.472 sec/step, loss=0.07322, avg_loss=0.07404]\n",
      "Step 526792  [5.472 sec/step, loss=0.07099, avg_loss=0.07404]\n",
      "Step 526793  [5.479 sec/step, loss=0.07558, avg_loss=0.07408]\n",
      "Step 526794  [5.477 sec/step, loss=0.07321, avg_loss=0.07406]\n",
      "Step 526795  [5.477 sec/step, loss=0.07531, avg_loss=0.07406]\n",
      "Step 526796  [5.481 sec/step, loss=0.07512, avg_loss=0.07408]\n",
      "Step 526797  [5.479 sec/step, loss=0.07121, avg_loss=0.07404]\n",
      "Step 526798  [5.498 sec/step, loss=0.07486, avg_loss=0.07407]\n",
      "Step 526799  [5.514 sec/step, loss=0.07548, avg_loss=0.07408]\n",
      "Step 526800  [5.490 sec/step, loss=0.07158, avg_loss=0.07404]\n",
      "Writing summary at step: 526800\n",
      "Step 526801  [5.478 sec/step, loss=0.07538, avg_loss=0.07403]\n",
      "Generated 32 batches of size 32 in 2.537 sec\n",
      "Step 526802  [5.463 sec/step, loss=0.07375, avg_loss=0.07401]\n",
      "Step 526803  [5.414 sec/step, loss=0.07236, avg_loss=0.07407]\n",
      "Step 526804  [5.394 sec/step, loss=0.07341, avg_loss=0.07406]\n",
      "Step 526805  [5.398 sec/step, loss=0.07639, avg_loss=0.07407]\n",
      "Step 526806  [5.400 sec/step, loss=0.07609, avg_loss=0.07407]\n",
      "Step 526807  [5.400 sec/step, loss=0.07624, avg_loss=0.07408]\n",
      "Step 526808  [5.405 sec/step, loss=0.07510, avg_loss=0.07409]\n",
      "Step 526809  [5.459 sec/step, loss=0.06666, avg_loss=0.07402]\n",
      "Step 526810  [5.430 sec/step, loss=0.06481, avg_loss=0.07392]\n",
      "Step 526811  [5.442 sec/step, loss=0.07464, avg_loss=0.07393]\n",
      "Step 526812  [5.449 sec/step, loss=0.07274, avg_loss=0.07391]\n",
      "Step 526813  [5.439 sec/step, loss=0.07562, avg_loss=0.07392]\n",
      "Step 526814  [5.450 sec/step, loss=0.07280, avg_loss=0.07391]\n",
      "Step 526815  [5.445 sec/step, loss=0.07528, avg_loss=0.07394]\n",
      "Step 526816  [5.443 sec/step, loss=0.07273, avg_loss=0.07391]\n",
      "Step 526817  [5.442 sec/step, loss=0.07524, avg_loss=0.07392]\n",
      "Step 526818  [5.432 sec/step, loss=0.07346, avg_loss=0.07390]\n",
      "Step 526819  [5.422 sec/step, loss=0.07444, avg_loss=0.07390]\n",
      "Step 526820  [5.410 sec/step, loss=0.07511, avg_loss=0.07390]\n",
      "Step 526821  [5.417 sec/step, loss=0.07387, avg_loss=0.07390]\n",
      "Step 526822  [5.441 sec/step, loss=0.07596, avg_loss=0.07399]\n",
      "Step 526823  [5.495 sec/step, loss=0.06698, avg_loss=0.07392]\n",
      "Step 526824  [5.510 sec/step, loss=0.07635, avg_loss=0.07394]\n",
      "Step 526825  [5.527 sec/step, loss=0.07396, avg_loss=0.07395]\n",
      "Step 526826  [5.534 sec/step, loss=0.07502, avg_loss=0.07395]\n",
      "Step 526827  [5.504 sec/step, loss=0.06667, avg_loss=0.07386]\n",
      "Step 526828  [5.498 sec/step, loss=0.07415, avg_loss=0.07384]\n",
      "Step 526829  [5.520 sec/step, loss=0.07689, avg_loss=0.07390]\n",
      "Step 526830  [5.507 sec/step, loss=0.07362, avg_loss=0.07388]\n",
      "Step 526831  [5.503 sec/step, loss=0.07375, avg_loss=0.07388]\n",
      "Step 526832  [5.527 sec/step, loss=0.07616, avg_loss=0.07392]\n",
      "Step 526833  [5.551 sec/step, loss=0.07616, avg_loss=0.07396]\n",
      "Generated 32 batches of size 32 in 2.428 sec\n",
      "Step 526834  [5.541 sec/step, loss=0.07539, avg_loss=0.07394]\n",
      "Step 526835  [5.520 sec/step, loss=0.07106, avg_loss=0.07389]\n",
      "Step 526836  [5.532 sec/step, loss=0.07437, avg_loss=0.07391]\n",
      "Step 526837  [5.533 sec/step, loss=0.07588, avg_loss=0.07392]\n",
      "Step 526838  [5.537 sec/step, loss=0.07340, avg_loss=0.07389]\n",
      "Step 526839  [5.529 sec/step, loss=0.07351, avg_loss=0.07388]\n",
      "Step 526840  [5.504 sec/step, loss=0.07220, avg_loss=0.07387]\n",
      "Step 526841  [5.493 sec/step, loss=0.07292, avg_loss=0.07384]\n",
      "Step 526842  [5.453 sec/step, loss=0.07616, avg_loss=0.07395]\n",
      "Step 526843  [5.445 sec/step, loss=0.07405, avg_loss=0.07395]\n",
      "Step 526844  [5.426 sec/step, loss=0.07115, avg_loss=0.07389]\n",
      "Step 526845  [5.417 sec/step, loss=0.07557, avg_loss=0.07390]\n",
      "Step 526846  [5.425 sec/step, loss=0.07504, avg_loss=0.07392]\n",
      "Step 526847  [5.424 sec/step, loss=0.07524, avg_loss=0.07392]\n",
      "Step 526848  [5.417 sec/step, loss=0.07495, avg_loss=0.07392]\n",
      "Step 526849  [5.375 sec/step, loss=0.07611, avg_loss=0.07402]\n",
      "Step 526850  [5.377 sec/step, loss=0.07654, avg_loss=0.07403]\n",
      "Step 526851  [5.415 sec/step, loss=0.06605, avg_loss=0.07396]\n",
      "Step 526852  [5.425 sec/step, loss=0.07116, avg_loss=0.07395]\n",
      "Step 526853  [5.437 sec/step, loss=0.07358, avg_loss=0.07397]\n",
      "Step 526854  [5.440 sec/step, loss=0.07602, avg_loss=0.07399]\n",
      "Step 526855  [5.422 sec/step, loss=0.07341, avg_loss=0.07395]\n",
      "Step 526856  [5.415 sec/step, loss=0.07242, avg_loss=0.07392]\n",
      "Step 526857  [5.406 sec/step, loss=0.07509, avg_loss=0.07390]\n",
      "Step 526858  [5.403 sec/step, loss=0.07629, avg_loss=0.07391]\n",
      "Step 526859  [5.409 sec/step, loss=0.07560, avg_loss=0.07392]\n",
      "Step 526860  [5.441 sec/step, loss=0.07591, avg_loss=0.07394]\n",
      "Step 526861  [5.435 sec/step, loss=0.07412, avg_loss=0.07395]\n",
      "Step 526862  [5.452 sec/step, loss=0.07459, avg_loss=0.07394]\n",
      "Step 526863  [5.439 sec/step, loss=0.07138, avg_loss=0.07392]\n",
      "Step 526864  [5.414 sec/step, loss=0.07577, avg_loss=0.07394]\n",
      "Step 526865  [5.418 sec/step, loss=0.07423, avg_loss=0.07396]\n",
      "Generated 32 batches of size 32 in 2.454 sec\n",
      "Step 526866  [5.443 sec/step, loss=0.07689, avg_loss=0.07399]\n",
      "Step 526867  [5.459 sec/step, loss=0.07592, avg_loss=0.07400]\n",
      "Step 526868  [5.438 sec/step, loss=0.07405, avg_loss=0.07397]\n",
      "Step 526869  [5.432 sec/step, loss=0.07246, avg_loss=0.07395]\n",
      "Step 526870  [5.414 sec/step, loss=0.07389, avg_loss=0.07394]\n",
      "Step 526871  [5.423 sec/step, loss=0.07473, avg_loss=0.07394]\n",
      "Step 526872  [5.422 sec/step, loss=0.07283, avg_loss=0.07393]\n",
      "Step 526873  [5.417 sec/step, loss=0.07436, avg_loss=0.07392]\n",
      "Step 526874  [5.416 sec/step, loss=0.06524, avg_loss=0.07391]\n",
      "Step 526875  [5.415 sec/step, loss=0.07577, avg_loss=0.07391]\n",
      "Step 526876  [5.416 sec/step, loss=0.07276, avg_loss=0.07390]\n",
      "Step 526877  [5.424 sec/step, loss=0.07550, avg_loss=0.07394]\n",
      "Step 526878  [5.431 sec/step, loss=0.07655, avg_loss=0.07397]\n",
      "Step 526879  [5.436 sec/step, loss=0.07588, avg_loss=0.07396]\n",
      "Step 526880  [5.438 sec/step, loss=0.07509, avg_loss=0.07395]\n",
      "Step 526881  [5.448 sec/step, loss=0.07631, avg_loss=0.07397]\n",
      "Step 526882  [5.426 sec/step, loss=0.07588, avg_loss=0.07399]\n",
      "Step 526883  [5.402 sec/step, loss=0.07432, avg_loss=0.07398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 526884  [5.409 sec/step, loss=0.07493, avg_loss=0.07400]\n",
      "Step 526885  [5.408 sec/step, loss=0.07458, avg_loss=0.07399]\n",
      "Step 526886  [5.404 sec/step, loss=0.07237, avg_loss=0.07397]\n",
      "Step 526887  [5.402 sec/step, loss=0.07416, avg_loss=0.07397]\n",
      "Step 526888  [5.394 sec/step, loss=0.07562, avg_loss=0.07397]\n",
      "Step 526889  [5.426 sec/step, loss=0.06649, avg_loss=0.07388]\n",
      "Step 526890  [5.407 sec/step, loss=0.07361, avg_loss=0.07388]\n",
      "Step 526891  [5.417 sec/step, loss=0.07524, avg_loss=0.07390]\n",
      "Step 526892  [5.439 sec/step, loss=0.07682, avg_loss=0.07396]\n",
      "Step 526893  [5.462 sec/step, loss=0.07593, avg_loss=0.07396]\n",
      "Step 526894  [5.455 sec/step, loss=0.07431, avg_loss=0.07397]\n",
      "Step 526895  [5.453 sec/step, loss=0.07413, avg_loss=0.07396]\n",
      "Step 526896  [5.462 sec/step, loss=0.07600, avg_loss=0.07397]\n",
      "Step 526897  [5.461 sec/step, loss=0.07191, avg_loss=0.07397]\n",
      "Generated 32 batches of size 32 in 2.528 sec\n",
      "Step 526898  [5.450 sec/step, loss=0.07240, avg_loss=0.07395]\n",
      "Step 526899  [5.439 sec/step, loss=0.07555, avg_loss=0.07395]\n",
      "Step 526900  [5.452 sec/step, loss=0.07505, avg_loss=0.07399]\n",
      "Writing summary at step: 526900\n",
      "Step 526901  [5.477 sec/step, loss=0.07324, avg_loss=0.07396]\n",
      "Step 526902  [5.463 sec/step, loss=0.06645, avg_loss=0.07389]\n",
      "Step 526903  [5.461 sec/step, loss=0.07422, avg_loss=0.07391]\n",
      "Step 526904  [5.469 sec/step, loss=0.07512, avg_loss=0.07393]\n",
      "Step 526905  [5.469 sec/step, loss=0.07618, avg_loss=0.07392]\n",
      "Step 526906  [5.458 sec/step, loss=0.07527, avg_loss=0.07392]\n",
      "Step 526907  [5.448 sec/step, loss=0.07452, avg_loss=0.07390]\n",
      "Step 526908  [5.454 sec/step, loss=0.07664, avg_loss=0.07391]\n",
      "Step 526909  [5.401 sec/step, loss=0.07547, avg_loss=0.07400]\n",
      "Step 526910  [5.428 sec/step, loss=0.07630, avg_loss=0.07412]\n",
      "Step 526911  [5.419 sec/step, loss=0.07262, avg_loss=0.07410]\n",
      "Step 526912  [5.424 sec/step, loss=0.07483, avg_loss=0.07412]\n",
      "Step 526913  [5.430 sec/step, loss=0.07563, avg_loss=0.07412]\n",
      "Step 526914  [5.416 sec/step, loss=0.06928, avg_loss=0.07408]\n",
      "Step 526915  [5.433 sec/step, loss=0.07566, avg_loss=0.07409]\n",
      "Step 526916  [5.431 sec/step, loss=0.07571, avg_loss=0.07412]\n",
      "Step 526917  [5.438 sec/step, loss=0.07366, avg_loss=0.07410]\n",
      "Step 526918  [5.453 sec/step, loss=0.07572, avg_loss=0.07412]\n",
      "Step 526919  [5.457 sec/step, loss=0.07349, avg_loss=0.07411]\n",
      "Step 526920  [5.452 sec/step, loss=0.07410, avg_loss=0.07410]\n",
      "Step 526921  [5.462 sec/step, loss=0.07405, avg_loss=0.07411]\n",
      "Step 526922  [5.466 sec/step, loss=0.07512, avg_loss=0.07410]\n",
      "Step 526923  [5.430 sec/step, loss=0.07596, avg_loss=0.07419]\n",
      "Step 526924  [5.405 sec/step, loss=0.06664, avg_loss=0.07409]\n",
      "Step 526925  [5.391 sec/step, loss=0.07528, avg_loss=0.07410]\n",
      "Step 526926  [5.383 sec/step, loss=0.07221, avg_loss=0.07407]\n",
      "Step 526927  [5.400 sec/step, loss=0.07571, avg_loss=0.07417]\n",
      "Step 526928  [5.386 sec/step, loss=0.07085, avg_loss=0.07413]\n",
      "Generated 32 batches of size 32 in 2.640 sec\n",
      "Step 526929  [5.426 sec/step, loss=0.06657, avg_loss=0.07403]\n",
      "Step 526930  [5.436 sec/step, loss=0.07517, avg_loss=0.07404]\n",
      "Step 526931  [5.448 sec/step, loss=0.07487, avg_loss=0.07406]\n",
      "Step 526932  [5.432 sec/step, loss=0.07274, avg_loss=0.07402]\n",
      "Step 526933  [5.420 sec/step, loss=0.07431, avg_loss=0.07400]\n",
      "Step 526934  [5.407 sec/step, loss=0.07444, avg_loss=0.07399]\n",
      "Step 526935  [5.445 sec/step, loss=0.07313, avg_loss=0.07401]\n",
      "Step 526936  [5.446 sec/step, loss=0.07431, avg_loss=0.07401]\n",
      "Step 526937  [5.441 sec/step, loss=0.07345, avg_loss=0.07399]\n",
      "Step 526938  [5.403 sec/step, loss=0.07377, avg_loss=0.07399]\n",
      "Step 526939  [5.419 sec/step, loss=0.07615, avg_loss=0.07402]\n",
      "Step 526940  [5.438 sec/step, loss=0.07383, avg_loss=0.07404]\n",
      "Step 526941  [5.425 sec/step, loss=0.07357, avg_loss=0.07404]\n",
      "Step 526942  [5.404 sec/step, loss=0.07234, avg_loss=0.07400]\n",
      "Step 526943  [5.420 sec/step, loss=0.07379, avg_loss=0.07400]\n",
      "Step 526944  [5.451 sec/step, loss=0.07359, avg_loss=0.07403]\n",
      "Step 526945  [5.445 sec/step, loss=0.07532, avg_loss=0.07402]\n",
      "Step 526946  [5.452 sec/step, loss=0.07631, avg_loss=0.07404]\n",
      "Step 526947  [5.456 sec/step, loss=0.07599, avg_loss=0.07404]\n",
      "Step 526948  [5.470 sec/step, loss=0.07677, avg_loss=0.07406]\n",
      "Step 526949  [5.460 sec/step, loss=0.07348, avg_loss=0.07404]\n",
      "Step 526950  [5.454 sec/step, loss=0.07509, avg_loss=0.07402]\n",
      "Step 526951  [5.417 sec/step, loss=0.07620, avg_loss=0.07412]\n",
      "Step 526952  [5.422 sec/step, loss=0.07550, avg_loss=0.07417]\n",
      "Step 526953  [5.401 sec/step, loss=0.07231, avg_loss=0.07415]\n",
      "Step 526954  [5.388 sec/step, loss=0.07335, avg_loss=0.07413]\n",
      "Step 526955  [5.372 sec/step, loss=0.06535, avg_loss=0.07405]\n",
      "Step 526956  [5.418 sec/step, loss=0.06603, avg_loss=0.07398]\n",
      "Step 526957  [5.422 sec/step, loss=0.07489, avg_loss=0.07398]\n",
      "Step 526958  [5.422 sec/step, loss=0.07433, avg_loss=0.07396]\n",
      "Step 526959  [5.420 sec/step, loss=0.07502, avg_loss=0.07395]\n",
      "Step 526960  [5.393 sec/step, loss=0.07507, avg_loss=0.07395]\n",
      "Generated 32 batches of size 32 in 2.509 sec\n",
      "Step 526961  [5.408 sec/step, loss=0.07361, avg_loss=0.07394]\n",
      "Step 526962  [5.392 sec/step, loss=0.07534, avg_loss=0.07395]\n",
      "Step 526963  [5.405 sec/step, loss=0.07426, avg_loss=0.07398]\n",
      "Step 526964  [5.396 sec/step, loss=0.07447, avg_loss=0.07396]\n",
      "Step 526965  [5.403 sec/step, loss=0.07604, avg_loss=0.07398]\n",
      "Step 526966  [5.416 sec/step, loss=0.07315, avg_loss=0.07395]\n",
      "Step 526967  [5.410 sec/step, loss=0.07543, avg_loss=0.07394]\n",
      "Step 526968  [5.427 sec/step, loss=0.07614, avg_loss=0.07396]\n",
      "Step 526969  [5.428 sec/step, loss=0.07204, avg_loss=0.07396]\n",
      "Step 526970  [5.461 sec/step, loss=0.07331, avg_loss=0.07395]\n",
      "Step 526971  [5.458 sec/step, loss=0.07432, avg_loss=0.07395]\n",
      "Step 526972  [5.502 sec/step, loss=0.06664, avg_loss=0.07389]\n",
      "Step 526973  [5.502 sec/step, loss=0.07487, avg_loss=0.07389]\n",
      "Step 526974  [5.527 sec/step, loss=0.07611, avg_loss=0.07400]\n",
      "Step 526975  [5.522 sec/step, loss=0.07333, avg_loss=0.07397]\n",
      "Step 526976  [5.513 sec/step, loss=0.06980, avg_loss=0.07394]\n",
      "Step 526977  [5.527 sec/step, loss=0.07563, avg_loss=0.07395]\n",
      "Step 526978  [5.533 sec/step, loss=0.07613, avg_loss=0.07394]\n",
      "Step 526979  [5.531 sec/step, loss=0.07596, avg_loss=0.07394]\n",
      "Step 526980  [5.537 sec/step, loss=0.07589, avg_loss=0.07395]\n",
      "Step 526981  [5.521 sec/step, loss=0.07536, avg_loss=0.07394]\n",
      "Step 526982  [5.525 sec/step, loss=0.07536, avg_loss=0.07394]\n",
      "Step 526983  [5.533 sec/step, loss=0.07508, avg_loss=0.07394]\n",
      "Step 526984  [5.512 sec/step, loss=0.06545, avg_loss=0.07385]\n",
      "Step 526985  [5.505 sec/step, loss=0.07414, avg_loss=0.07384]\n",
      "Step 526986  [5.514 sec/step, loss=0.07287, avg_loss=0.07385]\n",
      "Step 526987  [5.521 sec/step, loss=0.07542, avg_loss=0.07386]\n",
      "Step 526988  [5.519 sec/step, loss=0.07190, avg_loss=0.07382]\n",
      "Step 526989  [5.471 sec/step, loss=0.07548, avg_loss=0.07391]\n",
      "Step 526990  [5.468 sec/step, loss=0.07490, avg_loss=0.07393]\n",
      "Step 526991  [5.470 sec/step, loss=0.07218, avg_loss=0.07390]\n",
      "Step 526992  [5.447 sec/step, loss=0.07013, avg_loss=0.07383]\n",
      "Generated 32 batches of size 32 in 2.393 sec\n",
      "Step 526993  [5.431 sec/step, loss=0.07439, avg_loss=0.07381]\n",
      "Step 526994  [5.438 sec/step, loss=0.07417, avg_loss=0.07381]\n",
      "Step 526995  [5.435 sec/step, loss=0.07333, avg_loss=0.07381]\n",
      "Step 526996  [5.428 sec/step, loss=0.07581, avg_loss=0.07380]\n",
      "Step 526997  [5.436 sec/step, loss=0.07578, avg_loss=0.07384]\n",
      "Step 526998  [5.452 sec/step, loss=0.07466, avg_loss=0.07386]\n",
      "Step 526999  [5.468 sec/step, loss=0.07328, avg_loss=0.07384]\n",
      "Step 527000  [5.470 sec/step, loss=0.07457, avg_loss=0.07384]\n",
      "Writing summary at step: 527000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-527000\n",
      "Saving audio and alignment...\n",
      "Input: as moqaay pir kaenaydijan afraadd shaehhroon sae hanggaamii inxilaa bhugatd kur xavaar huuay~______________\n",
      "Step 527001  [5.446 sec/step, loss=0.07124, avg_loss=0.07382]\n",
      "Step 527002  [5.463 sec/step, loss=0.07463, avg_loss=0.07390]\n",
      "Step 527003  [5.458 sec/step, loss=0.07230, avg_loss=0.07388]\n",
      "Step 527004  [5.459 sec/step, loss=0.07479, avg_loss=0.07388]\n",
      "Step 527005  [5.448 sec/step, loss=0.07475, avg_loss=0.07386]\n",
      "Step 527006  [5.452 sec/step, loss=0.07558, avg_loss=0.07386]\n",
      "Step 527007  [5.476 sec/step, loss=0.07445, avg_loss=0.07386]\n",
      "Step 527008  [5.480 sec/step, loss=0.07542, avg_loss=0.07385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 527009  [5.491 sec/step, loss=0.07643, avg_loss=0.07386]\n",
      "Step 527010  [5.470 sec/step, loss=0.07427, avg_loss=0.07384]\n",
      "Step 527011  [5.466 sec/step, loss=0.07507, avg_loss=0.07387]\n",
      "Step 527012  [5.471 sec/step, loss=0.07557, avg_loss=0.07387]\n",
      "Step 527013  [5.507 sec/step, loss=0.06648, avg_loss=0.07378]\n",
      "Step 527014  [5.529 sec/step, loss=0.07469, avg_loss=0.07384]\n",
      "Step 527015  [5.503 sec/step, loss=0.07345, avg_loss=0.07381]\n",
      "Step 527016  [5.507 sec/step, loss=0.07513, avg_loss=0.07381]\n",
      "Step 527017  [5.512 sec/step, loss=0.07522, avg_loss=0.07382]\n",
      "Step 527018  [5.495 sec/step, loss=0.07396, avg_loss=0.07381]\n",
      "Step 527019  [5.511 sec/step, loss=0.07619, avg_loss=0.07383]\n",
      "Step 527020  [5.503 sec/step, loss=0.06730, avg_loss=0.07377]\n",
      "Step 527021  [5.494 sec/step, loss=0.07410, avg_loss=0.07377]\n",
      "Step 527022  [5.489 sec/step, loss=0.07496, avg_loss=0.07376]\n",
      "Generated 32 batches of size 32 in 2.365 sec\n",
      "Step 527023  [5.490 sec/step, loss=0.07610, avg_loss=0.07377]\n",
      "Step 527024  [5.507 sec/step, loss=0.07472, avg_loss=0.07385]\n",
      "Step 527025  [5.489 sec/step, loss=0.07358, avg_loss=0.07383]\n",
      "Step 527026  [5.488 sec/step, loss=0.07465, avg_loss=0.07385]\n",
      "Step 527027  [5.494 sec/step, loss=0.07688, avg_loss=0.07387]\n",
      "Step 527028  [5.510 sec/step, loss=0.07649, avg_loss=0.07392]\n",
      "Step 527029  [5.459 sec/step, loss=0.07553, avg_loss=0.07401]\n",
      "Step 527030  [5.443 sec/step, loss=0.07269, avg_loss=0.07399]\n",
      "Step 527031  [5.455 sec/step, loss=0.07570, avg_loss=0.07399]\n",
      "Step 527032  [5.469 sec/step, loss=0.07718, avg_loss=0.07404]\n",
      "Step 527033  [5.469 sec/step, loss=0.07361, avg_loss=0.07403]\n",
      "Step 527034  [5.480 sec/step, loss=0.07593, avg_loss=0.07405]\n",
      "Step 527035  [5.447 sec/step, loss=0.07278, avg_loss=0.07404]\n",
      "Step 527036  [5.482 sec/step, loss=0.06945, avg_loss=0.07399]\n",
      "Step 527037  [5.500 sec/step, loss=0.07767, avg_loss=0.07404]\n",
      "Step 527038  [5.537 sec/step, loss=0.07463, avg_loss=0.07405]\n",
      "Step 527039  [5.532 sec/step, loss=0.07741, avg_loss=0.07406]\n",
      "Step 527040  [5.535 sec/step, loss=0.07711, avg_loss=0.07409]\n",
      "Step 527041  [5.535 sec/step, loss=0.07502, avg_loss=0.07411]\n",
      "Step 527042  [5.545 sec/step, loss=0.07569, avg_loss=0.07414]\n",
      "Step 527043  [5.537 sec/step, loss=0.07361, avg_loss=0.07414]\n",
      "Step 527044  [5.513 sec/step, loss=0.07611, avg_loss=0.07416]\n",
      "Step 527045  [5.528 sec/step, loss=0.07737, avg_loss=0.07418]\n",
      "Step 527046  [5.500 sec/step, loss=0.06806, avg_loss=0.07410]\n",
      "Step 527047  [5.489 sec/step, loss=0.07483, avg_loss=0.07409]\n",
      "Step 527048  [5.482 sec/step, loss=0.07570, avg_loss=0.07408]\n",
      "Step 527049  [5.482 sec/step, loss=0.07573, avg_loss=0.07410]\n",
      "Step 527050  [5.470 sec/step, loss=0.07466, avg_loss=0.07410]\n",
      "Step 527051  [5.475 sec/step, loss=0.07438, avg_loss=0.07408]\n",
      "Step 527052  [5.468 sec/step, loss=0.07445, avg_loss=0.07407]\n",
      "Step 527053  [5.468 sec/step, loss=0.07265, avg_loss=0.07407]\n",
      "Step 527054  [5.482 sec/step, loss=0.07716, avg_loss=0.07411]\n",
      "Generated 32 batches of size 32 in 2.366 sec\n",
      "Step 527055  [5.514 sec/step, loss=0.07567, avg_loss=0.07421]\n",
      "Step 527056  [5.465 sec/step, loss=0.07622, avg_loss=0.07431]\n",
      "Step 527057  [5.474 sec/step, loss=0.07489, avg_loss=0.07431]\n",
      "Step 527058  [5.451 sec/step, loss=0.07140, avg_loss=0.07428]\n",
      "Step 527059  [5.448 sec/step, loss=0.07357, avg_loss=0.07427]\n",
      "Step 527060  [5.454 sec/step, loss=0.07657, avg_loss=0.07429]\n",
      "Step 527061  [5.455 sec/step, loss=0.07663, avg_loss=0.07432]\n",
      "Step 527062  [5.449 sec/step, loss=0.07624, avg_loss=0.07432]\n",
      "Step 527063  [5.458 sec/step, loss=0.07360, avg_loss=0.07432]\n",
      "Step 527064  [5.449 sec/step, loss=0.06575, avg_loss=0.07423]\n",
      "Step 527065  [5.452 sec/step, loss=0.07623, avg_loss=0.07423]\n",
      "Step 527066  [5.422 sec/step, loss=0.07213, avg_loss=0.07422]\n",
      "Step 527067  [5.409 sec/step, loss=0.07588, avg_loss=0.07423]\n",
      "Step 527068  [5.416 sec/step, loss=0.07642, avg_loss=0.07423]\n",
      "Step 527069  [5.412 sec/step, loss=0.07454, avg_loss=0.07426]\n",
      "Step 527070  [5.410 sec/step, loss=0.07345, avg_loss=0.07426]\n",
      "Step 527071  [5.400 sec/step, loss=0.07624, avg_loss=0.07428]\n",
      "Step 527072  [5.361 sec/step, loss=0.07347, avg_loss=0.07434]\n",
      "Step 527073  [5.360 sec/step, loss=0.07569, avg_loss=0.07435]\n",
      "Step 527074  [5.348 sec/step, loss=0.07326, avg_loss=0.07432]\n",
      "Step 527075  [5.360 sec/step, loss=0.07480, avg_loss=0.07434]\n",
      "Step 527076  [5.380 sec/step, loss=0.07375, avg_loss=0.07438]\n",
      "Step 527077  [5.370 sec/step, loss=0.07412, avg_loss=0.07436]\n",
      "Step 527078  [5.354 sec/step, loss=0.07339, avg_loss=0.07434]\n",
      "Step 527079  [5.358 sec/step, loss=0.07677, avg_loss=0.07434]\n",
      "Step 527080  [5.352 sec/step, loss=0.07649, avg_loss=0.07435]\n",
      "Step 527081  [5.343 sec/step, loss=0.07174, avg_loss=0.07431]\n",
      "Step 527082  [5.338 sec/step, loss=0.07513, avg_loss=0.07431]\n",
      "Step 527083  [5.340 sec/step, loss=0.07478, avg_loss=0.07431]\n",
      "Step 527084  [5.360 sec/step, loss=0.07311, avg_loss=0.07438]\n",
      "Step 527085  [5.368 sec/step, loss=0.07572, avg_loss=0.07440]\n",
      "Step 527086  [5.373 sec/step, loss=0.07593, avg_loss=0.07443]\n",
      "Generated 32 batches of size 32 in 2.586 sec\n",
      "Step 527087  [5.370 sec/step, loss=0.07547, avg_loss=0.07443]\n",
      "Step 527088  [5.367 sec/step, loss=0.07379, avg_loss=0.07445]\n",
      "Step 527089  [5.414 sec/step, loss=0.06618, avg_loss=0.07436]\n",
      "Step 527090  [5.417 sec/step, loss=0.07364, avg_loss=0.07434]\n",
      "Step 527091  [5.423 sec/step, loss=0.07635, avg_loss=0.07439]\n",
      "Step 527092  [5.421 sec/step, loss=0.07143, avg_loss=0.07440]\n",
      "Step 527093  [5.402 sec/step, loss=0.07459, avg_loss=0.07440]\n",
      "Step 527094  [5.413 sec/step, loss=0.07697, avg_loss=0.07443]\n",
      "Step 527095  [5.432 sec/step, loss=0.07614, avg_loss=0.07446]\n",
      "Step 527096  [5.421 sec/step, loss=0.07358, avg_loss=0.07444]\n",
      "Step 527097  [5.429 sec/step, loss=0.07634, avg_loss=0.07444]\n",
      "Step 527098  [5.428 sec/step, loss=0.07409, avg_loss=0.07444]\n",
      "Step 527099  [5.393 sec/step, loss=0.07169, avg_loss=0.07442]\n",
      "Step 527100  [5.394 sec/step, loss=0.07544, avg_loss=0.07443]\n",
      "Writing summary at step: 527100\n",
      "Step 527101  [5.401 sec/step, loss=0.07502, avg_loss=0.07447]\n",
      "Step 527102  [5.401 sec/step, loss=0.07285, avg_loss=0.07445]\n",
      "Step 527103  [5.405 sec/step, loss=0.07363, avg_loss=0.07446]\n",
      "Step 527104  [5.389 sec/step, loss=0.07041, avg_loss=0.07442]\n",
      "Step 527105  [5.405 sec/step, loss=0.07652, avg_loss=0.07444]\n",
      "Step 527106  [5.398 sec/step, loss=0.07137, avg_loss=0.07439]\n",
      "Step 527107  [5.395 sec/step, loss=0.07491, avg_loss=0.07440]\n",
      "Step 527108  [5.383 sec/step, loss=0.07603, avg_loss=0.07440]\n",
      "Step 527109  [5.373 sec/step, loss=0.07491, avg_loss=0.07439]\n",
      "Step 527110  [5.365 sec/step, loss=0.06597, avg_loss=0.07431]\n",
      "Step 527111  [5.360 sec/step, loss=0.07339, avg_loss=0.07429]\n",
      "Step 527112  [5.380 sec/step, loss=0.07374, avg_loss=0.07427]\n",
      "Step 527113  [5.324 sec/step, loss=0.07402, avg_loss=0.07435]\n",
      "Step 527114  [5.318 sec/step, loss=0.07470, avg_loss=0.07435]\n",
      "Step 527115  [5.324 sec/step, loss=0.07513, avg_loss=0.07436]\n",
      "Step 527116  [5.311 sec/step, loss=0.07431, avg_loss=0.07435]\n",
      "Step 527117  [5.299 sec/step, loss=0.07476, avg_loss=0.07435]\n",
      "Generated 32 batches of size 32 in 2.388 sec\n",
      "Step 527118  [5.321 sec/step, loss=0.07603, avg_loss=0.07437]\n",
      "Step 527119  [5.308 sec/step, loss=0.07570, avg_loss=0.07437]\n",
      "Step 527120  [5.332 sec/step, loss=0.07592, avg_loss=0.07445]\n",
      "Step 527121  [5.334 sec/step, loss=0.07503, avg_loss=0.07446]\n",
      "Step 527122  [5.335 sec/step, loss=0.07472, avg_loss=0.07446]\n",
      "Step 527123  [5.370 sec/step, loss=0.06740, avg_loss=0.07437]\n",
      "Step 527124  [5.358 sec/step, loss=0.07401, avg_loss=0.07436]\n",
      "Step 527125  [5.383 sec/step, loss=0.07619, avg_loss=0.07439]\n",
      "Step 527126  [5.396 sec/step, loss=0.07477, avg_loss=0.07439]\n",
      "Step 527127  [5.385 sec/step, loss=0.07560, avg_loss=0.07438]\n",
      "Step 527128  [5.380 sec/step, loss=0.07252, avg_loss=0.07434]\n",
      "Step 527129  [5.377 sec/step, loss=0.07576, avg_loss=0.07434]\n",
      "Step 527130  [5.438 sec/step, loss=0.06582, avg_loss=0.07427]\n",
      "Step 527131  [5.432 sec/step, loss=0.07420, avg_loss=0.07426]\n",
      "Step 527132  [5.405 sec/step, loss=0.06713, avg_loss=0.07416]\n",
      "Step 527133  [5.419 sec/step, loss=0.07583, avg_loss=0.07418]\n",
      "Step 527134  [5.410 sec/step, loss=0.07398, avg_loss=0.07416]\n",
      "Step 527135  [5.426 sec/step, loss=0.07592, avg_loss=0.07419]\n",
      "Step 527136  [5.402 sec/step, loss=0.07411, avg_loss=0.07424]\n",
      "Step 527137  [5.401 sec/step, loss=0.07374, avg_loss=0.07420]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 527138  [5.361 sec/step, loss=0.07238, avg_loss=0.07418]\n",
      "Step 527139  [5.353 sec/step, loss=0.07501, avg_loss=0.07415]\n",
      "Step 527140  [5.345 sec/step, loss=0.07508, avg_loss=0.07413]\n",
      "Step 527141  [5.349 sec/step, loss=0.07484, avg_loss=0.07413]\n",
      "Step 527142  [5.351 sec/step, loss=0.07510, avg_loss=0.07412]\n",
      "Step 527143  [5.365 sec/step, loss=0.07528, avg_loss=0.07414]\n",
      "Step 527144  [5.364 sec/step, loss=0.07342, avg_loss=0.07411]\n",
      "Step 527145  [5.348 sec/step, loss=0.07375, avg_loss=0.07408]\n",
      "Step 527146  [5.354 sec/step, loss=0.07227, avg_loss=0.07412]\n",
      "Step 527147  [5.357 sec/step, loss=0.07404, avg_loss=0.07411]\n",
      "Step 527148  [5.357 sec/step, loss=0.07508, avg_loss=0.07411]\n",
      "Step 527149  [5.371 sec/step, loss=0.07583, avg_loss=0.07411]\n",
      "Generated 32 batches of size 32 in 2.424 sec\n",
      "Step 527150  [5.384 sec/step, loss=0.07494, avg_loss=0.07411]\n",
      "Step 527151  [5.368 sec/step, loss=0.07483, avg_loss=0.07411]\n",
      "Step 527152  [5.381 sec/step, loss=0.07519, avg_loss=0.07412]\n",
      "Step 527153  [5.401 sec/step, loss=0.07219, avg_loss=0.07412]\n",
      "Step 527154  [5.396 sec/step, loss=0.07612, avg_loss=0.07411]\n",
      "Step 527155  [5.380 sec/step, loss=0.07346, avg_loss=0.07408]\n",
      "Step 527156  [5.385 sec/step, loss=0.07575, avg_loss=0.07408]\n",
      "Step 527157  [5.363 sec/step, loss=0.07072, avg_loss=0.07404]\n",
      "Step 527158  [5.380 sec/step, loss=0.07635, avg_loss=0.07409]\n",
      "Step 527159  [5.385 sec/step, loss=0.07541, avg_loss=0.07411]\n",
      "Step 527160  [5.378 sec/step, loss=0.07213, avg_loss=0.07406]\n",
      "Step 527161  [5.384 sec/step, loss=0.07602, avg_loss=0.07406]\n",
      "Step 527162  [5.437 sec/step, loss=0.06670, avg_loss=0.07396]\n",
      "Step 527163  [5.437 sec/step, loss=0.07575, avg_loss=0.07398]\n",
      "Step 527164  [5.437 sec/step, loss=0.06501, avg_loss=0.07397]\n",
      "Step 527165  [5.417 sec/step, loss=0.07412, avg_loss=0.07395]\n",
      "Step 527166  [5.432 sec/step, loss=0.07345, avg_loss=0.07397]\n",
      "Step 527167  [5.458 sec/step, loss=0.07442, avg_loss=0.07395]\n",
      "Step 527168  [5.439 sec/step, loss=0.07388, avg_loss=0.07393]\n",
      "Step 527169  [5.454 sec/step, loss=0.07566, avg_loss=0.07394]\n",
      "Step 527170  [5.456 sec/step, loss=0.07359, avg_loss=0.07394]\n",
      "Step 527171  [5.439 sec/step, loss=0.07190, avg_loss=0.07390]\n",
      "Step 527172  [5.424 sec/step, loss=0.07412, avg_loss=0.07390]\n",
      "Step 527173  [5.435 sec/step, loss=0.07574, avg_loss=0.07390]\n",
      "Step 527174  [5.450 sec/step, loss=0.07632, avg_loss=0.07393]\n",
      "Step 527175  [5.439 sec/step, loss=0.07355, avg_loss=0.07392]\n",
      "Step 527176  [5.435 sec/step, loss=0.07546, avg_loss=0.07394]\n",
      "Step 527177  [5.436 sec/step, loss=0.07316, avg_loss=0.07393]\n",
      "Step 527178  [5.440 sec/step, loss=0.07502, avg_loss=0.07394]\n",
      "Step 527179  [5.427 sec/step, loss=0.07355, avg_loss=0.07391]\n",
      "Step 527180  [5.430 sec/step, loss=0.07619, avg_loss=0.07391]\n",
      "Step 527181  [5.433 sec/step, loss=0.07407, avg_loss=0.07393]\n",
      "Generated 32 batches of size 32 in 2.471 sec\n",
      "Step 527182  [5.423 sec/step, loss=0.07227, avg_loss=0.07390]\n",
      "Step 527183  [5.423 sec/step, loss=0.07537, avg_loss=0.07391]\n",
      "Step 527184  [5.423 sec/step, loss=0.07517, avg_loss=0.07393]\n",
      "Step 527185  [5.425 sec/step, loss=0.07503, avg_loss=0.07392]\n",
      "Step 527186  [5.435 sec/step, loss=0.07444, avg_loss=0.07391]\n",
      "Step 527187  [5.431 sec/step, loss=0.07412, avg_loss=0.07390]\n",
      "Step 527188  [5.432 sec/step, loss=0.07505, avg_loss=0.07391]\n",
      "Step 527189  [5.371 sec/step, loss=0.07395, avg_loss=0.07399]\n",
      "Step 527190  [5.386 sec/step, loss=0.07693, avg_loss=0.07402]\n",
      "Step 527191  [5.401 sec/step, loss=0.07481, avg_loss=0.07400]\n",
      "Step 527192  [5.423 sec/step, loss=0.07631, avg_loss=0.07405]\n",
      "Step 527193  [5.429 sec/step, loss=0.07508, avg_loss=0.07406]\n",
      "Step 527194  [5.468 sec/step, loss=0.06590, avg_loss=0.07395]\n",
      "Step 527195  [5.467 sec/step, loss=0.07662, avg_loss=0.07395]\n",
      "Step 527196  [5.474 sec/step, loss=0.07541, avg_loss=0.07397]\n",
      "Step 527197  [5.467 sec/step, loss=0.07570, avg_loss=0.07396]\n",
      "Step 527198  [5.474 sec/step, loss=0.07551, avg_loss=0.07398]\n",
      "Step 527199  [5.484 sec/step, loss=0.07051, avg_loss=0.07397]\n",
      "Step 527200  [5.476 sec/step, loss=0.07392, avg_loss=0.07395]\n",
      "Writing summary at step: 527200\n",
      "Step 527201  [5.472 sec/step, loss=0.07250, avg_loss=0.07393]\n",
      "Step 527202  [5.481 sec/step, loss=0.07322, avg_loss=0.07393]\n",
      "Step 527203  [5.484 sec/step, loss=0.07483, avg_loss=0.07394]\n",
      "Step 527204  [5.495 sec/step, loss=0.07468, avg_loss=0.07398]\n",
      "Step 527205  [5.483 sec/step, loss=0.07559, avg_loss=0.07397]\n",
      "Step 527206  [5.488 sec/step, loss=0.07440, avg_loss=0.07400]\n",
      "Step 527207  [5.463 sec/step, loss=0.07201, avg_loss=0.07398]\n",
      "Step 527208  [5.451 sec/step, loss=0.07072, avg_loss=0.07392]\n",
      "Step 527209  [5.461 sec/step, loss=0.07443, avg_loss=0.07392]\n",
      "Step 527210  [5.485 sec/step, loss=0.07322, avg_loss=0.07399]\n",
      "Step 527211  [5.507 sec/step, loss=0.07651, avg_loss=0.07402]\n",
      "Step 527212  [5.470 sec/step, loss=0.07436, avg_loss=0.07403]\n",
      "Generated 32 batches of size 32 in 2.479 sec\n",
      "Step 527213  [5.483 sec/step, loss=0.07258, avg_loss=0.07401]\n",
      "Step 527214  [5.477 sec/step, loss=0.07345, avg_loss=0.07400]\n",
      "Step 527215  [5.464 sec/step, loss=0.07131, avg_loss=0.07396]\n",
      "Step 527216  [5.466 sec/step, loss=0.07302, avg_loss=0.07395]\n",
      "Step 527217  [5.475 sec/step, loss=0.07378, avg_loss=0.07394]\n",
      "Step 527218  [5.463 sec/step, loss=0.07482, avg_loss=0.07393]\n",
      "Step 527219  [5.451 sec/step, loss=0.06817, avg_loss=0.07385]\n",
      "Step 527220  [5.455 sec/step, loss=0.07612, avg_loss=0.07385]\n",
      "Step 527221  [5.458 sec/step, loss=0.07523, avg_loss=0.07386]\n",
      "Step 527222  [5.448 sec/step, loss=0.07486, avg_loss=0.07386]\n",
      "Step 527223  [5.402 sec/step, loss=0.07463, avg_loss=0.07393]\n",
      "Step 527224  [5.429 sec/step, loss=0.07524, avg_loss=0.07394]\n",
      "Step 527225  [5.432 sec/step, loss=0.07455, avg_loss=0.07393]\n",
      "Step 527226  [5.418 sec/step, loss=0.07590, avg_loss=0.07394]\n",
      "Step 527227  [5.420 sec/step, loss=0.07468, avg_loss=0.07393]\n",
      "Step 527228  [5.428 sec/step, loss=0.07528, avg_loss=0.07396]\n",
      "Step 527229  [5.452 sec/step, loss=0.07250, avg_loss=0.07392]\n",
      "Step 527230  [5.396 sec/step, loss=0.07360, avg_loss=0.07400]\n",
      "Step 527231  [5.435 sec/step, loss=0.06651, avg_loss=0.07392]\n",
      "Step 527232  [5.460 sec/step, loss=0.07635, avg_loss=0.07402]\n",
      "Step 527233  [5.458 sec/step, loss=0.07553, avg_loss=0.07401]\n",
      "Step 527234  [5.475 sec/step, loss=0.07482, avg_loss=0.07402]\n",
      "Step 527235  [5.457 sec/step, loss=0.07427, avg_loss=0.07401]\n",
      "Step 527236  [5.428 sec/step, loss=0.07381, avg_loss=0.07400]\n",
      "Step 527237  [5.400 sec/step, loss=0.07139, avg_loss=0.07398]\n",
      "Step 527238  [5.422 sec/step, loss=0.07634, avg_loss=0.07402]\n",
      "Step 527239  [5.406 sec/step, loss=0.06603, avg_loss=0.07393]\n",
      "Step 527240  [5.396 sec/step, loss=0.07067, avg_loss=0.07388]\n",
      "Step 527241  [5.392 sec/step, loss=0.07356, avg_loss=0.07387]\n",
      "Step 527242  [5.392 sec/step, loss=0.07577, avg_loss=0.07388]\n",
      "Step 527243  [5.389 sec/step, loss=0.07485, avg_loss=0.07387]\n",
      "Step 527244  [5.395 sec/step, loss=0.07543, avg_loss=0.07389]\n",
      "Generated 32 batches of size 32 in 2.555 sec\n",
      "Step 527245  [5.401 sec/step, loss=0.07259, avg_loss=0.07388]\n",
      "Step 527246  [5.422 sec/step, loss=0.07471, avg_loss=0.07391]\n",
      "Step 527247  [5.429 sec/step, loss=0.07419, avg_loss=0.07391]\n",
      "Step 527248  [5.427 sec/step, loss=0.07534, avg_loss=0.07391]\n",
      "Step 527249  [5.422 sec/step, loss=0.07633, avg_loss=0.07392]\n",
      "Step 527250  [5.427 sec/step, loss=0.07675, avg_loss=0.07393]\n",
      "Step 527251  [5.424 sec/step, loss=0.07225, avg_loss=0.07391]\n",
      "Step 527252  [5.406 sec/step, loss=0.07172, avg_loss=0.07387]\n",
      "Step 527253  [5.404 sec/step, loss=0.07539, avg_loss=0.07391]\n",
      "Step 527254  [5.422 sec/step, loss=0.07472, avg_loss=0.07389]\n",
      "Step 527255  [5.416 sec/step, loss=0.07443, avg_loss=0.07390]\n",
      "Step 527256  [5.415 sec/step, loss=0.07294, avg_loss=0.07387]\n",
      "Step 527257  [5.433 sec/step, loss=0.07640, avg_loss=0.07393]\n",
      "Step 527258  [5.426 sec/step, loss=0.07148, avg_loss=0.07388]\n",
      "Step 527259  [5.442 sec/step, loss=0.07480, avg_loss=0.07388]\n",
      "Step 527260  [5.458 sec/step, loss=0.07629, avg_loss=0.07392]\n",
      "Step 527261  [5.454 sec/step, loss=0.07554, avg_loss=0.07391]\n",
      "Step 527262  [5.400 sec/step, loss=0.07059, avg_loss=0.07395]\n",
      "Step 527263  [5.381 sec/step, loss=0.07429, avg_loss=0.07394]\n",
      "Step 527264  [5.393 sec/step, loss=0.07411, avg_loss=0.07403]\n",
      "Step 527265  [5.397 sec/step, loss=0.07506, avg_loss=0.07404]\n",
      "Step 527266  [5.385 sec/step, loss=0.07433, avg_loss=0.07405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 527267  [5.375 sec/step, loss=0.07441, avg_loss=0.07405]\n",
      "Step 527268  [5.375 sec/step, loss=0.07515, avg_loss=0.07406]\n",
      "Step 527269  [5.357 sec/step, loss=0.07055, avg_loss=0.07401]\n",
      "Step 527270  [5.341 sec/step, loss=0.07678, avg_loss=0.07404]\n",
      "Step 527271  [5.339 sec/step, loss=0.06546, avg_loss=0.07397]\n",
      "Step 527272  [5.336 sec/step, loss=0.07272, avg_loss=0.07396]\n",
      "Step 527273  [5.320 sec/step, loss=0.07404, avg_loss=0.07394]\n",
      "Step 527274  [5.308 sec/step, loss=0.07554, avg_loss=0.07394]\n",
      "Step 527275  [5.315 sec/step, loss=0.07486, avg_loss=0.07395]\n",
      "Step 527276  [5.315 sec/step, loss=0.07597, avg_loss=0.07395]\n",
      "Generated 32 batches of size 32 in 2.389 sec\n",
      "Step 527277  [5.328 sec/step, loss=0.07466, avg_loss=0.07397]\n",
      "Step 527278  [5.310 sec/step, loss=0.07114, avg_loss=0.07393]\n",
      "Step 527279  [5.360 sec/step, loss=0.06736, avg_loss=0.07387]\n",
      "Step 527280  [5.347 sec/step, loss=0.07295, avg_loss=0.07384]\n",
      "Step 527281  [5.364 sec/step, loss=0.07623, avg_loss=0.07386]\n",
      "Step 527282  [5.396 sec/step, loss=0.07375, avg_loss=0.07387]\n",
      "Step 527283  [5.400 sec/step, loss=0.07536, avg_loss=0.07387]\n",
      "Step 527284  [5.403 sec/step, loss=0.07641, avg_loss=0.07388]\n",
      "Step 527285  [5.403 sec/step, loss=0.07209, avg_loss=0.07386]\n",
      "Step 527286  [5.399 sec/step, loss=0.07604, avg_loss=0.07387]\n",
      "Step 527287  [5.405 sec/step, loss=0.07469, avg_loss=0.07388]\n",
      "Step 527288  [5.420 sec/step, loss=0.07364, avg_loss=0.07386]\n",
      "Step 527289  [5.428 sec/step, loss=0.07389, avg_loss=0.07386]\n",
      "Step 527290  [5.421 sec/step, loss=0.07470, avg_loss=0.07384]\n",
      "Step 527291  [5.412 sec/step, loss=0.07599, avg_loss=0.07385]\n",
      "Step 527292  [5.412 sec/step, loss=0.07522, avg_loss=0.07384]\n",
      "Step 527293  [5.415 sec/step, loss=0.07487, avg_loss=0.07384]\n",
      "Step 527294  [5.361 sec/step, loss=0.07358, avg_loss=0.07392]\n",
      "Step 527295  [5.336 sec/step, loss=0.07170, avg_loss=0.07387]\n",
      "Step 527296  [5.334 sec/step, loss=0.07250, avg_loss=0.07384]\n",
      "Step 527297  [5.345 sec/step, loss=0.07535, avg_loss=0.07383]\n",
      "Step 527298  [5.379 sec/step, loss=0.06520, avg_loss=0.07373]\n",
      "Step 527299  [5.379 sec/step, loss=0.07401, avg_loss=0.07377]\n",
      "Step 527300  [5.375 sec/step, loss=0.07408, avg_loss=0.07377]\n",
      "Writing summary at step: 527300\n",
      "Step 527301  [5.374 sec/step, loss=0.07391, avg_loss=0.07378]\n",
      "Step 527302  [5.353 sec/step, loss=0.07125, avg_loss=0.07376]\n",
      "Step 527303  [5.363 sec/step, loss=0.07613, avg_loss=0.07377]\n",
      "Step 527304  [5.363 sec/step, loss=0.07561, avg_loss=0.07378]\n",
      "Step 527305  [5.371 sec/step, loss=0.07403, avg_loss=0.07377]\n",
      "Step 527306  [5.368 sec/step, loss=0.07499, avg_loss=0.07377]\n",
      "Step 527307  [5.385 sec/step, loss=0.07586, avg_loss=0.07381]\n",
      "Generated 32 batches of size 32 in 2.356 sec\n",
      "Step 527308  [5.403 sec/step, loss=0.07413, avg_loss=0.07385]\n",
      "Step 527309  [5.399 sec/step, loss=0.07530, avg_loss=0.07386]\n",
      "Step 527310  [5.404 sec/step, loss=0.07503, avg_loss=0.07387]\n",
      "Step 527311  [5.383 sec/step, loss=0.07436, avg_loss=0.07385]\n",
      "Step 527312  [5.401 sec/step, loss=0.07615, avg_loss=0.07387]\n",
      "Step 527313  [5.390 sec/step, loss=0.07158, avg_loss=0.07386]\n",
      "Step 527314  [5.388 sec/step, loss=0.06996, avg_loss=0.07383]\n",
      "Step 527315  [5.403 sec/step, loss=0.07584, avg_loss=0.07387]\n",
      "Step 527316  [5.433 sec/step, loss=0.07485, avg_loss=0.07389]\n",
      "Step 527317  [5.433 sec/step, loss=0.07536, avg_loss=0.07390]\n",
      "Step 527318  [5.431 sec/step, loss=0.07516, avg_loss=0.07391]\n",
      "Step 527319  [5.451 sec/step, loss=0.07477, avg_loss=0.07397]\n",
      "Step 527320  [5.433 sec/step, loss=0.07308, avg_loss=0.07394]\n",
      "Step 527321  [5.479 sec/step, loss=0.06578, avg_loss=0.07385]\n",
      "Step 527322  [5.492 sec/step, loss=0.07610, avg_loss=0.07386]\n",
      "Step 527323  [5.490 sec/step, loss=0.07530, avg_loss=0.07387]\n",
      "Step 527324  [5.500 sec/step, loss=0.07300, avg_loss=0.07385]\n",
      "Step 527325  [5.492 sec/step, loss=0.07394, avg_loss=0.07384]\n",
      "Step 527326  [5.492 sec/step, loss=0.07371, avg_loss=0.07382]\n",
      "Step 527327  [5.484 sec/step, loss=0.07397, avg_loss=0.07381]\n",
      "Step 527328  [5.464 sec/step, loss=0.06713, avg_loss=0.07373]\n",
      "Step 527329  [5.441 sec/step, loss=0.07539, avg_loss=0.07376]\n",
      "Step 527330  [5.445 sec/step, loss=0.07332, avg_loss=0.07375]\n",
      "Step 527331  [5.405 sec/step, loss=0.07591, avg_loss=0.07385]\n",
      "Step 527332  [5.391 sec/step, loss=0.07412, avg_loss=0.07383]\n",
      "Step 527333  [5.388 sec/step, loss=0.07652, avg_loss=0.07384]\n",
      "Step 527334  [5.365 sec/step, loss=0.07181, avg_loss=0.07381]\n",
      "Step 527335  [5.389 sec/step, loss=0.07614, avg_loss=0.07383]\n",
      "Step 527336  [5.406 sec/step, loss=0.07606, avg_loss=0.07385]\n",
      "Step 527337  [5.421 sec/step, loss=0.07165, avg_loss=0.07385]\n",
      "Step 527338  [5.406 sec/step, loss=0.07333, avg_loss=0.07382]\n",
      "Step 527339  [5.411 sec/step, loss=0.07108, avg_loss=0.07387]\n",
      "Generated 32 batches of size 32 in 2.617 sec\n",
      "Step 527340  [5.423 sec/step, loss=0.07231, avg_loss=0.07389]\n",
      "Step 527341  [5.437 sec/step, loss=0.07548, avg_loss=0.07391]\n",
      "Step 527342  [5.440 sec/step, loss=0.07486, avg_loss=0.07390]\n",
      "Step 527343  [5.431 sec/step, loss=0.07528, avg_loss=0.07390]\n",
      "Step 527344  [5.442 sec/step, loss=0.07348, avg_loss=0.07388]\n",
      "Step 527345  [5.448 sec/step, loss=0.07347, avg_loss=0.07389]\n",
      "Step 527346  [5.435 sec/step, loss=0.07493, avg_loss=0.07389]\n",
      "Step 527347  [5.433 sec/step, loss=0.07509, avg_loss=0.07390]\n",
      "Step 527348  [5.430 sec/step, loss=0.07516, avg_loss=0.07390]\n",
      "Step 527349  [5.432 sec/step, loss=0.07582, avg_loss=0.07390]\n",
      "Step 527350  [5.414 sec/step, loss=0.07389, avg_loss=0.07387]\n",
      "Step 527351  [5.417 sec/step, loss=0.07443, avg_loss=0.07389]\n",
      "Step 527352  [5.476 sec/step, loss=0.06742, avg_loss=0.07385]\n",
      "Step 527353  [5.483 sec/step, loss=0.07560, avg_loss=0.07385]\n",
      "Step 527354  [5.456 sec/step, loss=0.07368, avg_loss=0.07384]\n",
      "Step 527355  [5.468 sec/step, loss=0.07546, avg_loss=0.07385]\n",
      "Step 527356  [5.461 sec/step, loss=0.07447, avg_loss=0.07386]\n",
      "Step 527357  [5.467 sec/step, loss=0.07549, avg_loss=0.07385]\n",
      "Step 527358  [5.474 sec/step, loss=0.07594, avg_loss=0.07390]\n",
      "Step 527359  [5.447 sec/step, loss=0.07191, avg_loss=0.07387]\n",
      "Step 527360  [5.443 sec/step, loss=0.07583, avg_loss=0.07386]\n",
      "Step 527361  [5.427 sec/step, loss=0.07351, avg_loss=0.07384]\n",
      "Step 527362  [5.428 sec/step, loss=0.07490, avg_loss=0.07389]\n",
      "Step 527363  [5.422 sec/step, loss=0.06569, avg_loss=0.07380]\n",
      "Step 527364  [5.435 sec/step, loss=0.07621, avg_loss=0.07382]\n",
      "Step 527365  [5.434 sec/step, loss=0.07530, avg_loss=0.07382]\n",
      "Step 527366  [5.436 sec/step, loss=0.07493, avg_loss=0.07383]\n",
      "Step 527367  [5.410 sec/step, loss=0.07162, avg_loss=0.07380]\n",
      "Step 527368  [5.407 sec/step, loss=0.07169, avg_loss=0.07377]\n",
      "Step 527369  [5.434 sec/step, loss=0.07558, avg_loss=0.07382]\n",
      "Step 527370  [5.449 sec/step, loss=0.07343, avg_loss=0.07378]\n",
      "Step 527371  [5.464 sec/step, loss=0.07450, avg_loss=0.07388]\n",
      "Generated 32 batches of size 32 in 2.343 sec\n",
      "Step 527372  [5.478 sec/step, loss=0.07550, avg_loss=0.07390]\n",
      "Step 527373  [5.489 sec/step, loss=0.07472, avg_loss=0.07391]\n",
      "Step 527374  [5.486 sec/step, loss=0.07271, avg_loss=0.07388]\n",
      "Step 527375  [5.493 sec/step, loss=0.07426, avg_loss=0.07388]\n",
      "Step 527376  [5.507 sec/step, loss=0.07543, avg_loss=0.07387]\n",
      "Step 527377  [5.493 sec/step, loss=0.07508, avg_loss=0.07387]\n",
      "Step 527378  [5.497 sec/step, loss=0.07422, avg_loss=0.07391]\n",
      "Step 527379  [5.459 sec/step, loss=0.07658, avg_loss=0.07400]\n",
      "Step 527380  [5.459 sec/step, loss=0.07344, avg_loss=0.07400]\n",
      "Step 527381  [5.464 sec/step, loss=0.07498, avg_loss=0.07399]\n",
      "Step 527382  [5.426 sec/step, loss=0.07112, avg_loss=0.07396]\n",
      "Step 527383  [5.427 sec/step, loss=0.07426, avg_loss=0.07395]\n",
      "Step 527384  [5.427 sec/step, loss=0.07639, avg_loss=0.07395]\n",
      "Step 527385  [5.419 sec/step, loss=0.07399, avg_loss=0.07397]\n",
      "Step 527386  [5.397 sec/step, loss=0.06493, avg_loss=0.07386]\n",
      "Step 527387  [5.400 sec/step, loss=0.07486, avg_loss=0.07386]\n",
      "Step 527388  [5.397 sec/step, loss=0.07624, avg_loss=0.07389]\n",
      "Step 527389  [5.424 sec/step, loss=0.07494, avg_loss=0.07390]\n",
      "Step 527390  [5.424 sec/step, loss=0.07196, avg_loss=0.07387]\n",
      "Step 527391  [5.405 sec/step, loss=0.07580, avg_loss=0.07387]\n",
      "Step 527392  [5.398 sec/step, loss=0.07536, avg_loss=0.07387]\n",
      "Step 527393  [5.400 sec/step, loss=0.07488, avg_loss=0.07387]\n",
      "Step 527394  [5.397 sec/step, loss=0.07393, avg_loss=0.07387]\n",
      "Step 527395  [5.410 sec/step, loss=0.07329, avg_loss=0.07389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 527396  [5.420 sec/step, loss=0.07373, avg_loss=0.07390]\n",
      "Step 527397  [5.412 sec/step, loss=0.07669, avg_loss=0.07392]\n",
      "Step 527398  [5.366 sec/step, loss=0.07541, avg_loss=0.07402]\n",
      "Step 527399  [5.376 sec/step, loss=0.07527, avg_loss=0.07403]\n",
      "Step 527400  [5.382 sec/step, loss=0.07201, avg_loss=0.07401]\n",
      "Writing summary at step: 527400\n",
      "Step 527401  [5.373 sec/step, loss=0.07405, avg_loss=0.07401]\n",
      "Step 527402  [5.380 sec/step, loss=0.07474, avg_loss=0.07405]\n",
      "Generated 32 batches of size 32 in 2.413 sec\n",
      "Step 527403  [5.372 sec/step, loss=0.07468, avg_loss=0.07403]\n",
      "Step 527404  [5.388 sec/step, loss=0.07515, avg_loss=0.07403]\n",
      "Step 527405  [5.376 sec/step, loss=0.07134, avg_loss=0.07400]\n",
      "Step 527406  [5.392 sec/step, loss=0.07591, avg_loss=0.07401]\n",
      "Step 527407  [5.430 sec/step, loss=0.06612, avg_loss=0.07391]\n",
      "Step 527408  [5.408 sec/step, loss=0.07197, avg_loss=0.07389]\n",
      "Step 527409  [5.397 sec/step, loss=0.07366, avg_loss=0.07387]\n",
      "Step 527410  [5.387 sec/step, loss=0.07578, avg_loss=0.07388]\n",
      "Step 527411  [5.397 sec/step, loss=0.07488, avg_loss=0.07389]\n",
      "Step 527412  [5.394 sec/step, loss=0.07480, avg_loss=0.07387]\n",
      "Step 527413  [5.385 sec/step, loss=0.07110, avg_loss=0.07387]\n",
      "Step 527414  [5.387 sec/step, loss=0.07529, avg_loss=0.07392]\n",
      "Step 527415  [5.386 sec/step, loss=0.07602, avg_loss=0.07392]\n",
      "Step 527416  [5.366 sec/step, loss=0.07374, avg_loss=0.07391]\n",
      "Step 527417  [5.349 sec/step, loss=0.07492, avg_loss=0.07391]\n",
      "Step 527418  [5.356 sec/step, loss=0.07466, avg_loss=0.07390]\n",
      "Step 527419  [5.402 sec/step, loss=0.06629, avg_loss=0.07382]\n",
      "Step 527420  [5.420 sec/step, loss=0.07397, avg_loss=0.07383]\n",
      "Step 527421  [5.365 sec/step, loss=0.07292, avg_loss=0.07390]\n",
      "Step 527422  [5.356 sec/step, loss=0.07413, avg_loss=0.07388]\n",
      "Step 527423  [5.337 sec/step, loss=0.06660, avg_loss=0.07379]\n",
      "Step 527424  [5.310 sec/step, loss=0.07526, avg_loss=0.07381]\n",
      "Step 527425  [5.296 sec/step, loss=0.07420, avg_loss=0.07382]\n",
      "Step 527426  [5.322 sec/step, loss=0.07363, avg_loss=0.07382]\n",
      "Step 527427  [5.332 sec/step, loss=0.07462, avg_loss=0.07382]\n",
      "Step 527428  [5.372 sec/step, loss=0.07378, avg_loss=0.07389]\n",
      "Step 527429  [5.382 sec/step, loss=0.07593, avg_loss=0.07389]\n",
      "Step 527430  [5.396 sec/step, loss=0.07620, avg_loss=0.07392]\n",
      "Step 527431  [5.397 sec/step, loss=0.07320, avg_loss=0.07390]\n",
      "Step 527432  [5.403 sec/step, loss=0.07400, avg_loss=0.07389]\n",
      "Step 527433  [5.380 sec/step, loss=0.07174, avg_loss=0.07385]\n",
      "Step 527434  [5.407 sec/step, loss=0.07661, avg_loss=0.07390]\n",
      "Generated 32 batches of size 32 in 2.377 sec\n",
      "Step 527435  [5.399 sec/step, loss=0.07548, avg_loss=0.07389]\n",
      "Step 527436  [5.383 sec/step, loss=0.07383, avg_loss=0.07387]\n",
      "Step 527437  [5.390 sec/step, loss=0.07659, avg_loss=0.07392]\n",
      "Step 527438  [5.400 sec/step, loss=0.07490, avg_loss=0.07393]\n",
      "Step 527439  [5.421 sec/step, loss=0.07567, avg_loss=0.07398]\n",
      "Step 527440  [5.416 sec/step, loss=0.07228, avg_loss=0.07398]\n",
      "Step 527441  [5.399 sec/step, loss=0.07398, avg_loss=0.07396]\n",
      "Step 527442  [5.388 sec/step, loss=0.07371, avg_loss=0.07395]\n",
      "Step 527443  [5.392 sec/step, loss=0.07629, avg_loss=0.07396]\n",
      "Step 527444  [5.375 sec/step, loss=0.07295, avg_loss=0.07396]\n",
      "Step 527445  [5.371 sec/step, loss=0.07252, avg_loss=0.07395]\n",
      "Step 527446  [5.369 sec/step, loss=0.07370, avg_loss=0.07393]\n",
      "Step 527447  [5.365 sec/step, loss=0.07406, avg_loss=0.07392]\n",
      "Step 527448  [5.357 sec/step, loss=0.07009, avg_loss=0.07387]\n",
      "Step 527449  [5.346 sec/step, loss=0.07458, avg_loss=0.07386]\n",
      "Step 527450  [5.337 sec/step, loss=0.06745, avg_loss=0.07380]\n",
      "Step 527451  [5.330 sec/step, loss=0.07157, avg_loss=0.07377]\n",
      "Step 527452  [5.291 sec/step, loss=0.07621, avg_loss=0.07385]\n",
      "Step 527453  [5.276 sec/step, loss=0.07513, avg_loss=0.07385]\n",
      "Step 527454  [5.283 sec/step, loss=0.07553, avg_loss=0.07387]\n",
      "Step 527455  [5.289 sec/step, loss=0.07552, avg_loss=0.07387]\n",
      "Step 527456  [5.278 sec/step, loss=0.07185, avg_loss=0.07384]\n",
      "Step 527457  [5.290 sec/step, loss=0.07300, avg_loss=0.07382]\n",
      "Step 527458  [5.332 sec/step, loss=0.06672, avg_loss=0.07373]\n",
      "Step 527459  [5.353 sec/step, loss=0.07451, avg_loss=0.07375]\n",
      "Step 527460  [5.345 sec/step, loss=0.07634, avg_loss=0.07376]\n",
      "Step 527461  [5.369 sec/step, loss=0.07582, avg_loss=0.07378]\n",
      "Step 527462  [5.379 sec/step, loss=0.07556, avg_loss=0.07379]\n",
      "Step 527463  [5.402 sec/step, loss=0.07593, avg_loss=0.07389]\n",
      "Step 527464  [5.389 sec/step, loss=0.07353, avg_loss=0.07386]\n",
      "Step 527465  [5.386 sec/step, loss=0.07391, avg_loss=0.07385]\n",
      "Step 527466  [5.385 sec/step, loss=0.07506, avg_loss=0.07385]\n",
      "Generated 32 batches of size 32 in 2.365 sec\n",
      "Step 527467  [5.416 sec/step, loss=0.07472, avg_loss=0.07388]\n",
      "Step 527468  [5.421 sec/step, loss=0.07214, avg_loss=0.07389]\n",
      "Step 527469  [5.418 sec/step, loss=0.07653, avg_loss=0.07389]\n",
      "Step 527470  [5.391 sec/step, loss=0.07441, avg_loss=0.07390]\n",
      "Step 527471  [5.397 sec/step, loss=0.07523, avg_loss=0.07391]\n",
      "Step 527472  [5.404 sec/step, loss=0.07618, avg_loss=0.07392]\n",
      "Step 527473  [5.403 sec/step, loss=0.07432, avg_loss=0.07391]\n",
      "Step 527474  [5.392 sec/step, loss=0.07189, avg_loss=0.07391]\n",
      "Step 527475  [5.390 sec/step, loss=0.07628, avg_loss=0.07393]\n",
      "Step 527476  [5.376 sec/step, loss=0.07270, avg_loss=0.07390]\n",
      "Step 527477  [5.423 sec/step, loss=0.06610, avg_loss=0.07381]\n",
      "Step 527478  [5.429 sec/step, loss=0.07211, avg_loss=0.07379]\n",
      "Step 527479  [5.421 sec/step, loss=0.07453, avg_loss=0.07377]\n",
      "Step 527480  [5.426 sec/step, loss=0.07401, avg_loss=0.07377]\n",
      "Step 527481  [5.422 sec/step, loss=0.07351, avg_loss=0.07376]\n",
      "Step 527482  [5.426 sec/step, loss=0.07282, avg_loss=0.07378]\n",
      "Step 527483  [5.419 sec/step, loss=0.07498, avg_loss=0.07378]\n",
      "Step 527484  [5.401 sec/step, loss=0.07385, avg_loss=0.07376]\n",
      "Step 527485  [5.420 sec/step, loss=0.07432, avg_loss=0.07376]\n",
      "Step 527486  [5.435 sec/step, loss=0.07477, avg_loss=0.07386]\n",
      "Step 527487  [5.428 sec/step, loss=0.07528, avg_loss=0.07386]\n",
      "Step 527488  [5.439 sec/step, loss=0.07295, avg_loss=0.07383]\n",
      "Step 527489  [5.400 sec/step, loss=0.07071, avg_loss=0.07379]\n",
      "Step 527490  [5.411 sec/step, loss=0.07622, avg_loss=0.07383]\n",
      "Step 527491  [5.426 sec/step, loss=0.07639, avg_loss=0.07384]\n",
      "Step 527492  [5.428 sec/step, loss=0.07419, avg_loss=0.07383]\n",
      "Step 527493  [5.429 sec/step, loss=0.07414, avg_loss=0.07382]\n",
      "Step 527494  [5.420 sec/step, loss=0.06439, avg_loss=0.07372]\n",
      "Step 527495  [5.427 sec/step, loss=0.07610, avg_loss=0.07375]\n",
      "Step 527496  [5.416 sec/step, loss=0.07495, avg_loss=0.07376]\n",
      "Step 527497  [5.433 sec/step, loss=0.07320, avg_loss=0.07373]\n",
      "Step 527498  [5.421 sec/step, loss=0.07386, avg_loss=0.07371]\n",
      "Generated 32 batches of size 32 in 2.407 sec\n",
      "Step 527499  [5.430 sec/step, loss=0.07367, avg_loss=0.07370]\n",
      "Step 527500  [5.442 sec/step, loss=0.07662, avg_loss=0.07374]\n",
      "Writing summary at step: 527500\n",
      "Step 527501  [5.450 sec/step, loss=0.07380, avg_loss=0.07374]\n",
      "Step 527502  [5.461 sec/step, loss=0.07433, avg_loss=0.07374]\n",
      "Step 527503  [5.458 sec/step, loss=0.07542, avg_loss=0.07374]\n",
      "Step 527504  [5.437 sec/step, loss=0.07332, avg_loss=0.07372]\n",
      "Step 527505  [5.438 sec/step, loss=0.07469, avg_loss=0.07376]\n",
      "Step 527506  [5.438 sec/step, loss=0.07525, avg_loss=0.07375]\n",
      "Step 527507  [5.396 sec/step, loss=0.07431, avg_loss=0.07383]\n",
      "Step 527508  [5.410 sec/step, loss=0.07456, avg_loss=0.07386]\n",
      "Step 527509  [5.415 sec/step, loss=0.07526, avg_loss=0.07388]\n",
      "Step 527510  [5.410 sec/step, loss=0.07329, avg_loss=0.07385]\n",
      "Step 527511  [5.406 sec/step, loss=0.07410, avg_loss=0.07384]\n",
      "Step 527512  [5.417 sec/step, loss=0.07617, avg_loss=0.07386]\n",
      "Step 527513  [5.435 sec/step, loss=0.07676, avg_loss=0.07391]\n",
      "Step 527514  [5.436 sec/step, loss=0.07284, avg_loss=0.07389]\n",
      "Step 527515  [5.461 sec/step, loss=0.07508, avg_loss=0.07388]\n",
      "Step 527516  [5.460 sec/step, loss=0.07500, avg_loss=0.07389]\n",
      "Step 527517  [5.447 sec/step, loss=0.06704, avg_loss=0.07381]\n",
      "Step 527518  [5.430 sec/step, loss=0.07411, avg_loss=0.07381]\n",
      "Step 527519  [5.377 sec/step, loss=0.07490, avg_loss=0.07389]\n",
      "Step 527520  [5.373 sec/step, loss=0.07617, avg_loss=0.07392]\n",
      "Step 527521  [5.363 sec/step, loss=0.07179, avg_loss=0.07390]\n",
      "Step 527522  [5.412 sec/step, loss=0.06520, avg_loss=0.07381]\n",
      "Step 527523  [5.423 sec/step, loss=0.07436, avg_loss=0.07389]\n",
      "Step 527524  [5.417 sec/step, loss=0.07387, avg_loss=0.07388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 527525  [5.428 sec/step, loss=0.07535, avg_loss=0.07389]\n",
      "Step 527526  [5.411 sec/step, loss=0.07466, avg_loss=0.07390]\n",
      "Step 527527  [5.402 sec/step, loss=0.07223, avg_loss=0.07388]\n",
      "Step 527528  [5.376 sec/step, loss=0.07465, avg_loss=0.07389]\n",
      "Step 527529  [5.370 sec/step, loss=0.07540, avg_loss=0.07388]\n",
      "Generated 32 batches of size 32 in 2.484 sec\n",
      "Step 527530  [5.361 sec/step, loss=0.07410, avg_loss=0.07386]\n",
      "Step 527531  [5.361 sec/step, loss=0.07322, avg_loss=0.07386]\n",
      "Step 527532  [5.373 sec/step, loss=0.07400, avg_loss=0.07386]\n",
      "Step 527533  [5.405 sec/step, loss=0.07604, avg_loss=0.07390]\n",
      "Step 527534  [5.404 sec/step, loss=0.07598, avg_loss=0.07390]\n",
      "Step 527535  [5.392 sec/step, loss=0.07393, avg_loss=0.07388]\n",
      "Step 527536  [5.409 sec/step, loss=0.07606, avg_loss=0.07390]\n",
      "Step 527537  [5.401 sec/step, loss=0.07611, avg_loss=0.07390]\n",
      "Step 527538  [5.402 sec/step, loss=0.07443, avg_loss=0.07389]\n",
      "Step 527539  [5.385 sec/step, loss=0.07358, avg_loss=0.07387]\n",
      "Step 527540  [5.381 sec/step, loss=0.07248, avg_loss=0.07387]\n",
      "Step 527541  [5.382 sec/step, loss=0.07438, avg_loss=0.07388]\n",
      "Step 527542  [5.398 sec/step, loss=0.07547, avg_loss=0.07390]\n",
      "Step 527543  [5.393 sec/step, loss=0.07412, avg_loss=0.07387]\n",
      "Step 527544  [5.401 sec/step, loss=0.07521, avg_loss=0.07390]\n",
      "Step 527545  [5.398 sec/step, loss=0.07511, avg_loss=0.07392]\n",
      "Step 527546  [5.452 sec/step, loss=0.06635, avg_loss=0.07385]\n",
      "Step 527547  [5.444 sec/step, loss=0.07151, avg_loss=0.07382]\n",
      "Step 527548  [5.461 sec/step, loss=0.07593, avg_loss=0.07388]\n",
      "Step 527549  [5.469 sec/step, loss=0.07449, avg_loss=0.07388]\n",
      "Step 527550  [5.478 sec/step, loss=0.07293, avg_loss=0.07394]\n",
      "Step 527551  [5.477 sec/step, loss=0.07405, avg_loss=0.07396]\n",
      "Step 527552  [5.469 sec/step, loss=0.07546, avg_loss=0.07395]\n",
      "Step 527553  [5.478 sec/step, loss=0.07454, avg_loss=0.07395]\n",
      "Step 527554  [5.472 sec/step, loss=0.07345, avg_loss=0.07393]\n",
      "Step 527555  [5.482 sec/step, loss=0.07258, avg_loss=0.07390]\n",
      "Step 527556  [5.503 sec/step, loss=0.07295, avg_loss=0.07391]\n",
      "Step 527557  [5.475 sec/step, loss=0.07107, avg_loss=0.07389]\n",
      "Step 527558  [5.414 sec/step, loss=0.07084, avg_loss=0.07393]\n",
      "Step 527559  [5.388 sec/step, loss=0.06570, avg_loss=0.07384]\n",
      "Step 527560  [5.387 sec/step, loss=0.07292, avg_loss=0.07381]\n",
      "Step 527561  [5.382 sec/step, loss=0.07575, avg_loss=0.07381]\n",
      "Generated 32 batches of size 32 in 2.454 sec\n",
      "Step 527562  [5.405 sec/step, loss=0.07363, avg_loss=0.07379]\n",
      "Step 527563  [5.396 sec/step, loss=0.07570, avg_loss=0.07379]\n",
      "Step 527564  [5.409 sec/step, loss=0.07624, avg_loss=0.07381]\n",
      "Step 527565  [5.423 sec/step, loss=0.07564, avg_loss=0.07383]\n",
      "Step 527566  [5.437 sec/step, loss=0.07500, avg_loss=0.07383]\n",
      "Step 527567  [5.432 sec/step, loss=0.07624, avg_loss=0.07384]\n",
      "Step 527568  [5.432 sec/step, loss=0.07550, avg_loss=0.07388]\n",
      "Step 527569  [5.416 sec/step, loss=0.07498, avg_loss=0.07386]\n",
      "Step 527570  [5.416 sec/step, loss=0.07462, avg_loss=0.07386]\n",
      "Step 527571  [5.421 sec/step, loss=0.07365, avg_loss=0.07385]\n",
      "Step 527572  [5.408 sec/step, loss=0.07568, avg_loss=0.07384]\n",
      "Step 527573  [5.412 sec/step, loss=0.07626, avg_loss=0.07386]\n",
      "Step 527574  [5.432 sec/step, loss=0.07221, avg_loss=0.07387]\n",
      "Step 527575  [5.417 sec/step, loss=0.07399, avg_loss=0.07384]\n",
      "Step 527576  [5.418 sec/step, loss=0.07214, avg_loss=0.07384]\n",
      "Step 527577  [5.375 sec/step, loss=0.07574, avg_loss=0.07393]\n",
      "Step 527578  [5.389 sec/step, loss=0.07436, avg_loss=0.07396]\n",
      "Step 527579  [5.435 sec/step, loss=0.06670, avg_loss=0.07388]\n",
      "Step 527580  [5.418 sec/step, loss=0.07183, avg_loss=0.07386]\n",
      "Step 527581  [5.416 sec/step, loss=0.07601, avg_loss=0.07388]\n",
      "Step 527582  [5.420 sec/step, loss=0.07529, avg_loss=0.07391]\n",
      "Step 527583  [5.424 sec/step, loss=0.07493, avg_loss=0.07391]\n",
      "Step 527584  [5.427 sec/step, loss=0.07420, avg_loss=0.07391]\n",
      "Step 527585  [5.408 sec/step, loss=0.07348, avg_loss=0.07390]\n",
      "Step 527586  [5.405 sec/step, loss=0.07299, avg_loss=0.07388]\n",
      "Step 527587  [5.421 sec/step, loss=0.07538, avg_loss=0.07388]\n",
      "Step 527588  [5.390 sec/step, loss=0.07190, avg_loss=0.07387]\n",
      "Step 527589  [5.430 sec/step, loss=0.07300, avg_loss=0.07390]\n",
      "Step 527590  [5.409 sec/step, loss=0.07408, avg_loss=0.07388]\n",
      "Step 527591  [5.397 sec/step, loss=0.07419, avg_loss=0.07385]\n",
      "Step 527592  [5.407 sec/step, loss=0.07562, avg_loss=0.07387]\n",
      "Step 527593  [5.413 sec/step, loss=0.07636, avg_loss=0.07389]\n",
      "Generated 32 batches of size 32 in 2.442 sec\n",
      "Step 527594  [5.446 sec/step, loss=0.07602, avg_loss=0.07401]\n",
      "Step 527595  [5.441 sec/step, loss=0.07452, avg_loss=0.07399]\n",
      "Step 527596  [5.446 sec/step, loss=0.07294, avg_loss=0.07397]\n",
      "Step 527597  [5.426 sec/step, loss=0.07493, avg_loss=0.07399]\n",
      "Step 527598  [5.445 sec/step, loss=0.07615, avg_loss=0.07401]\n",
      "Step 527599  [5.427 sec/step, loss=0.07524, avg_loss=0.07403]\n",
      "Step 527600  [5.423 sec/step, loss=0.07527, avg_loss=0.07401]\n",
      "Writing summary at step: 527600\n",
      "Step 527601  [5.409 sec/step, loss=0.06560, avg_loss=0.07393]\n",
      "Step 527602  [5.401 sec/step, loss=0.07507, avg_loss=0.07394]\n",
      "Step 527603  [5.389 sec/step, loss=0.07133, avg_loss=0.07390]\n",
      "Step 527604  [5.389 sec/step, loss=0.07305, avg_loss=0.07389]\n",
      "Step 527605  [5.390 sec/step, loss=0.07477, avg_loss=0.07390]\n",
      "Step 527606  [5.393 sec/step, loss=0.07634, avg_loss=0.07391]\n",
      "Step 527607  [5.436 sec/step, loss=0.06587, avg_loss=0.07382]\n",
      "Step 527608  [5.422 sec/step, loss=0.07230, avg_loss=0.07380]\n",
      "Step 527609  [5.425 sec/step, loss=0.07535, avg_loss=0.07380]\n",
      "Step 527610  [5.433 sec/step, loss=0.07510, avg_loss=0.07382]\n",
      "Step 527611  [5.441 sec/step, loss=0.07617, avg_loss=0.07384]\n",
      "Step 527612  [5.438 sec/step, loss=0.07602, avg_loss=0.07384]\n",
      "Step 527613  [5.444 sec/step, loss=0.07639, avg_loss=0.07383]\n",
      "Step 527614  [5.436 sec/step, loss=0.07433, avg_loss=0.07385]\n",
      "Step 527615  [5.420 sec/step, loss=0.07642, avg_loss=0.07386]\n",
      "Step 527616  [5.415 sec/step, loss=0.07535, avg_loss=0.07387]\n",
      "Step 527617  [5.432 sec/step, loss=0.07455, avg_loss=0.07394]\n",
      "Step 527618  [5.443 sec/step, loss=0.07521, avg_loss=0.07395]\n",
      "Step 527619  [5.452 sec/step, loss=0.07248, avg_loss=0.07393]\n",
      "Step 527620  [5.454 sec/step, loss=0.07330, avg_loss=0.07390]\n",
      "Step 527621  [5.460 sec/step, loss=0.07387, avg_loss=0.07392]\n",
      "Step 527622  [5.431 sec/step, loss=0.07319, avg_loss=0.07400]\n",
      "Step 527623  [5.432 sec/step, loss=0.07430, avg_loss=0.07400]\n",
      "Step 527624  [5.434 sec/step, loss=0.07416, avg_loss=0.07400]\n",
      "Generated 32 batches of size 32 in 2.443 sec\n",
      "Step 527625  [5.438 sec/step, loss=0.07399, avg_loss=0.07399]\n",
      "Step 527626  [5.441 sec/step, loss=0.07682, avg_loss=0.07401]\n",
      "Step 527627  [5.436 sec/step, loss=0.06724, avg_loss=0.07396]\n",
      "Step 527628  [5.441 sec/step, loss=0.07501, avg_loss=0.07396]\n",
      "Step 527629  [5.431 sec/step, loss=0.07382, avg_loss=0.07395]\n",
      "Step 527630  [5.426 sec/step, loss=0.07272, avg_loss=0.07393]\n",
      "Step 527631  [5.425 sec/step, loss=0.07650, avg_loss=0.07397]\n",
      "Step 527632  [5.415 sec/step, loss=0.07486, avg_loss=0.07398]\n",
      "Step 527633  [5.422 sec/step, loss=0.07390, avg_loss=0.07395]\n",
      "Step 527634  [5.420 sec/step, loss=0.07380, avg_loss=0.07393]\n",
      "Step 527635  [5.436 sec/step, loss=0.07616, avg_loss=0.07395]\n",
      "Step 527636  [5.421 sec/step, loss=0.07479, avg_loss=0.07394]\n",
      "Step 527637  [5.424 sec/step, loss=0.07557, avg_loss=0.07394]\n",
      "Step 527638  [5.427 sec/step, loss=0.07633, avg_loss=0.07396]\n",
      "Step 527639  [5.427 sec/step, loss=0.07277, avg_loss=0.07395]\n",
      "Step 527640  [5.438 sec/step, loss=0.07235, avg_loss=0.07395]\n",
      "Step 527641  [5.449 sec/step, loss=0.07550, avg_loss=0.07396]\n",
      "Step 527642  [5.457 sec/step, loss=0.07533, avg_loss=0.07396]\n",
      "Step 527643  [5.460 sec/step, loss=0.07490, avg_loss=0.07396]\n",
      "Step 527644  [5.458 sec/step, loss=0.07345, avg_loss=0.07395]\n",
      "Step 527645  [5.489 sec/step, loss=0.07290, avg_loss=0.07392]\n",
      "Step 527646  [5.430 sec/step, loss=0.07057, avg_loss=0.07397]\n",
      "Step 527647  [5.448 sec/step, loss=0.07279, avg_loss=0.07398]\n",
      "Step 527648  [5.435 sec/step, loss=0.07408, avg_loss=0.07396]\n",
      "Step 527649  [5.443 sec/step, loss=0.07590, avg_loss=0.07397]\n",
      "Step 527650  [5.455 sec/step, loss=0.07516, avg_loss=0.07400]\n",
      "Step 527651  [5.468 sec/step, loss=0.07636, avg_loss=0.07402]\n",
      "Step 527652  [5.455 sec/step, loss=0.07414, avg_loss=0.07401]\n",
      "Step 527653  [5.501 sec/step, loss=0.06631, avg_loss=0.07392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 527654  [5.511 sec/step, loss=0.07510, avg_loss=0.07394]\n",
      "Step 527655  [5.487 sec/step, loss=0.07355, avg_loss=0.07395]\n",
      "Step 527656  [5.462 sec/step, loss=0.07106, avg_loss=0.07393]\n",
      "Generated 32 batches of size 32 in 2.384 sec\n",
      "Step 527657  [5.478 sec/step, loss=0.07602, avg_loss=0.07398]\n",
      "Step 527658  [5.486 sec/step, loss=0.07524, avg_loss=0.07402]\n",
      "Step 527659  [5.486 sec/step, loss=0.06629, avg_loss=0.07403]\n",
      "Step 527660  [5.487 sec/step, loss=0.07556, avg_loss=0.07406]\n",
      "Step 527661  [5.471 sec/step, loss=0.07324, avg_loss=0.07403]\n",
      "Step 527662  [5.438 sec/step, loss=0.07529, avg_loss=0.07405]\n",
      "Step 527663  [5.455 sec/step, loss=0.07625, avg_loss=0.07405]\n",
      "Step 527664  [5.442 sec/step, loss=0.07248, avg_loss=0.07402]\n",
      "Step 527665  [5.447 sec/step, loss=0.07649, avg_loss=0.07403]\n",
      "Step 527666  [5.424 sec/step, loss=0.07402, avg_loss=0.07402]\n",
      "Step 527667  [5.419 sec/step, loss=0.07359, avg_loss=0.07399]\n",
      "Step 527668  [5.412 sec/step, loss=0.07290, avg_loss=0.07396]\n",
      "Step 527669  [5.422 sec/step, loss=0.07581, avg_loss=0.07397]\n",
      "Step 527670  [5.450 sec/step, loss=0.07452, avg_loss=0.07397]\n",
      "Step 527671  [5.444 sec/step, loss=0.07465, avg_loss=0.07398]\n",
      "Step 527672  [5.433 sec/step, loss=0.07408, avg_loss=0.07396]\n",
      "Step 527673  [5.425 sec/step, loss=0.07219, avg_loss=0.07392]\n",
      "Step 527674  [5.426 sec/step, loss=0.07622, avg_loss=0.07396]\n",
      "Step 527675  [5.435 sec/step, loss=0.07523, avg_loss=0.07398]\n",
      "Step 527676  [5.458 sec/step, loss=0.07499, avg_loss=0.07400]\n",
      "Step 527677  [5.451 sec/step, loss=0.07459, avg_loss=0.07399]\n",
      "Step 527678  [5.450 sec/step, loss=0.07622, avg_loss=0.07401]\n",
      "Step 527679  [5.404 sec/step, loss=0.07291, avg_loss=0.07407]\n",
      "Step 527680  [5.430 sec/step, loss=0.07421, avg_loss=0.07410]\n",
      "Step 527681  [5.422 sec/step, loss=0.07466, avg_loss=0.07408]\n",
      "Step 527682  [5.426 sec/step, loss=0.07568, avg_loss=0.07409]\n",
      "Step 527683  [5.422 sec/step, loss=0.07307, avg_loss=0.07407]\n",
      "Step 527684  [5.419 sec/step, loss=0.07140, avg_loss=0.07404]\n",
      "Step 527685  [5.410 sec/step, loss=0.07173, avg_loss=0.07402]\n",
      "Step 527686  [5.408 sec/step, loss=0.07404, avg_loss=0.07403]\n",
      "Step 527687  [5.397 sec/step, loss=0.07611, avg_loss=0.07404]\n",
      "Step 527688  [5.417 sec/step, loss=0.07641, avg_loss=0.07409]\n",
      "Generated 32 batches of size 32 in 2.333 sec\n",
      "Step 527689  [5.411 sec/step, loss=0.07403, avg_loss=0.07410]\n",
      "Step 527690  [5.432 sec/step, loss=0.07438, avg_loss=0.07410]\n",
      "Step 527691  [5.430 sec/step, loss=0.07492, avg_loss=0.07411]\n",
      "Step 527692  [5.466 sec/step, loss=0.06745, avg_loss=0.07403]\n",
      "Step 527693  [5.451 sec/step, loss=0.07389, avg_loss=0.07400]\n",
      "Step 527694  [5.450 sec/step, loss=0.07521, avg_loss=0.07399]\n",
      "Step 527695  [5.433 sec/step, loss=0.06670, avg_loss=0.07391]\n",
      "Step 527696  [5.427 sec/step, loss=0.07518, avg_loss=0.07394]\n",
      "Step 527697  [5.422 sec/step, loss=0.07523, avg_loss=0.07394]\n",
      "Step 527698  [5.410 sec/step, loss=0.07469, avg_loss=0.07393]\n",
      "Step 527699  [5.427 sec/step, loss=0.07492, avg_loss=0.07392]\n",
      "Step 527700  [5.433 sec/step, loss=0.07626, avg_loss=0.07393]\n",
      "Writing summary at step: 527700\n",
      "Step 527701  [5.442 sec/step, loss=0.07376, avg_loss=0.07401]\n",
      "Step 527702  [5.461 sec/step, loss=0.07456, avg_loss=0.07401]\n",
      "Step 527703  [5.475 sec/step, loss=0.07504, avg_loss=0.07405]\n",
      "Step 527704  [5.474 sec/step, loss=0.07391, avg_loss=0.07405]\n",
      "Step 527705  [5.474 sec/step, loss=0.07218, avg_loss=0.07403]\n",
      "Step 527706  [5.465 sec/step, loss=0.07600, avg_loss=0.07403]\n",
      "Step 527707  [5.404 sec/step, loss=0.07390, avg_loss=0.07411]\n",
      "Step 527708  [5.431 sec/step, loss=0.07548, avg_loss=0.07414]\n",
      "Step 527709  [5.435 sec/step, loss=0.07518, avg_loss=0.07414]\n",
      "Step 527710  [5.481 sec/step, loss=0.06517, avg_loss=0.07404]\n",
      "Step 527711  [5.479 sec/step, loss=0.07350, avg_loss=0.07401]\n",
      "Step 527712  [5.460 sec/step, loss=0.07317, avg_loss=0.07398]\n",
      "Step 527713  [5.447 sec/step, loss=0.07524, avg_loss=0.07397]\n",
      "Step 527714  [5.442 sec/step, loss=0.06527, avg_loss=0.07388]\n",
      "Step 527715  [5.419 sec/step, loss=0.07151, avg_loss=0.07383]\n",
      "Step 527716  [5.425 sec/step, loss=0.07339, avg_loss=0.07381]\n",
      "Step 527717  [5.420 sec/step, loss=0.07385, avg_loss=0.07380]\n",
      "Step 527718  [5.443 sec/step, loss=0.07382, avg_loss=0.07379]\n",
      "Step 527719  [5.435 sec/step, loss=0.07498, avg_loss=0.07381]\n",
      "Generated 32 batches of size 32 in 2.543 sec\n",
      "Step 527720  [5.426 sec/step, loss=0.07562, avg_loss=0.07384]\n",
      "Step 527721  [5.427 sec/step, loss=0.07195, avg_loss=0.07382]\n",
      "Step 527722  [5.408 sec/step, loss=0.07411, avg_loss=0.07383]\n",
      "Step 527723  [5.405 sec/step, loss=0.07135, avg_loss=0.07380]\n",
      "Step 527724  [5.418 sec/step, loss=0.07656, avg_loss=0.07382]\n",
      "Step 527725  [5.421 sec/step, loss=0.07629, avg_loss=0.07385]\n",
      "Step 527726  [5.420 sec/step, loss=0.07364, avg_loss=0.07381]\n",
      "Step 527727  [5.445 sec/step, loss=0.07597, avg_loss=0.07390]\n",
      "Step 527728  [5.442 sec/step, loss=0.07526, avg_loss=0.07390]\n",
      "Step 527729  [5.446 sec/step, loss=0.07564, avg_loss=0.07392]\n",
      "Step 527730  [5.459 sec/step, loss=0.07641, avg_loss=0.07396]\n",
      "Step 527731  [5.457 sec/step, loss=0.07603, avg_loss=0.07395]\n",
      "Step 527732  [5.473 sec/step, loss=0.07544, avg_loss=0.07396]\n",
      "Step 527733  [5.445 sec/step, loss=0.07486, avg_loss=0.07397]\n",
      "Step 527734  [5.444 sec/step, loss=0.07477, avg_loss=0.07398]\n",
      "Step 527735  [5.424 sec/step, loss=0.07380, avg_loss=0.07395]\n",
      "Step 527736  [5.409 sec/step, loss=0.06642, avg_loss=0.07387]\n",
      "Step 527737  [5.430 sec/step, loss=0.07509, avg_loss=0.07387]\n",
      "Step 527738  [5.423 sec/step, loss=0.07474, avg_loss=0.07385]\n",
      "Step 527739  [5.427 sec/step, loss=0.07545, avg_loss=0.07388]\n",
      "Step 527740  [5.425 sec/step, loss=0.07527, avg_loss=0.07391]\n",
      "Step 527741  [5.413 sec/step, loss=0.07407, avg_loss=0.07389]\n",
      "Step 527742  [5.391 sec/step, loss=0.07166, avg_loss=0.07386]\n",
      "Step 527743  [5.376 sec/step, loss=0.07459, avg_loss=0.07385]\n",
      "Step 527744  [5.378 sec/step, loss=0.07516, avg_loss=0.07387]\n",
      "Step 527745  [5.350 sec/step, loss=0.07450, avg_loss=0.07389]\n",
      "Step 527746  [5.348 sec/step, loss=0.07173, avg_loss=0.07390]\n",
      "Step 527747  [5.339 sec/step, loss=0.07409, avg_loss=0.07391]\n",
      "Step 527748  [5.357 sec/step, loss=0.07517, avg_loss=0.07392]\n",
      "Step 527749  [5.391 sec/step, loss=0.06551, avg_loss=0.07382]\n",
      "Step 527750  [5.384 sec/step, loss=0.07524, avg_loss=0.07382]\n",
      "Step 527751  [5.384 sec/step, loss=0.07672, avg_loss=0.07382]\n",
      "Generated 32 batches of size 32 in 2.406 sec\n",
      "Step 527752  [5.410 sec/step, loss=0.07641, avg_loss=0.07384]\n",
      "Step 527753  [5.359 sec/step, loss=0.07316, avg_loss=0.07391]\n",
      "Step 527754  [5.365 sec/step, loss=0.07580, avg_loss=0.07392]\n",
      "Step 527755  [5.365 sec/step, loss=0.07380, avg_loss=0.07392]\n",
      "Step 527756  [5.388 sec/step, loss=0.07452, avg_loss=0.07396]\n",
      "Step 527757  [5.379 sec/step, loss=0.07534, avg_loss=0.07395]\n",
      "Step 527758  [5.391 sec/step, loss=0.07593, avg_loss=0.07396]\n",
      "Step 527759  [5.409 sec/step, loss=0.07502, avg_loss=0.07404]\n",
      "Step 527760  [5.394 sec/step, loss=0.07100, avg_loss=0.07400]\n",
      "Step 527761  [5.397 sec/step, loss=0.07458, avg_loss=0.07401]\n",
      "Step 527762  [5.400 sec/step, loss=0.07205, avg_loss=0.07398]\n",
      "Step 527763  [5.391 sec/step, loss=0.07177, avg_loss=0.07393]\n",
      "Step 527764  [5.375 sec/step, loss=0.06672, avg_loss=0.07388]\n",
      "Step 527765  [5.363 sec/step, loss=0.07452, avg_loss=0.07386]\n",
      "Step 527766  [5.360 sec/step, loss=0.07144, avg_loss=0.07383]\n",
      "Step 527767  [5.365 sec/step, loss=0.07483, avg_loss=0.07384]\n",
      "Step 527768  [5.379 sec/step, loss=0.07607, avg_loss=0.07388]\n",
      "Step 527769  [5.364 sec/step, loss=0.07353, avg_loss=0.07385]\n",
      "Step 527770  [5.352 sec/step, loss=0.07454, avg_loss=0.07385]\n",
      "Step 527771  [5.359 sec/step, loss=0.07464, avg_loss=0.07385]\n",
      "Step 527772  [5.364 sec/step, loss=0.07046, avg_loss=0.07382]\n",
      "Step 527773  [5.385 sec/step, loss=0.07417, avg_loss=0.07384]\n",
      "Step 527774  [5.364 sec/step, loss=0.07239, avg_loss=0.07380]\n",
      "Step 527775  [5.361 sec/step, loss=0.07529, avg_loss=0.07380]\n",
      "Step 527776  [5.362 sec/step, loss=0.07389, avg_loss=0.07379]\n",
      "Step 527777  [5.374 sec/step, loss=0.07577, avg_loss=0.07380]\n",
      "Step 527778  [5.414 sec/step, loss=0.06816, avg_loss=0.07372]\n",
      "Step 527779  [5.415 sec/step, loss=0.07496, avg_loss=0.07374]\n",
      "Step 527780  [5.408 sec/step, loss=0.07314, avg_loss=0.07373]\n",
      "Step 527781  [5.424 sec/step, loss=0.07378, avg_loss=0.07372]\n",
      "Step 527782  [5.433 sec/step, loss=0.07540, avg_loss=0.07372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 527783  [5.435 sec/step, loss=0.07122, avg_loss=0.07370]\n",
      "Generated 32 batches of size 32 in 2.545 sec\n",
      "Step 527784  [5.445 sec/step, loss=0.07346, avg_loss=0.07372]\n",
      "Step 527785  [5.463 sec/step, loss=0.07453, avg_loss=0.07375]\n",
      "Step 527786  [5.459 sec/step, loss=0.07409, avg_loss=0.07375]\n",
      "Step 527787  [5.459 sec/step, loss=0.07589, avg_loss=0.07375]\n",
      "Step 527788  [5.445 sec/step, loss=0.07379, avg_loss=0.07372]\n",
      "Step 527789  [5.427 sec/step, loss=0.07605, avg_loss=0.07374]\n",
      "Step 527790  [5.424 sec/step, loss=0.07617, avg_loss=0.07376]\n",
      "Step 527791  [5.425 sec/step, loss=0.07441, avg_loss=0.07375]\n",
      "Step 527792  [5.372 sec/step, loss=0.07543, avg_loss=0.07383]\n",
      "Step 527793  [5.385 sec/step, loss=0.07635, avg_loss=0.07386]\n",
      "Step 527794  [5.369 sec/step, loss=0.07486, avg_loss=0.07385]\n",
      "Step 527795  [5.378 sec/step, loss=0.07403, avg_loss=0.07393]\n",
      "Step 527796  [5.380 sec/step, loss=0.07226, avg_loss=0.07390]\n",
      "Step 527797  [5.394 sec/step, loss=0.07556, avg_loss=0.07390]\n",
      "Step 527798  [5.399 sec/step, loss=0.07479, avg_loss=0.07390]\n",
      "Step 527799  [5.401 sec/step, loss=0.07578, avg_loss=0.07391]\n",
      "Step 527800  [5.389 sec/step, loss=0.07305, avg_loss=0.07388]\n",
      "Writing summary at step: 527800\n",
      "Step 527801  [5.404 sec/step, loss=0.07436, avg_loss=0.07388]\n",
      "Step 527802  [5.380 sec/step, loss=0.07346, avg_loss=0.07387]\n",
      "Step 527803  [5.385 sec/step, loss=0.07450, avg_loss=0.07387]\n",
      "Step 527804  [5.394 sec/step, loss=0.07437, avg_loss=0.07387]\n",
      "Step 527805  [5.392 sec/step, loss=0.07093, avg_loss=0.07386]\n",
      "Step 527806  [5.383 sec/step, loss=0.07465, avg_loss=0.07385]\n",
      "Step 527807  [5.443 sec/step, loss=0.06719, avg_loss=0.07378]\n",
      "Step 527808  [5.421 sec/step, loss=0.07395, avg_loss=0.07376]\n",
      "Step 527809  [5.418 sec/step, loss=0.07498, avg_loss=0.07376]\n",
      "Step 527810  [5.363 sec/step, loss=0.07237, avg_loss=0.07383]\n",
      "Step 527811  [5.346 sec/step, loss=0.07169, avg_loss=0.07382]\n",
      "Step 527812  [5.340 sec/step, loss=0.07187, avg_loss=0.07380]\n",
      "Step 527813  [5.323 sec/step, loss=0.06497, avg_loss=0.07370]\n",
      "Step 527814  [5.349 sec/step, loss=0.07391, avg_loss=0.07379]\n",
      "Generated 32 batches of size 32 in 2.415 sec\n",
      "Step 527815  [5.379 sec/step, loss=0.07358, avg_loss=0.07381]\n",
      "Step 527816  [5.384 sec/step, loss=0.07633, avg_loss=0.07384]\n",
      "Step 527817  [5.384 sec/step, loss=0.07389, avg_loss=0.07384]\n",
      "Step 527818  [5.373 sec/step, loss=0.07581, avg_loss=0.07386]\n",
      "Step 527819  [5.381 sec/step, loss=0.07527, avg_loss=0.07386]\n",
      "Step 527820  [5.377 sec/step, loss=0.07548, avg_loss=0.07386]\n",
      "Step 527821  [5.380 sec/step, loss=0.07498, avg_loss=0.07389]\n",
      "Step 527822  [5.381 sec/step, loss=0.07572, avg_loss=0.07391]\n",
      "Step 527823  [5.402 sec/step, loss=0.07611, avg_loss=0.07395]\n",
      "Step 527824  [5.417 sec/step, loss=0.07308, avg_loss=0.07392]\n",
      "Step 527825  [5.408 sec/step, loss=0.07475, avg_loss=0.07390]\n",
      "Step 527826  [5.399 sec/step, loss=0.07449, avg_loss=0.07391]\n",
      "Step 527827  [5.394 sec/step, loss=0.07431, avg_loss=0.07389]\n",
      "Step 527828  [5.383 sec/step, loss=0.07236, avg_loss=0.07387]\n",
      "Step 527829  [5.386 sec/step, loss=0.07509, avg_loss=0.07386]\n",
      "Step 527830  [5.385 sec/step, loss=0.07352, avg_loss=0.07383]\n",
      "Step 527831  [5.378 sec/step, loss=0.07473, avg_loss=0.07382]\n",
      "Step 527832  [5.363 sec/step, loss=0.07569, avg_loss=0.07382]\n",
      "Step 527833  [5.362 sec/step, loss=0.07471, avg_loss=0.07382]\n",
      "Step 527834  [5.364 sec/step, loss=0.07617, avg_loss=0.07383]\n",
      "Step 527835  [5.382 sec/step, loss=0.07623, avg_loss=0.07386]\n",
      "Step 527836  [5.394 sec/step, loss=0.07323, avg_loss=0.07393]\n",
      "Step 527837  [5.368 sec/step, loss=0.07351, avg_loss=0.07391]\n",
      "Step 527838  [5.367 sec/step, loss=0.07458, avg_loss=0.07391]\n",
      "Step 527839  [5.364 sec/step, loss=0.07398, avg_loss=0.07389]\n",
      "Step 527840  [5.368 sec/step, loss=0.07627, avg_loss=0.07390]\n",
      "Step 527841  [5.367 sec/step, loss=0.07414, avg_loss=0.07390]\n",
      "Step 527842  [5.368 sec/step, loss=0.07291, avg_loss=0.07392]\n",
      "Step 527843  [5.384 sec/step, loss=0.07657, avg_loss=0.07394]\n",
      "Step 527844  [5.393 sec/step, loss=0.07560, avg_loss=0.07394]\n",
      "Step 527845  [5.403 sec/step, loss=0.07543, avg_loss=0.07395]\n",
      "Step 527846  [5.411 sec/step, loss=0.07243, avg_loss=0.07396]\n",
      "Generated 32 batches of size 32 in 2.523 sec\n",
      "Step 527847  [5.415 sec/step, loss=0.07437, avg_loss=0.07396]\n",
      "Step 527848  [5.406 sec/step, loss=0.07353, avg_loss=0.07394]\n",
      "Step 527849  [5.361 sec/step, loss=0.07425, avg_loss=0.07403]\n",
      "Step 527850  [5.346 sec/step, loss=0.06793, avg_loss=0.07396]\n",
      "Step 527851  [5.355 sec/step, loss=0.07463, avg_loss=0.07394]\n",
      "Step 527852  [5.336 sec/step, loss=0.07493, avg_loss=0.07392]\n",
      "Step 527853  [5.323 sec/step, loss=0.07137, avg_loss=0.07390]\n",
      "Step 527854  [5.351 sec/step, loss=0.06872, avg_loss=0.07383]\n",
      "Step 527855  [5.362 sec/step, loss=0.07408, avg_loss=0.07384]\n",
      "Step 527856  [5.370 sec/step, loss=0.07655, avg_loss=0.07386]\n",
      "Step 527857  [5.357 sec/step, loss=0.07385, avg_loss=0.07384]\n",
      "Step 527858  [5.348 sec/step, loss=0.07178, avg_loss=0.07380]\n",
      "Step 527859  [5.355 sec/step, loss=0.07600, avg_loss=0.07381]\n",
      "Step 527860  [5.361 sec/step, loss=0.07266, avg_loss=0.07383]\n",
      "Step 527861  [5.367 sec/step, loss=0.07235, avg_loss=0.07380]\n",
      "Step 527862  [5.379 sec/step, loss=0.07403, avg_loss=0.07382]\n",
      "Step 527863  [5.386 sec/step, loss=0.07616, avg_loss=0.07387]\n",
      "Step 527864  [5.400 sec/step, loss=0.07079, avg_loss=0.07391]\n",
      "Step 527865  [5.400 sec/step, loss=0.07354, avg_loss=0.07390]\n",
      "Step 527866  [5.408 sec/step, loss=0.07532, avg_loss=0.07394]\n",
      "Step 527867  [5.384 sec/step, loss=0.07136, avg_loss=0.07390]\n",
      "Step 527868  [5.400 sec/step, loss=0.07541, avg_loss=0.07390]\n",
      "Step 527869  [5.402 sec/step, loss=0.07548, avg_loss=0.07392]\n",
      "Step 527870  [5.390 sec/step, loss=0.07283, avg_loss=0.07390]\n",
      "Step 527871  [5.378 sec/step, loss=0.07438, avg_loss=0.07390]\n",
      "Step 527872  [5.387 sec/step, loss=0.07545, avg_loss=0.07395]\n",
      "Step 527873  [5.373 sec/step, loss=0.07487, avg_loss=0.07395]\n",
      "Step 527874  [5.376 sec/step, loss=0.06973, avg_loss=0.07393]\n",
      "Step 527875  [5.375 sec/step, loss=0.07584, avg_loss=0.07393]\n",
      "Step 527876  [5.351 sec/step, loss=0.07489, avg_loss=0.07394]\n",
      "Step 527877  [5.343 sec/step, loss=0.07484, avg_loss=0.07393]\n",
      "Step 527878  [5.306 sec/step, loss=0.07612, avg_loss=0.07401]\n",
      "Generated 32 batches of size 32 in 2.573 sec\n",
      "Step 527879  [5.356 sec/step, loss=0.06702, avg_loss=0.07393]\n",
      "Step 527880  [5.358 sec/step, loss=0.07628, avg_loss=0.07396]\n",
      "Step 527881  [5.351 sec/step, loss=0.07597, avg_loss=0.07399]\n",
      "Step 527882  [5.356 sec/step, loss=0.07587, avg_loss=0.07399]\n",
      "Step 527883  [5.352 sec/step, loss=0.07375, avg_loss=0.07402]\n",
      "Step 527884  [5.366 sec/step, loss=0.07417, avg_loss=0.07402]\n",
      "Step 527885  [5.359 sec/step, loss=0.07231, avg_loss=0.07400]\n",
      "Step 527886  [5.363 sec/step, loss=0.07427, avg_loss=0.07400]\n",
      "Step 527887  [5.341 sec/step, loss=0.06534, avg_loss=0.07390]\n",
      "Step 527888  [5.350 sec/step, loss=0.07502, avg_loss=0.07391]\n",
      "Step 527889  [5.347 sec/step, loss=0.07422, avg_loss=0.07389]\n",
      "Step 527890  [5.340 sec/step, loss=0.07525, avg_loss=0.07388]\n",
      "Step 527891  [5.353 sec/step, loss=0.07480, avg_loss=0.07389]\n",
      "Step 527892  [5.349 sec/step, loss=0.07394, avg_loss=0.07387]\n",
      "Step 527893  [5.341 sec/step, loss=0.07269, avg_loss=0.07383]\n",
      "Step 527894  [5.335 sec/step, loss=0.07403, avg_loss=0.07383]\n",
      "Step 527895  [5.351 sec/step, loss=0.07411, avg_loss=0.07383]\n",
      "Step 527896  [5.377 sec/step, loss=0.07380, avg_loss=0.07384]\n",
      "Step 527897  [5.359 sec/step, loss=0.07525, avg_loss=0.07384]\n",
      "Step 527898  [5.353 sec/step, loss=0.07341, avg_loss=0.07383]\n",
      "Step 527899  [5.323 sec/step, loss=0.07166, avg_loss=0.07378]\n",
      "Step 527900  [5.330 sec/step, loss=0.07406, avg_loss=0.07379]\n",
      "Writing summary at step: 527900\n",
      "Step 527901  [5.315 sec/step, loss=0.07337, avg_loss=0.07378]\n",
      "Step 527902  [5.311 sec/step, loss=0.07169, avg_loss=0.07377]\n",
      "Step 527903  [5.293 sec/step, loss=0.07423, avg_loss=0.07376]\n",
      "Step 527904  [5.298 sec/step, loss=0.07320, avg_loss=0.07375]\n",
      "Step 527905  [5.319 sec/step, loss=0.07256, avg_loss=0.07377]\n",
      "Step 527906  [5.328 sec/step, loss=0.07592, avg_loss=0.07378]\n",
      "Step 527907  [5.328 sec/step, loss=0.06597, avg_loss=0.07377]\n",
      "Step 527908  [5.341 sec/step, loss=0.07576, avg_loss=0.07379]\n",
      "Step 527909  [5.340 sec/step, loss=0.07534, avg_loss=0.07379]\n",
      "Generated 32 batches of size 32 in 2.585 sec\n",
      "Step 527910  [5.346 sec/step, loss=0.07533, avg_loss=0.07382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 527911  [5.353 sec/step, loss=0.07296, avg_loss=0.07383]\n",
      "Step 527912  [5.378 sec/step, loss=0.07602, avg_loss=0.07387]\n",
      "Step 527913  [5.397 sec/step, loss=0.07518, avg_loss=0.07398]\n",
      "Step 527914  [5.401 sec/step, loss=0.07524, avg_loss=0.07399]\n",
      "Step 527915  [5.394 sec/step, loss=0.07635, avg_loss=0.07402]\n",
      "Step 527916  [5.386 sec/step, loss=0.07530, avg_loss=0.07401]\n",
      "Step 527917  [5.372 sec/step, loss=0.06477, avg_loss=0.07392]\n",
      "Step 527918  [5.363 sec/step, loss=0.07420, avg_loss=0.07390]\n",
      "Step 527919  [5.356 sec/step, loss=0.07445, avg_loss=0.07389]\n",
      "Step 527920  [5.366 sec/step, loss=0.07571, avg_loss=0.07389]\n",
      "Step 527921  [5.370 sec/step, loss=0.07309, avg_loss=0.07387]\n",
      "Step 527922  [5.366 sec/step, loss=0.07192, avg_loss=0.07384]\n",
      "Step 527923  [5.374 sec/step, loss=0.07337, avg_loss=0.07381]\n",
      "Step 527924  [5.364 sec/step, loss=0.07552, avg_loss=0.07383]\n",
      "Step 527925  [5.379 sec/step, loss=0.07660, avg_loss=0.07385]\n",
      "Step 527926  [5.396 sec/step, loss=0.07693, avg_loss=0.07388]\n",
      "Step 527927  [5.385 sec/step, loss=0.07426, avg_loss=0.07388]\n",
      "Step 527928  [5.402 sec/step, loss=0.07674, avg_loss=0.07392]\n",
      "Step 527929  [5.408 sec/step, loss=0.07460, avg_loss=0.07391]\n",
      "Step 527930  [5.380 sec/step, loss=0.06800, avg_loss=0.07386]\n",
      "Step 527931  [5.368 sec/step, loss=0.07192, avg_loss=0.07383]\n",
      "Step 527932  [5.357 sec/step, loss=0.07477, avg_loss=0.07382]\n",
      "Step 527933  [5.367 sec/step, loss=0.07658, avg_loss=0.07384]\n",
      "Step 527934  [5.362 sec/step, loss=0.07651, avg_loss=0.07384]\n",
      "Step 527935  [5.353 sec/step, loss=0.07543, avg_loss=0.07384]\n",
      "Step 527936  [5.358 sec/step, loss=0.07584, avg_loss=0.07386]\n",
      "Step 527937  [5.357 sec/step, loss=0.07424, avg_loss=0.07387]\n",
      "Step 527938  [5.382 sec/step, loss=0.07351, avg_loss=0.07386]\n",
      "Step 527939  [5.380 sec/step, loss=0.07445, avg_loss=0.07386]\n",
      "Step 527940  [5.366 sec/step, loss=0.07567, avg_loss=0.07386]\n",
      "Step 527941  [5.370 sec/step, loss=0.07360, avg_loss=0.07385]\n",
      "Generated 32 batches of size 32 in 2.290 sec\n",
      "Step 527942  [5.383 sec/step, loss=0.07288, avg_loss=0.07385]\n",
      "Step 527943  [5.360 sec/step, loss=0.07310, avg_loss=0.07382]\n",
      "Step 527944  [5.343 sec/step, loss=0.07554, avg_loss=0.07382]\n",
      "Step 527945  [5.328 sec/step, loss=0.07294, avg_loss=0.07379]\n",
      "Step 527946  [5.338 sec/step, loss=0.07621, avg_loss=0.07383]\n",
      "Step 527947  [5.340 sec/step, loss=0.07566, avg_loss=0.07384]\n",
      "Step 527948  [5.336 sec/step, loss=0.07429, avg_loss=0.07385]\n",
      "Step 527949  [5.382 sec/step, loss=0.06607, avg_loss=0.07377]\n",
      "Step 527950  [5.393 sec/step, loss=0.07427, avg_loss=0.07383]\n",
      "Step 527951  [5.372 sec/step, loss=0.07388, avg_loss=0.07382]\n",
      "Step 527952  [5.401 sec/step, loss=0.07318, avg_loss=0.07381]\n",
      "Step 527953  [5.429 sec/step, loss=0.07620, avg_loss=0.07386]\n",
      "Step 527954  [5.404 sec/step, loss=0.07579, avg_loss=0.07393]\n",
      "Step 527955  [5.447 sec/step, loss=0.06648, avg_loss=0.07385]\n",
      "Step 527956  [5.437 sec/step, loss=0.07612, avg_loss=0.07385]\n",
      "Step 527957  [5.445 sec/step, loss=0.07555, avg_loss=0.07386]\n",
      "Step 527958  [5.444 sec/step, loss=0.07421, avg_loss=0.07389]\n",
      "Step 527959  [5.438 sec/step, loss=0.07604, avg_loss=0.07389]\n",
      "Step 527960  [5.455 sec/step, loss=0.07424, avg_loss=0.07390]\n",
      "Step 527961  [5.458 sec/step, loss=0.07652, avg_loss=0.07394]\n",
      "Step 527962  [5.430 sec/step, loss=0.06479, avg_loss=0.07385]\n",
      "Step 527963  [5.408 sec/step, loss=0.06957, avg_loss=0.07379]\n",
      "Step 527964  [5.404 sec/step, loss=0.07476, avg_loss=0.07383]\n",
      "Step 527965  [5.407 sec/step, loss=0.07407, avg_loss=0.07383]\n",
      "Step 527966  [5.423 sec/step, loss=0.07574, avg_loss=0.07384]\n",
      "Step 527967  [5.448 sec/step, loss=0.07669, avg_loss=0.07389]\n",
      "Step 527968  [5.421 sec/step, loss=0.07526, avg_loss=0.07389]\n",
      "Step 527969  [5.411 sec/step, loss=0.07125, avg_loss=0.07385]\n",
      "Step 527970  [5.414 sec/step, loss=0.07535, avg_loss=0.07387]\n",
      "Step 527971  [5.425 sec/step, loss=0.07324, avg_loss=0.07386]\n",
      "Step 527972  [5.427 sec/step, loss=0.07506, avg_loss=0.07386]\n",
      "Step 527973  [5.409 sec/step, loss=0.07412, avg_loss=0.07385]\n",
      "Generated 32 batches of size 32 in 2.423 sec\n",
      "Step 527974  [5.428 sec/step, loss=0.07541, avg_loss=0.07390]\n",
      "Step 527975  [5.427 sec/step, loss=0.07489, avg_loss=0.07389]\n",
      "Step 527976  [5.426 sec/step, loss=0.07398, avg_loss=0.07389]\n",
      "Step 527977  [5.433 sec/step, loss=0.07608, avg_loss=0.07390]\n",
      "Step 527978  [5.419 sec/step, loss=0.07037, avg_loss=0.07384]\n",
      "Step 527979  [5.369 sec/step, loss=0.07437, avg_loss=0.07391]\n",
      "Step 527980  [5.357 sec/step, loss=0.07282, avg_loss=0.07388]\n",
      "Step 527981  [5.339 sec/step, loss=0.07280, avg_loss=0.07385]\n",
      "Step 527982  [5.333 sec/step, loss=0.07439, avg_loss=0.07383]\n",
      "Step 527983  [5.343 sec/step, loss=0.07565, avg_loss=0.07385]\n",
      "Step 527984  [5.349 sec/step, loss=0.07297, avg_loss=0.07384]\n",
      "Step 527985  [5.349 sec/step, loss=0.07490, avg_loss=0.07387]\n",
      "Step 527986  [5.353 sec/step, loss=0.07279, avg_loss=0.07385]\n",
      "Step 527987  [5.369 sec/step, loss=0.07152, avg_loss=0.07391]\n",
      "Step 527988  [5.368 sec/step, loss=0.07516, avg_loss=0.07391]\n",
      "Step 527989  [5.362 sec/step, loss=0.07447, avg_loss=0.07392]\n",
      "Step 527990  [5.362 sec/step, loss=0.07560, avg_loss=0.07392]\n",
      "Step 527991  [5.360 sec/step, loss=0.07650, avg_loss=0.07394]\n",
      "Step 527992  [5.375 sec/step, loss=0.07582, avg_loss=0.07396]\n",
      "Step 527993  [5.380 sec/step, loss=0.07510, avg_loss=0.07398]\n",
      "Step 527994  [5.411 sec/step, loss=0.07309, avg_loss=0.07397]\n",
      "Step 527995  [5.390 sec/step, loss=0.07266, avg_loss=0.07396]\n",
      "Step 527996  [5.367 sec/step, loss=0.07512, avg_loss=0.07397]\n",
      "Step 527997  [5.421 sec/step, loss=0.06624, avg_loss=0.07388]\n",
      "Step 527998  [5.430 sec/step, loss=0.07544, avg_loss=0.07390]\n",
      "Step 527999  [5.458 sec/step, loss=0.07657, avg_loss=0.07395]\n",
      "Step 528000  [5.454 sec/step, loss=0.07513, avg_loss=0.07396]\n",
      "Writing summary at step: 528000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-528000\n",
      "Saving audio and alignment...\n",
      "Input: mithan sarhanddii nay faedrayshan kay saatdh ilhaaq kii vafaaoon koo iifaa karnay miin dzalddii nahayn kii~_\n",
      "Step 528001  [5.453 sec/step, loss=0.07395, avg_loss=0.07397]\n",
      "Step 528002  [5.472 sec/step, loss=0.07534, avg_loss=0.07400]\n",
      "Step 528003  [5.481 sec/step, loss=0.07428, avg_loss=0.07400]\n",
      "Generated 32 batches of size 32 in 2.386 sec\n",
      "Step 528004  [5.486 sec/step, loss=0.07388, avg_loss=0.07401]\n",
      "Step 528005  [5.467 sec/step, loss=0.07397, avg_loss=0.07402]\n",
      "Step 528006  [5.463 sec/step, loss=0.07442, avg_loss=0.07401]\n",
      "Step 528007  [5.410 sec/step, loss=0.07334, avg_loss=0.07408]\n",
      "Step 528008  [5.422 sec/step, loss=0.07543, avg_loss=0.07408]\n",
      "Step 528009  [5.433 sec/step, loss=0.07578, avg_loss=0.07408]\n",
      "Step 528010  [5.414 sec/step, loss=0.06703, avg_loss=0.07400]\n",
      "Step 528011  [5.414 sec/step, loss=0.07295, avg_loss=0.07400]\n",
      "Step 528012  [5.393 sec/step, loss=0.07202, avg_loss=0.07396]\n",
      "Step 528013  [5.385 sec/step, loss=0.07037, avg_loss=0.07391]\n",
      "Step 528014  [5.372 sec/step, loss=0.07452, avg_loss=0.07390]\n",
      "Step 528015  [5.373 sec/step, loss=0.07579, avg_loss=0.07390]\n",
      "Step 528016  [5.374 sec/step, loss=0.07532, avg_loss=0.07390]\n",
      "Step 528017  [5.400 sec/step, loss=0.07634, avg_loss=0.07402]\n",
      "Step 528018  [5.393 sec/step, loss=0.07243, avg_loss=0.07400]\n",
      "Step 528019  [5.407 sec/step, loss=0.07623, avg_loss=0.07402]\n",
      "Step 528020  [5.395 sec/step, loss=0.07331, avg_loss=0.07399]\n",
      "Step 528021  [5.390 sec/step, loss=0.07574, avg_loss=0.07402]\n",
      "Step 528022  [5.376 sec/step, loss=0.06627, avg_loss=0.07396]\n",
      "Step 528023  [5.369 sec/step, loss=0.07687, avg_loss=0.07400]\n",
      "Step 528024  [5.351 sec/step, loss=0.07401, avg_loss=0.07398]\n",
      "Step 528025  [5.324 sec/step, loss=0.07290, avg_loss=0.07394]\n",
      "Step 528026  [5.333 sec/step, loss=0.07344, avg_loss=0.07391]\n",
      "Step 528027  [5.343 sec/step, loss=0.07336, avg_loss=0.07390]\n",
      "Step 528028  [5.327 sec/step, loss=0.07109, avg_loss=0.07384]\n",
      "Step 528029  [5.314 sec/step, loss=0.07528, avg_loss=0.07385]\n",
      "Step 528030  [5.336 sec/step, loss=0.07417, avg_loss=0.07391]\n",
      "Step 528031  [5.355 sec/step, loss=0.07324, avg_loss=0.07393]\n",
      "Step 528032  [5.414 sec/step, loss=0.06622, avg_loss=0.07384]\n",
      "Step 528033  [5.406 sec/step, loss=0.07495, avg_loss=0.07382]\n",
      "Step 528034  [5.412 sec/step, loss=0.07429, avg_loss=0.07380]\n",
      "Step 528035  [5.412 sec/step, loss=0.07460, avg_loss=0.07379]\n",
      "Generated 32 batches of size 32 in 2.511 sec\n",
      "Step 528036  [5.416 sec/step, loss=0.07314, avg_loss=0.07377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 528037  [5.420 sec/step, loss=0.07546, avg_loss=0.07378]\n",
      "Step 528038  [5.402 sec/step, loss=0.07608, avg_loss=0.07380]\n",
      "Step 528039  [5.421 sec/step, loss=0.07481, avg_loss=0.07381]\n",
      "Step 528040  [5.413 sec/step, loss=0.07286, avg_loss=0.07378]\n",
      "Step 528041  [5.411 sec/step, loss=0.07408, avg_loss=0.07378]\n",
      "Step 528042  [5.423 sec/step, loss=0.07374, avg_loss=0.07379]\n",
      "Step 528043  [5.445 sec/step, loss=0.07446, avg_loss=0.07381]\n",
      "Step 528044  [5.449 sec/step, loss=0.07418, avg_loss=0.07379]\n",
      "Step 528045  [5.446 sec/step, loss=0.07481, avg_loss=0.07381]\n",
      "Step 528046  [5.449 sec/step, loss=0.07597, avg_loss=0.07381]\n",
      "Step 528047  [5.433 sec/step, loss=0.06554, avg_loss=0.07371]\n",
      "Step 528048  [5.421 sec/step, loss=0.06966, avg_loss=0.07366]\n",
      "Step 528049  [5.369 sec/step, loss=0.07464, avg_loss=0.07375]\n",
      "Step 528050  [5.384 sec/step, loss=0.07610, avg_loss=0.07377]\n",
      "Step 528051  [5.398 sec/step, loss=0.07389, avg_loss=0.07377]\n",
      "Step 528052  [5.386 sec/step, loss=0.07584, avg_loss=0.07379]\n",
      "Step 528053  [5.386 sec/step, loss=0.07378, avg_loss=0.07377]\n",
      "Step 528054  [5.374 sec/step, loss=0.07489, avg_loss=0.07376]\n",
      "Step 528055  [5.333 sec/step, loss=0.07640, avg_loss=0.07386]\n",
      "Step 528056  [5.329 sec/step, loss=0.07508, avg_loss=0.07385]\n",
      "Step 528057  [5.329 sec/step, loss=0.07159, avg_loss=0.07381]\n",
      "Step 528058  [5.330 sec/step, loss=0.07562, avg_loss=0.07382]\n",
      "Step 528059  [5.323 sec/step, loss=0.07155, avg_loss=0.07378]\n",
      "Step 528060  [5.317 sec/step, loss=0.07519, avg_loss=0.07379]\n",
      "Step 528061  [5.300 sec/step, loss=0.07294, avg_loss=0.07375]\n",
      "Step 528062  [5.341 sec/step, loss=0.07466, avg_loss=0.07385]\n",
      "Step 528063  [5.359 sec/step, loss=0.07421, avg_loss=0.07390]\n",
      "Step 528064  [5.352 sec/step, loss=0.07146, avg_loss=0.07386]\n",
      "Step 528065  [5.350 sec/step, loss=0.07282, avg_loss=0.07385]\n",
      "Step 528066  [5.337 sec/step, loss=0.07512, avg_loss=0.07385]\n",
      "Step 528067  [5.331 sec/step, loss=0.07506, avg_loss=0.07383]\n",
      "Generated 32 batches of size 32 in 2.473 sec\n",
      "Step 528068  [5.331 sec/step, loss=0.07322, avg_loss=0.07381]\n",
      "Step 528069  [5.394 sec/step, loss=0.06619, avg_loss=0.07376]\n",
      "Step 528070  [5.378 sec/step, loss=0.07407, avg_loss=0.07375]\n",
      "Step 528071  [5.377 sec/step, loss=0.07612, avg_loss=0.07377]\n",
      "Step 528072  [5.369 sec/step, loss=0.07548, avg_loss=0.07378]\n",
      "Step 528073  [5.375 sec/step, loss=0.07319, avg_loss=0.07377]\n",
      "Step 528074  [5.366 sec/step, loss=0.07424, avg_loss=0.07376]\n",
      "Step 528075  [5.369 sec/step, loss=0.07267, avg_loss=0.07374]\n",
      "Step 528076  [5.386 sec/step, loss=0.07537, avg_loss=0.07375]\n",
      "Step 528077  [5.370 sec/step, loss=0.07312, avg_loss=0.07372]\n",
      "Step 528078  [5.375 sec/step, loss=0.07509, avg_loss=0.07377]\n",
      "Step 528079  [5.384 sec/step, loss=0.07632, avg_loss=0.07379]\n",
      "Step 528080  [5.387 sec/step, loss=0.07406, avg_loss=0.07380]\n",
      "Step 528081  [5.404 sec/step, loss=0.07575, avg_loss=0.07383]\n",
      "Step 528082  [5.396 sec/step, loss=0.07507, avg_loss=0.07383]\n",
      "Step 528083  [5.439 sec/step, loss=0.06608, avg_loss=0.07374]\n",
      "Step 528084  [5.429 sec/step, loss=0.07584, avg_loss=0.07377]\n",
      "Step 528085  [5.423 sec/step, loss=0.07012, avg_loss=0.07372]\n",
      "Step 528086  [5.424 sec/step, loss=0.07414, avg_loss=0.07373]\n",
      "Step 528087  [5.409 sec/step, loss=0.06643, avg_loss=0.07368]\n",
      "Step 528088  [5.410 sec/step, loss=0.07603, avg_loss=0.07369]\n",
      "Step 528089  [5.410 sec/step, loss=0.07308, avg_loss=0.07368]\n",
      "Step 528090  [5.434 sec/step, loss=0.07344, avg_loss=0.07366]\n",
      "Step 528091  [5.414 sec/step, loss=0.07196, avg_loss=0.07361]\n",
      "Step 528092  [5.406 sec/step, loss=0.07580, avg_loss=0.07361]\n",
      "Step 528093  [5.415 sec/step, loss=0.07324, avg_loss=0.07359]\n",
      "Step 528094  [5.393 sec/step, loss=0.07456, avg_loss=0.07361]\n",
      "Step 528095  [5.420 sec/step, loss=0.07625, avg_loss=0.07364]\n",
      "Step 528096  [5.433 sec/step, loss=0.07568, avg_loss=0.07365]\n",
      "Step 528097  [5.380 sec/step, loss=0.07552, avg_loss=0.07374]\n",
      "Step 528098  [5.366 sec/step, loss=0.07456, avg_loss=0.07373]\n",
      "Step 528099  [5.365 sec/step, loss=0.07329, avg_loss=0.07370]\n",
      "Generated 32 batches of size 32 in 2.565 sec\n",
      "Step 528100  [5.363 sec/step, loss=0.07345, avg_loss=0.07368]\n",
      "Writing summary at step: 528100\n",
      "Step 528101  [5.372 sec/step, loss=0.07497, avg_loss=0.07369]\n",
      "Step 528102  [5.366 sec/step, loss=0.07236, avg_loss=0.07366]\n",
      "Step 528103  [5.378 sec/step, loss=0.07575, avg_loss=0.07368]\n",
      "Step 528104  [5.351 sec/step, loss=0.07141, avg_loss=0.07365]\n",
      "Step 528105  [5.349 sec/step, loss=0.07146, avg_loss=0.07363]\n",
      "Step 528106  [5.350 sec/step, loss=0.07212, avg_loss=0.07360]\n",
      "Step 528107  [5.363 sec/step, loss=0.07621, avg_loss=0.07363]\n",
      "Step 528108  [5.354 sec/step, loss=0.07545, avg_loss=0.07363]\n",
      "Step 528109  [5.339 sec/step, loss=0.07377, avg_loss=0.07361]\n",
      "Step 528110  [5.361 sec/step, loss=0.07444, avg_loss=0.07369]\n",
      "Step 528111  [5.364 sec/step, loss=0.07217, avg_loss=0.07368]\n",
      "Step 528112  [5.360 sec/step, loss=0.07101, avg_loss=0.07367]\n",
      "Step 528113  [5.377 sec/step, loss=0.07683, avg_loss=0.07373]\n",
      "Step 528114  [5.398 sec/step, loss=0.07365, avg_loss=0.07373]\n",
      "Step 528115  [5.399 sec/step, loss=0.07475, avg_loss=0.07371]\n",
      "Step 528116  [5.389 sec/step, loss=0.07376, avg_loss=0.07370]\n",
      "Step 528117  [5.385 sec/step, loss=0.07600, avg_loss=0.07370]\n",
      "Step 528118  [5.389 sec/step, loss=0.07588, avg_loss=0.07373]\n",
      "Step 528119  [5.376 sec/step, loss=0.07440, avg_loss=0.07371]\n",
      "Step 528120  [5.376 sec/step, loss=0.07190, avg_loss=0.07370]\n",
      "Step 528121  [5.374 sec/step, loss=0.07536, avg_loss=0.07369]\n",
      "Step 528122  [5.373 sec/step, loss=0.06468, avg_loss=0.07368]\n",
      "Step 528123  [5.373 sec/step, loss=0.07575, avg_loss=0.07367]\n",
      "Step 528124  [5.378 sec/step, loss=0.07556, avg_loss=0.07368]\n",
      "Step 528125  [5.438 sec/step, loss=0.06674, avg_loss=0.07362]\n",
      "Step 528126  [5.412 sec/step, loss=0.07449, avg_loss=0.07363]\n",
      "Step 528127  [5.419 sec/step, loss=0.07608, avg_loss=0.07366]\n",
      "Step 528128  [5.425 sec/step, loss=0.07534, avg_loss=0.07370]\n",
      "Step 528129  [5.440 sec/step, loss=0.07301, avg_loss=0.07368]\n",
      "Step 528130  [5.462 sec/step, loss=0.07349, avg_loss=0.07367]\n",
      "Generated 32 batches of size 32 in 2.411 sec\n",
      "Step 528131  [5.463 sec/step, loss=0.07428, avg_loss=0.07368]\n",
      "Step 528132  [5.416 sec/step, loss=0.07461, avg_loss=0.07377]\n",
      "Step 528133  [5.425 sec/step, loss=0.07739, avg_loss=0.07379]\n",
      "Step 528134  [5.402 sec/step, loss=0.06971, avg_loss=0.07374]\n",
      "Step 528135  [5.390 sec/step, loss=0.07426, avg_loss=0.07374]\n",
      "Step 528136  [5.384 sec/step, loss=0.07080, avg_loss=0.07372]\n",
      "Step 528137  [5.373 sec/step, loss=0.07263, avg_loss=0.07369]\n",
      "Step 528138  [5.372 sec/step, loss=0.07318, avg_loss=0.07366]\n",
      "Step 528139  [5.376 sec/step, loss=0.07597, avg_loss=0.07367]\n",
      "Step 528140  [5.402 sec/step, loss=0.07619, avg_loss=0.07371]\n",
      "Step 528141  [5.421 sec/step, loss=0.07325, avg_loss=0.07370]\n",
      "Step 528142  [5.415 sec/step, loss=0.07526, avg_loss=0.07371]\n",
      "Step 528143  [5.413 sec/step, loss=0.07547, avg_loss=0.07372]\n",
      "Step 528144  [5.415 sec/step, loss=0.07507, avg_loss=0.07373]\n",
      "Step 528145  [5.415 sec/step, loss=0.07517, avg_loss=0.07373]\n",
      "Step 528146  [5.409 sec/step, loss=0.07426, avg_loss=0.07372]\n",
      "Step 528147  [5.415 sec/step, loss=0.07170, avg_loss=0.07378]\n",
      "Step 528148  [5.421 sec/step, loss=0.07543, avg_loss=0.07384]\n",
      "Step 528149  [5.414 sec/step, loss=0.07355, avg_loss=0.07383]\n",
      "Step 528150  [5.407 sec/step, loss=0.07504, avg_loss=0.07382]\n",
      "Step 528151  [5.406 sec/step, loss=0.07613, avg_loss=0.07384]\n",
      "Step 528152  [5.403 sec/step, loss=0.07598, avg_loss=0.07384]\n",
      "Step 528153  [5.391 sec/step, loss=0.07547, avg_loss=0.07386]\n",
      "Step 528154  [5.434 sec/step, loss=0.06497, avg_loss=0.07376]\n",
      "Step 528155  [5.423 sec/step, loss=0.07226, avg_loss=0.07372]\n",
      "Step 528156  [5.415 sec/step, loss=0.07374, avg_loss=0.07370]\n",
      "Step 528157  [5.434 sec/step, loss=0.07585, avg_loss=0.07374]\n",
      "Step 528158  [5.444 sec/step, loss=0.07633, avg_loss=0.07375]\n",
      "Step 528159  [5.440 sec/step, loss=0.07457, avg_loss=0.07378]\n",
      "Step 528160  [5.436 sec/step, loss=0.07406, avg_loss=0.07377]\n",
      "Step 528161  [5.438 sec/step, loss=0.07407, avg_loss=0.07378]\n",
      "Step 528162  [5.417 sec/step, loss=0.07525, avg_loss=0.07379]\n",
      "Generated 32 batches of size 32 in 2.805 sec\n",
      "Step 528163  [5.400 sec/step, loss=0.07156, avg_loss=0.07376]\n",
      "Step 528164  [5.415 sec/step, loss=0.07297, avg_loss=0.07378]\n",
      "Step 528165  [5.411 sec/step, loss=0.07358, avg_loss=0.07378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 528166  [5.418 sec/step, loss=0.07512, avg_loss=0.07378]\n",
      "Step 528167  [5.413 sec/step, loss=0.07495, avg_loss=0.07378]\n",
      "Step 528168  [5.440 sec/step, loss=0.07522, avg_loss=0.07380]\n",
      "Step 528169  [5.375 sec/step, loss=0.06516, avg_loss=0.07379]\n",
      "Step 528170  [5.386 sec/step, loss=0.07497, avg_loss=0.07380]\n",
      "Step 528171  [5.386 sec/step, loss=0.07652, avg_loss=0.07381]\n",
      "Step 528172  [5.390 sec/step, loss=0.07553, avg_loss=0.07381]\n",
      "Step 528173  [5.389 sec/step, loss=0.07391, avg_loss=0.07381]\n",
      "Step 528174  [5.390 sec/step, loss=0.07354, avg_loss=0.07381]\n",
      "Step 528175  [5.390 sec/step, loss=0.07485, avg_loss=0.07383]\n",
      "Step 528176  [5.385 sec/step, loss=0.07672, avg_loss=0.07384]\n",
      "Step 528177  [5.389 sec/step, loss=0.07464, avg_loss=0.07386]\n",
      "Step 528178  [5.394 sec/step, loss=0.07612, avg_loss=0.07387]\n",
      "Step 528179  [5.376 sec/step, loss=0.07084, avg_loss=0.07381]\n",
      "Step 528180  [5.374 sec/step, loss=0.07484, avg_loss=0.07382]\n",
      "Step 528181  [5.376 sec/step, loss=0.07308, avg_loss=0.07379]\n",
      "Step 528182  [5.382 sec/step, loss=0.07296, avg_loss=0.07377]\n",
      "Step 528183  [5.343 sec/step, loss=0.07389, avg_loss=0.07385]\n",
      "Step 528184  [5.330 sec/step, loss=0.07087, avg_loss=0.07380]\n",
      "Step 528185  [5.329 sec/step, loss=0.07128, avg_loss=0.07381]\n",
      "Step 528186  [5.340 sec/step, loss=0.07619, avg_loss=0.07383]\n",
      "Step 528187  [5.360 sec/step, loss=0.07506, avg_loss=0.07392]\n",
      "Step 528188  [5.363 sec/step, loss=0.07629, avg_loss=0.07392]\n",
      "Step 528189  [5.387 sec/step, loss=0.07522, avg_loss=0.07394]\n",
      "Step 528190  [5.357 sec/step, loss=0.07520, avg_loss=0.07396]\n",
      "Step 528191  [5.359 sec/step, loss=0.07360, avg_loss=0.07398]\n",
      "Step 528192  [5.342 sec/step, loss=0.06695, avg_loss=0.07389]\n",
      "Step 528193  [5.381 sec/step, loss=0.06585, avg_loss=0.07381]\n",
      "Step 528194  [5.403 sec/step, loss=0.07222, avg_loss=0.07379]\n",
      "Generated 32 batches of size 32 in 2.351 sec\n",
      "Step 528195  [5.398 sec/step, loss=0.07558, avg_loss=0.07378]\n",
      "Step 528196  [5.394 sec/step, loss=0.07672, avg_loss=0.07380]\n",
      "Step 528197  [5.396 sec/step, loss=0.07557, avg_loss=0.07380]\n",
      "Step 528198  [5.407 sec/step, loss=0.07321, avg_loss=0.07378]\n",
      "Step 528199  [5.393 sec/step, loss=0.07330, avg_loss=0.07378]\n",
      "Step 528200  [5.383 sec/step, loss=0.07381, avg_loss=0.07379]\n",
      "Writing summary at step: 528200\n",
      "Step 528201  [5.374 sec/step, loss=0.07434, avg_loss=0.07378]\n",
      "Step 528202  [5.359 sec/step, loss=0.07235, avg_loss=0.07378]\n",
      "Step 528203  [5.351 sec/step, loss=0.07527, avg_loss=0.07377]\n",
      "Step 528204  [5.366 sec/step, loss=0.07499, avg_loss=0.07381]\n",
      "Step 528205  [5.418 sec/step, loss=0.06750, avg_loss=0.07377]\n",
      "Step 528206  [5.422 sec/step, loss=0.07576, avg_loss=0.07381]\n",
      "Step 528207  [5.413 sec/step, loss=0.07537, avg_loss=0.07380]\n",
      "Step 528208  [5.397 sec/step, loss=0.07204, avg_loss=0.07376]\n",
      "Step 528209  [5.400 sec/step, loss=0.07395, avg_loss=0.07377]\n",
      "Step 528210  [5.404 sec/step, loss=0.07627, avg_loss=0.07378]\n",
      "Step 528211  [5.400 sec/step, loss=0.07102, avg_loss=0.07377]\n",
      "Step 528212  [5.410 sec/step, loss=0.07431, avg_loss=0.07381]\n",
      "Step 528213  [5.403 sec/step, loss=0.07435, avg_loss=0.07378]\n",
      "Step 528214  [5.393 sec/step, loss=0.07453, avg_loss=0.07379]\n",
      "Step 528215  [5.378 sec/step, loss=0.07531, avg_loss=0.07380]\n",
      "Step 528216  [5.383 sec/step, loss=0.07307, avg_loss=0.07379]\n",
      "Step 528217  [5.365 sec/step, loss=0.07217, avg_loss=0.07375]\n",
      "Step 528218  [5.369 sec/step, loss=0.07568, avg_loss=0.07375]\n",
      "Step 528219  [5.370 sec/step, loss=0.07198, avg_loss=0.07372]\n",
      "Step 528220  [5.384 sec/step, loss=0.07542, avg_loss=0.07376]\n",
      "Step 528221  [5.400 sec/step, loss=0.07572, avg_loss=0.07376]\n",
      "Step 528222  [5.398 sec/step, loss=0.06683, avg_loss=0.07378]\n",
      "Step 528223  [5.398 sec/step, loss=0.07390, avg_loss=0.07377]\n",
      "Step 528224  [5.393 sec/step, loss=0.07337, avg_loss=0.07374]\n",
      "Step 528225  [5.340 sec/step, loss=0.07527, avg_loss=0.07383]\n",
      "Generated 32 batches of size 32 in 2.358 sec\n",
      "Step 528226  [5.348 sec/step, loss=0.07249, avg_loss=0.07381]\n",
      "Step 528227  [5.329 sec/step, loss=0.07372, avg_loss=0.07379]\n",
      "Step 528228  [5.346 sec/step, loss=0.07641, avg_loss=0.07380]\n",
      "Step 528229  [5.325 sec/step, loss=0.07061, avg_loss=0.07377]\n",
      "Step 528230  [5.323 sec/step, loss=0.07365, avg_loss=0.07377]\n",
      "Step 528231  [5.318 sec/step, loss=0.07613, avg_loss=0.07379]\n",
      "Step 528232  [5.339 sec/step, loss=0.07348, avg_loss=0.07378]\n",
      "Step 528233  [5.330 sec/step, loss=0.07475, avg_loss=0.07376]\n",
      "Step 528234  [5.355 sec/step, loss=0.07625, avg_loss=0.07382]\n",
      "Step 528235  [5.358 sec/step, loss=0.07346, avg_loss=0.07381]\n",
      "Step 528236  [5.350 sec/step, loss=0.07361, avg_loss=0.07384]\n",
      "Step 528237  [5.357 sec/step, loss=0.07432, avg_loss=0.07386]\n",
      "Step 528238  [5.356 sec/step, loss=0.07411, avg_loss=0.07387]\n",
      "Step 528239  [5.329 sec/step, loss=0.07126, avg_loss=0.07382]\n",
      "Step 528240  [5.315 sec/step, loss=0.07330, avg_loss=0.07379]\n",
      "Step 528241  [5.317 sec/step, loss=0.07559, avg_loss=0.07381]\n",
      "Step 528242  [5.296 sec/step, loss=0.07411, avg_loss=0.07380]\n",
      "Step 528243  [5.298 sec/step, loss=0.07601, avg_loss=0.07381]\n",
      "Step 528244  [5.299 sec/step, loss=0.07561, avg_loss=0.07381]\n",
      "Step 528245  [5.299 sec/step, loss=0.07118, avg_loss=0.07377]\n",
      "Step 528246  [5.293 sec/step, loss=0.07502, avg_loss=0.07378]\n",
      "Step 528247  [5.305 sec/step, loss=0.07379, avg_loss=0.07380]\n",
      "Step 528248  [5.308 sec/step, loss=0.07470, avg_loss=0.07379]\n",
      "Step 528249  [5.306 sec/step, loss=0.07047, avg_loss=0.07376]\n",
      "Step 528250  [5.309 sec/step, loss=0.07628, avg_loss=0.07378]\n",
      "Step 528251  [5.310 sec/step, loss=0.07653, avg_loss=0.07378]\n",
      "Step 528252  [5.307 sec/step, loss=0.07570, avg_loss=0.07378]\n",
      "Step 528253  [5.300 sec/step, loss=0.07291, avg_loss=0.07375]\n",
      "Step 528254  [5.246 sec/step, loss=0.07493, avg_loss=0.07385]\n",
      "Step 528255  [5.250 sec/step, loss=0.07545, avg_loss=0.07388]\n",
      "Step 528256  [5.258 sec/step, loss=0.07435, avg_loss=0.07389]\n",
      "Step 528257  [5.290 sec/step, loss=0.06599, avg_loss=0.07379]\n",
      "Generated 32 batches of size 32 in 2.558 sec\n",
      "Step 528258  [5.279 sec/step, loss=0.07342, avg_loss=0.07376]\n",
      "Step 528259  [5.293 sec/step, loss=0.07462, avg_loss=0.07376]\n",
      "Step 528260  [5.308 sec/step, loss=0.07437, avg_loss=0.07377]\n",
      "Step 528261  [5.333 sec/step, loss=0.07484, avg_loss=0.07377]\n",
      "Step 528262  [5.337 sec/step, loss=0.07527, avg_loss=0.07377]\n",
      "Step 528263  [5.371 sec/step, loss=0.07477, avg_loss=0.07381]\n",
      "Step 528264  [5.382 sec/step, loss=0.07573, avg_loss=0.07383]\n",
      "Step 528265  [5.400 sec/step, loss=0.07383, avg_loss=0.07384]\n",
      "Step 528266  [5.377 sec/step, loss=0.06530, avg_loss=0.07374]\n",
      "Step 528267  [5.374 sec/step, loss=0.07199, avg_loss=0.07371]\n",
      "Step 528268  [5.341 sec/step, loss=0.07178, avg_loss=0.07367]\n",
      "Step 528269  [5.364 sec/step, loss=0.07546, avg_loss=0.07378]\n",
      "Step 528270  [5.349 sec/step, loss=0.07192, avg_loss=0.07375]\n",
      "Step 528271  [5.349 sec/step, loss=0.07608, avg_loss=0.07374]\n",
      "Step 528272  [5.344 sec/step, loss=0.07517, avg_loss=0.07374]\n",
      "Step 528273  [5.358 sec/step, loss=0.07507, avg_loss=0.07375]\n",
      "Step 528274  [5.352 sec/step, loss=0.07367, avg_loss=0.07375]\n",
      "Step 528275  [5.332 sec/step, loss=0.06609, avg_loss=0.07366]\n",
      "Step 528276  [5.347 sec/step, loss=0.07328, avg_loss=0.07363]\n",
      "Step 528277  [5.348 sec/step, loss=0.07549, avg_loss=0.07364]\n",
      "Step 528278  [5.390 sec/step, loss=0.06676, avg_loss=0.07354]\n",
      "Step 528279  [5.391 sec/step, loss=0.07571, avg_loss=0.07359]\n",
      "Step 528280  [5.394 sec/step, loss=0.07469, avg_loss=0.07359]\n",
      "Step 528281  [5.384 sec/step, loss=0.07202, avg_loss=0.07358]\n",
      "Step 528282  [5.379 sec/step, loss=0.07524, avg_loss=0.07360]\n",
      "Step 528283  [5.377 sec/step, loss=0.07616, avg_loss=0.07363]\n",
      "Step 528284  [5.373 sec/step, loss=0.07356, avg_loss=0.07365]\n",
      "Step 528285  [5.397 sec/step, loss=0.07684, avg_loss=0.07371]\n",
      "Step 528286  [5.376 sec/step, loss=0.07382, avg_loss=0.07368]\n",
      "Step 528287  [5.384 sec/step, loss=0.07460, avg_loss=0.07368]\n",
      "Step 528288  [5.380 sec/step, loss=0.07519, avg_loss=0.07367]\n",
      "Step 528289  [5.356 sec/step, loss=0.07364, avg_loss=0.07365]\n",
      "Generated 32 batches of size 32 in 2.438 sec\n",
      "Step 528290  [5.377 sec/step, loss=0.07617, avg_loss=0.07366]\n",
      "Step 528291  [5.395 sec/step, loss=0.07549, avg_loss=0.07368]\n",
      "Step 528292  [5.416 sec/step, loss=0.07518, avg_loss=0.07376]\n",
      "Step 528293  [5.386 sec/step, loss=0.07422, avg_loss=0.07385]\n",
      "Step 528294  [5.357 sec/step, loss=0.07138, avg_loss=0.07384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 528295  [5.349 sec/step, loss=0.07383, avg_loss=0.07382]\n",
      "Step 528296  [5.347 sec/step, loss=0.07606, avg_loss=0.07382]\n",
      "Step 528297  [5.349 sec/step, loss=0.07224, avg_loss=0.07378]\n",
      "Step 528298  [5.349 sec/step, loss=0.07483, avg_loss=0.07380]\n",
      "Step 528299  [5.347 sec/step, loss=0.07264, avg_loss=0.07379]\n",
      "Step 528300  [5.358 sec/step, loss=0.07502, avg_loss=0.07380]\n",
      "Writing summary at step: 528300\n",
      "Step 528301  [5.347 sec/step, loss=0.06547, avg_loss=0.07371]\n",
      "Step 528302  [5.364 sec/step, loss=0.07237, avg_loss=0.07372]\n",
      "Step 528303  [5.374 sec/step, loss=0.07630, avg_loss=0.07373]\n",
      "Step 528304  [5.377 sec/step, loss=0.07466, avg_loss=0.07372]\n",
      "Step 528305  [5.338 sec/step, loss=0.07406, avg_loss=0.07379]\n",
      "Step 528306  [5.327 sec/step, loss=0.07383, avg_loss=0.07377]\n",
      "Step 528307  [5.315 sec/step, loss=0.07010, avg_loss=0.07372]\n",
      "Step 528308  [5.329 sec/step, loss=0.07524, avg_loss=0.07375]\n",
      "Step 528309  [5.316 sec/step, loss=0.07105, avg_loss=0.07372]\n",
      "Step 528310  [5.315 sec/step, loss=0.07601, avg_loss=0.07372]\n",
      "Step 528311  [5.314 sec/step, loss=0.07377, avg_loss=0.07374]\n",
      "Step 528312  [5.319 sec/step, loss=0.07138, avg_loss=0.07371]\n",
      "Step 528313  [5.313 sec/step, loss=0.07441, avg_loss=0.07371]\n",
      "Step 528314  [5.351 sec/step, loss=0.06666, avg_loss=0.07364]\n",
      "Step 528315  [5.351 sec/step, loss=0.07198, avg_loss=0.07360]\n",
      "Step 528316  [5.347 sec/step, loss=0.07324, avg_loss=0.07360]\n",
      "Step 528317  [5.371 sec/step, loss=0.07610, avg_loss=0.07364]\n",
      "Step 528318  [5.363 sec/step, loss=0.07508, avg_loss=0.07364]\n",
      "Step 528319  [5.366 sec/step, loss=0.07481, avg_loss=0.07367]\n",
      "Step 528320  [5.371 sec/step, loss=0.07625, avg_loss=0.07367]\n",
      "Generated 32 batches of size 32 in 2.383 sec\n",
      "Step 528321  [5.365 sec/step, loss=0.07547, avg_loss=0.07367]\n",
      "Step 528322  [5.375 sec/step, loss=0.07431, avg_loss=0.07375]\n",
      "Step 528323  [5.369 sec/step, loss=0.07601, avg_loss=0.07377]\n",
      "Step 528324  [5.382 sec/step, loss=0.07680, avg_loss=0.07380]\n",
      "Step 528325  [5.375 sec/step, loss=0.07442, avg_loss=0.07379]\n",
      "Step 528326  [5.389 sec/step, loss=0.07347, avg_loss=0.07380]\n",
      "Step 528327  [5.409 sec/step, loss=0.07353, avg_loss=0.07380]\n",
      "Step 528328  [5.391 sec/step, loss=0.07508, avg_loss=0.07379]\n",
      "Step 528329  [5.409 sec/step, loss=0.07544, avg_loss=0.07384]\n",
      "Step 528330  [5.369 sec/step, loss=0.06647, avg_loss=0.07376]\n",
      "Step 528331  [5.367 sec/step, loss=0.07577, avg_loss=0.07376]\n",
      "Step 528332  [5.339 sec/step, loss=0.07346, avg_loss=0.07376]\n",
      "Step 528333  [5.337 sec/step, loss=0.07372, avg_loss=0.07375]\n",
      "Step 528334  [5.315 sec/step, loss=0.07405, avg_loss=0.07373]\n",
      "Step 528335  [5.328 sec/step, loss=0.07598, avg_loss=0.07375]\n",
      "Step 528336  [5.345 sec/step, loss=0.07460, avg_loss=0.07376]\n",
      "Step 528337  [5.353 sec/step, loss=0.07442, avg_loss=0.07376]\n",
      "Step 528338  [5.361 sec/step, loss=0.07570, avg_loss=0.07378]\n",
      "Step 528339  [5.378 sec/step, loss=0.07357, avg_loss=0.07380]\n",
      "Step 528340  [5.428 sec/step, loss=0.06566, avg_loss=0.07373]\n",
      "Step 528341  [5.440 sec/step, loss=0.07537, avg_loss=0.07373]\n",
      "Step 528342  [5.458 sec/step, loss=0.07347, avg_loss=0.07372]\n",
      "Step 528343  [5.447 sec/step, loss=0.07186, avg_loss=0.07368]\n",
      "Step 528344  [5.438 sec/step, loss=0.07307, avg_loss=0.07365]\n",
      "Step 528345  [5.451 sec/step, loss=0.07596, avg_loss=0.07370]\n",
      "Step 528346  [5.454 sec/step, loss=0.07488, avg_loss=0.07370]\n",
      "Step 528347  [5.444 sec/step, loss=0.07113, avg_loss=0.07367]\n",
      "Step 528348  [5.446 sec/step, loss=0.07171, avg_loss=0.07364]\n",
      "Step 528349  [5.458 sec/step, loss=0.07606, avg_loss=0.07370]\n",
      "Step 528350  [5.448 sec/step, loss=0.07337, avg_loss=0.07367]\n",
      "Step 528351  [5.438 sec/step, loss=0.07512, avg_loss=0.07365]\n",
      "Step 528352  [5.427 sec/step, loss=0.07578, avg_loss=0.07366]\n",
      "Generated 32 batches of size 32 in 2.425 sec\n",
      "Step 528353  [5.450 sec/step, loss=0.07615, avg_loss=0.07369]\n",
      "Step 528354  [5.473 sec/step, loss=0.07471, avg_loss=0.07369]\n",
      "Step 528355  [5.475 sec/step, loss=0.07489, avg_loss=0.07368]\n",
      "Step 528356  [5.468 sec/step, loss=0.07483, avg_loss=0.07368]\n",
      "Step 528357  [5.424 sec/step, loss=0.07544, avg_loss=0.07378]\n",
      "Step 528358  [5.413 sec/step, loss=0.07219, avg_loss=0.07377]\n",
      "Step 528359  [5.396 sec/step, loss=0.07231, avg_loss=0.07374]\n",
      "Step 528360  [5.394 sec/step, loss=0.07350, avg_loss=0.07373]\n",
      "Step 528361  [5.385 sec/step, loss=0.07624, avg_loss=0.07375]\n",
      "Step 528362  [5.381 sec/step, loss=0.07557, avg_loss=0.07375]\n",
      "Step 528363  [5.355 sec/step, loss=0.07413, avg_loss=0.07375]\n",
      "Step 528364  [5.334 sec/step, loss=0.07012, avg_loss=0.07369]\n",
      "Step 528365  [5.305 sec/step, loss=0.06583, avg_loss=0.07361]\n",
      "Step 528366  [5.310 sec/step, loss=0.07366, avg_loss=0.07369]\n",
      "Step 528367  [5.317 sec/step, loss=0.07456, avg_loss=0.07372]\n",
      "Step 528368  [5.345 sec/step, loss=0.07259, avg_loss=0.07373]\n",
      "Step 528369  [5.343 sec/step, loss=0.07562, avg_loss=0.07373]\n",
      "Step 528370  [5.382 sec/step, loss=0.07544, avg_loss=0.07376]\n",
      "Step 528371  [5.377 sec/step, loss=0.07131, avg_loss=0.07372]\n",
      "Step 528372  [5.378 sec/step, loss=0.07272, avg_loss=0.07369]\n",
      "Step 528373  [5.357 sec/step, loss=0.07080, avg_loss=0.07365]\n",
      "Step 528374  [5.367 sec/step, loss=0.07409, avg_loss=0.07365]\n",
      "Step 528375  [5.383 sec/step, loss=0.07588, avg_loss=0.07375]\n",
      "Step 528376  [5.367 sec/step, loss=0.07320, avg_loss=0.07375]\n",
      "Step 528377  [5.377 sec/step, loss=0.07607, avg_loss=0.07376]\n",
      "Step 528378  [5.325 sec/step, loss=0.07480, avg_loss=0.07384]\n",
      "Step 528379  [5.321 sec/step, loss=0.07329, avg_loss=0.07381]\n",
      "Step 528380  [5.306 sec/step, loss=0.07205, avg_loss=0.07379]\n",
      "Step 528381  [5.300 sec/step, loss=0.07138, avg_loss=0.07378]\n",
      "Step 528382  [5.307 sec/step, loss=0.07559, avg_loss=0.07378]\n",
      "Step 528383  [5.299 sec/step, loss=0.07535, avg_loss=0.07377]\n",
      "Step 528384  [5.314 sec/step, loss=0.07393, avg_loss=0.07378]\n",
      "Generated 32 batches of size 32 in 2.307 sec\n",
      "Step 528385  [5.310 sec/step, loss=0.07603, avg_loss=0.07377]\n",
      "Step 528386  [5.332 sec/step, loss=0.07546, avg_loss=0.07379]\n",
      "Step 528387  [5.371 sec/step, loss=0.06536, avg_loss=0.07369]\n",
      "Step 528388  [5.378 sec/step, loss=0.07632, avg_loss=0.07371]\n",
      "Step 528389  [5.384 sec/step, loss=0.07550, avg_loss=0.07372]\n",
      "Step 528390  [5.362 sec/step, loss=0.07031, avg_loss=0.07367]\n",
      "Step 528391  [5.361 sec/step, loss=0.07565, avg_loss=0.07367]\n",
      "Step 528392  [5.353 sec/step, loss=0.07407, avg_loss=0.07366]\n",
      "Step 528393  [5.335 sec/step, loss=0.07309, avg_loss=0.07364]\n",
      "Step 528394  [5.329 sec/step, loss=0.07335, avg_loss=0.07366]\n",
      "Step 528395  [5.338 sec/step, loss=0.07621, avg_loss=0.07369]\n",
      "Step 528396  [5.326 sec/step, loss=0.07200, avg_loss=0.07365]\n",
      "Step 528397  [5.328 sec/step, loss=0.07391, avg_loss=0.07366]\n",
      "Step 528398  [5.335 sec/step, loss=0.07416, avg_loss=0.07366]\n",
      "Step 528399  [5.348 sec/step, loss=0.07302, avg_loss=0.07366]\n",
      "Step 528400  [5.374 sec/step, loss=0.07297, avg_loss=0.07364]\n",
      "Writing summary at step: 528400\n",
      "Step 528401  [5.396 sec/step, loss=0.07501, avg_loss=0.07374]\n",
      "Step 528402  [5.379 sec/step, loss=0.07227, avg_loss=0.07374]\n",
      "Step 528403  [5.357 sec/step, loss=0.07304, avg_loss=0.07370]\n",
      "Step 528404  [5.366 sec/step, loss=0.07567, avg_loss=0.07371]\n",
      "Step 528405  [5.361 sec/step, loss=0.07457, avg_loss=0.07372]\n",
      "Step 528406  [5.374 sec/step, loss=0.07614, avg_loss=0.07374]\n",
      "Step 528407  [5.386 sec/step, loss=0.07464, avg_loss=0.07379]\n",
      "Step 528408  [5.394 sec/step, loss=0.07557, avg_loss=0.07379]\n",
      "Step 528409  [5.411 sec/step, loss=0.07588, avg_loss=0.07384]\n",
      "Step 528410  [5.407 sec/step, loss=0.07342, avg_loss=0.07381]\n",
      "Step 528411  [5.411 sec/step, loss=0.07495, avg_loss=0.07382]\n",
      "Step 528412  [5.411 sec/step, loss=0.07466, avg_loss=0.07386]\n",
      "Step 528413  [5.402 sec/step, loss=0.07118, avg_loss=0.07382]\n",
      "Step 528414  [5.349 sec/step, loss=0.07335, avg_loss=0.07389]\n",
      "Step 528415  [5.348 sec/step, loss=0.07253, avg_loss=0.07390]\n",
      "Generated 32 batches of size 32 in 2.362 sec\n",
      "Step 528416  [5.372 sec/step, loss=0.07592, avg_loss=0.07392]\n",
      "Step 528417  [5.361 sec/step, loss=0.07464, avg_loss=0.07391]\n",
      "Step 528418  [5.371 sec/step, loss=0.07606, avg_loss=0.07392]\n",
      "Step 528419  [5.363 sec/step, loss=0.07074, avg_loss=0.07388]\n",
      "Step 528420  [5.347 sec/step, loss=0.07512, avg_loss=0.07387]\n",
      "Step 528421  [5.391 sec/step, loss=0.06719, avg_loss=0.07378]\n",
      "Step 528422  [5.397 sec/step, loss=0.07403, avg_loss=0.07378]\n",
      "Step 528423  [5.373 sec/step, loss=0.06634, avg_loss=0.07368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 528424  [5.385 sec/step, loss=0.07328, avg_loss=0.07365]\n",
      "Step 528425  [5.386 sec/step, loss=0.07142, avg_loss=0.07362]\n",
      "Step 528426  [5.369 sec/step, loss=0.07515, avg_loss=0.07364]\n",
      "Step 528427  [5.405 sec/step, loss=0.06541, avg_loss=0.07356]\n",
      "Step 528428  [5.412 sec/step, loss=0.07217, avg_loss=0.07353]\n",
      "Step 528429  [5.418 sec/step, loss=0.07606, avg_loss=0.07353]\n",
      "Step 528430  [5.424 sec/step, loss=0.07370, avg_loss=0.07360]\n",
      "Step 528431  [5.432 sec/step, loss=0.07631, avg_loss=0.07361]\n",
      "Step 528432  [5.423 sec/step, loss=0.07074, avg_loss=0.07358]\n",
      "Step 528433  [5.421 sec/step, loss=0.07259, avg_loss=0.07357]\n",
      "Step 528434  [5.427 sec/step, loss=0.07495, avg_loss=0.07358]\n",
      "Step 528435  [5.411 sec/step, loss=0.07396, avg_loss=0.07356]\n",
      "Step 528436  [5.416 sec/step, loss=0.07379, avg_loss=0.07355]\n",
      "Step 528437  [5.393 sec/step, loss=0.06580, avg_loss=0.07347]\n",
      "Step 528438  [5.387 sec/step, loss=0.07608, avg_loss=0.07347]\n",
      "Step 528439  [5.392 sec/step, loss=0.07580, avg_loss=0.07349]\n",
      "Step 528440  [5.347 sec/step, loss=0.07549, avg_loss=0.07359]\n",
      "Step 528441  [5.323 sec/step, loss=0.07461, avg_loss=0.07358]\n",
      "Step 528442  [5.320 sec/step, loss=0.07627, avg_loss=0.07361]\n",
      "Step 528443  [5.323 sec/step, loss=0.07533, avg_loss=0.07365]\n",
      "Step 528444  [5.356 sec/step, loss=0.07283, avg_loss=0.07364]\n",
      "Step 528445  [5.363 sec/step, loss=0.07601, avg_loss=0.07364]\n",
      "Step 528446  [5.363 sec/step, loss=0.07148, avg_loss=0.07361]\n",
      "Step 528447  [5.378 sec/step, loss=0.07563, avg_loss=0.07365]\n",
      "Generated 32 batches of size 32 in 2.496 sec\n",
      "Step 528448  [5.378 sec/step, loss=0.07358, avg_loss=0.07367]\n",
      "Step 528449  [5.381 sec/step, loss=0.07398, avg_loss=0.07365]\n",
      "Step 528450  [5.385 sec/step, loss=0.07407, avg_loss=0.07366]\n",
      "Step 528451  [5.400 sec/step, loss=0.07650, avg_loss=0.07367]\n",
      "Step 528452  [5.399 sec/step, loss=0.07220, avg_loss=0.07364]\n",
      "Step 528453  [5.391 sec/step, loss=0.07338, avg_loss=0.07361]\n",
      "Step 528454  [5.375 sec/step, loss=0.07513, avg_loss=0.07361]\n",
      "Step 528455  [5.365 sec/step, loss=0.07392, avg_loss=0.07360]\n",
      "Step 528456  [5.364 sec/step, loss=0.07537, avg_loss=0.07361]\n",
      "Step 528457  [5.407 sec/step, loss=0.06682, avg_loss=0.07352]\n",
      "Step 528458  [5.420 sec/step, loss=0.07533, avg_loss=0.07355]\n",
      "Step 528459  [5.441 sec/step, loss=0.07362, avg_loss=0.07357]\n",
      "Step 528460  [5.442 sec/step, loss=0.07595, avg_loss=0.07359]\n",
      "Step 528461  [5.438 sec/step, loss=0.07471, avg_loss=0.07358]\n",
      "Step 528462  [5.437 sec/step, loss=0.07471, avg_loss=0.07357]\n",
      "Step 528463  [5.445 sec/step, loss=0.07442, avg_loss=0.07357]\n",
      "Step 528464  [5.462 sec/step, loss=0.07629, avg_loss=0.07363]\n",
      "Step 528465  [5.464 sec/step, loss=0.07174, avg_loss=0.07369]\n",
      "Step 528466  [5.457 sec/step, loss=0.06567, avg_loss=0.07361]\n",
      "Step 528467  [5.452 sec/step, loss=0.07075, avg_loss=0.07357]\n",
      "Step 528468  [5.435 sec/step, loss=0.07546, avg_loss=0.07360]\n",
      "Step 528469  [5.425 sec/step, loss=0.07500, avg_loss=0.07360]\n",
      "Step 528470  [5.416 sec/step, loss=0.07638, avg_loss=0.07361]\n",
      "Step 528471  [5.412 sec/step, loss=0.07422, avg_loss=0.07364]\n",
      "Step 528472  [5.424 sec/step, loss=0.07588, avg_loss=0.07367]\n",
      "Step 528473  [5.440 sec/step, loss=0.07394, avg_loss=0.07370]\n",
      "Step 528474  [5.436 sec/step, loss=0.07286, avg_loss=0.07369]\n",
      "Step 528475  [5.426 sec/step, loss=0.07405, avg_loss=0.07367]\n",
      "Step 528476  [5.427 sec/step, loss=0.07304, avg_loss=0.07367]\n",
      "Step 528477  [5.412 sec/step, loss=0.07358, avg_loss=0.07364]\n",
      "Step 528478  [5.408 sec/step, loss=0.07434, avg_loss=0.07364]\n",
      "Step 528479  [5.407 sec/step, loss=0.07394, avg_loss=0.07364]\n",
      "Generated 32 batches of size 32 in 2.352 sec\n",
      "Step 528480  [5.429 sec/step, loss=0.07506, avg_loss=0.07367]\n",
      "Step 528481  [5.429 sec/step, loss=0.07314, avg_loss=0.07369]\n",
      "Step 528482  [5.441 sec/step, loss=0.07324, avg_loss=0.07367]\n",
      "Step 528483  [5.429 sec/step, loss=0.07117, avg_loss=0.07363]\n",
      "Step 528484  [5.418 sec/step, loss=0.07552, avg_loss=0.07364]\n",
      "Step 528485  [5.419 sec/step, loss=0.07355, avg_loss=0.07362]\n",
      "Step 528486  [5.405 sec/step, loss=0.07486, avg_loss=0.07361]\n",
      "Step 528487  [5.366 sec/step, loss=0.07617, avg_loss=0.07372]\n",
      "Step 528488  [5.381 sec/step, loss=0.07417, avg_loss=0.07370]\n",
      "Step 528489  [5.391 sec/step, loss=0.07626, avg_loss=0.07370]\n",
      "Step 528490  [5.394 sec/step, loss=0.07427, avg_loss=0.07374]\n",
      "Step 528491  [5.397 sec/step, loss=0.07598, avg_loss=0.07375]\n",
      "Step 528492  [5.407 sec/step, loss=0.07569, avg_loss=0.07376]\n",
      "Step 528493  [5.410 sec/step, loss=0.07416, avg_loss=0.07377]\n",
      "Step 528494  [5.434 sec/step, loss=0.07553, avg_loss=0.07380]\n",
      "Step 528495  [5.431 sec/step, loss=0.07381, avg_loss=0.07377]\n",
      "Step 528496  [5.443 sec/step, loss=0.07276, avg_loss=0.07378]\n",
      "Step 528497  [5.435 sec/step, loss=0.07528, avg_loss=0.07379]\n",
      "Step 528498  [5.423 sec/step, loss=0.07452, avg_loss=0.07380]\n",
      "Step 528499  [5.414 sec/step, loss=0.07513, avg_loss=0.07382]\n",
      "Step 528500  [5.439 sec/step, loss=0.06536, avg_loss=0.07374]\n",
      "Writing summary at step: 528500\n",
      "Step 528501  [5.438 sec/step, loss=0.07406, avg_loss=0.07373]\n",
      "Step 528502  [5.434 sec/step, loss=0.06566, avg_loss=0.07367]\n",
      "Step 528503  [5.438 sec/step, loss=0.07389, avg_loss=0.07368]\n",
      "Step 528504  [5.428 sec/step, loss=0.07546, avg_loss=0.07367]\n",
      "Step 528505  [5.420 sec/step, loss=0.07500, avg_loss=0.07368]\n",
      "Step 528506  [5.401 sec/step, loss=0.07375, avg_loss=0.07365]\n",
      "Step 528507  [5.389 sec/step, loss=0.07410, avg_loss=0.07365]\n",
      "Step 528508  [5.387 sec/step, loss=0.07550, avg_loss=0.07365]\n",
      "Step 528509  [5.371 sec/step, loss=0.07118, avg_loss=0.07360]\n",
      "Step 528510  [5.375 sec/step, loss=0.07508, avg_loss=0.07362]\n",
      "Generated 32 batches of size 32 in 2.441 sec\n",
      "Step 528511  [5.382 sec/step, loss=0.07529, avg_loss=0.07362]\n",
      "Step 528512  [5.407 sec/step, loss=0.07449, avg_loss=0.07362]\n",
      "Step 528513  [5.413 sec/step, loss=0.07094, avg_loss=0.07362]\n",
      "Step 528514  [5.416 sec/step, loss=0.07337, avg_loss=0.07362]\n",
      "Step 528515  [5.435 sec/step, loss=0.07546, avg_loss=0.07365]\n",
      "Step 528516  [5.430 sec/step, loss=0.07625, avg_loss=0.07365]\n",
      "Step 528517  [5.441 sec/step, loss=0.07582, avg_loss=0.07366]\n",
      "Step 528518  [5.432 sec/step, loss=0.07268, avg_loss=0.07363]\n",
      "Step 528519  [5.429 sec/step, loss=0.07347, avg_loss=0.07365]\n",
      "Step 528520  [5.416 sec/step, loss=0.06761, avg_loss=0.07358]\n",
      "Step 528521  [5.370 sec/step, loss=0.07456, avg_loss=0.07365]\n",
      "Step 528522  [5.385 sec/step, loss=0.07606, avg_loss=0.07367]\n",
      "Step 528523  [5.413 sec/step, loss=0.07545, avg_loss=0.07376]\n",
      "Step 528524  [5.398 sec/step, loss=0.07510, avg_loss=0.07378]\n",
      "Step 528525  [5.404 sec/step, loss=0.07311, avg_loss=0.07380]\n",
      "Step 528526  [5.396 sec/step, loss=0.07497, avg_loss=0.07380]\n",
      "Step 528527  [5.359 sec/step, loss=0.07354, avg_loss=0.07388]\n",
      "Step 528528  [5.352 sec/step, loss=0.07232, avg_loss=0.07388]\n",
      "Step 528529  [5.328 sec/step, loss=0.07435, avg_loss=0.07386]\n",
      "Step 528530  [5.387 sec/step, loss=0.06619, avg_loss=0.07379]\n",
      "Step 528531  [5.386 sec/step, loss=0.07452, avg_loss=0.07377]\n",
      "Step 528532  [5.406 sec/step, loss=0.07559, avg_loss=0.07382]\n",
      "Step 528533  [5.411 sec/step, loss=0.07524, avg_loss=0.07385]\n",
      "Step 528534  [5.412 sec/step, loss=0.07442, avg_loss=0.07384]\n",
      "Step 528535  [5.448 sec/step, loss=0.07303, avg_loss=0.07383]\n",
      "Step 528536  [5.426 sec/step, loss=0.07143, avg_loss=0.07381]\n",
      "Step 528537  [5.443 sec/step, loss=0.07311, avg_loss=0.07388]\n",
      "Step 528538  [5.453 sec/step, loss=0.07578, avg_loss=0.07388]\n",
      "Step 528539  [5.442 sec/step, loss=0.07375, avg_loss=0.07386]\n",
      "Step 528540  [5.437 sec/step, loss=0.07391, avg_loss=0.07384]\n",
      "Step 528541  [5.443 sec/step, loss=0.07628, avg_loss=0.07386]\n",
      "Step 528542  [5.428 sec/step, loss=0.07389, avg_loss=0.07383]\n",
      "Generated 32 batches of size 32 in 2.701 sec\n",
      "Step 528543  [5.418 sec/step, loss=0.07221, avg_loss=0.07380]\n",
      "Step 528544  [5.406 sec/step, loss=0.07547, avg_loss=0.07383]\n",
      "Step 528545  [5.382 sec/step, loss=0.07384, avg_loss=0.07381]\n",
      "Step 528546  [5.382 sec/step, loss=0.07563, avg_loss=0.07385]\n",
      "Step 528547  [5.379 sec/step, loss=0.07472, avg_loss=0.07384]\n",
      "Step 528548  [5.388 sec/step, loss=0.07604, avg_loss=0.07386]\n",
      "Step 528549  [5.392 sec/step, loss=0.07637, avg_loss=0.07389]\n",
      "Step 528550  [5.388 sec/step, loss=0.07543, avg_loss=0.07390]\n",
      "Step 528551  [5.376 sec/step, loss=0.07229, avg_loss=0.07386]\n",
      "Step 528552  [5.394 sec/step, loss=0.07591, avg_loss=0.07390]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 528553  [5.434 sec/step, loss=0.06677, avg_loss=0.07383]\n",
      "Step 528554  [5.430 sec/step, loss=0.07507, avg_loss=0.07383]\n",
      "Step 528555  [5.441 sec/step, loss=0.07478, avg_loss=0.07384]\n",
      "Step 528556  [5.427 sec/step, loss=0.06472, avg_loss=0.07373]\n",
      "Step 528557  [5.385 sec/step, loss=0.07580, avg_loss=0.07382]\n",
      "Step 528558  [5.384 sec/step, loss=0.07506, avg_loss=0.07382]\n",
      "Step 528559  [5.386 sec/step, loss=0.07402, avg_loss=0.07382]\n",
      "Step 528560  [5.398 sec/step, loss=0.07317, avg_loss=0.07380]\n",
      "Step 528561  [5.395 sec/step, loss=0.07552, avg_loss=0.07380]\n",
      "Step 528562  [5.398 sec/step, loss=0.07476, avg_loss=0.07380]\n",
      "Step 528563  [5.390 sec/step, loss=0.07490, avg_loss=0.07381]\n",
      "Step 528564  [5.369 sec/step, loss=0.07033, avg_loss=0.07375]\n",
      "Step 528565  [5.385 sec/step, loss=0.07116, avg_loss=0.07374]\n",
      "Step 528566  [5.411 sec/step, loss=0.07359, avg_loss=0.07382]\n",
      "Step 528567  [5.411 sec/step, loss=0.07154, avg_loss=0.07383]\n",
      "Step 528568  [5.408 sec/step, loss=0.07497, avg_loss=0.07383]\n",
      "Step 528569  [5.415 sec/step, loss=0.07462, avg_loss=0.07382]\n",
      "Step 528570  [5.412 sec/step, loss=0.07660, avg_loss=0.07382]\n",
      "Step 528571  [5.398 sec/step, loss=0.07107, avg_loss=0.07379]\n",
      "Step 528572  [5.408 sec/step, loss=0.07347, avg_loss=0.07377]\n",
      "Step 528573  [5.401 sec/step, loss=0.07374, avg_loss=0.07377]\n",
      "Step 528574  [5.397 sec/step, loss=0.07215, avg_loss=0.07376]\n",
      "Generated 32 batches of size 32 in 2.521 sec\n",
      "Step 528575  [5.404 sec/step, loss=0.07397, avg_loss=0.07376]\n",
      "Step 528576  [5.385 sec/step, loss=0.07298, avg_loss=0.07376]\n",
      "Step 528577  [5.391 sec/step, loss=0.07497, avg_loss=0.07377]\n",
      "Step 528578  [5.388 sec/step, loss=0.07428, avg_loss=0.07377]\n",
      "Step 528579  [5.394 sec/step, loss=0.07399, avg_loss=0.07377]\n",
      "Step 528580  [5.399 sec/step, loss=0.07566, avg_loss=0.07378]\n",
      "Step 528581  [5.412 sec/step, loss=0.07590, avg_loss=0.07381]\n",
      "Step 528582  [5.396 sec/step, loss=0.07481, avg_loss=0.07382]\n",
      "Step 528583  [5.418 sec/step, loss=0.07635, avg_loss=0.07387]\n",
      "Step 528584  [5.409 sec/step, loss=0.07242, avg_loss=0.07384]\n",
      "Step 528585  [5.394 sec/step, loss=0.07530, avg_loss=0.07386]\n",
      "Step 528586  [5.391 sec/step, loss=0.07452, avg_loss=0.07386]\n",
      "Step 528587  [5.381 sec/step, loss=0.07522, avg_loss=0.07385]\n",
      "Step 528588  [5.358 sec/step, loss=0.07478, avg_loss=0.07385]\n",
      "Step 528589  [5.363 sec/step, loss=0.07618, avg_loss=0.07385]\n",
      "Step 528590  [5.416 sec/step, loss=0.06522, avg_loss=0.07376]\n",
      "Step 528591  [5.421 sec/step, loss=0.07547, avg_loss=0.07376]\n",
      "Step 528592  [5.420 sec/step, loss=0.07485, avg_loss=0.07375]\n",
      "Step 528593  [5.412 sec/step, loss=0.07305, avg_loss=0.07374]\n",
      "Step 528594  [5.395 sec/step, loss=0.07566, avg_loss=0.07374]\n",
      "Step 528595  [5.398 sec/step, loss=0.07569, avg_loss=0.07376]\n",
      "Step 528596  [5.383 sec/step, loss=0.07387, avg_loss=0.07377]\n",
      "Step 528597  [5.386 sec/step, loss=0.07386, avg_loss=0.07375]\n",
      "Step 528598  [5.390 sec/step, loss=0.07422, avg_loss=0.07375]\n",
      "Step 528599  [5.416 sec/step, loss=0.07305, avg_loss=0.07373]\n",
      "Step 528600  [5.362 sec/step, loss=0.07368, avg_loss=0.07381]\n",
      "Writing summary at step: 528600\n",
      "Step 528601  [5.375 sec/step, loss=0.07579, avg_loss=0.07383]\n",
      "Step 528602  [5.393 sec/step, loss=0.07450, avg_loss=0.07392]\n",
      "Step 528603  [5.380 sec/step, loss=0.06730, avg_loss=0.07385]\n",
      "Step 528604  [5.369 sec/step, loss=0.07380, avg_loss=0.07384]\n",
      "Step 528605  [5.378 sec/step, loss=0.07593, avg_loss=0.07385]\n",
      "Generated 32 batches of size 32 in 2.365 sec\n",
      "Step 528606  [5.394 sec/step, loss=0.07202, avg_loss=0.07383]\n",
      "Step 528607  [5.394 sec/step, loss=0.07412, avg_loss=0.07383]\n",
      "Step 528608  [5.398 sec/step, loss=0.07362, avg_loss=0.07381]\n",
      "Step 528609  [5.404 sec/step, loss=0.07327, avg_loss=0.07383]\n",
      "Step 528610  [5.407 sec/step, loss=0.07378, avg_loss=0.07382]\n",
      "Step 528611  [5.414 sec/step, loss=0.07613, avg_loss=0.07383]\n",
      "Step 528612  [5.391 sec/step, loss=0.07515, avg_loss=0.07383]\n",
      "Step 528613  [5.404 sec/step, loss=0.07523, avg_loss=0.07388]\n",
      "Step 528614  [5.393 sec/step, loss=0.07154, avg_loss=0.07386]\n",
      "Step 528615  [5.377 sec/step, loss=0.07523, avg_loss=0.07386]\n",
      "Step 528616  [5.374 sec/step, loss=0.07438, avg_loss=0.07384]\n",
      "Step 528617  [5.378 sec/step, loss=0.07589, avg_loss=0.07384]\n",
      "Step 528618  [5.387 sec/step, loss=0.07511, avg_loss=0.07386]\n",
      "Step 528619  [5.390 sec/step, loss=0.07402, avg_loss=0.07387]\n",
      "Step 528620  [5.409 sec/step, loss=0.07467, avg_loss=0.07394]\n",
      "Step 528621  [5.409 sec/step, loss=0.07418, avg_loss=0.07393]\n",
      "Step 528622  [5.392 sec/step, loss=0.07334, avg_loss=0.07391]\n",
      "Step 528623  [5.384 sec/step, loss=0.07554, avg_loss=0.07391]\n",
      "Step 528624  [5.366 sec/step, loss=0.07128, avg_loss=0.07387]\n",
      "Step 528625  [5.382 sec/step, loss=0.07599, avg_loss=0.07390]\n",
      "Step 528626  [5.379 sec/step, loss=0.07403, avg_loss=0.07389]\n",
      "Step 528627  [5.376 sec/step, loss=0.07631, avg_loss=0.07392]\n",
      "Step 528628  [5.369 sec/step, loss=0.07098, avg_loss=0.07390]\n",
      "Step 528629  [5.376 sec/step, loss=0.07440, avg_loss=0.07390]\n",
      "Step 528630  [5.325 sec/step, loss=0.07324, avg_loss=0.07397]\n",
      "Step 528631  [5.341 sec/step, loss=0.07267, avg_loss=0.07396]\n",
      "Step 528632  [5.337 sec/step, loss=0.07499, avg_loss=0.07395]\n",
      "Step 528633  [5.347 sec/step, loss=0.07543, avg_loss=0.07395]\n",
      "Step 528634  [5.345 sec/step, loss=0.07225, avg_loss=0.07393]\n",
      "Step 528635  [5.311 sec/step, loss=0.07363, avg_loss=0.07394]\n",
      "Step 528636  [5.332 sec/step, loss=0.07320, avg_loss=0.07395]\n",
      "Step 528637  [5.342 sec/step, loss=0.07618, avg_loss=0.07398]\n",
      "Generated 32 batches of size 32 in 2.380 sec\n",
      "Step 528638  [5.346 sec/step, loss=0.07467, avg_loss=0.07397]\n",
      "Step 528639  [5.350 sec/step, loss=0.07457, avg_loss=0.07398]\n",
      "Step 528640  [5.335 sec/step, loss=0.06466, avg_loss=0.07389]\n",
      "Step 528641  [5.319 sec/step, loss=0.07408, avg_loss=0.07387]\n",
      "Step 528642  [5.339 sec/step, loss=0.07341, avg_loss=0.07386]\n",
      "Step 528643  [5.353 sec/step, loss=0.07498, avg_loss=0.07389]\n",
      "Step 528644  [5.349 sec/step, loss=0.07590, avg_loss=0.07389]\n",
      "Step 528645  [5.358 sec/step, loss=0.07462, avg_loss=0.07390]\n",
      "Step 528646  [5.408 sec/step, loss=0.06688, avg_loss=0.07381]\n",
      "Step 528647  [5.397 sec/step, loss=0.07415, avg_loss=0.07381]\n",
      "Step 528648  [5.396 sec/step, loss=0.07315, avg_loss=0.07378]\n",
      "Step 528649  [5.370 sec/step, loss=0.06517, avg_loss=0.07367]\n",
      "Step 528650  [5.372 sec/step, loss=0.07484, avg_loss=0.07366]\n",
      "Step 528651  [5.359 sec/step, loss=0.07021, avg_loss=0.07364]\n",
      "Step 528652  [5.342 sec/step, loss=0.07310, avg_loss=0.07361]\n",
      "Step 528653  [5.304 sec/step, loss=0.07565, avg_loss=0.07370]\n",
      "Step 528654  [5.295 sec/step, loss=0.07393, avg_loss=0.07369]\n",
      "Step 528655  [5.301 sec/step, loss=0.07419, avg_loss=0.07368]\n",
      "Step 528656  [5.323 sec/step, loss=0.07490, avg_loss=0.07379]\n",
      "Step 528657  [5.313 sec/step, loss=0.07379, avg_loss=0.07377]\n",
      "Step 528658  [5.308 sec/step, loss=0.07504, avg_loss=0.07377]\n",
      "Step 528659  [5.313 sec/step, loss=0.07513, avg_loss=0.07378]\n",
      "Step 528660  [5.290 sec/step, loss=0.07356, avg_loss=0.07378]\n",
      "Step 528661  [5.306 sec/step, loss=0.07493, avg_loss=0.07377]\n",
      "Step 528662  [5.285 sec/step, loss=0.07105, avg_loss=0.07374]\n",
      "Step 528663  [5.288 sec/step, loss=0.07445, avg_loss=0.07373]\n",
      "Step 528664  [5.300 sec/step, loss=0.07549, avg_loss=0.07378]\n",
      "Step 528665  [5.305 sec/step, loss=0.07578, avg_loss=0.07383]\n",
      "Step 528666  [5.306 sec/step, loss=0.07618, avg_loss=0.07386]\n",
      "Step 528667  [5.313 sec/step, loss=0.07477, avg_loss=0.07389]\n",
      "Step 528668  [5.315 sec/step, loss=0.07476, avg_loss=0.07389]\n",
      "Step 528669  [5.335 sec/step, loss=0.07265, avg_loss=0.07387]\n",
      "Generated 32 batches of size 32 in 2.541 sec\n",
      "Step 528670  [5.321 sec/step, loss=0.07342, avg_loss=0.07384]\n",
      "Step 528671  [5.334 sec/step, loss=0.07430, avg_loss=0.07387]\n",
      "Step 528672  [5.315 sec/step, loss=0.07301, avg_loss=0.07386]\n",
      "Step 528673  [5.314 sec/step, loss=0.07209, avg_loss=0.07385]\n",
      "Step 528674  [5.369 sec/step, loss=0.06532, avg_loss=0.07378]\n",
      "Step 528675  [5.373 sec/step, loss=0.07530, avg_loss=0.07379]\n",
      "Step 528676  [5.393 sec/step, loss=0.07551, avg_loss=0.07382]\n",
      "Step 528677  [5.402 sec/step, loss=0.07371, avg_loss=0.07380]\n",
      "Step 528678  [5.421 sec/step, loss=0.07653, avg_loss=0.07383]\n",
      "Step 528679  [5.431 sec/step, loss=0.07521, avg_loss=0.07384]\n",
      "Step 528680  [5.419 sec/step, loss=0.07435, avg_loss=0.07383]\n",
      "Step 528681  [5.411 sec/step, loss=0.07429, avg_loss=0.07381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 528682  [5.421 sec/step, loss=0.07646, avg_loss=0.07383]\n",
      "Step 528683  [5.411 sec/step, loss=0.07569, avg_loss=0.07382]\n",
      "Step 528684  [5.421 sec/step, loss=0.07306, avg_loss=0.07383]\n",
      "Step 528685  [5.476 sec/step, loss=0.06721, avg_loss=0.07375]\n",
      "Step 528686  [5.489 sec/step, loss=0.07570, avg_loss=0.07376]\n",
      "Step 528687  [5.486 sec/step, loss=0.07469, avg_loss=0.07375]\n",
      "Step 528688  [5.496 sec/step, loss=0.07601, avg_loss=0.07376]\n",
      "Step 528689  [5.469 sec/step, loss=0.07210, avg_loss=0.07372]\n",
      "Step 528690  [5.432 sec/step, loss=0.07470, avg_loss=0.07382]\n",
      "Step 528691  [5.404 sec/step, loss=0.07396, avg_loss=0.07380]\n",
      "Step 528692  [5.390 sec/step, loss=0.07166, avg_loss=0.07377]\n",
      "Step 528693  [5.389 sec/step, loss=0.07401, avg_loss=0.07378]\n",
      "Step 528694  [5.370 sec/step, loss=0.06600, avg_loss=0.07368]\n",
      "Step 528695  [5.358 sec/step, loss=0.07340, avg_loss=0.07366]\n",
      "Step 528696  [5.370 sec/step, loss=0.07604, avg_loss=0.07368]\n",
      "Step 528697  [5.363 sec/step, loss=0.07374, avg_loss=0.07368]\n",
      "Step 528698  [5.369 sec/step, loss=0.07375, avg_loss=0.07368]\n",
      "Step 528699  [5.353 sec/step, loss=0.07563, avg_loss=0.07370]\n",
      "Step 528700  [5.360 sec/step, loss=0.07618, avg_loss=0.07373]\n",
      "Writing summary at step: 528700\n",
      "Generated 32 batches of size 32 in 2.411 sec\n",
      "Step 528701  [5.350 sec/step, loss=0.07259, avg_loss=0.07370]\n",
      "Step 528702  [5.352 sec/step, loss=0.07270, avg_loss=0.07368]\n",
      "Step 528703  [5.371 sec/step, loss=0.07537, avg_loss=0.07376]\n",
      "Step 528704  [5.380 sec/step, loss=0.07405, avg_loss=0.07376]\n",
      "Step 528705  [5.366 sec/step, loss=0.07395, avg_loss=0.07374]\n",
      "Step 528706  [5.363 sec/step, loss=0.07460, avg_loss=0.07377]\n",
      "Step 528707  [5.378 sec/step, loss=0.07512, avg_loss=0.07378]\n",
      "Step 528708  [5.392 sec/step, loss=0.07370, avg_loss=0.07378]\n",
      "Step 528709  [5.406 sec/step, loss=0.07348, avg_loss=0.07378]\n",
      "Step 528710  [5.399 sec/step, loss=0.07436, avg_loss=0.07379]\n",
      "Step 528711  [5.401 sec/step, loss=0.07622, avg_loss=0.07379]\n",
      "Step 528712  [5.395 sec/step, loss=0.07305, avg_loss=0.07377]\n",
      "Step 528713  [5.392 sec/step, loss=0.07456, avg_loss=0.07376]\n",
      "Step 528714  [5.412 sec/step, loss=0.07580, avg_loss=0.07380]\n",
      "Step 528715  [5.411 sec/step, loss=0.07402, avg_loss=0.07379]\n",
      "Step 528716  [5.395 sec/step, loss=0.07024, avg_loss=0.07375]\n",
      "Step 528717  [5.381 sec/step, loss=0.07549, avg_loss=0.07374]\n",
      "Step 528718  [5.393 sec/step, loss=0.07566, avg_loss=0.07375]\n",
      "Step 528719  [5.394 sec/step, loss=0.07272, avg_loss=0.07374]\n",
      "Step 528720  [5.403 sec/step, loss=0.07578, avg_loss=0.07375]\n",
      "Step 528721  [5.397 sec/step, loss=0.07520, avg_loss=0.07376]\n",
      "Step 528722  [5.394 sec/step, loss=0.07337, avg_loss=0.07376]\n",
      "Step 528723  [5.393 sec/step, loss=0.07411, avg_loss=0.07374]\n",
      "Step 528724  [5.391 sec/step, loss=0.06752, avg_loss=0.07371]\n",
      "Step 528725  [5.364 sec/step, loss=0.07323, avg_loss=0.07368]\n",
      "Step 528726  [5.367 sec/step, loss=0.07517, avg_loss=0.07369]\n",
      "Step 528727  [5.369 sec/step, loss=0.07594, avg_loss=0.07369]\n",
      "Step 528728  [5.381 sec/step, loss=0.07456, avg_loss=0.07372]\n",
      "Step 528729  [5.395 sec/step, loss=0.07378, avg_loss=0.07372]\n",
      "Step 528730  [5.398 sec/step, loss=0.07558, avg_loss=0.07374]\n",
      "Step 528731  [5.363 sec/step, loss=0.07417, avg_loss=0.07375]\n",
      "Step 528732  [5.362 sec/step, loss=0.07176, avg_loss=0.07372]\n",
      "Generated 32 batches of size 32 in 2.345 sec\n",
      "Step 528733  [5.363 sec/step, loss=0.07575, avg_loss=0.07373]\n",
      "Step 528734  [5.365 sec/step, loss=0.07272, avg_loss=0.07373]\n",
      "Step 528735  [5.421 sec/step, loss=0.06575, avg_loss=0.07365]\n",
      "Step 528736  [5.421 sec/step, loss=0.07362, avg_loss=0.07366]\n",
      "Step 528737  [5.415 sec/step, loss=0.07374, avg_loss=0.07363]\n",
      "Step 528738  [5.384 sec/step, loss=0.07271, avg_loss=0.07361]\n",
      "Step 528739  [5.409 sec/step, loss=0.07261, avg_loss=0.07359]\n",
      "Step 528740  [5.439 sec/step, loss=0.07566, avg_loss=0.07370]\n",
      "Step 528741  [5.454 sec/step, loss=0.07488, avg_loss=0.07371]\n",
      "Step 528742  [5.443 sec/step, loss=0.07543, avg_loss=0.07373]\n",
      "Step 528743  [5.436 sec/step, loss=0.07509, avg_loss=0.07373]\n",
      "Step 528744  [5.437 sec/step, loss=0.07590, avg_loss=0.07373]\n",
      "Step 528745  [5.445 sec/step, loss=0.07598, avg_loss=0.07374]\n",
      "Step 528746  [5.398 sec/step, loss=0.07457, avg_loss=0.07382]\n",
      "Step 528747  [5.407 sec/step, loss=0.07578, avg_loss=0.07384]\n",
      "Step 528748  [5.423 sec/step, loss=0.07291, avg_loss=0.07384]\n",
      "Step 528749  [5.430 sec/step, loss=0.07050, avg_loss=0.07389]\n",
      "Step 528750  [5.427 sec/step, loss=0.07122, avg_loss=0.07385]\n",
      "Step 528751  [5.437 sec/step, loss=0.07479, avg_loss=0.07390]\n",
      "Step 528752  [5.453 sec/step, loss=0.07608, avg_loss=0.07393]\n",
      "Step 528753  [5.451 sec/step, loss=0.07330, avg_loss=0.07390]\n",
      "Step 528754  [5.460 sec/step, loss=0.07556, avg_loss=0.07392]\n",
      "Step 528755  [5.441 sec/step, loss=0.07437, avg_loss=0.07392]\n",
      "Step 528756  [5.487 sec/step, loss=0.06596, avg_loss=0.07383]\n",
      "Step 528757  [5.475 sec/step, loss=0.06495, avg_loss=0.07374]\n",
      "Step 528758  [5.486 sec/step, loss=0.07496, avg_loss=0.07374]\n",
      "Step 528759  [5.476 sec/step, loss=0.07438, avg_loss=0.07374]\n",
      "Step 528760  [5.468 sec/step, loss=0.07368, avg_loss=0.07374]\n",
      "Step 528761  [5.440 sec/step, loss=0.07414, avg_loss=0.07373]\n",
      "Step 528762  [5.468 sec/step, loss=0.07526, avg_loss=0.07377]\n",
      "Step 528763  [5.460 sec/step, loss=0.07297, avg_loss=0.07376]\n",
      "Step 528764  [5.445 sec/step, loss=0.07067, avg_loss=0.07371]\n",
      "Generated 32 batches of size 32 in 2.437 sec\n",
      "Step 528765  [5.443 sec/step, loss=0.07453, avg_loss=0.07370]\n",
      "Step 528766  [5.439 sec/step, loss=0.07534, avg_loss=0.07369]\n",
      "Step 528767  [5.439 sec/step, loss=0.07471, avg_loss=0.07369]\n",
      "Step 528768  [5.437 sec/step, loss=0.07336, avg_loss=0.07367]\n",
      "Step 528769  [5.419 sec/step, loss=0.07584, avg_loss=0.07371]\n",
      "Step 528770  [5.424 sec/step, loss=0.07420, avg_loss=0.07371]\n",
      "Step 528771  [5.445 sec/step, loss=0.07371, avg_loss=0.07371]\n",
      "Step 528772  [5.446 sec/step, loss=0.07209, avg_loss=0.07370]\n",
      "Step 528773  [5.463 sec/step, loss=0.07582, avg_loss=0.07374]\n",
      "Step 528774  [5.403 sec/step, loss=0.07386, avg_loss=0.07382]\n",
      "Step 528775  [5.385 sec/step, loss=0.06520, avg_loss=0.07372]\n",
      "Step 528776  [5.422 sec/step, loss=0.06725, avg_loss=0.07364]\n",
      "Step 528777  [5.414 sec/step, loss=0.07446, avg_loss=0.07364]\n",
      "Step 528778  [5.399 sec/step, loss=0.07425, avg_loss=0.07362]\n",
      "Step 528779  [5.388 sec/step, loss=0.07462, avg_loss=0.07362]\n",
      "Step 528780  [5.385 sec/step, loss=0.07349, avg_loss=0.07361]\n",
      "Step 528781  [5.400 sec/step, loss=0.07609, avg_loss=0.07363]\n",
      "Step 528782  [5.383 sec/step, loss=0.07214, avg_loss=0.07358]\n",
      "Step 528783  [5.374 sec/step, loss=0.07338, avg_loss=0.07356]\n",
      "Step 528784  [5.381 sec/step, loss=0.07498, avg_loss=0.07358]\n",
      "Step 528785  [5.321 sec/step, loss=0.07102, avg_loss=0.07362]\n",
      "Step 528786  [5.322 sec/step, loss=0.07614, avg_loss=0.07362]\n",
      "Step 528787  [5.336 sec/step, loss=0.07416, avg_loss=0.07362]\n",
      "Step 528788  [5.321 sec/step, loss=0.07447, avg_loss=0.07360]\n",
      "Step 528789  [5.362 sec/step, loss=0.07411, avg_loss=0.07362]\n",
      "Step 528790  [5.352 sec/step, loss=0.07473, avg_loss=0.07362]\n",
      "Step 528791  [5.371 sec/step, loss=0.07492, avg_loss=0.07363]\n",
      "Step 528792  [5.391 sec/step, loss=0.07352, avg_loss=0.07365]\n",
      "Step 528793  [5.392 sec/step, loss=0.07490, avg_loss=0.07366]\n",
      "Step 528794  [5.433 sec/step, loss=0.07298, avg_loss=0.07373]\n",
      "Step 528795  [5.442 sec/step, loss=0.07467, avg_loss=0.07374]\n",
      "Step 528796  [5.444 sec/step, loss=0.07630, avg_loss=0.07374]\n",
      "Generated 32 batches of size 32 in 2.408 sec\n",
      "Step 528797  [5.468 sec/step, loss=0.07562, avg_loss=0.07376]\n",
      "Step 528798  [5.461 sec/step, loss=0.07508, avg_loss=0.07377]\n",
      "Step 528799  [5.454 sec/step, loss=0.07555, avg_loss=0.07377]\n",
      "Step 528800  [5.450 sec/step, loss=0.07441, avg_loss=0.07376]\n",
      "Writing summary at step: 528800\n",
      "Step 528801  [5.450 sec/step, loss=0.07435, avg_loss=0.07377]\n",
      "Step 528802  [5.456 sec/step, loss=0.07534, avg_loss=0.07380]\n",
      "Step 528803  [5.452 sec/step, loss=0.07552, avg_loss=0.07380]\n",
      "Step 528804  [5.446 sec/step, loss=0.07314, avg_loss=0.07379]\n",
      "Step 528805  [5.460 sec/step, loss=0.07628, avg_loss=0.07382]\n",
      "Step 528806  [5.456 sec/step, loss=0.07476, avg_loss=0.07382]\n",
      "Step 528807  [5.448 sec/step, loss=0.07273, avg_loss=0.07379]\n",
      "Step 528808  [5.425 sec/step, loss=0.07533, avg_loss=0.07381]\n",
      "Step 528809  [5.427 sec/step, loss=0.07606, avg_loss=0.07384]\n",
      "Step 528810  [5.412 sec/step, loss=0.07370, avg_loss=0.07383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 528811  [5.395 sec/step, loss=0.07506, avg_loss=0.07382]\n",
      "Step 528812  [5.408 sec/step, loss=0.07370, avg_loss=0.07382]\n",
      "Step 528813  [5.395 sec/step, loss=0.07366, avg_loss=0.07382]\n",
      "Step 528814  [5.400 sec/step, loss=0.07554, avg_loss=0.07381]\n",
      "Step 528815  [5.404 sec/step, loss=0.07184, avg_loss=0.07379]\n",
      "Step 528816  [5.413 sec/step, loss=0.07541, avg_loss=0.07384]\n",
      "Step 528817  [5.419 sec/step, loss=0.07593, avg_loss=0.07385]\n",
      "Step 528818  [5.406 sec/step, loss=0.07518, avg_loss=0.07384]\n",
      "Step 528819  [5.436 sec/step, loss=0.07268, avg_loss=0.07384]\n",
      "Step 528820  [5.421 sec/step, loss=0.07421, avg_loss=0.07383]\n",
      "Step 528821  [5.433 sec/step, loss=0.07644, avg_loss=0.07384]\n",
      "Step 528822  [5.434 sec/step, loss=0.07316, avg_loss=0.07384]\n",
      "Step 528823  [5.429 sec/step, loss=0.07500, avg_loss=0.07385]\n",
      "Step 528824  [5.429 sec/step, loss=0.06600, avg_loss=0.07383]\n",
      "Step 528825  [5.485 sec/step, loss=0.06930, avg_loss=0.07379]\n",
      "Step 528826  [5.475 sec/step, loss=0.07186, avg_loss=0.07376]\n",
      "Step 528827  [5.454 sec/step, loss=0.07161, avg_loss=0.07371]\n",
      "Generated 32 batches of size 32 in 2.486 sec\n",
      "Step 528828  [5.458 sec/step, loss=0.07448, avg_loss=0.07371]\n",
      "Step 528829  [5.449 sec/step, loss=0.07556, avg_loss=0.07373]\n",
      "Step 528830  [5.460 sec/step, loss=0.07578, avg_loss=0.07373]\n",
      "Step 528831  [5.464 sec/step, loss=0.07101, avg_loss=0.07370]\n",
      "Step 528832  [5.472 sec/step, loss=0.07424, avg_loss=0.07373]\n",
      "Step 528833  [5.479 sec/step, loss=0.07521, avg_loss=0.07372]\n",
      "Step 528834  [5.494 sec/step, loss=0.07552, avg_loss=0.07375]\n",
      "Step 528835  [5.443 sec/step, loss=0.07322, avg_loss=0.07382]\n",
      "Step 528836  [5.437 sec/step, loss=0.07392, avg_loss=0.07383]\n",
      "Step 528837  [5.422 sec/step, loss=0.07382, avg_loss=0.07383]\n",
      "Step 528838  [5.430 sec/step, loss=0.07126, avg_loss=0.07381]\n",
      "Step 528839  [5.428 sec/step, loss=0.07528, avg_loss=0.07384]\n",
      "Step 528840  [5.415 sec/step, loss=0.07554, avg_loss=0.07384]\n",
      "Step 528841  [5.403 sec/step, loss=0.07400, avg_loss=0.07383]\n",
      "Step 528842  [5.383 sec/step, loss=0.06579, avg_loss=0.07373]\n",
      "Step 528843  [5.381 sec/step, loss=0.07488, avg_loss=0.07373]\n",
      "Step 528844  [5.420 sec/step, loss=0.06582, avg_loss=0.07363]\n",
      "Step 528845  [5.418 sec/step, loss=0.07628, avg_loss=0.07363]\n",
      "Step 528846  [5.427 sec/step, loss=0.07540, avg_loss=0.07364]\n",
      "Step 528847  [5.425 sec/step, loss=0.07424, avg_loss=0.07363]\n",
      "Step 528848  [5.403 sec/step, loss=0.07540, avg_loss=0.07365]\n",
      "Step 528849  [5.417 sec/step, loss=0.07458, avg_loss=0.07369]\n",
      "Step 528850  [5.440 sec/step, loss=0.07519, avg_loss=0.07373]\n",
      "Step 528851  [5.443 sec/step, loss=0.07516, avg_loss=0.07374]\n",
      "Step 528852  [5.438 sec/step, loss=0.07569, avg_loss=0.07373]\n",
      "Step 528853  [5.418 sec/step, loss=0.07158, avg_loss=0.07371]\n",
      "Step 528854  [5.431 sec/step, loss=0.07488, avg_loss=0.07371]\n",
      "Step 528855  [5.445 sec/step, loss=0.07439, avg_loss=0.07371]\n",
      "Step 528856  [5.392 sec/step, loss=0.07363, avg_loss=0.07378]\n",
      "Step 528857  [5.421 sec/step, loss=0.07354, avg_loss=0.07387]\n",
      "Step 528858  [5.422 sec/step, loss=0.07474, avg_loss=0.07387]\n",
      "Step 528859  [5.430 sec/step, loss=0.07568, avg_loss=0.07388]\n",
      "Generated 32 batches of size 32 in 2.604 sec\n",
      "Step 528860  [5.435 sec/step, loss=0.07269, avg_loss=0.07387]\n",
      "Step 528861  [5.462 sec/step, loss=0.07662, avg_loss=0.07390]\n",
      "Step 528862  [5.449 sec/step, loss=0.07369, avg_loss=0.07388]\n",
      "Step 528863  [5.458 sec/step, loss=0.07468, avg_loss=0.07390]\n",
      "Step 528864  [5.470 sec/step, loss=0.07563, avg_loss=0.07395]\n",
      "Step 528865  [5.476 sec/step, loss=0.07382, avg_loss=0.07394]\n",
      "Step 528866  [5.469 sec/step, loss=0.07426, avg_loss=0.07393]\n",
      "Step 528867  [5.455 sec/step, loss=0.07371, avg_loss=0.07392]\n",
      "Step 528868  [5.445 sec/step, loss=0.07188, avg_loss=0.07390]\n",
      "Step 528869  [5.437 sec/step, loss=0.07446, avg_loss=0.07389]\n",
      "Step 528870  [5.432 sec/step, loss=0.07531, avg_loss=0.07390]\n",
      "Step 528871  [5.414 sec/step, loss=0.07515, avg_loss=0.07392]\n",
      "Step 528872  [5.420 sec/step, loss=0.07552, avg_loss=0.07395]\n",
      "Step 528873  [5.420 sec/step, loss=0.07606, avg_loss=0.07395]\n",
      "Step 528874  [5.443 sec/step, loss=0.07590, avg_loss=0.07397]\n",
      "Step 528875  [5.452 sec/step, loss=0.07402, avg_loss=0.07406]\n",
      "Step 528876  [5.389 sec/step, loss=0.07185, avg_loss=0.07411]\n",
      "Step 528877  [5.388 sec/step, loss=0.07246, avg_loss=0.07409]\n",
      "Step 528878  [5.385 sec/step, loss=0.07245, avg_loss=0.07407]\n",
      "Step 528879  [5.434 sec/step, loss=0.06640, avg_loss=0.07399]\n",
      "Step 528880  [5.435 sec/step, loss=0.07320, avg_loss=0.07398]\n",
      "Step 528881  [5.413 sec/step, loss=0.07046, avg_loss=0.07393]\n",
      "Step 528882  [5.429 sec/step, loss=0.07608, avg_loss=0.07397]\n",
      "Step 528883  [5.444 sec/step, loss=0.07608, avg_loss=0.07399]\n",
      "Step 528884  [5.449 sec/step, loss=0.07633, avg_loss=0.07401]\n",
      "Step 528885  [5.458 sec/step, loss=0.07109, avg_loss=0.07401]\n",
      "Step 528886  [5.447 sec/step, loss=0.07416, avg_loss=0.07399]\n",
      "Step 528887  [5.437 sec/step, loss=0.07509, avg_loss=0.07400]\n",
      "Step 528888  [5.446 sec/step, loss=0.07522, avg_loss=0.07401]\n",
      "Step 528889  [5.416 sec/step, loss=0.07352, avg_loss=0.07400]\n",
      "Step 528890  [5.437 sec/step, loss=0.07376, avg_loss=0.07399]\n",
      "Step 528891  [5.434 sec/step, loss=0.07468, avg_loss=0.07399]\n",
      "Generated 32 batches of size 32 in 2.803 sec\n",
      "Step 528892  [5.413 sec/step, loss=0.06653, avg_loss=0.07392]\n",
      "Step 528893  [5.420 sec/step, loss=0.07269, avg_loss=0.07390]\n",
      "Step 528894  [5.402 sec/step, loss=0.07517, avg_loss=0.07392]\n",
      "Step 528895  [5.389 sec/step, loss=0.07433, avg_loss=0.07391]\n",
      "Step 528896  [5.375 sec/step, loss=0.07439, avg_loss=0.07390]\n",
      "Step 528897  [5.362 sec/step, loss=0.07480, avg_loss=0.07389]\n",
      "Step 528898  [5.377 sec/step, loss=0.07602, avg_loss=0.07390]\n",
      "Step 528899  [5.384 sec/step, loss=0.07473, avg_loss=0.07389]\n",
      "Step 528900  [5.396 sec/step, loss=0.07385, avg_loss=0.07388]\n",
      "Writing summary at step: 528900\n",
      "Step 528901  [5.393 sec/step, loss=0.07522, avg_loss=0.07389]\n",
      "Step 528902  [5.393 sec/step, loss=0.07662, avg_loss=0.07390]\n",
      "Step 528903  [5.419 sec/step, loss=0.07312, avg_loss=0.07388]\n",
      "Step 528904  [5.435 sec/step, loss=0.07397, avg_loss=0.07389]\n",
      "Step 528905  [5.417 sec/step, loss=0.07372, avg_loss=0.07386]\n",
      "Step 528906  [5.401 sec/step, loss=0.06511, avg_loss=0.07377]\n",
      "Step 528907  [5.397 sec/step, loss=0.07320, avg_loss=0.07377]\n",
      "Step 528908  [5.409 sec/step, loss=0.07458, avg_loss=0.07376]\n",
      "Step 528909  [5.403 sec/step, loss=0.07520, avg_loss=0.07376]\n",
      "Step 528910  [5.411 sec/step, loss=0.07515, avg_loss=0.07377]\n",
      "Step 528911  [5.400 sec/step, loss=0.07146, avg_loss=0.07373]\n",
      "Step 528912  [5.399 sec/step, loss=0.07579, avg_loss=0.07375]\n",
      "Step 528913  [5.412 sec/step, loss=0.07618, avg_loss=0.07378]\n",
      "Step 528914  [5.393 sec/step, loss=0.07195, avg_loss=0.07374]\n",
      "Step 528915  [5.401 sec/step, loss=0.07369, avg_loss=0.07376]\n",
      "Step 528916  [5.399 sec/step, loss=0.07512, avg_loss=0.07376]\n",
      "Step 528917  [5.441 sec/step, loss=0.06589, avg_loss=0.07366]\n",
      "Step 528918  [5.428 sec/step, loss=0.07404, avg_loss=0.07365]\n",
      "Step 528919  [5.405 sec/step, loss=0.07370, avg_loss=0.07366]\n",
      "Step 528920  [5.407 sec/step, loss=0.07326, avg_loss=0.07365]\n",
      "Step 528921  [5.396 sec/step, loss=0.07383, avg_loss=0.07362]\n",
      "Step 528922  [5.399 sec/step, loss=0.07617, avg_loss=0.07365]\n",
      "Generated 32 batches of size 32 in 2.374 sec\n",
      "Step 528923  [5.420 sec/step, loss=0.07518, avg_loss=0.07365]\n",
      "Step 528924  [5.438 sec/step, loss=0.07522, avg_loss=0.07375]\n",
      "Step 528925  [5.403 sec/step, loss=0.07543, avg_loss=0.07381]\n",
      "Step 528926  [5.409 sec/step, loss=0.07028, avg_loss=0.07379]\n",
      "Step 528927  [5.420 sec/step, loss=0.07420, avg_loss=0.07382]\n",
      "Step 528928  [5.421 sec/step, loss=0.07533, avg_loss=0.07383]\n",
      "Step 528929  [5.434 sec/step, loss=0.07548, avg_loss=0.07383]\n",
      "Step 528930  [5.434 sec/step, loss=0.07616, avg_loss=0.07383]\n",
      "Step 528931  [5.448 sec/step, loss=0.07624, avg_loss=0.07388]\n",
      "Step 528932  [5.437 sec/step, loss=0.07289, avg_loss=0.07387]\n",
      "Step 528933  [5.408 sec/step, loss=0.07026, avg_loss=0.07382]\n",
      "Step 528934  [5.378 sec/step, loss=0.06538, avg_loss=0.07372]\n",
      "Step 528935  [5.381 sec/step, loss=0.07503, avg_loss=0.07374]\n",
      "Step 528936  [5.364 sec/step, loss=0.07125, avg_loss=0.07371]\n",
      "Step 528937  [5.373 sec/step, loss=0.07397, avg_loss=0.07371]\n",
      "Step 528938  [5.374 sec/step, loss=0.07048, avg_loss=0.07370]\n",
      "Step 528939  [5.352 sec/step, loss=0.07575, avg_loss=0.07371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 528940  [5.355 sec/step, loss=0.07536, avg_loss=0.07370]\n",
      "Step 528941  [5.352 sec/step, loss=0.07407, avg_loss=0.07371]\n",
      "Step 528942  [5.377 sec/step, loss=0.07601, avg_loss=0.07381]\n",
      "Step 528943  [5.378 sec/step, loss=0.07558, avg_loss=0.07381]\n",
      "Step 528944  [5.342 sec/step, loss=0.07600, avg_loss=0.07392]\n",
      "Step 528945  [5.330 sec/step, loss=0.07501, avg_loss=0.07390]\n",
      "Step 528946  [5.329 sec/step, loss=0.07325, avg_loss=0.07388]\n",
      "Step 528947  [5.340 sec/step, loss=0.07320, avg_loss=0.07387]\n",
      "Step 528948  [5.339 sec/step, loss=0.07195, avg_loss=0.07384]\n",
      "Step 528949  [5.345 sec/step, loss=0.07651, avg_loss=0.07386]\n",
      "Step 528950  [5.333 sec/step, loss=0.07440, avg_loss=0.07385]\n",
      "Step 528951  [5.331 sec/step, loss=0.07354, avg_loss=0.07383]\n",
      "Step 528952  [5.339 sec/step, loss=0.07630, avg_loss=0.07384]\n",
      "Step 528953  [5.366 sec/step, loss=0.07544, avg_loss=0.07388]\n",
      "Step 528954  [5.363 sec/step, loss=0.07564, avg_loss=0.07388]\n",
      "Generated 32 batches of size 32 in 2.556 sec\n",
      "Step 528955  [5.358 sec/step, loss=0.07357, avg_loss=0.07388]\n",
      "Step 528956  [5.363 sec/step, loss=0.07171, avg_loss=0.07386]\n",
      "Step 528957  [5.361 sec/step, loss=0.07544, avg_loss=0.07388]\n",
      "Step 528958  [5.345 sec/step, loss=0.07320, avg_loss=0.07386]\n",
      "Step 528959  [5.337 sec/step, loss=0.07431, avg_loss=0.07385]\n",
      "Step 528960  [5.342 sec/step, loss=0.07589, avg_loss=0.07388]\n",
      "Step 528961  [5.318 sec/step, loss=0.07424, avg_loss=0.07386]\n",
      "Step 528962  [5.344 sec/step, loss=0.07286, avg_loss=0.07385]\n",
      "Step 528963  [5.392 sec/step, loss=0.06528, avg_loss=0.07375]\n",
      "Step 528964  [5.407 sec/step, loss=0.07346, avg_loss=0.07373]\n",
      "Step 528965  [5.401 sec/step, loss=0.07447, avg_loss=0.07374]\n",
      "Step 528966  [5.405 sec/step, loss=0.07484, avg_loss=0.07374]\n",
      "Step 528967  [5.428 sec/step, loss=0.07607, avg_loss=0.07377]\n",
      "Step 528968  [5.441 sec/step, loss=0.07377, avg_loss=0.07379]\n",
      "Step 528969  [5.434 sec/step, loss=0.07313, avg_loss=0.07377]\n",
      "Step 528970  [5.450 sec/step, loss=0.07553, avg_loss=0.07378]\n",
      "Step 528971  [5.474 sec/step, loss=0.07351, avg_loss=0.07376]\n",
      "Step 528972  [5.470 sec/step, loss=0.07532, avg_loss=0.07376]\n",
      "Step 528973  [5.453 sec/step, loss=0.07324, avg_loss=0.07373]\n",
      "Step 528974  [5.452 sec/step, loss=0.07326, avg_loss=0.07370]\n",
      "Step 528975  [5.470 sec/step, loss=0.07570, avg_loss=0.07372]\n",
      "Step 528976  [5.485 sec/step, loss=0.07596, avg_loss=0.07376]\n",
      "Step 528977  [5.476 sec/step, loss=0.07430, avg_loss=0.07378]\n",
      "Step 528978  [5.476 sec/step, loss=0.07108, avg_loss=0.07377]\n",
      "Step 528979  [5.430 sec/step, loss=0.07334, avg_loss=0.07383]\n",
      "Step 528980  [5.430 sec/step, loss=0.07479, avg_loss=0.07385]\n",
      "Step 528981  [5.428 sec/step, loss=0.07391, avg_loss=0.07389]\n",
      "Step 528982  [5.412 sec/step, loss=0.07452, avg_loss=0.07387]\n",
      "Step 528983  [5.455 sec/step, loss=0.06691, avg_loss=0.07378]\n",
      "Step 528984  [5.454 sec/step, loss=0.07615, avg_loss=0.07378]\n",
      "Step 528985  [5.457 sec/step, loss=0.07176, avg_loss=0.07378]\n",
      "Step 528986  [5.443 sec/step, loss=0.07147, avg_loss=0.07376]\n",
      "Generated 32 batches of size 32 in 2.374 sec\n",
      "Step 528987  [5.452 sec/step, loss=0.07535, avg_loss=0.07376]\n",
      "Step 528988  [5.453 sec/step, loss=0.07469, avg_loss=0.07375]\n",
      "Step 528989  [5.441 sec/step, loss=0.06680, avg_loss=0.07369]\n",
      "Step 528990  [5.413 sec/step, loss=0.07347, avg_loss=0.07368]\n",
      "Step 528991  [5.415 sec/step, loss=0.07595, avg_loss=0.07370]\n",
      "Step 528992  [5.422 sec/step, loss=0.07334, avg_loss=0.07376]\n",
      "Step 528993  [5.415 sec/step, loss=0.07315, avg_loss=0.07377]\n",
      "Step 528994  [5.432 sec/step, loss=0.07299, avg_loss=0.07375]\n",
      "Step 528995  [5.442 sec/step, loss=0.07546, avg_loss=0.07376]\n",
      "Step 528996  [5.446 sec/step, loss=0.07374, avg_loss=0.07375]\n",
      "Step 528997  [5.459 sec/step, loss=0.07514, avg_loss=0.07375]\n",
      "Step 528998  [5.426 sec/step, loss=0.06641, avg_loss=0.07366]\n",
      "Step 528999  [5.426 sec/step, loss=0.07601, avg_loss=0.07367]\n",
      "Step 529000  [5.414 sec/step, loss=0.07219, avg_loss=0.07365]\n",
      "Writing summary at step: 529000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-529000\n",
      "Saving audio and alignment...\n",
      "Input: sangiin dzhagrroon kay kaesoon sae batsnay vaaloo kharray hoo kur aankh dzhukaaoo~_________\n",
      "Step 529001  [5.402 sec/step, loss=0.07412, avg_loss=0.07364]\n",
      "Step 529002  [5.401 sec/step, loss=0.07586, avg_loss=0.07364]\n",
      "Step 529003  [5.385 sec/step, loss=0.07591, avg_loss=0.07366]\n",
      "Step 529004  [5.386 sec/step, loss=0.07610, avg_loss=0.07369]\n",
      "Step 529005  [5.410 sec/step, loss=0.07607, avg_loss=0.07371]\n",
      "Step 529006  [5.430 sec/step, loss=0.07523, avg_loss=0.07381]\n",
      "Step 529007  [5.438 sec/step, loss=0.07594, avg_loss=0.07384]\n",
      "Step 529008  [5.417 sec/step, loss=0.07057, avg_loss=0.07380]\n",
      "Step 529009  [5.422 sec/step, loss=0.07642, avg_loss=0.07381]\n",
      "Step 529010  [5.422 sec/step, loss=0.07420, avg_loss=0.07380]\n",
      "Step 529011  [5.438 sec/step, loss=0.07193, avg_loss=0.07380]\n",
      "Step 529012  [5.421 sec/step, loss=0.07400, avg_loss=0.07379]\n",
      "Step 529013  [5.425 sec/step, loss=0.07422, avg_loss=0.07377]\n",
      "Step 529014  [5.430 sec/step, loss=0.07495, avg_loss=0.07380]\n",
      "Step 529015  [5.471 sec/step, loss=0.06571, avg_loss=0.07372]\n",
      "Step 529016  [5.483 sec/step, loss=0.07415, avg_loss=0.07371]\n",
      "Generated 32 batches of size 32 in 2.748 sec\n",
      "Step 529017  [5.427 sec/step, loss=0.07189, avg_loss=0.07377]\n",
      "Step 529018  [5.438 sec/step, loss=0.07518, avg_loss=0.07378]\n",
      "Step 529019  [5.436 sec/step, loss=0.07463, avg_loss=0.07379]\n",
      "Step 529020  [5.467 sec/step, loss=0.07214, avg_loss=0.07378]\n",
      "Step 529021  [5.476 sec/step, loss=0.07574, avg_loss=0.07380]\n",
      "Step 529022  [5.472 sec/step, loss=0.07351, avg_loss=0.07377]\n",
      "Step 529023  [5.452 sec/step, loss=0.07560, avg_loss=0.07377]\n",
      "Step 529024  [5.446 sec/step, loss=0.07493, avg_loss=0.07377]\n",
      "Step 529025  [5.435 sec/step, loss=0.07184, avg_loss=0.07374]\n",
      "Step 529026  [5.446 sec/step, loss=0.07521, avg_loss=0.07378]\n",
      "Step 529027  [5.445 sec/step, loss=0.07483, avg_loss=0.07379]\n",
      "Step 529028  [5.441 sec/step, loss=0.07507, avg_loss=0.07379]\n",
      "Step 529029  [5.411 sec/step, loss=0.06737, avg_loss=0.07371]\n",
      "Step 529030  [5.396 sec/step, loss=0.07189, avg_loss=0.07366]\n",
      "Step 529031  [5.400 sec/step, loss=0.07611, avg_loss=0.07366]\n",
      "Step 529032  [5.454 sec/step, loss=0.06588, avg_loss=0.07359]\n",
      "Step 529033  [5.466 sec/step, loss=0.07539, avg_loss=0.07364]\n",
      "Step 529034  [5.492 sec/step, loss=0.07394, avg_loss=0.07373]\n",
      "Step 529035  [5.488 sec/step, loss=0.07324, avg_loss=0.07371]\n",
      "Step 529036  [5.515 sec/step, loss=0.07557, avg_loss=0.07376]\n",
      "Step 529037  [5.512 sec/step, loss=0.07331, avg_loss=0.07375]\n",
      "Step 529038  [5.540 sec/step, loss=0.07293, avg_loss=0.07377]\n",
      "Step 529039  [5.543 sec/step, loss=0.07333, avg_loss=0.07375]\n",
      "Step 529040  [5.540 sec/step, loss=0.07506, avg_loss=0.07375]\n",
      "Step 529041  [5.559 sec/step, loss=0.07579, avg_loss=0.07376]\n",
      "Step 529042  [5.555 sec/step, loss=0.07287, avg_loss=0.07373]\n",
      "Step 529043  [5.549 sec/step, loss=0.07431, avg_loss=0.07372]\n",
      "Step 529044  [5.533 sec/step, loss=0.07551, avg_loss=0.07371]\n",
      "Step 529045  [5.548 sec/step, loss=0.07577, avg_loss=0.07372]\n",
      "Step 529046  [5.545 sec/step, loss=0.07646, avg_loss=0.07375]\n",
      "Step 529047  [5.532 sec/step, loss=0.07475, avg_loss=0.07377]\n",
      "Step 529048  [5.515 sec/step, loss=0.07120, avg_loss=0.07376]\n",
      "Generated 32 batches of size 32 in 2.395 sec\n",
      "Step 529049  [5.524 sec/step, loss=0.07535, avg_loss=0.07375]\n",
      "Step 529050  [5.520 sec/step, loss=0.07439, avg_loss=0.07375]\n",
      "Step 529051  [5.532 sec/step, loss=0.07621, avg_loss=0.07378]\n",
      "Step 529052  [5.513 sec/step, loss=0.07398, avg_loss=0.07375]\n",
      "Step 529053  [5.501 sec/step, loss=0.07533, avg_loss=0.07375]\n",
      "Step 529054  [5.503 sec/step, loss=0.07597, avg_loss=0.07376]\n",
      "Step 529055  [5.496 sec/step, loss=0.07366, avg_loss=0.07376]\n",
      "Step 529056  [5.484 sec/step, loss=0.07114, avg_loss=0.07375]\n",
      "Step 529057  [5.466 sec/step, loss=0.07400, avg_loss=0.07374]\n",
      "Step 529058  [5.492 sec/step, loss=0.07328, avg_loss=0.07374]\n",
      "Step 529059  [5.480 sec/step, loss=0.07282, avg_loss=0.07372]\n",
      "Step 529060  [5.476 sec/step, loss=0.07444, avg_loss=0.07371]\n",
      "Step 529061  [5.484 sec/step, loss=0.07589, avg_loss=0.07372]\n",
      "Step 529062  [5.468 sec/step, loss=0.07260, avg_loss=0.07372]\n",
      "Step 529063  [5.407 sec/step, loss=0.07391, avg_loss=0.07381]\n",
      "Step 529064  [5.377 sec/step, loss=0.06395, avg_loss=0.07371]\n",
      "Step 529065  [5.384 sec/step, loss=0.07355, avg_loss=0.07370]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 529066  [5.392 sec/step, loss=0.07585, avg_loss=0.07371]\n",
      "Step 529067  [5.382 sec/step, loss=0.07578, avg_loss=0.07371]\n",
      "Step 529068  [5.382 sec/step, loss=0.07431, avg_loss=0.07372]\n",
      "Step 529069  [5.388 sec/step, loss=0.07313, avg_loss=0.07372]\n",
      "Step 529070  [5.426 sec/step, loss=0.06670, avg_loss=0.07363]\n",
      "Step 529071  [5.404 sec/step, loss=0.07471, avg_loss=0.07364]\n",
      "Step 529072  [5.403 sec/step, loss=0.07501, avg_loss=0.07364]\n",
      "Step 529073  [5.417 sec/step, loss=0.07459, avg_loss=0.07365]\n",
      "Step 529074  [5.434 sec/step, loss=0.07297, avg_loss=0.07365]\n",
      "Step 529075  [5.414 sec/step, loss=0.06980, avg_loss=0.07359]\n",
      "Step 529076  [5.409 sec/step, loss=0.07211, avg_loss=0.07355]\n",
      "Step 529077  [5.412 sec/step, loss=0.07397, avg_loss=0.07355]\n",
      "Step 529078  [5.409 sec/step, loss=0.07016, avg_loss=0.07354]\n",
      "Step 529079  [5.402 sec/step, loss=0.07503, avg_loss=0.07355]\n",
      "Step 529080  [5.414 sec/step, loss=0.07615, avg_loss=0.07357]\n",
      "Generated 32 batches of size 32 in 2.609 sec\n",
      "Step 529081  [5.427 sec/step, loss=0.07083, avg_loss=0.07354]\n",
      "Step 529082  [5.428 sec/step, loss=0.07551, avg_loss=0.07355]\n",
      "Step 529083  [5.370 sec/step, loss=0.07397, avg_loss=0.07362]\n",
      "Step 529084  [5.372 sec/step, loss=0.07633, avg_loss=0.07362]\n",
      "Step 529085  [5.381 sec/step, loss=0.07661, avg_loss=0.07367]\n",
      "Step 529086  [5.401 sec/step, loss=0.07523, avg_loss=0.07371]\n",
      "Step 529087  [5.391 sec/step, loss=0.07428, avg_loss=0.07369]\n",
      "Step 529088  [5.397 sec/step, loss=0.07624, avg_loss=0.07371]\n",
      "Step 529089  [5.417 sec/step, loss=0.07444, avg_loss=0.07379]\n",
      "Step 529090  [5.423 sec/step, loss=0.07412, avg_loss=0.07379]\n",
      "Step 529091  [5.421 sec/step, loss=0.07338, avg_loss=0.07377]\n",
      "Step 529092  [5.436 sec/step, loss=0.07374, avg_loss=0.07377]\n",
      "Step 529093  [5.439 sec/step, loss=0.07432, avg_loss=0.07378]\n",
      "Step 529094  [5.410 sec/step, loss=0.07380, avg_loss=0.07379]\n",
      "Step 529095  [5.405 sec/step, loss=0.07330, avg_loss=0.07377]\n",
      "Step 529096  [5.419 sec/step, loss=0.07602, avg_loss=0.07379]\n",
      "Step 529097  [5.407 sec/step, loss=0.07165, avg_loss=0.07376]\n",
      "Step 529098  [5.424 sec/step, loss=0.07542, avg_loss=0.07385]\n",
      "Step 529099  [5.395 sec/step, loss=0.06564, avg_loss=0.07374]\n",
      "Step 529100  [5.399 sec/step, loss=0.07605, avg_loss=0.07378]\n",
      "Writing summary at step: 529100\n",
      "Step 529101  [5.426 sec/step, loss=0.07557, avg_loss=0.07380]\n",
      "Step 529102  [5.426 sec/step, loss=0.07624, avg_loss=0.07380]\n",
      "Step 529103  [5.432 sec/step, loss=0.07614, avg_loss=0.07380]\n",
      "Step 529104  [5.409 sec/step, loss=0.07204, avg_loss=0.07376]\n",
      "Step 529105  [5.421 sec/step, loss=0.07312, avg_loss=0.07373]\n",
      "Step 529106  [5.467 sec/step, loss=0.06555, avg_loss=0.07364]\n",
      "Step 529107  [5.473 sec/step, loss=0.07649, avg_loss=0.07364]\n",
      "Step 529108  [5.475 sec/step, loss=0.07502, avg_loss=0.07369]\n",
      "Step 529109  [5.467 sec/step, loss=0.07106, avg_loss=0.07363]\n",
      "Step 529110  [5.456 sec/step, loss=0.07302, avg_loss=0.07362]\n",
      "Step 529111  [5.456 sec/step, loss=0.07533, avg_loss=0.07365]\n",
      "Generated 32 batches of size 32 in 2.566 sec\n",
      "Step 529112  [5.463 sec/step, loss=0.07418, avg_loss=0.07366]\n",
      "Step 529113  [5.444 sec/step, loss=0.07042, avg_loss=0.07362]\n",
      "Step 529114  [5.453 sec/step, loss=0.07607, avg_loss=0.07363]\n",
      "Step 529115  [5.404 sec/step, loss=0.07279, avg_loss=0.07370]\n",
      "Step 529116  [5.405 sec/step, loss=0.07591, avg_loss=0.07372]\n",
      "Step 529117  [5.406 sec/step, loss=0.07409, avg_loss=0.07374]\n",
      "Step 529118  [5.398 sec/step, loss=0.07363, avg_loss=0.07372]\n",
      "Step 529119  [5.406 sec/step, loss=0.07470, avg_loss=0.07373]\n",
      "Step 529120  [5.378 sec/step, loss=0.07509, avg_loss=0.07375]\n",
      "Step 529121  [5.380 sec/step, loss=0.07427, avg_loss=0.07374]\n",
      "Step 529122  [5.385 sec/step, loss=0.07075, avg_loss=0.07371]\n",
      "Step 529123  [5.401 sec/step, loss=0.07644, avg_loss=0.07372]\n",
      "Step 529124  [5.416 sec/step, loss=0.07327, avg_loss=0.07370]\n",
      "Step 529125  [5.425 sec/step, loss=0.07464, avg_loss=0.07373]\n",
      "Step 529126  [5.411 sec/step, loss=0.07408, avg_loss=0.07372]\n",
      "Step 529127  [5.412 sec/step, loss=0.07510, avg_loss=0.07372]\n",
      "Step 529128  [5.395 sec/step, loss=0.06732, avg_loss=0.07365]\n",
      "Step 529129  [5.416 sec/step, loss=0.07469, avg_loss=0.07372]\n",
      "Step 529130  [5.416 sec/step, loss=0.07329, avg_loss=0.07373]\n",
      "Step 529131  [5.403 sec/step, loss=0.07407, avg_loss=0.07371]\n",
      "Step 529132  [5.363 sec/step, loss=0.07591, avg_loss=0.07381]\n",
      "Step 529133  [5.379 sec/step, loss=0.07499, avg_loss=0.07381]\n",
      "Step 529134  [5.420 sec/step, loss=0.06639, avg_loss=0.07373]\n",
      "Step 529135  [5.449 sec/step, loss=0.07290, avg_loss=0.07373]\n",
      "Step 529136  [5.432 sec/step, loss=0.07107, avg_loss=0.07369]\n",
      "Step 529137  [5.426 sec/step, loss=0.07103, avg_loss=0.07366]\n",
      "Step 529138  [5.400 sec/step, loss=0.07312, avg_loss=0.07366]\n",
      "Step 529139  [5.381 sec/step, loss=0.07192, avg_loss=0.07365]\n",
      "Step 529140  [5.379 sec/step, loss=0.07453, avg_loss=0.07365]\n",
      "Step 529141  [5.368 sec/step, loss=0.07517, avg_loss=0.07364]\n",
      "Step 529142  [5.370 sec/step, loss=0.07587, avg_loss=0.07367]\n",
      "Step 529143  [5.376 sec/step, loss=0.07545, avg_loss=0.07368]\n",
      "Generated 32 batches of size 32 in 2.433 sec\n",
      "Step 529144  [5.385 sec/step, loss=0.07441, avg_loss=0.07367]\n",
      "Step 529145  [5.367 sec/step, loss=0.07421, avg_loss=0.07365]\n",
      "Step 529146  [5.371 sec/step, loss=0.07391, avg_loss=0.07363]\n",
      "Step 529147  [5.366 sec/step, loss=0.07314, avg_loss=0.07361]\n",
      "Step 529148  [5.383 sec/step, loss=0.07592, avg_loss=0.07366]\n",
      "Step 529149  [5.368 sec/step, loss=0.07507, avg_loss=0.07366]\n",
      "Step 529150  [5.376 sec/step, loss=0.07579, avg_loss=0.07367]\n",
      "Step 529151  [5.366 sec/step, loss=0.07515, avg_loss=0.07366]\n",
      "Step 529152  [5.385 sec/step, loss=0.07571, avg_loss=0.07368]\n",
      "Step 529153  [5.378 sec/step, loss=0.07503, avg_loss=0.07367]\n",
      "Step 529154  [5.366 sec/step, loss=0.07036, avg_loss=0.07362]\n",
      "Step 529155  [5.366 sec/step, loss=0.07353, avg_loss=0.07362]\n",
      "Step 529156  [5.367 sec/step, loss=0.07297, avg_loss=0.07363]\n",
      "Step 529157  [5.375 sec/step, loss=0.07427, avg_loss=0.07364]\n",
      "Step 529158  [5.361 sec/step, loss=0.07390, avg_loss=0.07364]\n",
      "Step 529159  [5.374 sec/step, loss=0.07534, avg_loss=0.07367]\n",
      "Step 529160  [5.380 sec/step, loss=0.07565, avg_loss=0.07368]\n",
      "Step 529161  [5.392 sec/step, loss=0.07378, avg_loss=0.07366]\n",
      "Step 529162  [5.379 sec/step, loss=0.07533, avg_loss=0.07369]\n",
      "Step 529163  [5.397 sec/step, loss=0.07597, avg_loss=0.07371]\n",
      "Step 529164  [5.465 sec/step, loss=0.06626, avg_loss=0.07373]\n",
      "Step 529165  [5.479 sec/step, loss=0.07271, avg_loss=0.07372]\n",
      "Step 529166  [5.475 sec/step, loss=0.07466, avg_loss=0.07371]\n",
      "Step 529167  [5.481 sec/step, loss=0.07385, avg_loss=0.07369]\n",
      "Step 529168  [5.464 sec/step, loss=0.06617, avg_loss=0.07361]\n",
      "Step 529169  [5.451 sec/step, loss=0.07189, avg_loss=0.07360]\n",
      "Step 529170  [5.412 sec/step, loss=0.07590, avg_loss=0.07369]\n",
      "Step 529171  [5.412 sec/step, loss=0.07365, avg_loss=0.07368]\n",
      "Step 529172  [5.406 sec/step, loss=0.07236, avg_loss=0.07365]\n",
      "Step 529173  [5.412 sec/step, loss=0.07557, avg_loss=0.07366]\n",
      "Step 529174  [5.387 sec/step, loss=0.07563, avg_loss=0.07369]\n",
      "Step 529175  [5.396 sec/step, loss=0.07454, avg_loss=0.07374]\n",
      "Generated 32 batches of size 32 in 2.420 sec\n",
      "Step 529176  [5.405 sec/step, loss=0.07369, avg_loss=0.07375]\n",
      "Step 529177  [5.399 sec/step, loss=0.07409, avg_loss=0.07375]\n",
      "Step 529178  [5.427 sec/step, loss=0.07590, avg_loss=0.07381]\n",
      "Step 529179  [5.433 sec/step, loss=0.07592, avg_loss=0.07382]\n",
      "Step 529180  [5.414 sec/step, loss=0.07110, avg_loss=0.07377]\n",
      "Step 529181  [5.426 sec/step, loss=0.07668, avg_loss=0.07383]\n",
      "Step 529182  [5.424 sec/step, loss=0.07366, avg_loss=0.07381]\n",
      "Step 529183  [5.431 sec/step, loss=0.07362, avg_loss=0.07381]\n",
      "Step 529184  [5.428 sec/step, loss=0.07501, avg_loss=0.07379]\n",
      "Step 529185  [5.429 sec/step, loss=0.07570, avg_loss=0.07378]\n",
      "Step 529186  [5.413 sec/step, loss=0.07014, avg_loss=0.07373]\n",
      "Step 529187  [5.407 sec/step, loss=0.07380, avg_loss=0.07373]\n",
      "Step 529188  [5.390 sec/step, loss=0.07458, avg_loss=0.07371]\n",
      "Step 529189  [5.398 sec/step, loss=0.07669, avg_loss=0.07373]\n",
      "Step 529190  [5.389 sec/step, loss=0.07397, avg_loss=0.07373]\n",
      "Step 529191  [5.393 sec/step, loss=0.07450, avg_loss=0.07374]\n",
      "Step 529192  [5.385 sec/step, loss=0.07427, avg_loss=0.07375]\n",
      "Step 529193  [5.382 sec/step, loss=0.07367, avg_loss=0.07374]\n",
      "Step 529194  [5.415 sec/step, loss=0.07326, avg_loss=0.07374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 529195  [5.405 sec/step, loss=0.07117, avg_loss=0.07372]\n",
      "Step 529196  [5.442 sec/step, loss=0.06599, avg_loss=0.07362]\n",
      "Step 529197  [5.439 sec/step, loss=0.07474, avg_loss=0.07365]\n",
      "Step 529198  [5.442 sec/step, loss=0.07479, avg_loss=0.07364]\n",
      "Step 529199  [5.444 sec/step, loss=0.06532, avg_loss=0.07364]\n",
      "Step 529200  [5.448 sec/step, loss=0.07591, avg_loss=0.07364]\n",
      "Writing summary at step: 529200\n",
      "Step 529201  [5.427 sec/step, loss=0.07531, avg_loss=0.07363]\n",
      "Step 529202  [5.421 sec/step, loss=0.07485, avg_loss=0.07362]\n",
      "Step 529203  [5.404 sec/step, loss=0.07520, avg_loss=0.07361]\n",
      "Step 529204  [5.419 sec/step, loss=0.07456, avg_loss=0.07363]\n",
      "Step 529205  [5.386 sec/step, loss=0.07416, avg_loss=0.07365]\n",
      "Step 529206  [5.333 sec/step, loss=0.07234, avg_loss=0.07371]\n",
      "Generated 32 batches of size 32 in 2.386 sec\n",
      "Step 529207  [5.331 sec/step, loss=0.07270, avg_loss=0.07368]\n",
      "Step 529208  [5.332 sec/step, loss=0.07367, avg_loss=0.07366]\n",
      "Step 529209  [5.335 sec/step, loss=0.07580, avg_loss=0.07371]\n",
      "Step 529210  [5.362 sec/step, loss=0.07491, avg_loss=0.07373]\n",
      "Step 529211  [5.380 sec/step, loss=0.07306, avg_loss=0.07371]\n",
      "Step 529212  [5.391 sec/step, loss=0.07517, avg_loss=0.07372]\n",
      "Step 529213  [5.410 sec/step, loss=0.07597, avg_loss=0.07377]\n",
      "Step 529214  [5.392 sec/step, loss=0.07115, avg_loss=0.07372]\n",
      "Step 529215  [5.399 sec/step, loss=0.07637, avg_loss=0.07376]\n",
      "Step 529216  [5.386 sec/step, loss=0.07351, avg_loss=0.07373]\n",
      "Step 529217  [5.393 sec/step, loss=0.07440, avg_loss=0.07374]\n",
      "Step 529218  [5.393 sec/step, loss=0.07372, avg_loss=0.07374]\n",
      "Step 529219  [5.374 sec/step, loss=0.07397, avg_loss=0.07373]\n",
      "Step 529220  [5.384 sec/step, loss=0.07281, avg_loss=0.07371]\n",
      "Step 529221  [5.374 sec/step, loss=0.07169, avg_loss=0.07368]\n",
      "Step 529222  [5.380 sec/step, loss=0.07446, avg_loss=0.07372]\n",
      "Step 529223  [5.357 sec/step, loss=0.07146, avg_loss=0.07367]\n",
      "Step 529224  [5.395 sec/step, loss=0.06536, avg_loss=0.07359]\n",
      "Step 529225  [5.395 sec/step, loss=0.07470, avg_loss=0.07359]\n",
      "Step 529226  [5.405 sec/step, loss=0.07556, avg_loss=0.07360]\n",
      "Step 529227  [5.415 sec/step, loss=0.07572, avg_loss=0.07361]\n",
      "Step 529228  [5.440 sec/step, loss=0.07636, avg_loss=0.07370]\n",
      "Step 529229  [5.440 sec/step, loss=0.07503, avg_loss=0.07370]\n",
      "Step 529230  [5.453 sec/step, loss=0.07580, avg_loss=0.07373]\n",
      "Step 529231  [5.440 sec/step, loss=0.07327, avg_loss=0.07372]\n",
      "Step 529232  [5.457 sec/step, loss=0.07322, avg_loss=0.07370]\n",
      "Step 529233  [5.439 sec/step, loss=0.07485, avg_loss=0.07369]\n",
      "Step 529234  [5.403 sec/step, loss=0.07553, avg_loss=0.07379]\n",
      "Step 529235  [5.389 sec/step, loss=0.07535, avg_loss=0.07381]\n",
      "Step 529236  [5.398 sec/step, loss=0.07508, avg_loss=0.07385]\n",
      "Step 529237  [5.390 sec/step, loss=0.06641, avg_loss=0.07380]\n",
      "Step 529238  [5.385 sec/step, loss=0.07207, avg_loss=0.07379]\n",
      "Generated 32 batches of size 32 in 2.424 sec\n",
      "Step 529239  [5.415 sec/step, loss=0.07594, avg_loss=0.07383]\n",
      "Step 529240  [5.412 sec/step, loss=0.07557, avg_loss=0.07384]\n",
      "Step 529241  [5.412 sec/step, loss=0.07527, avg_loss=0.07384]\n",
      "Step 529242  [5.424 sec/step, loss=0.07563, avg_loss=0.07384]\n",
      "Step 529243  [5.414 sec/step, loss=0.07169, avg_loss=0.07380]\n",
      "Step 529244  [5.417 sec/step, loss=0.07465, avg_loss=0.07381]\n",
      "Step 529245  [5.413 sec/step, loss=0.07359, avg_loss=0.07380]\n",
      "Step 529246  [5.399 sec/step, loss=0.07187, avg_loss=0.07378]\n",
      "Step 529247  [5.410 sec/step, loss=0.07546, avg_loss=0.07380]\n",
      "Step 529248  [5.419 sec/step, loss=0.07591, avg_loss=0.07380]\n",
      "Step 529249  [5.419 sec/step, loss=0.07426, avg_loss=0.07380]\n",
      "Step 529250  [5.423 sec/step, loss=0.07429, avg_loss=0.07378]\n",
      "Step 529251  [5.423 sec/step, loss=0.07525, avg_loss=0.07378]\n",
      "Step 529252  [5.408 sec/step, loss=0.07419, avg_loss=0.07377]\n",
      "Step 529253  [5.413 sec/step, loss=0.07581, avg_loss=0.07377]\n",
      "Step 529254  [5.408 sec/step, loss=0.07383, avg_loss=0.07381]\n",
      "Step 529255  [5.402 sec/step, loss=0.07132, avg_loss=0.07379]\n",
      "Step 529256  [5.419 sec/step, loss=0.07577, avg_loss=0.07381]\n",
      "Step 529257  [5.426 sec/step, loss=0.07604, avg_loss=0.07383]\n",
      "Step 529258  [5.421 sec/step, loss=0.07472, avg_loss=0.07384]\n",
      "Step 529259  [5.421 sec/step, loss=0.07507, avg_loss=0.07384]\n",
      "Step 529260  [5.409 sec/step, loss=0.07531, avg_loss=0.07383]\n",
      "Step 529261  [5.385 sec/step, loss=0.07152, avg_loss=0.07381]\n",
      "Step 529262  [5.402 sec/step, loss=0.07659, avg_loss=0.07382]\n",
      "Step 529263  [5.377 sec/step, loss=0.06506, avg_loss=0.07371]\n",
      "Step 529264  [5.377 sec/step, loss=0.06674, avg_loss=0.07372]\n",
      "Step 529265  [5.357 sec/step, loss=0.07595, avg_loss=0.07375]\n",
      "Step 529266  [5.376 sec/step, loss=0.07316, avg_loss=0.07374]\n",
      "Step 529267  [5.370 sec/step, loss=0.07492, avg_loss=0.07375]\n",
      "Step 529268  [5.384 sec/step, loss=0.07220, avg_loss=0.07381]\n",
      "Step 529269  [5.412 sec/step, loss=0.07610, avg_loss=0.07385]\n",
      "Step 529270  [5.396 sec/step, loss=0.07371, avg_loss=0.07383]\n",
      "Generated 32 batches of size 32 in 2.415 sec\n",
      "Step 529271  [5.415 sec/step, loss=0.07520, avg_loss=0.07384]\n",
      "Step 529272  [5.419 sec/step, loss=0.07457, avg_loss=0.07387]\n",
      "Step 529273  [5.405 sec/step, loss=0.07167, avg_loss=0.07383]\n",
      "Step 529274  [5.400 sec/step, loss=0.07523, avg_loss=0.07382]\n",
      "Step 529275  [5.397 sec/step, loss=0.07395, avg_loss=0.07382]\n",
      "Step 529276  [5.400 sec/step, loss=0.07579, avg_loss=0.07384]\n",
      "Step 529277  [5.422 sec/step, loss=0.07461, avg_loss=0.07384]\n",
      "Step 529278  [5.401 sec/step, loss=0.07349, avg_loss=0.07382]\n",
      "Step 529279  [5.388 sec/step, loss=0.07389, avg_loss=0.07380]\n",
      "Step 529280  [5.402 sec/step, loss=0.07516, avg_loss=0.07384]\n",
      "Step 529281  [5.385 sec/step, loss=0.07062, avg_loss=0.07378]\n",
      "Step 529282  [5.390 sec/step, loss=0.07486, avg_loss=0.07379]\n",
      "Step 529283  [5.400 sec/step, loss=0.07481, avg_loss=0.07380]\n",
      "Step 529284  [5.408 sec/step, loss=0.07582, avg_loss=0.07381]\n",
      "Step 529285  [5.403 sec/step, loss=0.07578, avg_loss=0.07381]\n",
      "Step 529286  [5.398 sec/step, loss=0.07134, avg_loss=0.07382]\n",
      "Step 529287  [5.425 sec/step, loss=0.07312, avg_loss=0.07382]\n",
      "Step 529288  [5.420 sec/step, loss=0.07405, avg_loss=0.07381]\n",
      "Step 529289  [5.407 sec/step, loss=0.07482, avg_loss=0.07379]\n",
      "Step 529290  [5.420 sec/step, loss=0.07540, avg_loss=0.07381]\n",
      "Step 529291  [5.408 sec/step, loss=0.07324, avg_loss=0.07379]\n",
      "Step 529292  [5.403 sec/step, loss=0.07310, avg_loss=0.07378]\n",
      "Step 529293  [5.391 sec/step, loss=0.06587, avg_loss=0.07370]\n",
      "Step 529294  [5.376 sec/step, loss=0.07568, avg_loss=0.07373]\n",
      "Step 529295  [5.394 sec/step, loss=0.07433, avg_loss=0.07376]\n",
      "Step 529296  [5.339 sec/step, loss=0.07353, avg_loss=0.07384]\n",
      "Step 529297  [5.329 sec/step, loss=0.07392, avg_loss=0.07383]\n",
      "Step 529298  [5.350 sec/step, loss=0.07316, avg_loss=0.07381]\n",
      "Step 529299  [5.417 sec/step, loss=0.06732, avg_loss=0.07383]\n",
      "Step 529300  [5.405 sec/step, loss=0.07515, avg_loss=0.07382]\n",
      "Writing summary at step: 529300\n",
      "Step 529301  [5.417 sec/step, loss=0.07622, avg_loss=0.07383]\n",
      "Generated 32 batches of size 32 in 2.362 sec\n",
      "Step 529302  [5.425 sec/step, loss=0.07532, avg_loss=0.07384]\n",
      "Step 529303  [5.418 sec/step, loss=0.07390, avg_loss=0.07382]\n",
      "Step 529304  [5.416 sec/step, loss=0.07557, avg_loss=0.07383]\n",
      "Step 529305  [5.435 sec/step, loss=0.07610, avg_loss=0.07385]\n",
      "Step 529306  [5.428 sec/step, loss=0.07092, avg_loss=0.07384]\n",
      "Step 529307  [5.424 sec/step, loss=0.07502, avg_loss=0.07386]\n",
      "Step 529308  [5.423 sec/step, loss=0.07498, avg_loss=0.07388]\n",
      "Step 529309  [5.421 sec/step, loss=0.07423, avg_loss=0.07386]\n",
      "Step 529310  [5.419 sec/step, loss=0.07622, avg_loss=0.07387]\n",
      "Step 529311  [5.398 sec/step, loss=0.07472, avg_loss=0.07389]\n",
      "Step 529312  [5.394 sec/step, loss=0.07356, avg_loss=0.07387]\n",
      "Step 529313  [5.390 sec/step, loss=0.07430, avg_loss=0.07386]\n",
      "Step 529314  [5.386 sec/step, loss=0.07212, avg_loss=0.07387]\n",
      "Step 529315  [5.388 sec/step, loss=0.07609, avg_loss=0.07386]\n",
      "Step 529316  [5.390 sec/step, loss=0.07532, avg_loss=0.07388]\n",
      "Step 529317  [5.398 sec/step, loss=0.07380, avg_loss=0.07388]\n",
      "Step 529318  [5.452 sec/step, loss=0.06602, avg_loss=0.07380]\n",
      "Step 529319  [5.460 sec/step, loss=0.07511, avg_loss=0.07381]\n",
      "Step 529320  [5.445 sec/step, loss=0.07403, avg_loss=0.07382]\n",
      "Step 529321  [5.453 sec/step, loss=0.07480, avg_loss=0.07385]\n",
      "Step 529322  [5.475 sec/step, loss=0.07253, avg_loss=0.07383]\n",
      "Step 529323  [5.477 sec/step, loss=0.07005, avg_loss=0.07382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 529324  [5.431 sec/step, loss=0.07435, avg_loss=0.07391]\n",
      "Step 529325  [5.439 sec/step, loss=0.07561, avg_loss=0.07392]\n",
      "Step 529326  [5.452 sec/step, loss=0.07608, avg_loss=0.07392]\n",
      "Step 529327  [5.441 sec/step, loss=0.07317, avg_loss=0.07390]\n",
      "Step 529328  [5.436 sec/step, loss=0.07554, avg_loss=0.07389]\n",
      "Step 529329  [5.447 sec/step, loss=0.07518, avg_loss=0.07389]\n",
      "Step 529330  [5.435 sec/step, loss=0.07487, avg_loss=0.07388]\n",
      "Step 529331  [5.446 sec/step, loss=0.07557, avg_loss=0.07391]\n",
      "Step 529332  [5.417 sec/step, loss=0.07510, avg_loss=0.07393]\n",
      "Step 529333  [5.411 sec/step, loss=0.07377, avg_loss=0.07391]\n",
      "Generated 32 batches of size 32 in 2.558 sec\n",
      "Step 529334  [5.391 sec/step, loss=0.07234, avg_loss=0.07388]\n",
      "Step 529335  [5.383 sec/step, loss=0.07620, avg_loss=0.07389]\n",
      "Step 529336  [5.377 sec/step, loss=0.07440, avg_loss=0.07388]\n",
      "Step 529337  [5.379 sec/step, loss=0.06552, avg_loss=0.07388]\n",
      "Step 529338  [5.380 sec/step, loss=0.07247, avg_loss=0.07388]\n",
      "Step 529339  [5.374 sec/step, loss=0.07663, avg_loss=0.07389]\n",
      "Step 529340  [5.391 sec/step, loss=0.07459, avg_loss=0.07388]\n",
      "Step 529341  [5.389 sec/step, loss=0.07281, avg_loss=0.07385]\n",
      "Step 529342  [5.364 sec/step, loss=0.07381, avg_loss=0.07383]\n",
      "Step 529343  [5.376 sec/step, loss=0.07379, avg_loss=0.07385]\n",
      "Step 529344  [5.382 sec/step, loss=0.07658, avg_loss=0.07387]\n",
      "Step 529345  [5.401 sec/step, loss=0.07483, avg_loss=0.07389]\n",
      "Step 529346  [5.400 sec/step, loss=0.07495, avg_loss=0.07392]\n",
      "Step 529347  [5.390 sec/step, loss=0.07299, avg_loss=0.07389]\n",
      "Step 529348  [5.383 sec/step, loss=0.07533, avg_loss=0.07389]\n",
      "Step 529349  [5.374 sec/step, loss=0.07295, avg_loss=0.07387]\n",
      "Step 529350  [5.375 sec/step, loss=0.07573, avg_loss=0.07389]\n",
      "Step 529351  [5.389 sec/step, loss=0.07533, avg_loss=0.07389]\n",
      "Step 529352  [5.398 sec/step, loss=0.07496, avg_loss=0.07390]\n",
      "Step 529353  [5.398 sec/step, loss=0.07629, avg_loss=0.07390]\n",
      "Step 529354  [5.391 sec/step, loss=0.07180, avg_loss=0.07388]\n",
      "Step 529355  [5.402 sec/step, loss=0.07198, avg_loss=0.07389]\n",
      "Step 529356  [5.377 sec/step, loss=0.06714, avg_loss=0.07380]\n",
      "Step 529357  [5.373 sec/step, loss=0.07542, avg_loss=0.07380]\n",
      "Step 529358  [5.379 sec/step, loss=0.07681, avg_loss=0.07382]\n",
      "Step 529359  [5.372 sec/step, loss=0.07402, avg_loss=0.07381]\n",
      "Step 529360  [5.372 sec/step, loss=0.07531, avg_loss=0.07381]\n",
      "Step 529361  [5.383 sec/step, loss=0.07529, avg_loss=0.07384]\n",
      "Step 529362  [5.380 sec/step, loss=0.07625, avg_loss=0.07384]\n",
      "Step 529363  [5.394 sec/step, loss=0.07476, avg_loss=0.07394]\n",
      "Step 529364  [5.353 sec/step, loss=0.07620, avg_loss=0.07403]\n",
      "Step 529365  [5.350 sec/step, loss=0.07425, avg_loss=0.07401]\n",
      "Generated 32 batches of size 32 in 2.341 sec\n",
      "Step 529366  [5.338 sec/step, loss=0.07360, avg_loss=0.07402]\n",
      "Step 529367  [5.337 sec/step, loss=0.07571, avg_loss=0.07403]\n",
      "Step 529368  [5.354 sec/step, loss=0.07412, avg_loss=0.07405]\n",
      "Step 529369  [5.341 sec/step, loss=0.07261, avg_loss=0.07401]\n",
      "Step 529370  [5.337 sec/step, loss=0.07407, avg_loss=0.07401]\n",
      "Step 529371  [5.365 sec/step, loss=0.06599, avg_loss=0.07392]\n",
      "Step 529372  [5.354 sec/step, loss=0.07039, avg_loss=0.07388]\n",
      "Step 529373  [5.344 sec/step, loss=0.07405, avg_loss=0.07390]\n",
      "Step 529374  [5.375 sec/step, loss=0.07322, avg_loss=0.07388]\n",
      "Step 529375  [5.364 sec/step, loss=0.07184, avg_loss=0.07386]\n",
      "Step 529376  [5.348 sec/step, loss=0.07425, avg_loss=0.07385]\n",
      "Step 529377  [5.320 sec/step, loss=0.06609, avg_loss=0.07376]\n",
      "Step 529378  [5.322 sec/step, loss=0.07135, avg_loss=0.07374]\n",
      "Step 529379  [5.344 sec/step, loss=0.07589, avg_loss=0.07376]\n",
      "Step 529380  [5.333 sec/step, loss=0.07328, avg_loss=0.07374]\n",
      "Step 529381  [5.346 sec/step, loss=0.07621, avg_loss=0.07380]\n",
      "Step 529382  [5.345 sec/step, loss=0.07411, avg_loss=0.07379]\n",
      "Step 529383  [5.325 sec/step, loss=0.07187, avg_loss=0.07376]\n",
      "Step 529384  [5.323 sec/step, loss=0.07618, avg_loss=0.07377]\n",
      "Step 529385  [5.312 sec/step, loss=0.07363, avg_loss=0.07374]\n",
      "Step 529386  [5.328 sec/step, loss=0.07397, avg_loss=0.07377]\n",
      "Step 529387  [5.305 sec/step, loss=0.07526, avg_loss=0.07379]\n",
      "Step 529388  [5.312 sec/step, loss=0.07490, avg_loss=0.07380]\n",
      "Step 529389  [5.331 sec/step, loss=0.07392, avg_loss=0.07379]\n",
      "Step 529390  [5.327 sec/step, loss=0.07529, avg_loss=0.07379]\n",
      "Step 529391  [5.338 sec/step, loss=0.07517, avg_loss=0.07381]\n",
      "Step 529392  [5.390 sec/step, loss=0.06581, avg_loss=0.07374]\n",
      "Step 529393  [5.402 sec/step, loss=0.07491, avg_loss=0.07383]\n",
      "Step 529394  [5.391 sec/step, loss=0.07509, avg_loss=0.07382]\n",
      "Step 529395  [5.413 sec/step, loss=0.07335, avg_loss=0.07381]\n",
      "Step 529396  [5.419 sec/step, loss=0.07527, avg_loss=0.07383]\n",
      "Step 529397  [5.431 sec/step, loss=0.07443, avg_loss=0.07383]\n",
      "Generated 32 batches of size 32 in 2.327 sec\n",
      "Step 529398  [5.415 sec/step, loss=0.07585, avg_loss=0.07386]\n",
      "Step 529399  [5.363 sec/step, loss=0.07103, avg_loss=0.07390]\n",
      "Step 529400  [5.365 sec/step, loss=0.07465, avg_loss=0.07389]\n",
      "Writing summary at step: 529400\n",
      "Step 529401  [5.366 sec/step, loss=0.07567, avg_loss=0.07389]\n",
      "Step 529402  [5.371 sec/step, loss=0.07645, avg_loss=0.07390]\n",
      "Step 529403  [5.381 sec/step, loss=0.07357, avg_loss=0.07390]\n",
      "Step 529404  [5.388 sec/step, loss=0.07607, avg_loss=0.07390]\n",
      "Step 529405  [5.388 sec/step, loss=0.07356, avg_loss=0.07387]\n",
      "Step 529406  [5.397 sec/step, loss=0.07422, avg_loss=0.07391]\n",
      "Step 529407  [5.391 sec/step, loss=0.07114, avg_loss=0.07387]\n",
      "Step 529408  [5.383 sec/step, loss=0.07035, avg_loss=0.07382]\n",
      "Step 529409  [5.399 sec/step, loss=0.07527, avg_loss=0.07383]\n",
      "Step 529410  [5.394 sec/step, loss=0.07597, avg_loss=0.07383]\n",
      "Step 529411  [5.419 sec/step, loss=0.07462, avg_loss=0.07383]\n",
      "Step 529412  [5.414 sec/step, loss=0.07508, avg_loss=0.07384]\n",
      "Step 529413  [5.411 sec/step, loss=0.07514, avg_loss=0.07385]\n",
      "Step 529414  [5.431 sec/step, loss=0.07620, avg_loss=0.07389]\n",
      "Step 529415  [5.432 sec/step, loss=0.07580, avg_loss=0.07389]\n",
      "Step 529416  [5.425 sec/step, loss=0.07361, avg_loss=0.07387]\n",
      "Step 529417  [5.418 sec/step, loss=0.07545, avg_loss=0.07389]\n",
      "Step 529418  [5.367 sec/step, loss=0.07442, avg_loss=0.07397]\n",
      "Step 529419  [5.415 sec/step, loss=0.06617, avg_loss=0.07388]\n",
      "Step 529420  [5.422 sec/step, loss=0.07517, avg_loss=0.07390]\n",
      "Step 529421  [5.407 sec/step, loss=0.07401, avg_loss=0.07389]\n",
      "Step 529422  [5.377 sec/step, loss=0.07356, avg_loss=0.07390]\n",
      "Step 529423  [5.396 sec/step, loss=0.07624, avg_loss=0.07396]\n",
      "Step 529424  [5.402 sec/step, loss=0.07435, avg_loss=0.07396]\n",
      "Step 529425  [5.386 sec/step, loss=0.07367, avg_loss=0.07394]\n",
      "Step 529426  [5.387 sec/step, loss=0.07602, avg_loss=0.07394]\n",
      "Step 529427  [5.395 sec/step, loss=0.07607, avg_loss=0.07397]\n",
      "Step 529428  [5.377 sec/step, loss=0.07126, avg_loss=0.07393]\n",
      "Generated 32 batches of size 32 in 2.446 sec\n",
      "Step 529429  [5.383 sec/step, loss=0.07541, avg_loss=0.07393]\n",
      "Step 529430  [5.384 sec/step, loss=0.07506, avg_loss=0.07393]\n",
      "Step 529431  [5.384 sec/step, loss=0.07558, avg_loss=0.07393]\n",
      "Step 529432  [5.383 sec/step, loss=0.07530, avg_loss=0.07393]\n",
      "Step 529433  [5.373 sec/step, loss=0.06564, avg_loss=0.07385]\n",
      "Step 529434  [5.378 sec/step, loss=0.07319, avg_loss=0.07386]\n",
      "Step 529435  [5.363 sec/step, loss=0.07410, avg_loss=0.07384]\n",
      "Step 529436  [5.367 sec/step, loss=0.07539, avg_loss=0.07385]\n",
      "Step 529437  [5.394 sec/step, loss=0.07462, avg_loss=0.07394]\n",
      "Step 529438  [5.398 sec/step, loss=0.07539, avg_loss=0.07397]\n",
      "Step 529439  [5.402 sec/step, loss=0.07563, avg_loss=0.07396]\n",
      "Step 529440  [5.378 sec/step, loss=0.07250, avg_loss=0.07394]\n",
      "Step 529441  [5.399 sec/step, loss=0.07503, avg_loss=0.07396]\n",
      "Step 529442  [5.403 sec/step, loss=0.07462, avg_loss=0.07397]\n",
      "Step 529443  [5.404 sec/step, loss=0.07515, avg_loss=0.07398]\n",
      "Step 529444  [5.380 sec/step, loss=0.07409, avg_loss=0.07396]\n",
      "Step 529445  [5.375 sec/step, loss=0.07479, avg_loss=0.07396]\n",
      "Step 529446  [5.388 sec/step, loss=0.07618, avg_loss=0.07397]\n",
      "Step 529447  [5.445 sec/step, loss=0.06685, avg_loss=0.07391]\n",
      "Step 529448  [5.437 sec/step, loss=0.07372, avg_loss=0.07389]\n",
      "Step 529449  [5.438 sec/step, loss=0.07484, avg_loss=0.07391]\n",
      "Step 529450  [5.436 sec/step, loss=0.07388, avg_loss=0.07389]\n",
      "Step 529451  [5.418 sec/step, loss=0.07039, avg_loss=0.07384]\n",
      "Step 529452  [5.422 sec/step, loss=0.07596, avg_loss=0.07385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 529453  [5.403 sec/step, loss=0.06665, avg_loss=0.07376]\n",
      "Step 529454  [5.419 sec/step, loss=0.07515, avg_loss=0.07379]\n",
      "Step 529455  [5.428 sec/step, loss=0.07489, avg_loss=0.07382]\n",
      "Step 529456  [5.433 sec/step, loss=0.07041, avg_loss=0.07385]\n",
      "Step 529457  [5.441 sec/step, loss=0.07462, avg_loss=0.07384]\n",
      "Step 529458  [5.427 sec/step, loss=0.07467, avg_loss=0.07382]\n",
      "Step 529459  [5.426 sec/step, loss=0.07191, avg_loss=0.07380]\n",
      "Step 529460  [5.434 sec/step, loss=0.07346, avg_loss=0.07378]\n",
      "Generated 32 batches of size 32 in 2.403 sec\n",
      "Step 529461  [5.438 sec/step, loss=0.07456, avg_loss=0.07378]\n",
      "Step 529462  [5.436 sec/step, loss=0.07591, avg_loss=0.07377]\n",
      "Step 529463  [5.447 sec/step, loss=0.07645, avg_loss=0.07379]\n",
      "Step 529464  [5.440 sec/step, loss=0.07433, avg_loss=0.07377]\n",
      "Step 529465  [5.436 sec/step, loss=0.07289, avg_loss=0.07376]\n",
      "Step 529466  [5.408 sec/step, loss=0.07191, avg_loss=0.07374]\n",
      "Step 529467  [5.429 sec/step, loss=0.07284, avg_loss=0.07371]\n",
      "Step 529468  [5.422 sec/step, loss=0.07493, avg_loss=0.07372]\n",
      "Step 529469  [5.435 sec/step, loss=0.07646, avg_loss=0.07376]\n",
      "Step 529470  [5.450 sec/step, loss=0.07611, avg_loss=0.07378]\n",
      "Step 529471  [5.385 sec/step, loss=0.07153, avg_loss=0.07383]\n",
      "Step 529472  [5.397 sec/step, loss=0.07240, avg_loss=0.07385]\n",
      "Step 529473  [5.399 sec/step, loss=0.07437, avg_loss=0.07386]\n",
      "Step 529474  [5.398 sec/step, loss=0.07335, avg_loss=0.07386]\n",
      "Step 529475  [5.412 sec/step, loss=0.07473, avg_loss=0.07389]\n",
      "Step 529476  [5.431 sec/step, loss=0.07615, avg_loss=0.07391]\n",
      "Step 529477  [5.444 sec/step, loss=0.07348, avg_loss=0.07398]\n",
      "Step 529478  [5.453 sec/step, loss=0.07216, avg_loss=0.07399]\n",
      "Step 529479  [5.437 sec/step, loss=0.06963, avg_loss=0.07393]\n",
      "Step 529480  [5.428 sec/step, loss=0.06481, avg_loss=0.07384]\n",
      "Step 529481  [5.413 sec/step, loss=0.07491, avg_loss=0.07383]\n",
      "Step 529482  [5.406 sec/step, loss=0.07400, avg_loss=0.07383]\n",
      "Step 529483  [5.422 sec/step, loss=0.07497, avg_loss=0.07386]\n",
      "Step 529484  [5.406 sec/step, loss=0.07411, avg_loss=0.07384]\n",
      "Step 529485  [5.411 sec/step, loss=0.07610, avg_loss=0.07386]\n",
      "Step 529486  [5.409 sec/step, loss=0.07592, avg_loss=0.07388]\n",
      "Step 529487  [5.412 sec/step, loss=0.07489, avg_loss=0.07388]\n",
      "Step 529488  [5.431 sec/step, loss=0.07405, avg_loss=0.07387]\n",
      "Step 529489  [5.401 sec/step, loss=0.07260, avg_loss=0.07386]\n",
      "Step 529490  [5.409 sec/step, loss=0.07712, avg_loss=0.07387]\n",
      "Step 529491  [5.409 sec/step, loss=0.07697, avg_loss=0.07389]\n",
      "Step 529492  [5.367 sec/step, loss=0.07545, avg_loss=0.07399]\n",
      "Generated 32 batches of size 32 in 2.361 sec\n",
      "Step 529493  [5.375 sec/step, loss=0.07222, avg_loss=0.07396]\n",
      "Step 529494  [5.378 sec/step, loss=0.07587, avg_loss=0.07397]\n",
      "Step 529495  [5.361 sec/step, loss=0.07632, avg_loss=0.07400]\n",
      "Step 529496  [5.373 sec/step, loss=0.07690, avg_loss=0.07402]\n",
      "Step 529497  [5.373 sec/step, loss=0.07513, avg_loss=0.07402]\n",
      "Step 529498  [5.414 sec/step, loss=0.06632, avg_loss=0.07393]\n",
      "Step 529499  [5.429 sec/step, loss=0.07465, avg_loss=0.07396]\n",
      "Step 529500  [5.441 sec/step, loss=0.07423, avg_loss=0.07396]\n",
      "Writing summary at step: 529500\n",
      "Step 529501  [5.441 sec/step, loss=0.07649, avg_loss=0.07397]\n",
      "Step 529502  [5.431 sec/step, loss=0.07532, avg_loss=0.07396]\n",
      "Step 529503  [5.429 sec/step, loss=0.07537, avg_loss=0.07397]\n",
      "Step 529504  [5.404 sec/step, loss=0.06716, avg_loss=0.07388]\n",
      "Step 529505  [5.401 sec/step, loss=0.07626, avg_loss=0.07391]\n",
      "Step 529506  [5.394 sec/step, loss=0.07404, avg_loss=0.07391]\n",
      "Step 529507  [5.409 sec/step, loss=0.07604, avg_loss=0.07396]\n",
      "Step 529508  [5.420 sec/step, loss=0.07581, avg_loss=0.07401]\n",
      "Step 529509  [5.400 sec/step, loss=0.07423, avg_loss=0.07400]\n",
      "Step 529510  [5.412 sec/step, loss=0.07318, avg_loss=0.07398]\n",
      "Step 529511  [5.389 sec/step, loss=0.07544, avg_loss=0.07398]\n",
      "Step 529512  [5.440 sec/step, loss=0.06679, avg_loss=0.07390]\n",
      "Step 529513  [5.450 sec/step, loss=0.07626, avg_loss=0.07391]\n",
      "Step 529514  [5.443 sec/step, loss=0.07125, avg_loss=0.07386]\n",
      "Step 529515  [5.432 sec/step, loss=0.07321, avg_loss=0.07384]\n",
      "Step 529516  [5.431 sec/step, loss=0.07400, avg_loss=0.07384]\n",
      "Step 529517  [5.424 sec/step, loss=0.07554, avg_loss=0.07384]\n",
      "Step 529518  [5.430 sec/step, loss=0.07514, avg_loss=0.07385]\n",
      "Step 529519  [5.381 sec/step, loss=0.07482, avg_loss=0.07393]\n",
      "Step 529520  [5.382 sec/step, loss=0.07542, avg_loss=0.07394]\n",
      "Step 529521  [5.402 sec/step, loss=0.07398, avg_loss=0.07394]\n",
      "Step 529522  [5.405 sec/step, loss=0.07566, avg_loss=0.07396]\n",
      "Step 529523  [5.383 sec/step, loss=0.07165, avg_loss=0.07391]\n",
      "Generated 32 batches of size 32 in 2.371 sec\n",
      "Step 529524  [5.384 sec/step, loss=0.07467, avg_loss=0.07392]\n",
      "Step 529525  [5.378 sec/step, loss=0.07428, avg_loss=0.07392]\n",
      "Step 529526  [5.375 sec/step, loss=0.07388, avg_loss=0.07390]\n",
      "Step 529527  [5.353 sec/step, loss=0.07190, avg_loss=0.07386]\n",
      "Step 529528  [5.375 sec/step, loss=0.07506, avg_loss=0.07390]\n",
      "Step 529529  [5.348 sec/step, loss=0.07293, avg_loss=0.07387]\n",
      "Step 529530  [5.355 sec/step, loss=0.07484, avg_loss=0.07387]\n",
      "Step 529531  [5.367 sec/step, loss=0.07658, avg_loss=0.07388]\n",
      "Step 529532  [5.397 sec/step, loss=0.07398, avg_loss=0.07387]\n",
      "Step 529533  [5.428 sec/step, loss=0.07564, avg_loss=0.07397]\n",
      "Step 529534  [5.440 sec/step, loss=0.07638, avg_loss=0.07400]\n",
      "Step 529535  [5.449 sec/step, loss=0.07492, avg_loss=0.07401]\n",
      "Step 529536  [5.451 sec/step, loss=0.07643, avg_loss=0.07402]\n",
      "Step 529537  [5.437 sec/step, loss=0.07573, avg_loss=0.07403]\n",
      "Step 529538  [5.434 sec/step, loss=0.07284, avg_loss=0.07400]\n",
      "Step 529539  [5.418 sec/step, loss=0.07390, avg_loss=0.07399]\n",
      "Step 529540  [5.425 sec/step, loss=0.07447, avg_loss=0.07400]\n",
      "Step 529541  [5.413 sec/step, loss=0.07562, avg_loss=0.07401]\n",
      "Step 529542  [5.434 sec/step, loss=0.07639, avg_loss=0.07403]\n",
      "Step 529543  [5.444 sec/step, loss=0.07443, avg_loss=0.07402]\n",
      "Step 529544  [5.504 sec/step, loss=0.06634, avg_loss=0.07394]\n",
      "Step 529545  [5.513 sec/step, loss=0.07609, avg_loss=0.07396]\n",
      "Step 529546  [5.504 sec/step, loss=0.07495, avg_loss=0.07394]\n",
      "Step 529547  [5.440 sec/step, loss=0.07155, avg_loss=0.07399]\n",
      "Step 529548  [5.448 sec/step, loss=0.07615, avg_loss=0.07402]\n",
      "Step 529549  [5.444 sec/step, loss=0.07383, avg_loss=0.07401]\n",
      "Step 529550  [5.444 sec/step, loss=0.07684, avg_loss=0.07404]\n",
      "Step 529551  [5.441 sec/step, loss=0.07162, avg_loss=0.07405]\n",
      "Step 529552  [5.425 sec/step, loss=0.07450, avg_loss=0.07403]\n",
      "Step 529553  [5.431 sec/step, loss=0.07462, avg_loss=0.07411]\n",
      "Step 529554  [5.423 sec/step, loss=0.07504, avg_loss=0.07411]\n",
      "Step 529555  [5.423 sec/step, loss=0.07654, avg_loss=0.07413]\n",
      "Generated 32 batches of size 32 in 2.391 sec\n",
      "Step 529556  [5.441 sec/step, loss=0.07581, avg_loss=0.07418]\n",
      "Step 529557  [5.421 sec/step, loss=0.07115, avg_loss=0.07415]\n",
      "Step 529558  [5.454 sec/step, loss=0.07320, avg_loss=0.07413]\n",
      "Step 529559  [5.469 sec/step, loss=0.07292, avg_loss=0.07414]\n",
      "Step 529560  [5.462 sec/step, loss=0.07570, avg_loss=0.07416]\n",
      "Step 529561  [5.439 sec/step, loss=0.06535, avg_loss=0.07407]\n",
      "Step 529562  [5.432 sec/step, loss=0.07521, avg_loss=0.07407]\n",
      "Step 529563  [5.432 sec/step, loss=0.07535, avg_loss=0.07405]\n",
      "Step 529564  [5.436 sec/step, loss=0.07440, avg_loss=0.07406]\n",
      "Step 529565  [5.453 sec/step, loss=0.07572, avg_loss=0.07408]\n",
      "Step 529566  [5.516 sec/step, loss=0.06676, avg_loss=0.07403]\n",
      "Step 529567  [5.502 sec/step, loss=0.07646, avg_loss=0.07407]\n",
      "Step 529568  [5.488 sec/step, loss=0.07147, avg_loss=0.07403]\n",
      "Step 529569  [5.469 sec/step, loss=0.07359, avg_loss=0.07401]\n",
      "Step 529570  [5.473 sec/step, loss=0.07457, avg_loss=0.07399]\n",
      "Step 529571  [5.491 sec/step, loss=0.07460, avg_loss=0.07402]\n",
      "Step 529572  [5.486 sec/step, loss=0.07519, avg_loss=0.07405]\n",
      "Step 529573  [5.504 sec/step, loss=0.07621, avg_loss=0.07407]\n",
      "Step 529574  [5.490 sec/step, loss=0.07661, avg_loss=0.07410]\n",
      "Step 529575  [5.492 sec/step, loss=0.07559, avg_loss=0.07411]\n",
      "Step 529576  [5.489 sec/step, loss=0.07631, avg_loss=0.07411]\n",
      "Step 529577  [5.482 sec/step, loss=0.07444, avg_loss=0.07412]\n",
      "Step 529578  [5.488 sec/step, loss=0.07412, avg_loss=0.07414]\n",
      "Step 529579  [5.492 sec/step, loss=0.07454, avg_loss=0.07419]\n",
      "Step 529580  [5.509 sec/step, loss=0.07539, avg_loss=0.07429]\n",
      "Step 529581  [5.499 sec/step, loss=0.07138, avg_loss=0.07426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 529582  [5.507 sec/step, loss=0.07573, avg_loss=0.07428]\n",
      "Step 529583  [5.501 sec/step, loss=0.07323, avg_loss=0.07426]\n",
      "Step 529584  [5.515 sec/step, loss=0.07578, avg_loss=0.07428]\n",
      "Step 529585  [5.520 sec/step, loss=0.07437, avg_loss=0.07426]\n",
      "Step 529586  [5.529 sec/step, loss=0.07605, avg_loss=0.07426]\n",
      "Step 529587  [5.536 sec/step, loss=0.07582, avg_loss=0.07427]\n",
      "Generated 32 batches of size 32 in 2.408 sec\n",
      "Step 529588  [5.531 sec/step, loss=0.07645, avg_loss=0.07429]\n",
      "Step 529589  [5.535 sec/step, loss=0.07475, avg_loss=0.07431]\n",
      "Step 529590  [5.521 sec/step, loss=0.07531, avg_loss=0.07430]\n",
      "Step 529591  [5.497 sec/step, loss=0.06588, avg_loss=0.07419]\n",
      "Step 529592  [5.514 sec/step, loss=0.07503, avg_loss=0.07418]\n",
      "Step 529593  [5.511 sec/step, loss=0.07235, avg_loss=0.07418]\n",
      "Step 529594  [5.506 sec/step, loss=0.07456, avg_loss=0.07417]\n",
      "Step 529595  [5.495 sec/step, loss=0.07534, avg_loss=0.07416]\n",
      "Step 529596  [5.473 sec/step, loss=0.07324, avg_loss=0.07412]\n",
      "Step 529597  [5.469 sec/step, loss=0.07252, avg_loss=0.07410]\n",
      "Step 529598  [5.468 sec/step, loss=0.06671, avg_loss=0.07410]\n",
      "Step 529599  [5.447 sec/step, loss=0.07458, avg_loss=0.07410]\n",
      "Step 529600  [5.444 sec/step, loss=0.07628, avg_loss=0.07412]\n",
      "Writing summary at step: 529600\n",
      "Step 529601  [5.434 sec/step, loss=0.07441, avg_loss=0.07410]\n",
      "Step 529602  [5.434 sec/step, loss=0.07544, avg_loss=0.07410]\n",
      "Step 529603  [5.439 sec/step, loss=0.07529, avg_loss=0.07410]\n",
      "Step 529604  [5.466 sec/step, loss=0.07638, avg_loss=0.07419]\n",
      "Step 529605  [5.460 sec/step, loss=0.07519, avg_loss=0.07418]\n",
      "Step 529606  [5.472 sec/step, loss=0.07203, avg_loss=0.07416]\n",
      "Step 529607  [5.458 sec/step, loss=0.07520, avg_loss=0.07415]\n",
      "Step 529608  [5.463 sec/step, loss=0.07208, avg_loss=0.07412]\n",
      "Step 529609  [5.483 sec/step, loss=0.07709, avg_loss=0.07414]\n",
      "Step 529610  [5.462 sec/step, loss=0.07301, avg_loss=0.07414]\n",
      "Step 529611  [5.452 sec/step, loss=0.07073, avg_loss=0.07410]\n",
      "Step 529612  [5.413 sec/step, loss=0.07577, avg_loss=0.07419]\n",
      "Step 529613  [5.387 sec/step, loss=0.07192, avg_loss=0.07414]\n",
      "Step 529614  [5.385 sec/step, loss=0.07415, avg_loss=0.07417]\n",
      "Step 529615  [5.368 sec/step, loss=0.06620, avg_loss=0.07410]\n",
      "Step 529616  [5.374 sec/step, loss=0.07522, avg_loss=0.07411]\n",
      "Step 529617  [5.373 sec/step, loss=0.07324, avg_loss=0.07409]\n",
      "Step 529618  [5.376 sec/step, loss=0.07567, avg_loss=0.07409]\n",
      "Generated 32 batches of size 32 in 2.394 sec\n",
      "Step 529619  [5.399 sec/step, loss=0.07434, avg_loss=0.07409]\n",
      "Step 529620  [5.390 sec/step, loss=0.07383, avg_loss=0.07407]\n",
      "Step 529621  [5.385 sec/step, loss=0.07352, avg_loss=0.07407]\n",
      "Step 529622  [5.374 sec/step, loss=0.07264, avg_loss=0.07404]\n",
      "Step 529623  [5.385 sec/step, loss=0.07591, avg_loss=0.07408]\n",
      "Step 529624  [5.386 sec/step, loss=0.07466, avg_loss=0.07408]\n",
      "Step 529625  [5.396 sec/step, loss=0.07408, avg_loss=0.07408]\n",
      "Step 529626  [5.410 sec/step, loss=0.07394, avg_loss=0.07408]\n",
      "Step 529627  [5.433 sec/step, loss=0.07625, avg_loss=0.07412]\n",
      "Step 529628  [5.410 sec/step, loss=0.07209, avg_loss=0.07409]\n",
      "Step 529629  [5.432 sec/step, loss=0.07584, avg_loss=0.07412]\n",
      "Step 529630  [5.452 sec/step, loss=0.07387, avg_loss=0.07411]\n",
      "Step 529631  [5.451 sec/step, loss=0.07497, avg_loss=0.07410]\n",
      "Step 529632  [5.431 sec/step, loss=0.07708, avg_loss=0.07413]\n",
      "Step 529633  [5.414 sec/step, loss=0.07525, avg_loss=0.07412]\n",
      "Step 529634  [5.395 sec/step, loss=0.07405, avg_loss=0.07410]\n",
      "Step 529635  [5.446 sec/step, loss=0.06478, avg_loss=0.07400]\n",
      "Step 529636  [5.442 sec/step, loss=0.07339, avg_loss=0.07397]\n",
      "Step 529637  [5.435 sec/step, loss=0.07185, avg_loss=0.07393]\n",
      "Step 529638  [5.435 sec/step, loss=0.07534, avg_loss=0.07396]\n",
      "Step 529639  [5.448 sec/step, loss=0.07419, avg_loss=0.07396]\n",
      "Step 529640  [5.448 sec/step, loss=0.07498, avg_loss=0.07396]\n",
      "Step 529641  [5.435 sec/step, loss=0.07081, avg_loss=0.07392]\n",
      "Step 529642  [5.413 sec/step, loss=0.07341, avg_loss=0.07389]\n",
      "Step 529643  [5.407 sec/step, loss=0.07522, avg_loss=0.07389]\n",
      "Step 529644  [5.353 sec/step, loss=0.07363, avg_loss=0.07397]\n",
      "Step 529645  [5.355 sec/step, loss=0.07495, avg_loss=0.07396]\n",
      "Step 529646  [5.358 sec/step, loss=0.07430, avg_loss=0.07395]\n",
      "Step 529647  [5.381 sec/step, loss=0.07510, avg_loss=0.07398]\n",
      "Step 529648  [5.375 sec/step, loss=0.07387, avg_loss=0.07396]\n",
      "Step 529649  [5.390 sec/step, loss=0.07560, avg_loss=0.07398]\n",
      "Step 529650  [5.380 sec/step, loss=0.07554, avg_loss=0.07397]\n",
      "Generated 32 batches of size 32 in 2.396 sec\n",
      "Step 529651  [5.401 sec/step, loss=0.07531, avg_loss=0.07400]\n",
      "Step 529652  [5.388 sec/step, loss=0.06692, avg_loss=0.07393]\n",
      "Step 529653  [5.408 sec/step, loss=0.07654, avg_loss=0.07395]\n",
      "Step 529654  [5.408 sec/step, loss=0.07363, avg_loss=0.07393]\n",
      "Step 529655  [5.397 sec/step, loss=0.07268, avg_loss=0.07389]\n",
      "Step 529656  [5.398 sec/step, loss=0.07621, avg_loss=0.07390]\n",
      "Step 529657  [5.411 sec/step, loss=0.07564, avg_loss=0.07394]\n",
      "Step 529658  [5.387 sec/step, loss=0.07564, avg_loss=0.07397]\n",
      "Step 529659  [5.381 sec/step, loss=0.07560, avg_loss=0.07399]\n",
      "Step 529660  [5.395 sec/step, loss=0.07609, avg_loss=0.07400]\n",
      "Step 529661  [5.423 sec/step, loss=0.07431, avg_loss=0.07409]\n",
      "Step 529662  [5.449 sec/step, loss=0.07314, avg_loss=0.07407]\n",
      "Step 529663  [5.426 sec/step, loss=0.07115, avg_loss=0.07403]\n",
      "Step 529664  [5.421 sec/step, loss=0.07443, avg_loss=0.07403]\n",
      "Step 529665  [5.401 sec/step, loss=0.07334, avg_loss=0.07400]\n",
      "Step 529666  [5.348 sec/step, loss=0.07199, avg_loss=0.07405]\n",
      "Step 529667  [5.333 sec/step, loss=0.07548, avg_loss=0.07404]\n",
      "Step 529668  [5.351 sec/step, loss=0.07499, avg_loss=0.07408]\n",
      "Step 529669  [5.337 sec/step, loss=0.06524, avg_loss=0.07400]\n",
      "Step 529670  [5.329 sec/step, loss=0.07524, avg_loss=0.07400]\n",
      "Step 529671  [5.329 sec/step, loss=0.07491, avg_loss=0.07401]\n",
      "Step 529672  [5.340 sec/step, loss=0.07359, avg_loss=0.07399]\n",
      "Step 529673  [5.346 sec/step, loss=0.07356, avg_loss=0.07396]\n",
      "Step 529674  [5.326 sec/step, loss=0.07214, avg_loss=0.07392]\n",
      "Step 529675  [5.324 sec/step, loss=0.07263, avg_loss=0.07389]\n",
      "Step 529676  [5.313 sec/step, loss=0.07514, avg_loss=0.07388]\n",
      "Step 529677  [5.328 sec/step, loss=0.07406, avg_loss=0.07387]\n",
      "Step 529678  [5.323 sec/step, loss=0.07481, avg_loss=0.07388]\n",
      "Step 529679  [5.319 sec/step, loss=0.07393, avg_loss=0.07387]\n",
      "Step 529680  [5.310 sec/step, loss=0.07414, avg_loss=0.07386]\n",
      "Step 529681  [5.314 sec/step, loss=0.07065, avg_loss=0.07385]\n",
      "Step 529682  [5.323 sec/step, loss=0.07608, avg_loss=0.07386]\n",
      "Generated 32 batches of size 32 in 2.367 sec\n",
      "Step 529683  [5.329 sec/step, loss=0.07462, avg_loss=0.07387]\n",
      "Step 529684  [5.308 sec/step, loss=0.07423, avg_loss=0.07386]\n",
      "Step 529685  [5.312 sec/step, loss=0.07607, avg_loss=0.07387]\n",
      "Step 529686  [5.309 sec/step, loss=0.07649, avg_loss=0.07388]\n",
      "Step 529687  [5.315 sec/step, loss=0.07639, avg_loss=0.07388]\n",
      "Step 529688  [5.352 sec/step, loss=0.06561, avg_loss=0.07378]\n",
      "Step 529689  [5.361 sec/step, loss=0.07468, avg_loss=0.07377]\n",
      "Step 529690  [5.365 sec/step, loss=0.07456, avg_loss=0.07377]\n",
      "Step 529691  [5.397 sec/step, loss=0.07588, avg_loss=0.07387]\n",
      "Step 529692  [5.356 sec/step, loss=0.06686, avg_loss=0.07379]\n",
      "Step 529693  [5.358 sec/step, loss=0.07508, avg_loss=0.07381]\n",
      "Step 529694  [5.357 sec/step, loss=0.07493, avg_loss=0.07382]\n",
      "Step 529695  [5.351 sec/step, loss=0.07045, avg_loss=0.07377]\n",
      "Step 529696  [5.351 sec/step, loss=0.07393, avg_loss=0.07377]\n",
      "Step 529697  [5.351 sec/step, loss=0.07376, avg_loss=0.07379]\n",
      "Step 529698  [5.306 sec/step, loss=0.07619, avg_loss=0.07388]\n",
      "Step 529699  [5.321 sec/step, loss=0.07560, avg_loss=0.07389]\n",
      "Step 529700  [5.325 sec/step, loss=0.07621, avg_loss=0.07389]\n",
      "Writing summary at step: 529700\n",
      "Step 529701  [5.338 sec/step, loss=0.07376, avg_loss=0.07388]\n",
      "Step 529702  [5.342 sec/step, loss=0.07566, avg_loss=0.07389]\n",
      "Step 529703  [5.345 sec/step, loss=0.07430, avg_loss=0.07388]\n",
      "Step 529704  [5.335 sec/step, loss=0.07526, avg_loss=0.07387]\n",
      "Step 529705  [5.344 sec/step, loss=0.07566, avg_loss=0.07387]\n",
      "Step 529706  [5.351 sec/step, loss=0.07372, avg_loss=0.07389]\n",
      "Step 529707  [5.356 sec/step, loss=0.07512, avg_loss=0.07389]\n",
      "Step 529708  [5.375 sec/step, loss=0.07339, avg_loss=0.07390]\n",
      "Step 529709  [5.356 sec/step, loss=0.07375, avg_loss=0.07387]\n",
      "Step 529710  [5.409 sec/step, loss=0.06824, avg_loss=0.07382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 529711  [5.420 sec/step, loss=0.07595, avg_loss=0.07387]\n",
      "Step 529712  [5.411 sec/step, loss=0.07157, avg_loss=0.07383]\n",
      "Step 529713  [5.417 sec/step, loss=0.07328, avg_loss=0.07384]\n",
      "Generated 32 batches of size 32 in 2.368 sec\n",
      "Step 529714  [5.431 sec/step, loss=0.07480, avg_loss=0.07385]\n",
      "Step 529715  [5.453 sec/step, loss=0.07465, avg_loss=0.07393]\n",
      "Step 529716  [5.453 sec/step, loss=0.07174, avg_loss=0.07390]\n",
      "Step 529717  [5.458 sec/step, loss=0.07421, avg_loss=0.07391]\n",
      "Step 529718  [5.443 sec/step, loss=0.07350, avg_loss=0.07389]\n",
      "Step 529719  [5.409 sec/step, loss=0.07190, avg_loss=0.07386]\n",
      "Step 529720  [5.402 sec/step, loss=0.07197, avg_loss=0.07384]\n",
      "Step 529721  [5.412 sec/step, loss=0.07606, avg_loss=0.07387]\n",
      "Step 529722  [5.422 sec/step, loss=0.07539, avg_loss=0.07390]\n",
      "Step 529723  [5.439 sec/step, loss=0.07417, avg_loss=0.07388]\n",
      "Step 529724  [5.424 sec/step, loss=0.07079, avg_loss=0.07384]\n",
      "Step 529725  [5.475 sec/step, loss=0.06449, avg_loss=0.07374]\n",
      "Step 529726  [5.474 sec/step, loss=0.07474, avg_loss=0.07375]\n",
      "Step 529727  [5.464 sec/step, loss=0.07531, avg_loss=0.07374]\n",
      "Step 529728  [5.464 sec/step, loss=0.07098, avg_loss=0.07373]\n",
      "Step 529729  [5.448 sec/step, loss=0.07557, avg_loss=0.07373]\n",
      "Step 529730  [5.434 sec/step, loss=0.07377, avg_loss=0.07373]\n",
      "Step 529731  [5.411 sec/step, loss=0.06983, avg_loss=0.07368]\n",
      "Step 529732  [5.409 sec/step, loss=0.07495, avg_loss=0.07366]\n",
      "Step 529733  [5.410 sec/step, loss=0.07497, avg_loss=0.07365]\n",
      "Step 529734  [5.422 sec/step, loss=0.07515, avg_loss=0.07366]\n",
      "Step 529735  [5.353 sec/step, loss=0.06611, avg_loss=0.07368]\n",
      "Step 529736  [5.358 sec/step, loss=0.07420, avg_loss=0.07368]\n",
      "Step 529737  [5.364 sec/step, loss=0.07115, avg_loss=0.07368]\n",
      "Step 529738  [5.375 sec/step, loss=0.07600, avg_loss=0.07368]\n",
      "Step 529739  [5.375 sec/step, loss=0.07601, avg_loss=0.07370]\n",
      "Step 529740  [5.367 sec/step, loss=0.07458, avg_loss=0.07370]\n",
      "Step 529741  [5.384 sec/step, loss=0.07635, avg_loss=0.07375]\n",
      "Step 529742  [5.390 sec/step, loss=0.07492, avg_loss=0.07377]\n",
      "Step 529743  [5.388 sec/step, loss=0.07482, avg_loss=0.07376]\n",
      "Step 529744  [5.395 sec/step, loss=0.07315, avg_loss=0.07376]\n",
      "Step 529745  [5.383 sec/step, loss=0.07521, avg_loss=0.07376]\n",
      "Generated 32 batches of size 32 in 2.405 sec\n",
      "Step 529746  [5.394 sec/step, loss=0.07584, avg_loss=0.07378]\n",
      "Step 529747  [5.381 sec/step, loss=0.07389, avg_loss=0.07377]\n",
      "Step 529748  [5.374 sec/step, loss=0.07239, avg_loss=0.07375]\n",
      "Step 529749  [5.378 sec/step, loss=0.07576, avg_loss=0.07375]\n",
      "Step 529750  [5.376 sec/step, loss=0.07276, avg_loss=0.07372]\n",
      "Step 529751  [5.350 sec/step, loss=0.07406, avg_loss=0.07371]\n",
      "Step 529752  [5.368 sec/step, loss=0.07411, avg_loss=0.07378]\n",
      "Step 529753  [5.371 sec/step, loss=0.07604, avg_loss=0.07378]\n",
      "Step 529754  [5.394 sec/step, loss=0.07485, avg_loss=0.07379]\n",
      "Step 529755  [5.395 sec/step, loss=0.07202, avg_loss=0.07378]\n",
      "Step 529756  [5.372 sec/step, loss=0.07161, avg_loss=0.07374]\n",
      "Step 529757  [5.377 sec/step, loss=0.07543, avg_loss=0.07374]\n",
      "Step 529758  [5.379 sec/step, loss=0.07487, avg_loss=0.07373]\n",
      "Step 529759  [5.369 sec/step, loss=0.07320, avg_loss=0.07371]\n",
      "Step 529760  [5.351 sec/step, loss=0.07347, avg_loss=0.07368]\n",
      "Step 529761  [5.346 sec/step, loss=0.07218, avg_loss=0.07366]\n",
      "Step 529762  [5.370 sec/step, loss=0.06676, avg_loss=0.07359]\n",
      "Step 529763  [5.385 sec/step, loss=0.07241, avg_loss=0.07361]\n",
      "Step 529764  [5.385 sec/step, loss=0.07415, avg_loss=0.07360]\n",
      "Step 529765  [5.400 sec/step, loss=0.07576, avg_loss=0.07363]\n",
      "Step 529766  [5.416 sec/step, loss=0.07608, avg_loss=0.07367]\n",
      "Step 529767  [5.415 sec/step, loss=0.07471, avg_loss=0.07366]\n",
      "Step 529768  [5.409 sec/step, loss=0.07229, avg_loss=0.07363]\n",
      "Step 529769  [5.416 sec/step, loss=0.07150, avg_loss=0.07370]\n",
      "Step 529770  [5.416 sec/step, loss=0.07528, avg_loss=0.07370]\n",
      "Step 529771  [5.414 sec/step, loss=0.07471, avg_loss=0.07369]\n",
      "Step 529772  [5.413 sec/step, loss=0.07598, avg_loss=0.07372]\n",
      "Step 529773  [5.416 sec/step, loss=0.07384, avg_loss=0.07372]\n",
      "Step 529774  [5.434 sec/step, loss=0.07446, avg_loss=0.07374]\n",
      "Step 529775  [5.444 sec/step, loss=0.07610, avg_loss=0.07378]\n",
      "Step 529776  [5.452 sec/step, loss=0.07479, avg_loss=0.07378]\n",
      "Step 529777  [5.445 sec/step, loss=0.07568, avg_loss=0.07379]\n",
      "Generated 32 batches of size 32 in 2.664 sec\n",
      "Step 529778  [5.434 sec/step, loss=0.07365, avg_loss=0.07378]\n",
      "Step 529779  [5.466 sec/step, loss=0.07394, avg_loss=0.07378]\n",
      "Step 529780  [5.457 sec/step, loss=0.06538, avg_loss=0.07369]\n",
      "Step 529781  [5.464 sec/step, loss=0.07361, avg_loss=0.07372]\n",
      "Step 529782  [5.446 sec/step, loss=0.07376, avg_loss=0.07370]\n",
      "Step 529783  [5.441 sec/step, loss=0.07438, avg_loss=0.07370]\n",
      "Step 529784  [5.459 sec/step, loss=0.07575, avg_loss=0.07371]\n",
      "Step 529785  [5.466 sec/step, loss=0.07626, avg_loss=0.07371]\n",
      "Step 529786  [5.467 sec/step, loss=0.07600, avg_loss=0.07371]\n",
      "Step 529787  [5.456 sec/step, loss=0.07573, avg_loss=0.07370]\n",
      "Step 529788  [5.410 sec/step, loss=0.07425, avg_loss=0.07379]\n",
      "Step 529789  [5.404 sec/step, loss=0.07361, avg_loss=0.07378]\n",
      "Step 529790  [5.396 sec/step, loss=0.07152, avg_loss=0.07375]\n",
      "Step 529791  [5.380 sec/step, loss=0.07234, avg_loss=0.07371]\n",
      "Step 529792  [5.386 sec/step, loss=0.07387, avg_loss=0.07378]\n",
      "Step 529793  [5.431 sec/step, loss=0.06515, avg_loss=0.07368]\n",
      "Step 529794  [5.426 sec/step, loss=0.07337, avg_loss=0.07367]\n",
      "Step 529795  [5.443 sec/step, loss=0.07591, avg_loss=0.07372]\n",
      "Step 529796  [5.452 sec/step, loss=0.07547, avg_loss=0.07374]\n",
      "Step 529797  [5.479 sec/step, loss=0.07280, avg_loss=0.07373]\n",
      "Step 529798  [5.482 sec/step, loss=0.07461, avg_loss=0.07371]\n",
      "Step 529799  [5.487 sec/step, loss=0.07505, avg_loss=0.07371]\n",
      "Step 529800  [5.490 sec/step, loss=0.07438, avg_loss=0.07369]\n",
      "Writing summary at step: 529800\n",
      "Step 529801  [5.484 sec/step, loss=0.07544, avg_loss=0.07371]\n",
      "Step 529802  [5.466 sec/step, loss=0.07179, avg_loss=0.07367]\n",
      "Step 529803  [5.443 sec/step, loss=0.06534, avg_loss=0.07358]\n",
      "Step 529804  [5.427 sec/step, loss=0.07177, avg_loss=0.07354]\n",
      "Step 529805  [5.414 sec/step, loss=0.07456, avg_loss=0.07353]\n",
      "Step 529806  [5.408 sec/step, loss=0.07614, avg_loss=0.07356]\n",
      "Step 529807  [5.416 sec/step, loss=0.07572, avg_loss=0.07356]\n",
      "Step 529808  [5.389 sec/step, loss=0.07446, avg_loss=0.07357]\n",
      "Generated 32 batches of size 32 in 2.631 sec\n",
      "Step 529809  [5.394 sec/step, loss=0.07094, avg_loss=0.07354]\n",
      "Step 529810  [5.346 sec/step, loss=0.07482, avg_loss=0.07361]\n",
      "Step 529811  [5.354 sec/step, loss=0.07596, avg_loss=0.07361]\n",
      "Step 529812  [5.357 sec/step, loss=0.07519, avg_loss=0.07365]\n",
      "Step 529813  [5.370 sec/step, loss=0.07518, avg_loss=0.07367]\n",
      "Step 529814  [5.357 sec/step, loss=0.07564, avg_loss=0.07367]\n",
      "Step 529815  [5.352 sec/step, loss=0.07306, avg_loss=0.07366]\n",
      "Step 529816  [5.365 sec/step, loss=0.07594, avg_loss=0.07370]\n",
      "Step 529817  [5.358 sec/step, loss=0.07423, avg_loss=0.07370]\n",
      "Step 529818  [5.378 sec/step, loss=0.07605, avg_loss=0.07373]\n",
      "Step 529819  [5.386 sec/step, loss=0.07300, avg_loss=0.07374]\n",
      "Step 529820  [5.449 sec/step, loss=0.06682, avg_loss=0.07368]\n",
      "Step 529821  [5.421 sec/step, loss=0.07371, avg_loss=0.07366]\n",
      "Step 529822  [5.421 sec/step, loss=0.07495, avg_loss=0.07366]\n",
      "Step 529823  [5.404 sec/step, loss=0.07413, avg_loss=0.07366]\n",
      "Step 529824  [5.419 sec/step, loss=0.07433, avg_loss=0.07369]\n",
      "Step 529825  [5.374 sec/step, loss=0.07477, avg_loss=0.07379]\n",
      "Step 529826  [5.361 sec/step, loss=0.07599, avg_loss=0.07381]\n",
      "Step 529827  [5.363 sec/step, loss=0.07359, avg_loss=0.07379]\n",
      "Step 529828  [5.373 sec/step, loss=0.07336, avg_loss=0.07381]\n",
      "Step 529829  [5.381 sec/step, loss=0.07546, avg_loss=0.07381]\n",
      "Step 529830  [5.375 sec/step, loss=0.07396, avg_loss=0.07381]\n",
      "Step 529831  [5.372 sec/step, loss=0.07072, avg_loss=0.07382]\n",
      "Step 529832  [5.358 sec/step, loss=0.07387, avg_loss=0.07381]\n",
      "Step 529833  [5.362 sec/step, loss=0.07556, avg_loss=0.07382]\n",
      "Step 529834  [5.357 sec/step, loss=0.07481, avg_loss=0.07382]\n",
      "Step 529835  [5.357 sec/step, loss=0.06573, avg_loss=0.07381]\n",
      "Step 529836  [5.346 sec/step, loss=0.06981, avg_loss=0.07377]\n",
      "Step 529837  [5.339 sec/step, loss=0.07038, avg_loss=0.07376]\n",
      "Step 529838  [5.339 sec/step, loss=0.07545, avg_loss=0.07375]\n",
      "Step 529839  [5.326 sec/step, loss=0.07416, avg_loss=0.07374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 529840  [5.340 sec/step, loss=0.07569, avg_loss=0.07375]\n",
      "Generated 32 batches of size 32 in 2.471 sec\n",
      "Step 529841  [5.336 sec/step, loss=0.07522, avg_loss=0.07374]\n",
      "Step 529842  [5.349 sec/step, loss=0.07690, avg_loss=0.07376]\n",
      "Step 529843  [5.359 sec/step, loss=0.07443, avg_loss=0.07375]\n",
      "Step 529844  [5.353 sec/step, loss=0.07511, avg_loss=0.07377]\n",
      "Step 529845  [5.371 sec/step, loss=0.07339, avg_loss=0.07375]\n",
      "Step 529846  [5.364 sec/step, loss=0.07666, avg_loss=0.07376]\n",
      "Step 529847  [5.372 sec/step, loss=0.07437, avg_loss=0.07377]\n",
      "Step 529848  [5.406 sec/step, loss=0.07304, avg_loss=0.07377]\n",
      "Step 529849  [5.395 sec/step, loss=0.07354, avg_loss=0.07375]\n",
      "Step 529850  [5.412 sec/step, loss=0.07501, avg_loss=0.07377]\n",
      "Step 529851  [5.433 sec/step, loss=0.07625, avg_loss=0.07379]\n",
      "Step 529852  [5.428 sec/step, loss=0.07140, avg_loss=0.07377]\n",
      "Step 529853  [5.442 sec/step, loss=0.07241, avg_loss=0.07373]\n",
      "Step 529854  [5.432 sec/step, loss=0.07483, avg_loss=0.07373]\n",
      "Step 529855  [5.438 sec/step, loss=0.07191, avg_loss=0.07373]\n",
      "Step 529856  [5.447 sec/step, loss=0.07276, avg_loss=0.07374]\n",
      "Step 529857  [5.440 sec/step, loss=0.07437, avg_loss=0.07373]\n",
      "Step 529858  [5.448 sec/step, loss=0.07600, avg_loss=0.07374]\n",
      "Step 529859  [5.468 sec/step, loss=0.07516, avg_loss=0.07376]\n",
      "Step 529860  [5.467 sec/step, loss=0.07407, avg_loss=0.07377]\n",
      "Step 529861  [5.458 sec/step, loss=0.07521, avg_loss=0.07380]\n",
      "Step 529862  [5.408 sec/step, loss=0.07443, avg_loss=0.07387]\n",
      "Step 529863  [5.400 sec/step, loss=0.07343, avg_loss=0.07388]\n",
      "Step 529864  [5.404 sec/step, loss=0.07526, avg_loss=0.07390]\n",
      "Step 529865  [5.401 sec/step, loss=0.07598, avg_loss=0.07390]\n",
      "Step 529866  [5.396 sec/step, loss=0.07493, avg_loss=0.07389]\n",
      "Step 529867  [5.389 sec/step, loss=0.07129, avg_loss=0.07385]\n",
      "Step 529868  [5.387 sec/step, loss=0.07249, avg_loss=0.07385]\n",
      "Step 529869  [5.409 sec/step, loss=0.07375, avg_loss=0.07388]\n",
      "Step 529870  [5.413 sec/step, loss=0.07442, avg_loss=0.07387]\n",
      "Step 529871  [5.420 sec/step, loss=0.07597, avg_loss=0.07388]\n",
      "Step 529872  [5.427 sec/step, loss=0.07329, avg_loss=0.07385]\n",
      "Generated 32 batches of size 32 in 2.406 sec\n",
      "Step 529873  [5.411 sec/step, loss=0.07499, avg_loss=0.07387]\n",
      "Step 529874  [5.451 sec/step, loss=0.06615, avg_loss=0.07378]\n",
      "Step 529875  [5.426 sec/step, loss=0.06543, avg_loss=0.07368]\n",
      "Step 529876  [5.417 sec/step, loss=0.07541, avg_loss=0.07368]\n",
      "Step 529877  [5.406 sec/step, loss=0.07134, avg_loss=0.07364]\n",
      "Step 529878  [5.412 sec/step, loss=0.07528, avg_loss=0.07365]\n",
      "Step 529879  [5.375 sec/step, loss=0.07413, avg_loss=0.07366]\n",
      "Step 529880  [5.397 sec/step, loss=0.07579, avg_loss=0.07376]\n",
      "Step 529881  [5.395 sec/step, loss=0.07374, avg_loss=0.07376]\n",
      "Step 529882  [5.410 sec/step, loss=0.07658, avg_loss=0.07379]\n",
      "Step 529883  [5.423 sec/step, loss=0.07625, avg_loss=0.07381]\n",
      "Step 529884  [5.405 sec/step, loss=0.07432, avg_loss=0.07379]\n",
      "Step 529885  [5.409 sec/step, loss=0.07372, avg_loss=0.07377]\n",
      "Step 529886  [5.407 sec/step, loss=0.07330, avg_loss=0.07374]\n",
      "Step 529887  [5.456 sec/step, loss=0.06652, avg_loss=0.07365]\n",
      "Step 529888  [5.452 sec/step, loss=0.07489, avg_loss=0.07366]\n",
      "Step 529889  [5.470 sec/step, loss=0.07641, avg_loss=0.07368]\n",
      "Step 529890  [5.504 sec/step, loss=0.07306, avg_loss=0.07370]\n",
      "Step 529891  [5.497 sec/step, loss=0.07436, avg_loss=0.07372]\n",
      "Step 529892  [5.493 sec/step, loss=0.07223, avg_loss=0.07370]\n",
      "Step 529893  [5.449 sec/step, loss=0.07484, avg_loss=0.07380]\n",
      "Step 529894  [5.451 sec/step, loss=0.07192, avg_loss=0.07379]\n",
      "Step 529895  [5.454 sec/step, loss=0.07630, avg_loss=0.07379]\n",
      "Step 529896  [5.450 sec/step, loss=0.07393, avg_loss=0.07377]\n",
      "Step 529897  [5.416 sec/step, loss=0.07282, avg_loss=0.07377]\n",
      "Step 529898  [5.405 sec/step, loss=0.07506, avg_loss=0.07378]\n",
      "Step 529899  [5.404 sec/step, loss=0.07566, avg_loss=0.07379]\n",
      "Step 529900  [5.390 sec/step, loss=0.07490, avg_loss=0.07379]\n",
      "Writing summary at step: 529900\n",
      "Step 529901  [5.386 sec/step, loss=0.07388, avg_loss=0.07378]\n",
      "Step 529902  [5.396 sec/step, loss=0.07447, avg_loss=0.07380]\n",
      "Step 529903  [5.412 sec/step, loss=0.07247, avg_loss=0.07387]\n",
      "Generated 32 batches of size 32 in 2.558 sec\n",
      "Step 529904  [5.427 sec/step, loss=0.07425, avg_loss=0.07390]\n",
      "Step 529905  [5.432 sec/step, loss=0.07567, avg_loss=0.07391]\n",
      "Step 529906  [5.410 sec/step, loss=0.06756, avg_loss=0.07382]\n",
      "Step 529907  [5.411 sec/step, loss=0.07294, avg_loss=0.07380]\n",
      "Step 529908  [5.407 sec/step, loss=0.07399, avg_loss=0.07379]\n",
      "Step 529909  [5.395 sec/step, loss=0.07104, avg_loss=0.07379]\n",
      "Step 529910  [5.398 sec/step, loss=0.07503, avg_loss=0.07379]\n",
      "Step 529911  [5.393 sec/step, loss=0.07607, avg_loss=0.07380]\n",
      "Step 529912  [5.398 sec/step, loss=0.07402, avg_loss=0.07378]\n",
      "Step 529913  [5.390 sec/step, loss=0.07490, avg_loss=0.07378]\n",
      "Step 529914  [5.389 sec/step, loss=0.07533, avg_loss=0.07378]\n",
      "Step 529915  [5.416 sec/step, loss=0.07247, avg_loss=0.07377]\n",
      "Step 529916  [5.396 sec/step, loss=0.07047, avg_loss=0.07372]\n",
      "Step 529917  [5.384 sec/step, loss=0.06584, avg_loss=0.07363]\n",
      "Step 529918  [5.381 sec/step, loss=0.07515, avg_loss=0.07362]\n",
      "Step 529919  [5.389 sec/step, loss=0.07614, avg_loss=0.07366]\n",
      "Step 529920  [5.357 sec/step, loss=0.07501, avg_loss=0.07374]\n",
      "Step 529921  [5.356 sec/step, loss=0.07105, avg_loss=0.07371]\n",
      "Step 529922  [5.370 sec/step, loss=0.07619, avg_loss=0.07372]\n",
      "Step 529923  [5.379 sec/step, loss=0.07404, avg_loss=0.07372]\n",
      "Step 529924  [5.365 sec/step, loss=0.07504, avg_loss=0.07373]\n",
      "Step 529925  [5.362 sec/step, loss=0.07547, avg_loss=0.07374]\n",
      "Step 529926  [5.347 sec/step, loss=0.07169, avg_loss=0.07369]\n",
      "Step 529927  [5.346 sec/step, loss=0.07302, avg_loss=0.07369]\n",
      "Step 529928  [5.362 sec/step, loss=0.07642, avg_loss=0.07372]\n",
      "Step 529929  [5.358 sec/step, loss=0.07449, avg_loss=0.07371]\n",
      "Step 529930  [5.362 sec/step, loss=0.07568, avg_loss=0.07373]\n",
      "Step 529931  [5.380 sec/step, loss=0.07502, avg_loss=0.07377]\n",
      "Step 529932  [5.393 sec/step, loss=0.07558, avg_loss=0.07379]\n",
      "Step 529933  [5.394 sec/step, loss=0.07433, avg_loss=0.07377]\n",
      "Step 529934  [5.380 sec/step, loss=0.07142, avg_loss=0.07374]\n",
      "Step 529935  [5.410 sec/step, loss=0.07628, avg_loss=0.07385]\n",
      "Generated 32 batches of size 32 in 2.505 sec\n",
      "Step 529936  [5.419 sec/step, loss=0.07409, avg_loss=0.07389]\n",
      "Step 529937  [5.427 sec/step, loss=0.07298, avg_loss=0.07391]\n",
      "Step 529938  [5.412 sec/step, loss=0.07416, avg_loss=0.07390]\n",
      "Step 529939  [5.431 sec/step, loss=0.07465, avg_loss=0.07391]\n",
      "Step 529940  [5.421 sec/step, loss=0.07351, avg_loss=0.07388]\n",
      "Step 529941  [5.409 sec/step, loss=0.07401, avg_loss=0.07387]\n",
      "Step 529942  [5.395 sec/step, loss=0.07493, avg_loss=0.07385]\n",
      "Step 529943  [5.394 sec/step, loss=0.07415, avg_loss=0.07385]\n",
      "Step 529944  [5.447 sec/step, loss=0.06516, avg_loss=0.07375]\n",
      "Step 529945  [5.434 sec/step, loss=0.07619, avg_loss=0.07378]\n",
      "Step 529946  [5.426 sec/step, loss=0.07449, avg_loss=0.07376]\n",
      "Step 529947  [5.433 sec/step, loss=0.07596, avg_loss=0.07377]\n",
      "Step 529948  [5.398 sec/step, loss=0.07309, avg_loss=0.07377]\n",
      "Step 529949  [5.382 sec/step, loss=0.06687, avg_loss=0.07371]\n",
      "Step 529950  [5.355 sec/step, loss=0.07423, avg_loss=0.07370]\n",
      "Step 529951  [5.333 sec/step, loss=0.07085, avg_loss=0.07364]\n",
      "Step 529952  [5.349 sec/step, loss=0.07580, avg_loss=0.07369]\n",
      "Step 529953  [5.320 sec/step, loss=0.07552, avg_loss=0.07372]\n",
      "Step 529954  [5.311 sec/step, loss=0.07205, avg_loss=0.07369]\n",
      "Step 529955  [5.333 sec/step, loss=0.07401, avg_loss=0.07371]\n",
      "Step 529956  [5.338 sec/step, loss=0.07260, avg_loss=0.07371]\n",
      "Step 529957  [5.347 sec/step, loss=0.07501, avg_loss=0.07372]\n",
      "Step 529958  [5.340 sec/step, loss=0.07486, avg_loss=0.07371]\n",
      "Step 529959  [5.333 sec/step, loss=0.07616, avg_loss=0.07372]\n",
      "Step 529960  [5.334 sec/step, loss=0.07370, avg_loss=0.07371]\n",
      "Step 529961  [5.322 sec/step, loss=0.07190, avg_loss=0.07368]\n",
      "Step 529962  [5.323 sec/step, loss=0.07466, avg_loss=0.07368]\n",
      "Step 529963  [5.381 sec/step, loss=0.06653, avg_loss=0.07361]\n",
      "Step 529964  [5.387 sec/step, loss=0.07396, avg_loss=0.07360]\n",
      "Step 529965  [5.384 sec/step, loss=0.07496, avg_loss=0.07359]\n",
      "Step 529966  [5.380 sec/step, loss=0.07458, avg_loss=0.07359]\n",
      "Step 529967  [5.395 sec/step, loss=0.07572, avg_loss=0.07363]\n",
      "Generated 32 batches of size 32 in 2.430 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 529968  [5.410 sec/step, loss=0.07393, avg_loss=0.07364]\n",
      "Step 529969  [5.398 sec/step, loss=0.07464, avg_loss=0.07365]\n",
      "Step 529970  [5.391 sec/step, loss=0.07505, avg_loss=0.07366]\n",
      "Step 529971  [5.378 sec/step, loss=0.07283, avg_loss=0.07363]\n",
      "Step 529972  [5.367 sec/step, loss=0.07533, avg_loss=0.07365]\n",
      "Step 529973  [5.361 sec/step, loss=0.07545, avg_loss=0.07365]\n",
      "Step 529974  [5.306 sec/step, loss=0.07388, avg_loss=0.07373]\n",
      "Step 529975  [5.336 sec/step, loss=0.07666, avg_loss=0.07384]\n",
      "Step 529976  [5.359 sec/step, loss=0.07357, avg_loss=0.07382]\n",
      "Step 529977  [5.370 sec/step, loss=0.07246, avg_loss=0.07384]\n",
      "Step 529978  [5.379 sec/step, loss=0.07328, avg_loss=0.07382]\n",
      "Step 529979  [5.397 sec/step, loss=0.07590, avg_loss=0.07383]\n",
      "Step 529980  [5.382 sec/step, loss=0.07195, avg_loss=0.07379]\n",
      "Step 529981  [5.388 sec/step, loss=0.07534, avg_loss=0.07381]\n",
      "Step 529982  [5.393 sec/step, loss=0.07599, avg_loss=0.07380]\n",
      "Step 529983  [5.431 sec/step, loss=0.06700, avg_loss=0.07371]\n",
      "Step 529984  [5.444 sec/step, loss=0.07533, avg_loss=0.07372]\n",
      "Step 529985  [5.409 sec/step, loss=0.06492, avg_loss=0.07363]\n",
      "Step 529986  [5.405 sec/step, loss=0.07501, avg_loss=0.07365]\n",
      "Step 529987  [5.345 sec/step, loss=0.07398, avg_loss=0.07373]\n",
      "Step 529988  [5.353 sec/step, loss=0.07489, avg_loss=0.07373]\n",
      "Step 529989  [5.348 sec/step, loss=0.07621, avg_loss=0.07372]\n",
      "Step 529990  [5.306 sec/step, loss=0.07171, avg_loss=0.07371]\n",
      "Step 529991  [5.315 sec/step, loss=0.07499, avg_loss=0.07372]\n",
      "Step 529992  [5.329 sec/step, loss=0.07423, avg_loss=0.07374]\n",
      "Step 529993  [5.329 sec/step, loss=0.07487, avg_loss=0.07374]\n",
      "Step 529994  [5.361 sec/step, loss=0.07254, avg_loss=0.07374]\n",
      "Step 529995  [5.361 sec/step, loss=0.07617, avg_loss=0.07374]\n",
      "Step 529996  [5.361 sec/step, loss=0.07364, avg_loss=0.07374]\n",
      "Step 529997  [5.363 sec/step, loss=0.07299, avg_loss=0.07374]\n",
      "Step 529998  [5.358 sec/step, loss=0.07392, avg_loss=0.07373]\n",
      "Step 529999  [5.346 sec/step, loss=0.07439, avg_loss=0.07372]\n",
      "Generated 32 batches of size 32 in 2.428 sec\n",
      "Step 530000  [5.347 sec/step, loss=0.07510, avg_loss=0.07372]\n",
      "Writing summary at step: 530000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-530000\n",
      "Saving audio and alignment...\n",
      "Input: saathvayn sauuddii bayrray kay booing tsaardziz miin tdaxfiif zayrayggor hae~_____________________________________\n",
      "Step 530001  [5.358 sec/step, loss=0.07603, avg_loss=0.07374]\n",
      "Step 530002  [5.351 sec/step, loss=0.07383, avg_loss=0.07373]\n",
      "Step 530003  [5.346 sec/step, loss=0.07504, avg_loss=0.07376]\n",
      "Step 530004  [5.363 sec/step, loss=0.07585, avg_loss=0.07378]\n",
      "Step 530005  [5.364 sec/step, loss=0.07471, avg_loss=0.07377]\n",
      "Step 530006  [5.390 sec/step, loss=0.07352, avg_loss=0.07383]\n",
      "Step 530007  [5.389 sec/step, loss=0.07364, avg_loss=0.07383]\n",
      "Step 530008  [5.410 sec/step, loss=0.07565, avg_loss=0.07385]\n",
      "Step 530009  [5.417 sec/step, loss=0.07504, avg_loss=0.07389]\n",
      "Step 530010  [5.424 sec/step, loss=0.07349, avg_loss=0.07387]\n",
      "Step 530011  [5.416 sec/step, loss=0.07408, avg_loss=0.07385]\n",
      "Step 530012  [5.431 sec/step, loss=0.07536, avg_loss=0.07387]\n",
      "Step 530013  [5.432 sec/step, loss=0.07551, avg_loss=0.07387]\n",
      "Step 530014  [5.444 sec/step, loss=0.07601, avg_loss=0.07388]\n",
      "Step 530015  [5.426 sec/step, loss=0.07433, avg_loss=0.07390]\n",
      "Step 530016  [5.426 sec/step, loss=0.07210, avg_loss=0.07392]\n",
      "Step 530017  [5.450 sec/step, loss=0.07610, avg_loss=0.07402]\n",
      "Step 530018  [5.444 sec/step, loss=0.07493, avg_loss=0.07402]\n",
      "Step 530019  [5.437 sec/step, loss=0.07460, avg_loss=0.07400]\n",
      "Step 530020  [5.430 sec/step, loss=0.07619, avg_loss=0.07401]\n",
      "Step 530021  [5.440 sec/step, loss=0.07057, avg_loss=0.07401]\n",
      "Step 530022  [5.417 sec/step, loss=0.07044, avg_loss=0.07395]\n",
      "Step 530023  [5.408 sec/step, loss=0.07456, avg_loss=0.07395]\n",
      "Step 530024  [5.408 sec/step, loss=0.07511, avg_loss=0.07396]\n",
      "Step 530025  [5.402 sec/step, loss=0.07160, avg_loss=0.07392]\n",
      "Step 530026  [5.454 sec/step, loss=0.06555, avg_loss=0.07386]\n",
      "Step 530027  [5.455 sec/step, loss=0.07493, avg_loss=0.07387]\n",
      "Step 530028  [5.447 sec/step, loss=0.07514, avg_loss=0.07386]\n",
      "Step 530029  [5.432 sec/step, loss=0.07387, avg_loss=0.07386]\n",
      "Generated 32 batches of size 32 in 2.503 sec\n",
      "Step 530030  [5.420 sec/step, loss=0.07402, avg_loss=0.07384]\n",
      "Step 530031  [5.402 sec/step, loss=0.07192, avg_loss=0.07381]\n",
      "Step 530032  [5.381 sec/step, loss=0.06560, avg_loss=0.07371]\n",
      "Step 530033  [5.386 sec/step, loss=0.07444, avg_loss=0.07371]\n",
      "Step 530034  [5.398 sec/step, loss=0.07266, avg_loss=0.07372]\n",
      "Step 530035  [5.401 sec/step, loss=0.07639, avg_loss=0.07372]\n",
      "Step 530036  [5.400 sec/step, loss=0.07431, avg_loss=0.07372]\n",
      "Step 530037  [5.416 sec/step, loss=0.07691, avg_loss=0.07376]\n",
      "Step 530038  [5.418 sec/step, loss=0.07338, avg_loss=0.07376]\n",
      "Step 530039  [5.416 sec/step, loss=0.07351, avg_loss=0.07375]\n",
      "Step 530040  [5.431 sec/step, loss=0.07367, avg_loss=0.07375]\n",
      "Step 530041  [5.430 sec/step, loss=0.07384, avg_loss=0.07375]\n",
      "Step 530042  [5.423 sec/step, loss=0.07382, avg_loss=0.07373]\n",
      "Step 530043  [5.410 sec/step, loss=0.07464, avg_loss=0.07374]\n",
      "Step 530044  [5.355 sec/step, loss=0.07392, avg_loss=0.07383]\n",
      "Step 530045  [5.355 sec/step, loss=0.07528, avg_loss=0.07382]\n",
      "Step 530046  [5.362 sec/step, loss=0.07636, avg_loss=0.07384]\n",
      "Step 530047  [5.354 sec/step, loss=0.07541, avg_loss=0.07383]\n",
      "Step 530048  [5.363 sec/step, loss=0.07530, avg_loss=0.07385]\n",
      "Step 530049  [5.365 sec/step, loss=0.07145, avg_loss=0.07390]\n",
      "Step 530050  [5.377 sec/step, loss=0.07197, avg_loss=0.07388]\n",
      "Step 530051  [5.415 sec/step, loss=0.07366, avg_loss=0.07390]\n",
      "Step 530052  [5.401 sec/step, loss=0.07384, avg_loss=0.07388]\n",
      "Step 530053  [5.400 sec/step, loss=0.07498, avg_loss=0.07388]\n",
      "Step 530054  [5.401 sec/step, loss=0.07254, avg_loss=0.07388]\n",
      "Step 530055  [5.389 sec/step, loss=0.07615, avg_loss=0.07391]\n",
      "Step 530056  [5.379 sec/step, loss=0.07030, avg_loss=0.07388]\n",
      "Step 530057  [5.355 sec/step, loss=0.06648, avg_loss=0.07380]\n",
      "Step 530058  [5.352 sec/step, loss=0.07455, avg_loss=0.07379]\n",
      "Step 530059  [5.348 sec/step, loss=0.07486, avg_loss=0.07378]\n",
      "Step 530060  [5.352 sec/step, loss=0.07517, avg_loss=0.07380]\n",
      "Step 530061  [5.384 sec/step, loss=0.07268, avg_loss=0.07380]\n",
      "Generated 32 batches of size 32 in 2.368 sec\n",
      "Step 530062  [5.388 sec/step, loss=0.07637, avg_loss=0.07382]\n",
      "Step 530063  [5.348 sec/step, loss=0.07612, avg_loss=0.07392]\n",
      "Step 530064  [5.344 sec/step, loss=0.07580, avg_loss=0.07393]\n",
      "Step 530065  [5.389 sec/step, loss=0.06620, avg_loss=0.07385]\n",
      "Step 530066  [5.395 sec/step, loss=0.07642, avg_loss=0.07387]\n",
      "Step 530067  [5.387 sec/step, loss=0.07336, avg_loss=0.07384]\n",
      "Step 530068  [5.358 sec/step, loss=0.07134, avg_loss=0.07382]\n",
      "Step 530069  [5.361 sec/step, loss=0.07465, avg_loss=0.07382]\n",
      "Step 530070  [5.378 sec/step, loss=0.07546, avg_loss=0.07382]\n",
      "Step 530071  [5.381 sec/step, loss=0.07342, avg_loss=0.07383]\n",
      "Step 530072  [5.394 sec/step, loss=0.07534, avg_loss=0.07383]\n",
      "Step 530073  [5.388 sec/step, loss=0.07369, avg_loss=0.07381]\n",
      "Step 530074  [5.402 sec/step, loss=0.07539, avg_loss=0.07382]\n",
      "Step 530075  [5.383 sec/step, loss=0.07061, avg_loss=0.07376]\n",
      "Step 530076  [5.353 sec/step, loss=0.07163, avg_loss=0.07374]\n",
      "Step 530077  [5.348 sec/step, loss=0.07170, avg_loss=0.07374]\n",
      "Step 530078  [5.365 sec/step, loss=0.07242, avg_loss=0.07373]\n",
      "Step 530079  [5.362 sec/step, loss=0.07602, avg_loss=0.07373]\n",
      "Step 530080  [5.422 sec/step, loss=0.06607, avg_loss=0.07367]\n",
      "Step 530081  [5.419 sec/step, loss=0.07506, avg_loss=0.07367]\n",
      "Step 530082  [5.404 sec/step, loss=0.07358, avg_loss=0.07364]\n",
      "Step 530083  [5.335 sec/step, loss=0.06554, avg_loss=0.07363]\n",
      "Step 530084  [5.349 sec/step, loss=0.07543, avg_loss=0.07363]\n",
      "Step 530085  [5.366 sec/step, loss=0.07539, avg_loss=0.07373]\n",
      "Step 530086  [5.360 sec/step, loss=0.07167, avg_loss=0.07370]\n",
      "Step 530087  [5.373 sec/step, loss=0.07231, avg_loss=0.07368]\n",
      "Step 530088  [5.351 sec/step, loss=0.07126, avg_loss=0.07365]\n",
      "Step 530089  [5.351 sec/step, loss=0.07572, avg_loss=0.07364]\n",
      "Step 530090  [5.366 sec/step, loss=0.07506, avg_loss=0.07368]\n",
      "Step 530091  [5.369 sec/step, loss=0.07480, avg_loss=0.07367]\n",
      "Step 530092  [5.364 sec/step, loss=0.07372, avg_loss=0.07367]\n",
      "Step 530093  [5.369 sec/step, loss=0.07327, avg_loss=0.07365]\n",
      "Generated 32 batches of size 32 in 2.353 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 530094  [5.355 sec/step, loss=0.07614, avg_loss=0.07369]\n",
      "Step 530095  [5.343 sec/step, loss=0.07445, avg_loss=0.07367]\n",
      "Step 530096  [5.338 sec/step, loss=0.07338, avg_loss=0.07367]\n",
      "Step 530097  [5.351 sec/step, loss=0.07510, avg_loss=0.07369]\n",
      "Step 530098  [5.357 sec/step, loss=0.07438, avg_loss=0.07370]\n",
      "Step 530099  [5.363 sec/step, loss=0.07471, avg_loss=0.07370]\n",
      "Step 530100  [5.372 sec/step, loss=0.07579, avg_loss=0.07371]\n",
      "Writing summary at step: 530100\n",
      "Step 530101  [5.365 sec/step, loss=0.07467, avg_loss=0.07369]\n",
      "Step 530102  [5.358 sec/step, loss=0.07149, avg_loss=0.07367]\n",
      "Step 530103  [5.356 sec/step, loss=0.07046, avg_loss=0.07362]\n",
      "Step 530104  [5.356 sec/step, loss=0.07317, avg_loss=0.07360]\n",
      "Step 530105  [5.354 sec/step, loss=0.07417, avg_loss=0.07359]\n",
      "Step 530106  [5.329 sec/step, loss=0.06653, avg_loss=0.07352]\n",
      "Step 530107  [5.321 sec/step, loss=0.07512, avg_loss=0.07354]\n",
      "Step 530108  [5.292 sec/step, loss=0.06960, avg_loss=0.07347]\n",
      "Step 530109  [5.307 sec/step, loss=0.07445, avg_loss=0.07347]\n",
      "Step 530110  [5.303 sec/step, loss=0.07550, avg_loss=0.07349]\n",
      "Step 530111  [5.353 sec/step, loss=0.06624, avg_loss=0.07341]\n",
      "Step 530112  [5.342 sec/step, loss=0.07586, avg_loss=0.07342]\n",
      "Step 530113  [5.354 sec/step, loss=0.07650, avg_loss=0.07343]\n",
      "Step 530114  [5.353 sec/step, loss=0.07396, avg_loss=0.07340]\n",
      "Step 530115  [5.356 sec/step, loss=0.07352, avg_loss=0.07340]\n",
      "Step 530116  [5.363 sec/step, loss=0.07228, avg_loss=0.07340]\n",
      "Step 530117  [5.365 sec/step, loss=0.07701, avg_loss=0.07341]\n",
      "Step 530118  [5.359 sec/step, loss=0.07486, avg_loss=0.07341]\n",
      "Step 530119  [5.351 sec/step, loss=0.07173, avg_loss=0.07338]\n",
      "Step 530120  [5.344 sec/step, loss=0.07500, avg_loss=0.07337]\n",
      "Step 530121  [5.356 sec/step, loss=0.07462, avg_loss=0.07341]\n",
      "Step 530122  [5.362 sec/step, loss=0.07376, avg_loss=0.07344]\n",
      "Step 530123  [5.363 sec/step, loss=0.07487, avg_loss=0.07344]\n",
      "Step 530124  [5.369 sec/step, loss=0.07414, avg_loss=0.07343]\n",
      "Generated 32 batches of size 32 in 2.497 sec\n",
      "Step 530125  [5.368 sec/step, loss=0.07419, avg_loss=0.07346]\n",
      "Step 530126  [5.317 sec/step, loss=0.07415, avg_loss=0.07355]\n",
      "Step 530127  [5.310 sec/step, loss=0.07399, avg_loss=0.07354]\n",
      "Step 530128  [5.310 sec/step, loss=0.07351, avg_loss=0.07352]\n",
      "Step 530129  [5.316 sec/step, loss=0.07346, avg_loss=0.07352]\n",
      "Step 530130  [5.344 sec/step, loss=0.07370, avg_loss=0.07351]\n",
      "Step 530131  [5.368 sec/step, loss=0.07615, avg_loss=0.07355]\n",
      "Step 530132  [5.383 sec/step, loss=0.07402, avg_loss=0.07364]\n",
      "Step 530133  [5.373 sec/step, loss=0.07333, avg_loss=0.07363]\n",
      "Step 530134  [5.385 sec/step, loss=0.07582, avg_loss=0.07366]\n",
      "Step 530135  [5.374 sec/step, loss=0.07478, avg_loss=0.07364]\n",
      "Step 530136  [5.378 sec/step, loss=0.07583, avg_loss=0.07366]\n",
      "Step 530137  [5.370 sec/step, loss=0.07300, avg_loss=0.07362]\n",
      "Step 530138  [5.391 sec/step, loss=0.07511, avg_loss=0.07364]\n",
      "Step 530139  [5.374 sec/step, loss=0.06956, avg_loss=0.07360]\n",
      "Step 530140  [5.375 sec/step, loss=0.07572, avg_loss=0.07362]\n",
      "Step 530141  [5.385 sec/step, loss=0.07542, avg_loss=0.07363]\n",
      "Step 530142  [5.389 sec/step, loss=0.07333, avg_loss=0.07363]\n",
      "Step 530143  [5.386 sec/step, loss=0.07349, avg_loss=0.07362]\n",
      "Step 530144  [5.390 sec/step, loss=0.07454, avg_loss=0.07362]\n",
      "Step 530145  [5.373 sec/step, loss=0.07377, avg_loss=0.07361]\n",
      "Step 530146  [5.384 sec/step, loss=0.07554, avg_loss=0.07360]\n",
      "Step 530147  [5.373 sec/step, loss=0.07349, avg_loss=0.07358]\n",
      "Step 530148  [5.370 sec/step, loss=0.07417, avg_loss=0.07357]\n",
      "Step 530149  [5.373 sec/step, loss=0.07127, avg_loss=0.07357]\n",
      "Step 530150  [5.379 sec/step, loss=0.07603, avg_loss=0.07361]\n",
      "Step 530151  [5.362 sec/step, loss=0.07287, avg_loss=0.07360]\n",
      "Step 530152  [5.360 sec/step, loss=0.07520, avg_loss=0.07361]\n",
      "Step 530153  [5.372 sec/step, loss=0.07590, avg_loss=0.07362]\n",
      "Step 530154  [5.378 sec/step, loss=0.07494, avg_loss=0.07365]\n",
      "Step 530155  [5.348 sec/step, loss=0.06648, avg_loss=0.07355]\n",
      "Step 530156  [5.358 sec/step, loss=0.07539, avg_loss=0.07360]\n",
      "Generated 32 batches of size 32 in 2.312 sec\n",
      "Step 530157  [5.378 sec/step, loss=0.07283, avg_loss=0.07367]\n",
      "Step 530158  [5.372 sec/step, loss=0.07134, avg_loss=0.07363]\n",
      "Step 530159  [5.354 sec/step, loss=0.07217, avg_loss=0.07361]\n",
      "Step 530160  [5.370 sec/step, loss=0.07528, avg_loss=0.07361]\n",
      "Step 530161  [5.402 sec/step, loss=0.06628, avg_loss=0.07354]\n",
      "Step 530162  [5.422 sec/step, loss=0.07300, avg_loss=0.07351]\n",
      "Step 530163  [5.416 sec/step, loss=0.07440, avg_loss=0.07349]\n",
      "Step 530164  [5.419 sec/step, loss=0.07446, avg_loss=0.07348]\n",
      "Step 530165  [5.365 sec/step, loss=0.07288, avg_loss=0.07355]\n",
      "Step 530166  [5.353 sec/step, loss=0.07179, avg_loss=0.07350]\n",
      "Step 530167  [5.362 sec/step, loss=0.07515, avg_loss=0.07352]\n",
      "Step 530168  [5.359 sec/step, loss=0.07084, avg_loss=0.07351]\n",
      "Step 530169  [5.368 sec/step, loss=0.07580, avg_loss=0.07352]\n",
      "Step 530170  [5.360 sec/step, loss=0.07565, avg_loss=0.07353]\n",
      "Step 530171  [5.371 sec/step, loss=0.07578, avg_loss=0.07355]\n",
      "Step 530172  [5.352 sec/step, loss=0.07364, avg_loss=0.07353]\n",
      "Step 530173  [5.383 sec/step, loss=0.07270, avg_loss=0.07352]\n",
      "Step 530174  [5.369 sec/step, loss=0.07484, avg_loss=0.07352]\n",
      "Step 530175  [5.369 sec/step, loss=0.07274, avg_loss=0.07354]\n",
      "Step 530176  [5.397 sec/step, loss=0.07295, avg_loss=0.07355]\n",
      "Step 530177  [5.398 sec/step, loss=0.07340, avg_loss=0.07357]\n",
      "Step 530178  [5.376 sec/step, loss=0.07495, avg_loss=0.07359]\n",
      "Step 530179  [5.382 sec/step, loss=0.07423, avg_loss=0.07358]\n",
      "Step 530180  [5.345 sec/step, loss=0.07617, avg_loss=0.07368]\n",
      "Step 530181  [5.357 sec/step, loss=0.07394, avg_loss=0.07367]\n",
      "Step 530182  [5.359 sec/step, loss=0.07280, avg_loss=0.07366]\n",
      "Step 530183  [5.387 sec/step, loss=0.07559, avg_loss=0.07376]\n",
      "Step 530184  [5.379 sec/step, loss=0.07566, avg_loss=0.07376]\n",
      "Step 530185  [5.371 sec/step, loss=0.07387, avg_loss=0.07375]\n",
      "Step 530186  [5.374 sec/step, loss=0.07439, avg_loss=0.07377]\n",
      "Step 530187  [5.371 sec/step, loss=0.07463, avg_loss=0.07380]\n",
      "Step 530188  [5.380 sec/step, loss=0.07512, avg_loss=0.07383]\n",
      "Generated 32 batches of size 32 in 2.325 sec\n",
      "Step 530189  [5.377 sec/step, loss=0.07577, avg_loss=0.07384]\n",
      "Step 530190  [5.357 sec/step, loss=0.06460, avg_loss=0.07373]\n",
      "Step 530191  [5.352 sec/step, loss=0.07383, avg_loss=0.07372]\n",
      "Step 530192  [5.349 sec/step, loss=0.07075, avg_loss=0.07369]\n",
      "Step 530193  [5.344 sec/step, loss=0.07362, avg_loss=0.07369]\n",
      "Step 530194  [5.336 sec/step, loss=0.07504, avg_loss=0.07368]\n",
      "Step 530195  [5.326 sec/step, loss=0.07158, avg_loss=0.07366]\n",
      "Step 530196  [5.335 sec/step, loss=0.07502, avg_loss=0.07367]\n",
      "Step 530197  [5.377 sec/step, loss=0.06629, avg_loss=0.07358]\n",
      "Step 530198  [5.388 sec/step, loss=0.07572, avg_loss=0.07360]\n",
      "Step 530199  [5.385 sec/step, loss=0.07516, avg_loss=0.07360]\n",
      "Step 530200  [5.386 sec/step, loss=0.07536, avg_loss=0.07360]\n",
      "Writing summary at step: 530200\n",
      "Step 530201  [5.378 sec/step, loss=0.07348, avg_loss=0.07359]\n",
      "Step 530202  [5.387 sec/step, loss=0.07343, avg_loss=0.07360]\n",
      "Step 530203  [5.402 sec/step, loss=0.07599, avg_loss=0.07366]\n",
      "Step 530204  [5.394 sec/step, loss=0.07581, avg_loss=0.07369]\n",
      "Step 530205  [5.393 sec/step, loss=0.07412, avg_loss=0.07369]\n",
      "Step 530206  [5.402 sec/step, loss=0.07280, avg_loss=0.07375]\n",
      "Step 530207  [5.405 sec/step, loss=0.07418, avg_loss=0.07374]\n",
      "Step 530208  [5.410 sec/step, loss=0.07356, avg_loss=0.07378]\n",
      "Step 530209  [5.416 sec/step, loss=0.07512, avg_loss=0.07379]\n",
      "Step 530210  [5.409 sec/step, loss=0.07380, avg_loss=0.07377]\n",
      "Step 530211  [5.366 sec/step, loss=0.07614, avg_loss=0.07387]\n",
      "Step 530212  [5.353 sec/step, loss=0.07448, avg_loss=0.07385]\n",
      "Step 530213  [5.323 sec/step, loss=0.06602, avg_loss=0.07375]\n",
      "Step 530214  [5.365 sec/step, loss=0.06481, avg_loss=0.07366]\n",
      "Step 530215  [5.367 sec/step, loss=0.07584, avg_loss=0.07368]\n",
      "Step 530216  [5.366 sec/step, loss=0.07506, avg_loss=0.07371]\n",
      "Step 530217  [5.369 sec/step, loss=0.07401, avg_loss=0.07368]\n",
      "Step 530218  [5.359 sec/step, loss=0.07230, avg_loss=0.07365]\n",
      "Step 530219  [5.364 sec/step, loss=0.07531, avg_loss=0.07369]\n",
      "Generated 32 batches of size 32 in 2.381 sec\n",
      "Step 530220  [5.372 sec/step, loss=0.07656, avg_loss=0.07370]\n",
      "Step 530221  [5.386 sec/step, loss=0.07467, avg_loss=0.07370]\n",
      "Step 530222  [5.397 sec/step, loss=0.07390, avg_loss=0.07371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 530223  [5.385 sec/step, loss=0.07411, avg_loss=0.07370]\n",
      "Step 530224  [5.382 sec/step, loss=0.07525, avg_loss=0.07371]\n",
      "Step 530225  [5.376 sec/step, loss=0.07093, avg_loss=0.07368]\n",
      "Step 530226  [5.376 sec/step, loss=0.07278, avg_loss=0.07366]\n",
      "Step 530227  [5.387 sec/step, loss=0.07535, avg_loss=0.07368]\n",
      "Step 530228  [5.388 sec/step, loss=0.07390, avg_loss=0.07368]\n",
      "Step 530229  [5.383 sec/step, loss=0.07105, avg_loss=0.07366]\n",
      "Step 530230  [5.411 sec/step, loss=0.06709, avg_loss=0.07359]\n",
      "Step 530231  [5.412 sec/step, loss=0.07322, avg_loss=0.07356]\n",
      "Step 530232  [5.424 sec/step, loss=0.07633, avg_loss=0.07358]\n",
      "Step 530233  [5.424 sec/step, loss=0.07507, avg_loss=0.07360]\n",
      "Step 530234  [5.416 sec/step, loss=0.07352, avg_loss=0.07358]\n",
      "Step 530235  [5.412 sec/step, loss=0.07200, avg_loss=0.07355]\n",
      "Step 530236  [5.397 sec/step, loss=0.07394, avg_loss=0.07353]\n",
      "Step 530237  [5.399 sec/step, loss=0.07480, avg_loss=0.07355]\n",
      "Step 530238  [5.383 sec/step, loss=0.07445, avg_loss=0.07354]\n",
      "Step 530239  [5.416 sec/step, loss=0.07342, avg_loss=0.07358]\n",
      "Step 530240  [5.419 sec/step, loss=0.07520, avg_loss=0.07358]\n",
      "Step 530241  [5.428 sec/step, loss=0.07424, avg_loss=0.07356]\n",
      "Step 530242  [5.418 sec/step, loss=0.06727, avg_loss=0.07350]\n",
      "Step 530243  [5.411 sec/step, loss=0.07170, avg_loss=0.07349]\n",
      "Step 530244  [5.409 sec/step, loss=0.07373, avg_loss=0.07348]\n",
      "Step 530245  [5.410 sec/step, loss=0.07232, avg_loss=0.07346]\n",
      "Step 530246  [5.388 sec/step, loss=0.07371, avg_loss=0.07345]\n",
      "Step 530247  [5.400 sec/step, loss=0.07445, avg_loss=0.07345]\n",
      "Step 530248  [5.397 sec/step, loss=0.07010, avg_loss=0.07341]\n",
      "Step 530249  [5.409 sec/step, loss=0.07469, avg_loss=0.07345]\n",
      "Step 530250  [5.406 sec/step, loss=0.07195, avg_loss=0.07341]\n",
      "Step 530251  [5.412 sec/step, loss=0.07560, avg_loss=0.07343]\n",
      "Generated 32 batches of size 32 in 2.406 sec\n",
      "Step 530252  [5.432 sec/step, loss=0.07595, avg_loss=0.07344]\n",
      "Step 530253  [5.421 sec/step, loss=0.07368, avg_loss=0.07342]\n",
      "Step 530254  [5.418 sec/step, loss=0.07495, avg_loss=0.07342]\n",
      "Step 530255  [5.436 sec/step, loss=0.07541, avg_loss=0.07351]\n",
      "Step 530256  [5.443 sec/step, loss=0.07515, avg_loss=0.07351]\n",
      "Step 530257  [5.444 sec/step, loss=0.07512, avg_loss=0.07353]\n",
      "Step 530258  [5.456 sec/step, loss=0.07607, avg_loss=0.07358]\n",
      "Step 530259  [5.492 sec/step, loss=0.07288, avg_loss=0.07358]\n",
      "Step 530260  [5.488 sec/step, loss=0.07603, avg_loss=0.07359]\n",
      "Step 530261  [5.452 sec/step, loss=0.07590, avg_loss=0.07369]\n",
      "Step 530262  [5.440 sec/step, loss=0.07339, avg_loss=0.07369]\n",
      "Step 530263  [5.438 sec/step, loss=0.07444, avg_loss=0.07369]\n",
      "Step 530264  [5.423 sec/step, loss=0.07425, avg_loss=0.07369]\n",
      "Step 530265  [5.419 sec/step, loss=0.07200, avg_loss=0.07368]\n",
      "Step 530266  [5.408 sec/step, loss=0.07188, avg_loss=0.07368]\n",
      "Step 530267  [5.413 sec/step, loss=0.07425, avg_loss=0.07367]\n",
      "Step 530268  [5.429 sec/step, loss=0.07484, avg_loss=0.07371]\n",
      "Step 530269  [5.416 sec/step, loss=0.07494, avg_loss=0.07370]\n",
      "Step 530270  [5.429 sec/step, loss=0.07569, avg_loss=0.07371]\n",
      "Step 530271  [5.427 sec/step, loss=0.07492, avg_loss=0.07370]\n",
      "Step 530272  [5.437 sec/step, loss=0.07391, avg_loss=0.07370]\n",
      "Step 530273  [5.427 sec/step, loss=0.07533, avg_loss=0.07373]\n",
      "Step 530274  [5.427 sec/step, loss=0.07182, avg_loss=0.07370]\n",
      "Step 530275  [5.442 sec/step, loss=0.07424, avg_loss=0.07371]\n",
      "Step 530276  [5.425 sec/step, loss=0.07427, avg_loss=0.07372]\n",
      "Step 530277  [5.422 sec/step, loss=0.07037, avg_loss=0.07369]\n",
      "Step 530278  [5.427 sec/step, loss=0.07601, avg_loss=0.07370]\n",
      "Step 530279  [5.412 sec/step, loss=0.07473, avg_loss=0.07371]\n",
      "Step 530280  [5.390 sec/step, loss=0.07405, avg_loss=0.07369]\n",
      "Step 530281  [5.390 sec/step, loss=0.07588, avg_loss=0.07371]\n",
      "Step 530282  [5.391 sec/step, loss=0.07170, avg_loss=0.07370]\n",
      "Step 530283  [5.378 sec/step, loss=0.07351, avg_loss=0.07368]\n",
      "Generated 32 batches of size 32 in 2.797 sec\n",
      "Step 530284  [5.358 sec/step, loss=0.06467, avg_loss=0.07357]\n",
      "Step 530285  [5.359 sec/step, loss=0.07361, avg_loss=0.07356]\n",
      "Step 530286  [5.356 sec/step, loss=0.07440, avg_loss=0.07356]\n",
      "Step 530287  [5.382 sec/step, loss=0.07237, avg_loss=0.07354]\n",
      "Step 530288  [5.397 sec/step, loss=0.07602, avg_loss=0.07355]\n",
      "Step 530289  [5.441 sec/step, loss=0.06567, avg_loss=0.07345]\n",
      "Step 530290  [5.464 sec/step, loss=0.07517, avg_loss=0.07355]\n",
      "Step 530291  [5.469 sec/step, loss=0.07424, avg_loss=0.07356]\n",
      "Step 530292  [5.479 sec/step, loss=0.07555, avg_loss=0.07361]\n",
      "Step 530293  [5.501 sec/step, loss=0.07319, avg_loss=0.07360]\n",
      "Step 530294  [5.549 sec/step, loss=0.06651, avg_loss=0.07352]\n",
      "Step 530295  [5.543 sec/step, loss=0.06630, avg_loss=0.07346]\n",
      "Step 530296  [5.551 sec/step, loss=0.07577, avg_loss=0.07347]\n",
      "Step 530297  [5.504 sec/step, loss=0.07500, avg_loss=0.07356]\n",
      "Step 530298  [5.514 sec/step, loss=0.07265, avg_loss=0.07353]\n",
      "Step 530299  [5.514 sec/step, loss=0.07150, avg_loss=0.07349]\n",
      "Step 530300  [5.506 sec/step, loss=0.07186, avg_loss=0.07346]\n",
      "Writing summary at step: 530300\n",
      "Step 530301  [5.514 sec/step, loss=0.07490, avg_loss=0.07347]\n",
      "Step 530302  [5.514 sec/step, loss=0.07296, avg_loss=0.07347]\n",
      "Step 530303  [5.496 sec/step, loss=0.07080, avg_loss=0.07341]\n",
      "Step 530304  [5.492 sec/step, loss=0.07562, avg_loss=0.07341]\n",
      "Step 530305  [5.502 sec/step, loss=0.07612, avg_loss=0.07343]\n",
      "Step 530306  [5.519 sec/step, loss=0.07356, avg_loss=0.07344]\n",
      "Step 530307  [5.517 sec/step, loss=0.07553, avg_loss=0.07345]\n",
      "Step 530308  [5.529 sec/step, loss=0.07492, avg_loss=0.07347]\n",
      "Step 530309  [5.509 sec/step, loss=0.07522, avg_loss=0.07347]\n",
      "Step 530310  [5.506 sec/step, loss=0.07496, avg_loss=0.07348]\n",
      "Step 530311  [5.513 sec/step, loss=0.07405, avg_loss=0.07346]\n",
      "Step 530312  [5.506 sec/step, loss=0.07450, avg_loss=0.07346]\n",
      "Step 530313  [5.533 sec/step, loss=0.07604, avg_loss=0.07356]\n",
      "Step 530314  [5.497 sec/step, loss=0.07646, avg_loss=0.07367]\n",
      "Generated 32 batches of size 32 in 2.435 sec\n",
      "Step 530315  [5.503 sec/step, loss=0.07608, avg_loss=0.07368]\n",
      "Step 530316  [5.515 sec/step, loss=0.07515, avg_loss=0.07368]\n",
      "Step 530317  [5.509 sec/step, loss=0.07507, avg_loss=0.07369]\n",
      "Step 530318  [5.524 sec/step, loss=0.07468, avg_loss=0.07371]\n",
      "Step 530319  [5.526 sec/step, loss=0.07211, avg_loss=0.07368]\n",
      "Step 530320  [5.511 sec/step, loss=0.07363, avg_loss=0.07365]\n",
      "Step 530321  [5.474 sec/step, loss=0.07163, avg_loss=0.07362]\n",
      "Step 530322  [5.468 sec/step, loss=0.07386, avg_loss=0.07362]\n",
      "Step 530323  [5.471 sec/step, loss=0.07361, avg_loss=0.07362]\n",
      "Step 530324  [5.470 sec/step, loss=0.07515, avg_loss=0.07361]\n",
      "Step 530325  [5.484 sec/step, loss=0.07461, avg_loss=0.07365]\n",
      "Step 530326  [5.489 sec/step, loss=0.07313, avg_loss=0.07365]\n",
      "Step 530327  [5.485 sec/step, loss=0.07314, avg_loss=0.07363]\n",
      "Step 530328  [5.478 sec/step, loss=0.07485, avg_loss=0.07364]\n",
      "Step 530329  [5.483 sec/step, loss=0.07349, avg_loss=0.07367]\n",
      "Step 530330  [5.440 sec/step, loss=0.07327, avg_loss=0.07373]\n",
      "Step 530331  [5.430 sec/step, loss=0.07487, avg_loss=0.07374]\n",
      "Step 530332  [5.438 sec/step, loss=0.07320, avg_loss=0.07371]\n",
      "Step 530333  [5.450 sec/step, loss=0.07574, avg_loss=0.07372]\n",
      "Step 530334  [5.448 sec/step, loss=0.07401, avg_loss=0.07373]\n",
      "Step 530335  [5.498 sec/step, loss=0.06619, avg_loss=0.07367]\n",
      "Step 530336  [5.505 sec/step, loss=0.07465, avg_loss=0.07367]\n",
      "Step 530337  [5.496 sec/step, loss=0.07288, avg_loss=0.07365]\n",
      "Step 530338  [5.511 sec/step, loss=0.07630, avg_loss=0.07367]\n",
      "Step 530339  [5.480 sec/step, loss=0.07103, avg_loss=0.07365]\n",
      "Step 530340  [5.473 sec/step, loss=0.07445, avg_loss=0.07364]\n",
      "Step 530341  [5.452 sec/step, loss=0.07187, avg_loss=0.07362]\n",
      "Step 530342  [5.456 sec/step, loss=0.07391, avg_loss=0.07368]\n",
      "Step 530343  [5.454 sec/step, loss=0.07179, avg_loss=0.07369]\n",
      "Step 530344  [5.470 sec/step, loss=0.07609, avg_loss=0.07371]\n",
      "Step 530345  [5.508 sec/step, loss=0.07325, avg_loss=0.07372]\n",
      "Step 530346  [5.495 sec/step, loss=0.06590, avg_loss=0.07364]\n",
      "Generated 32 batches of size 32 in 2.391 sec\n",
      "Step 530347  [5.503 sec/step, loss=0.07681, avg_loss=0.07366]\n",
      "Step 530348  [5.513 sec/step, loss=0.07451, avg_loss=0.07371]\n",
      "Step 530349  [5.514 sec/step, loss=0.07599, avg_loss=0.07372]\n",
      "Step 530350  [5.519 sec/step, loss=0.07539, avg_loss=0.07376]\n",
      "Step 530351  [5.495 sec/step, loss=0.07427, avg_loss=0.07374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 530352  [5.491 sec/step, loss=0.07616, avg_loss=0.07374]\n",
      "Step 530353  [5.499 sec/step, loss=0.07530, avg_loss=0.07376]\n",
      "Step 530354  [5.510 sec/step, loss=0.07598, avg_loss=0.07377]\n",
      "Step 530355  [5.499 sec/step, loss=0.07302, avg_loss=0.07375]\n",
      "Step 530356  [5.497 sec/step, loss=0.07453, avg_loss=0.07374]\n",
      "Step 530357  [5.475 sec/step, loss=0.06541, avg_loss=0.07364]\n",
      "Step 530358  [5.477 sec/step, loss=0.07580, avg_loss=0.07364]\n",
      "Step 530359  [5.469 sec/step, loss=0.07632, avg_loss=0.07367]\n",
      "Step 530360  [5.471 sec/step, loss=0.07415, avg_loss=0.07366]\n",
      "Step 530361  [5.448 sec/step, loss=0.07098, avg_loss=0.07361]\n",
      "Step 530362  [5.444 sec/step, loss=0.07602, avg_loss=0.07363]\n",
      "Step 530363  [5.446 sec/step, loss=0.07529, avg_loss=0.07364]\n",
      "Step 530364  [5.443 sec/step, loss=0.07441, avg_loss=0.07364]\n",
      "Step 530365  [5.469 sec/step, loss=0.07585, avg_loss=0.07368]\n",
      "Step 530366  [5.474 sec/step, loss=0.07365, avg_loss=0.07370]\n",
      "Step 530367  [5.463 sec/step, loss=0.07462, avg_loss=0.07370]\n",
      "Step 530368  [5.453 sec/step, loss=0.07398, avg_loss=0.07369]\n",
      "Step 530369  [5.454 sec/step, loss=0.07494, avg_loss=0.07369]\n",
      "Step 530370  [5.450 sec/step, loss=0.07580, avg_loss=0.07370]\n",
      "Step 530371  [5.435 sec/step, loss=0.07429, avg_loss=0.07369]\n",
      "Step 530372  [5.431 sec/step, loss=0.07525, avg_loss=0.07370]\n",
      "Step 530373  [5.465 sec/step, loss=0.06605, avg_loss=0.07361]\n",
      "Step 530374  [5.471 sec/step, loss=0.07603, avg_loss=0.07365]\n",
      "Step 530375  [5.470 sec/step, loss=0.07546, avg_loss=0.07366]\n",
      "Step 530376  [5.496 sec/step, loss=0.07242, avg_loss=0.07365]\n",
      "Step 530377  [5.503 sec/step, loss=0.07312, avg_loss=0.07367]\n",
      "Step 530378  [5.503 sec/step, loss=0.07563, avg_loss=0.07367]\n",
      "Generated 32 batches of size 32 in 2.582 sec\n",
      "Step 530379  [5.509 sec/step, loss=0.07208, avg_loss=0.07364]\n",
      "Step 530380  [5.521 sec/step, loss=0.07238, avg_loss=0.07363]\n",
      "Step 530381  [5.511 sec/step, loss=0.07471, avg_loss=0.07361]\n",
      "Step 530382  [5.524 sec/step, loss=0.07361, avg_loss=0.07363]\n",
      "Step 530383  [5.514 sec/step, loss=0.07106, avg_loss=0.07361]\n",
      "Step 530384  [5.532 sec/step, loss=0.07646, avg_loss=0.07373]\n",
      "Step 530385  [5.536 sec/step, loss=0.07579, avg_loss=0.07375]\n",
      "Step 530386  [5.542 sec/step, loss=0.07170, avg_loss=0.07372]\n",
      "Step 530387  [5.524 sec/step, loss=0.07273, avg_loss=0.07373]\n",
      "Step 530388  [5.528 sec/step, loss=0.07606, avg_loss=0.07373]\n",
      "Step 530389  [5.483 sec/step, loss=0.07505, avg_loss=0.07382]\n",
      "Step 530390  [5.477 sec/step, loss=0.07547, avg_loss=0.07382]\n",
      "Step 530391  [5.500 sec/step, loss=0.07310, avg_loss=0.07381]\n",
      "Step 530392  [5.490 sec/step, loss=0.07317, avg_loss=0.07379]\n",
      "Step 530393  [5.463 sec/step, loss=0.07465, avg_loss=0.07380]\n",
      "Step 530394  [5.408 sec/step, loss=0.07353, avg_loss=0.07387]\n",
      "Step 530395  [5.414 sec/step, loss=0.07208, avg_loss=0.07393]\n",
      "Step 530396  [5.401 sec/step, loss=0.07234, avg_loss=0.07390]\n",
      "Step 530397  [5.403 sec/step, loss=0.07438, avg_loss=0.07389]\n",
      "Step 530398  [5.394 sec/step, loss=0.07334, avg_loss=0.07390]\n",
      "Step 530399  [5.390 sec/step, loss=0.07329, avg_loss=0.07391]\n",
      "Step 530400  [5.391 sec/step, loss=0.07444, avg_loss=0.07394]\n",
      "Writing summary at step: 530400\n",
      "Step 530401  [5.391 sec/step, loss=0.07461, avg_loss=0.07394]\n",
      "Step 530402  [5.397 sec/step, loss=0.07510, avg_loss=0.07396]\n",
      "Step 530403  [5.415 sec/step, loss=0.07465, avg_loss=0.07400]\n",
      "Step 530404  [5.458 sec/step, loss=0.06614, avg_loss=0.07390]\n",
      "Step 530405  [5.469 sec/step, loss=0.07335, avg_loss=0.07387]\n",
      "Step 530406  [5.469 sec/step, loss=0.07617, avg_loss=0.07390]\n",
      "Step 530407  [5.468 sec/step, loss=0.07454, avg_loss=0.07389]\n",
      "Step 530408  [5.472 sec/step, loss=0.07628, avg_loss=0.07390]\n",
      "Step 530409  [5.486 sec/step, loss=0.07620, avg_loss=0.07391]\n",
      "Generated 32 batches of size 32 in 2.516 sec\n",
      "Step 530410  [5.491 sec/step, loss=0.07477, avg_loss=0.07391]\n",
      "Step 530411  [5.464 sec/step, loss=0.07245, avg_loss=0.07390]\n",
      "Step 530412  [5.474 sec/step, loss=0.07555, avg_loss=0.07391]\n",
      "Step 530413  [5.478 sec/step, loss=0.07598, avg_loss=0.07391]\n",
      "Step 530414  [5.459 sec/step, loss=0.07431, avg_loss=0.07389]\n",
      "Step 530415  [5.449 sec/step, loss=0.07610, avg_loss=0.07389]\n",
      "Step 530416  [5.436 sec/step, loss=0.07323, avg_loss=0.07387]\n",
      "Step 530417  [5.434 sec/step, loss=0.07519, avg_loss=0.07387]\n",
      "Step 530418  [5.422 sec/step, loss=0.07396, avg_loss=0.07386]\n",
      "Step 530419  [5.434 sec/step, loss=0.07593, avg_loss=0.07390]\n",
      "Step 530420  [5.430 sec/step, loss=0.07381, avg_loss=0.07390]\n",
      "Step 530421  [5.433 sec/step, loss=0.07058, avg_loss=0.07389]\n",
      "Step 530422  [5.439 sec/step, loss=0.07547, avg_loss=0.07391]\n",
      "Step 530423  [5.460 sec/step, loss=0.07563, avg_loss=0.07393]\n",
      "Step 530424  [5.457 sec/step, loss=0.07480, avg_loss=0.07392]\n",
      "Step 530425  [5.456 sec/step, loss=0.07517, avg_loss=0.07393]\n",
      "Step 530426  [5.453 sec/step, loss=0.07438, avg_loss=0.07394]\n",
      "Step 530427  [5.437 sec/step, loss=0.06624, avg_loss=0.07387]\n",
      "Step 530428  [5.447 sec/step, loss=0.07496, avg_loss=0.07387]\n",
      "Step 530429  [5.466 sec/step, loss=0.07614, avg_loss=0.07390]\n",
      "Step 530430  [5.468 sec/step, loss=0.07610, avg_loss=0.07393]\n",
      "Step 530431  [5.464 sec/step, loss=0.07411, avg_loss=0.07392]\n",
      "Step 530432  [5.450 sec/step, loss=0.07477, avg_loss=0.07394]\n",
      "Step 530433  [5.451 sec/step, loss=0.07336, avg_loss=0.07391]\n",
      "Step 530434  [5.453 sec/step, loss=0.07516, avg_loss=0.07392]\n",
      "Step 530435  [5.405 sec/step, loss=0.07563, avg_loss=0.07402]\n",
      "Step 530436  [5.432 sec/step, loss=0.07349, avg_loss=0.07401]\n",
      "Step 530437  [5.432 sec/step, loss=0.07199, avg_loss=0.07400]\n",
      "Step 530438  [5.417 sec/step, loss=0.07243, avg_loss=0.07396]\n",
      "Step 530439  [5.418 sec/step, loss=0.07444, avg_loss=0.07399]\n",
      "Step 530440  [5.415 sec/step, loss=0.07328, avg_loss=0.07398]\n",
      "Step 530441  [5.422 sec/step, loss=0.07365, avg_loss=0.07400]\n",
      "Generated 32 batches of size 32 in 2.346 sec\n",
      "Step 530442  [5.456 sec/step, loss=0.07352, avg_loss=0.07399]\n",
      "Step 530443  [5.460 sec/step, loss=0.07390, avg_loss=0.07402]\n",
      "Step 530444  [5.450 sec/step, loss=0.07491, avg_loss=0.07400]\n",
      "Step 530445  [5.423 sec/step, loss=0.07375, avg_loss=0.07401]\n",
      "Step 530446  [5.447 sec/step, loss=0.07544, avg_loss=0.07410]\n",
      "Step 530447  [5.420 sec/step, loss=0.07105, avg_loss=0.07405]\n",
      "Step 530448  [5.407 sec/step, loss=0.07285, avg_loss=0.07403]\n",
      "Step 530449  [5.417 sec/step, loss=0.07436, avg_loss=0.07401]\n",
      "Step 530450  [5.459 sec/step, loss=0.06616, avg_loss=0.07392]\n",
      "Step 530451  [5.471 sec/step, loss=0.07520, avg_loss=0.07393]\n",
      "Step 530452  [5.457 sec/step, loss=0.07527, avg_loss=0.07392]\n",
      "Step 530453  [5.464 sec/step, loss=0.07577, avg_loss=0.07393]\n",
      "Step 530454  [5.445 sec/step, loss=0.07366, avg_loss=0.07390]\n",
      "Step 530455  [5.440 sec/step, loss=0.07170, avg_loss=0.07389]\n",
      "Step 530456  [5.447 sec/step, loss=0.07576, avg_loss=0.07390]\n",
      "Step 530457  [5.464 sec/step, loss=0.07433, avg_loss=0.07399]\n",
      "Step 530458  [5.477 sec/step, loss=0.07449, avg_loss=0.07398]\n",
      "Step 530459  [5.463 sec/step, loss=0.07263, avg_loss=0.07394]\n",
      "Step 530460  [5.454 sec/step, loss=0.07482, avg_loss=0.07395]\n",
      "Step 530461  [5.459 sec/step, loss=0.07305, avg_loss=0.07397]\n",
      "Step 530462  [5.454 sec/step, loss=0.07374, avg_loss=0.07395]\n",
      "Step 530463  [5.445 sec/step, loss=0.07515, avg_loss=0.07395]\n",
      "Step 530464  [5.456 sec/step, loss=0.07530, avg_loss=0.07395]\n",
      "Step 530465  [5.447 sec/step, loss=0.07382, avg_loss=0.07393]\n",
      "Step 530466  [5.464 sec/step, loss=0.07374, avg_loss=0.07393]\n",
      "Step 530467  [5.474 sec/step, loss=0.07626, avg_loss=0.07395]\n",
      "Step 530468  [5.495 sec/step, loss=0.07560, avg_loss=0.07397]\n",
      "Step 530469  [5.502 sec/step, loss=0.07453, avg_loss=0.07396]\n",
      "Step 530470  [5.480 sec/step, loss=0.07373, avg_loss=0.07394]\n",
      "Step 530471  [5.535 sec/step, loss=0.06574, avg_loss=0.07386]\n",
      "Step 530472  [5.536 sec/step, loss=0.07363, avg_loss=0.07384]\n",
      "Step 530473  [5.482 sec/step, loss=0.07249, avg_loss=0.07391]\n",
      "Generated 32 batches of size 32 in 2.548 sec\n",
      "Step 530474  [5.477 sec/step, loss=0.07046, avg_loss=0.07385]\n",
      "Step 530475  [5.484 sec/step, loss=0.07548, avg_loss=0.07385]\n",
      "Step 530476  [5.458 sec/step, loss=0.07481, avg_loss=0.07387]\n",
      "Step 530477  [5.470 sec/step, loss=0.07824, avg_loss=0.07392]\n",
      "Step 530478  [5.471 sec/step, loss=0.07585, avg_loss=0.07393]\n",
      "Step 530479  [5.450 sec/step, loss=0.06610, avg_loss=0.07387]\n",
      "Step 530480  [5.437 sec/step, loss=0.07088, avg_loss=0.07385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 530481  [5.439 sec/step, loss=0.07564, avg_loss=0.07386]\n",
      "Step 530482  [5.443 sec/step, loss=0.07620, avg_loss=0.07389]\n",
      "Step 530483  [5.472 sec/step, loss=0.07626, avg_loss=0.07394]\n",
      "Step 530484  [5.471 sec/step, loss=0.07505, avg_loss=0.07393]\n",
      "Step 530485  [5.459 sec/step, loss=0.07275, avg_loss=0.07389]\n",
      "Step 530486  [5.465 sec/step, loss=0.07425, avg_loss=0.07392]\n",
      "Step 530487  [5.442 sec/step, loss=0.06658, avg_loss=0.07386]\n",
      "Step 530488  [5.429 sec/step, loss=0.07554, avg_loss=0.07385]\n",
      "Step 530489  [5.418 sec/step, loss=0.07431, avg_loss=0.07385]\n",
      "Step 530490  [5.409 sec/step, loss=0.07131, avg_loss=0.07380]\n",
      "Step 530491  [5.375 sec/step, loss=0.07321, avg_loss=0.07381]\n",
      "Step 530492  [5.388 sec/step, loss=0.07491, avg_loss=0.07382]\n",
      "Step 530493  [5.398 sec/step, loss=0.07683, avg_loss=0.07384]\n",
      "Step 530494  [5.398 sec/step, loss=0.07573, avg_loss=0.07387]\n",
      "Step 530495  [5.415 sec/step, loss=0.07498, avg_loss=0.07390]\n",
      "Step 530496  [5.435 sec/step, loss=0.07586, avg_loss=0.07393]\n",
      "Step 530497  [5.458 sec/step, loss=0.07379, avg_loss=0.07393]\n",
      "Step 530498  [5.455 sec/step, loss=0.07672, avg_loss=0.07396]\n",
      "Step 530499  [5.458 sec/step, loss=0.07554, avg_loss=0.07398]\n",
      "Step 530500  [5.459 sec/step, loss=0.07760, avg_loss=0.07401]\n",
      "Writing summary at step: 530500\n",
      "Step 530501  [5.450 sec/step, loss=0.07450, avg_loss=0.07401]\n",
      "Step 530502  [5.461 sec/step, loss=0.07424, avg_loss=0.07400]\n",
      "Step 530503  [5.452 sec/step, loss=0.07333, avg_loss=0.07399]\n",
      "Step 530504  [5.412 sec/step, loss=0.07622, avg_loss=0.07409]\n",
      "Generated 32 batches of size 32 in 2.552 sec\n",
      "Step 530505  [5.399 sec/step, loss=0.07247, avg_loss=0.07408]\n",
      "Step 530506  [5.394 sec/step, loss=0.07633, avg_loss=0.07408]\n",
      "Step 530507  [5.443 sec/step, loss=0.06834, avg_loss=0.07402]\n",
      "Step 530508  [5.433 sec/step, loss=0.07659, avg_loss=0.07403]\n",
      "Step 530509  [5.425 sec/step, loss=0.07692, avg_loss=0.07403]\n",
      "Step 530510  [5.437 sec/step, loss=0.07807, avg_loss=0.07407]\n",
      "Step 530511  [5.440 sec/step, loss=0.07596, avg_loss=0.07410]\n",
      "Step 530512  [5.437 sec/step, loss=0.07548, avg_loss=0.07410]\n",
      "Step 530513  [5.418 sec/step, loss=0.07569, avg_loss=0.07410]\n",
      "Step 530514  [5.426 sec/step, loss=0.07666, avg_loss=0.07412]\n",
      "Step 530515  [5.428 sec/step, loss=0.07445, avg_loss=0.07410]\n",
      "Step 530516  [5.436 sec/step, loss=0.07684, avg_loss=0.07414]\n",
      "Step 530517  [5.433 sec/step, loss=0.07574, avg_loss=0.07415]\n",
      "Step 530518  [5.455 sec/step, loss=0.07781, avg_loss=0.07418]\n",
      "Step 530519  [5.446 sec/step, loss=0.07719, avg_loss=0.07420]\n",
      "Step 530520  [5.471 sec/step, loss=0.07748, avg_loss=0.07423]\n",
      "Step 530521  [5.496 sec/step, loss=0.07649, avg_loss=0.07429]\n",
      "Step 530522  [5.502 sec/step, loss=0.07575, avg_loss=0.07430]\n",
      "Step 530523  [5.484 sec/step, loss=0.07514, avg_loss=0.07429]\n",
      "Step 530524  [5.490 sec/step, loss=0.07553, avg_loss=0.07430]\n",
      "Step 530525  [5.490 sec/step, loss=0.07504, avg_loss=0.07430]\n",
      "Step 530526  [5.498 sec/step, loss=0.07824, avg_loss=0.07433]\n",
      "Step 530527  [5.519 sec/step, loss=0.07559, avg_loss=0.07443]\n",
      "Step 530528  [5.522 sec/step, loss=0.07739, avg_loss=0.07445]\n",
      "Step 530529  [5.517 sec/step, loss=0.07723, avg_loss=0.07446]\n",
      "Step 530530  [5.512 sec/step, loss=0.07635, avg_loss=0.07447]\n",
      "Step 530531  [5.511 sec/step, loss=0.07522, avg_loss=0.07448]\n",
      "Step 530532  [5.499 sec/step, loss=0.07439, avg_loss=0.07447]\n",
      "Step 530533  [5.470 sec/step, loss=0.06702, avg_loss=0.07441]\n",
      "Step 530534  [5.473 sec/step, loss=0.07562, avg_loss=0.07441]\n",
      "Step 530535  [5.466 sec/step, loss=0.07166, avg_loss=0.07437]\n",
      "Step 530536  [5.437 sec/step, loss=0.07634, avg_loss=0.07440]\n",
      "Generated 32 batches of size 32 in 2.516 sec\n",
      "Step 530537  [5.443 sec/step, loss=0.07598, avg_loss=0.07444]\n",
      "Step 530538  [5.443 sec/step, loss=0.07407, avg_loss=0.07446]\n",
      "Step 530539  [5.496 sec/step, loss=0.06715, avg_loss=0.07439]\n",
      "Step 530540  [5.520 sec/step, loss=0.07288, avg_loss=0.07438]\n",
      "Step 530541  [5.510 sec/step, loss=0.07297, avg_loss=0.07438]\n",
      "Step 530542  [5.475 sec/step, loss=0.07178, avg_loss=0.07436]\n",
      "Step 530543  [5.498 sec/step, loss=0.07646, avg_loss=0.07438]\n",
      "Step 530544  [5.487 sec/step, loss=0.07465, avg_loss=0.07438]\n",
      "Step 530545  [5.476 sec/step, loss=0.07461, avg_loss=0.07439]\n",
      "Step 530546  [5.470 sec/step, loss=0.07500, avg_loss=0.07439]\n",
      "Step 530547  [5.474 sec/step, loss=0.07297, avg_loss=0.07440]\n",
      "Step 530548  [5.494 sec/step, loss=0.07662, avg_loss=0.07444]\n",
      "Step 530549  [5.481 sec/step, loss=0.07534, avg_loss=0.07445]\n",
      "Step 530550  [5.442 sec/step, loss=0.07666, avg_loss=0.07456]\n",
      "Step 530551  [5.429 sec/step, loss=0.07165, avg_loss=0.07452]\n",
      "Step 530552  [5.436 sec/step, loss=0.07587, avg_loss=0.07453]\n",
      "Step 530553  [5.418 sec/step, loss=0.07157, avg_loss=0.07449]\n",
      "Step 530554  [5.441 sec/step, loss=0.07402, avg_loss=0.07449]\n",
      "Step 530555  [5.457 sec/step, loss=0.07605, avg_loss=0.07453]\n",
      "Step 530556  [5.445 sec/step, loss=0.07579, avg_loss=0.07453]\n",
      "Step 530557  [5.451 sec/step, loss=0.07494, avg_loss=0.07454]\n",
      "Step 530558  [5.430 sec/step, loss=0.07497, avg_loss=0.07454]\n",
      "Step 530559  [5.415 sec/step, loss=0.06628, avg_loss=0.07448]\n",
      "Step 530560  [5.422 sec/step, loss=0.07659, avg_loss=0.07450]\n",
      "Step 530561  [5.434 sec/step, loss=0.07650, avg_loss=0.07453]\n",
      "Step 530562  [5.437 sec/step, loss=0.07702, avg_loss=0.07457]\n",
      "Step 530563  [5.433 sec/step, loss=0.07470, avg_loss=0.07456]\n",
      "Step 530564  [5.451 sec/step, loss=0.07457, avg_loss=0.07455]\n",
      "Step 530565  [5.452 sec/step, loss=0.07462, avg_loss=0.07456]\n",
      "Step 530566  [5.440 sec/step, loss=0.07531, avg_loss=0.07458]\n",
      "Step 530567  [5.431 sec/step, loss=0.07464, avg_loss=0.07456]\n",
      "Step 530568  [5.468 sec/step, loss=0.06667, avg_loss=0.07447]\n",
      "Generated 32 batches of size 32 in 2.391 sec\n",
      "Step 530569  [5.479 sec/step, loss=0.07443, avg_loss=0.07447]\n",
      "Step 530570  [5.496 sec/step, loss=0.07640, avg_loss=0.07450]\n",
      "Step 530571  [5.442 sec/step, loss=0.07435, avg_loss=0.07458]\n",
      "Step 530572  [5.436 sec/step, loss=0.07417, avg_loss=0.07459]\n",
      "Step 530573  [5.468 sec/step, loss=0.07450, avg_loss=0.07461]\n",
      "Step 530574  [5.457 sec/step, loss=0.07311, avg_loss=0.07464]\n",
      "Step 530575  [5.441 sec/step, loss=0.07330, avg_loss=0.07461]\n",
      "Step 530576  [5.446 sec/step, loss=0.07347, avg_loss=0.07460]\n",
      "Step 530577  [5.434 sec/step, loss=0.07425, avg_loss=0.07456]\n",
      "Step 530578  [5.419 sec/step, loss=0.07202, avg_loss=0.07452]\n",
      "Step 530579  [5.448 sec/step, loss=0.07415, avg_loss=0.07460]\n",
      "Step 530580  [5.476 sec/step, loss=0.07486, avg_loss=0.07464]\n",
      "Step 530581  [5.459 sec/step, loss=0.07112, avg_loss=0.07460]\n",
      "Step 530582  [5.456 sec/step, loss=0.07598, avg_loss=0.07459]\n",
      "Step 530583  [5.443 sec/step, loss=0.07479, avg_loss=0.07458]\n",
      "Step 530584  [5.426 sec/step, loss=0.07414, avg_loss=0.07457]\n",
      "Step 530585  [5.436 sec/step, loss=0.07380, avg_loss=0.07458]\n",
      "Step 530586  [5.426 sec/step, loss=0.07552, avg_loss=0.07459]\n",
      "Step 530587  [5.447 sec/step, loss=0.07529, avg_loss=0.07468]\n",
      "Step 530588  [5.453 sec/step, loss=0.07574, avg_loss=0.07468]\n",
      "Step 530589  [5.453 sec/step, loss=0.07347, avg_loss=0.07467]\n",
      "Step 530590  [5.467 sec/step, loss=0.07609, avg_loss=0.07472]\n",
      "Step 530591  [5.475 sec/step, loss=0.07515, avg_loss=0.07474]\n",
      "Step 530592  [5.496 sec/step, loss=0.07521, avg_loss=0.07475]\n",
      "Step 530593  [5.498 sec/step, loss=0.07587, avg_loss=0.07474]\n",
      "Step 530594  [5.494 sec/step, loss=0.07378, avg_loss=0.07472]\n",
      "Step 530595  [5.498 sec/step, loss=0.07641, avg_loss=0.07473]\n",
      "Step 530596  [5.480 sec/step, loss=0.07454, avg_loss=0.07472]\n",
      "Step 530597  [5.457 sec/step, loss=0.07444, avg_loss=0.07472]\n",
      "Step 530598  [5.446 sec/step, loss=0.07433, avg_loss=0.07470]\n",
      "Step 530599  [5.444 sec/step, loss=0.07156, avg_loss=0.07466]\n",
      "Step 530600  [5.442 sec/step, loss=0.07427, avg_loss=0.07463]\n",
      "Writing summary at step: 530600\n",
      "Generated 32 batches of size 32 in 2.396 sec\n",
      "Step 530601  [5.496 sec/step, loss=0.06559, avg_loss=0.07454]\n",
      "Step 530602  [5.483 sec/step, loss=0.07455, avg_loss=0.07454]\n",
      "Step 530603  [5.483 sec/step, loss=0.07398, avg_loss=0.07455]\n",
      "Step 530604  [5.483 sec/step, loss=0.07397, avg_loss=0.07452]\n",
      "Step 530605  [5.465 sec/step, loss=0.07160, avg_loss=0.07452]\n",
      "Step 530606  [5.467 sec/step, loss=0.07331, avg_loss=0.07449]\n",
      "Step 530607  [5.428 sec/step, loss=0.07601, avg_loss=0.07456]\n",
      "Step 530608  [5.412 sec/step, loss=0.06668, avg_loss=0.07446]\n",
      "Step 530609  [5.409 sec/step, loss=0.07429, avg_loss=0.07444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 530610  [5.389 sec/step, loss=0.07410, avg_loss=0.07440]\n",
      "Step 530611  [5.412 sec/step, loss=0.07326, avg_loss=0.07437]\n",
      "Step 530612  [5.414 sec/step, loss=0.07557, avg_loss=0.07437]\n",
      "Step 530613  [5.411 sec/step, loss=0.07415, avg_loss=0.07436]\n",
      "Step 530614  [5.412 sec/step, loss=0.07486, avg_loss=0.07434]\n",
      "Step 530615  [5.398 sec/step, loss=0.07283, avg_loss=0.07432]\n",
      "Step 530616  [5.398 sec/step, loss=0.07537, avg_loss=0.07431]\n",
      "Step 530617  [5.399 sec/step, loss=0.07383, avg_loss=0.07429]\n",
      "Step 530618  [5.378 sec/step, loss=0.07181, avg_loss=0.07423]\n",
      "Step 530619  [5.390 sec/step, loss=0.07622, avg_loss=0.07422]\n",
      "Step 530620  [5.374 sec/step, loss=0.07568, avg_loss=0.07420]\n",
      "Step 530621  [5.366 sec/step, loss=0.07618, avg_loss=0.07420]\n",
      "Step 530622  [5.345 sec/step, loss=0.07401, avg_loss=0.07418]\n",
      "Step 530623  [5.345 sec/step, loss=0.07532, avg_loss=0.07418]\n",
      "Step 530624  [5.338 sec/step, loss=0.07365, avg_loss=0.07416]\n",
      "Step 530625  [5.330 sec/step, loss=0.07376, avg_loss=0.07415]\n",
      "Step 530626  [5.318 sec/step, loss=0.07482, avg_loss=0.07412]\n",
      "Step 530627  [5.296 sec/step, loss=0.06555, avg_loss=0.07402]\n",
      "Step 530628  [5.285 sec/step, loss=0.07204, avg_loss=0.07396]\n",
      "Step 530629  [5.262 sec/step, loss=0.07197, avg_loss=0.07391]\n",
      "Step 530630  [5.266 sec/step, loss=0.07604, avg_loss=0.07391]\n",
      "Step 530631  [5.319 sec/step, loss=0.06636, avg_loss=0.07382]\n",
      "Generated 32 batches of size 32 in 2.308 sec\n",
      "Step 530632  [5.350 sec/step, loss=0.07284, avg_loss=0.07380]\n",
      "Step 530633  [5.372 sec/step, loss=0.07447, avg_loss=0.07388]\n",
      "Step 530634  [5.375 sec/step, loss=0.07651, avg_loss=0.07389]\n",
      "Step 530635  [5.394 sec/step, loss=0.07582, avg_loss=0.07393]\n",
      "Step 530636  [5.405 sec/step, loss=0.07573, avg_loss=0.07392]\n",
      "Step 530637  [5.415 sec/step, loss=0.07373, avg_loss=0.07390]\n",
      "Step 530638  [5.422 sec/step, loss=0.07512, avg_loss=0.07391]\n",
      "Step 530639  [5.397 sec/step, loss=0.07346, avg_loss=0.07397]\n",
      "Step 530640  [5.373 sec/step, loss=0.07525, avg_loss=0.07400]\n",
      "Step 530641  [5.389 sec/step, loss=0.07509, avg_loss=0.07402]\n",
      "Step 530642  [5.396 sec/step, loss=0.07357, avg_loss=0.07404]\n",
      "Step 530643  [5.392 sec/step, loss=0.07631, avg_loss=0.07403]\n",
      "Step 530644  [5.394 sec/step, loss=0.07489, avg_loss=0.07404]\n",
      "Step 530645  [5.405 sec/step, loss=0.07526, avg_loss=0.07404]\n",
      "Step 530646  [5.418 sec/step, loss=0.07582, avg_loss=0.07405]\n",
      "Step 530647  [5.456 sec/step, loss=0.07318, avg_loss=0.07405]\n",
      "Step 530648  [5.439 sec/step, loss=0.07158, avg_loss=0.07400]\n",
      "Step 530649  [5.452 sec/step, loss=0.07556, avg_loss=0.07400]\n",
      "Step 530650  [5.430 sec/step, loss=0.07001, avg_loss=0.07394]\n",
      "Step 530651  [5.441 sec/step, loss=0.07415, avg_loss=0.07396]\n",
      "Step 530652  [5.450 sec/step, loss=0.07522, avg_loss=0.07396]\n",
      "Step 530653  [5.446 sec/step, loss=0.07387, avg_loss=0.07398]\n",
      "Step 530654  [5.423 sec/step, loss=0.07405, avg_loss=0.07398]\n",
      "Step 530655  [5.423 sec/step, loss=0.07445, avg_loss=0.07396]\n",
      "Step 530656  [5.404 sec/step, loss=0.06454, avg_loss=0.07385]\n",
      "Step 530657  [5.403 sec/step, loss=0.07503, avg_loss=0.07385]\n",
      "Step 530658  [5.407 sec/step, loss=0.07445, avg_loss=0.07385]\n",
      "Step 530659  [5.472 sec/step, loss=0.06736, avg_loss=0.07386]\n",
      "Step 530660  [5.463 sec/step, loss=0.07340, avg_loss=0.07383]\n",
      "Step 530661  [5.455 sec/step, loss=0.07470, avg_loss=0.07381]\n",
      "Step 530662  [5.457 sec/step, loss=0.07357, avg_loss=0.07377]\n",
      "Step 530663  [5.463 sec/step, loss=0.07498, avg_loss=0.07378]\n",
      "Generated 32 batches of size 32 in 2.521 sec\n",
      "Step 530664  [5.448 sec/step, loss=0.07060, avg_loss=0.07374]\n",
      "Step 530665  [5.429 sec/step, loss=0.07219, avg_loss=0.07371]\n",
      "Step 530666  [5.452 sec/step, loss=0.07344, avg_loss=0.07369]\n",
      "Step 530667  [5.458 sec/step, loss=0.07444, avg_loss=0.07369]\n",
      "Step 530668  [5.417 sec/step, loss=0.07410, avg_loss=0.07377]\n",
      "Step 530669  [5.387 sec/step, loss=0.07020, avg_loss=0.07372]\n",
      "Step 530670  [5.378 sec/step, loss=0.07413, avg_loss=0.07370]\n",
      "Step 530671  [5.392 sec/step, loss=0.07645, avg_loss=0.07372]\n",
      "Step 530672  [5.403 sec/step, loss=0.07606, avg_loss=0.07374]\n",
      "Step 530673  [5.384 sec/step, loss=0.07654, avg_loss=0.07376]\n",
      "Step 530674  [5.394 sec/step, loss=0.07393, avg_loss=0.07377]\n",
      "Step 530675  [5.402 sec/step, loss=0.07400, avg_loss=0.07378]\n",
      "Step 530676  [5.396 sec/step, loss=0.07442, avg_loss=0.07379]\n",
      "Step 530677  [5.417 sec/step, loss=0.07525, avg_loss=0.07380]\n",
      "Step 530678  [5.421 sec/step, loss=0.07154, avg_loss=0.07379]\n",
      "Step 530679  [5.424 sec/step, loss=0.07640, avg_loss=0.07381]\n",
      "Step 530680  [5.417 sec/step, loss=0.07596, avg_loss=0.07382]\n",
      "Step 530681  [5.423 sec/step, loss=0.07301, avg_loss=0.07384]\n",
      "Step 530682  [5.403 sec/step, loss=0.07363, avg_loss=0.07382]\n",
      "Step 530683  [5.395 sec/step, loss=0.07175, avg_loss=0.07379]\n",
      "Step 530684  [5.428 sec/step, loss=0.07435, avg_loss=0.07379]\n",
      "Step 530685  [5.422 sec/step, loss=0.07239, avg_loss=0.07378]\n",
      "Step 530686  [5.425 sec/step, loss=0.07481, avg_loss=0.07377]\n",
      "Step 530687  [5.430 sec/step, loss=0.07278, avg_loss=0.07375]\n",
      "Step 530688  [5.430 sec/step, loss=0.07562, avg_loss=0.07374]\n",
      "Step 530689  [5.451 sec/step, loss=0.07542, avg_loss=0.07376]\n",
      "Step 530690  [5.441 sec/step, loss=0.07015, avg_loss=0.07370]\n",
      "Step 530691  [5.443 sec/step, loss=0.07515, avg_loss=0.07370]\n",
      "Step 530692  [5.402 sec/step, loss=0.06753, avg_loss=0.07363]\n",
      "Step 530693  [5.439 sec/step, loss=0.06536, avg_loss=0.07352]\n",
      "Step 530694  [5.447 sec/step, loss=0.07482, avg_loss=0.07353]\n",
      "Step 530695  [5.423 sec/step, loss=0.07309, avg_loss=0.07350]\n",
      "Generated 32 batches of size 32 in 2.386 sec\n",
      "Step 530696  [5.440 sec/step, loss=0.07391, avg_loss=0.07349]\n",
      "Step 530697  [5.445 sec/step, loss=0.07597, avg_loss=0.07351]\n",
      "Step 530698  [5.449 sec/step, loss=0.07447, avg_loss=0.07351]\n",
      "Step 530699  [5.454 sec/step, loss=0.07533, avg_loss=0.07355]\n",
      "Step 530700  [5.452 sec/step, loss=0.07294, avg_loss=0.07353]\n",
      "Writing summary at step: 530700\n",
      "Step 530701  [5.406 sec/step, loss=0.07533, avg_loss=0.07363]\n",
      "Step 530702  [5.405 sec/step, loss=0.07351, avg_loss=0.07362]\n",
      "Step 530703  [5.412 sec/step, loss=0.07213, avg_loss=0.07360]\n",
      "Step 530704  [5.400 sec/step, loss=0.07522, avg_loss=0.07362]\n",
      "Step 530705  [5.412 sec/step, loss=0.07498, avg_loss=0.07365]\n",
      "Step 530706  [5.401 sec/step, loss=0.07389, avg_loss=0.07366]\n",
      "Step 530707  [5.387 sec/step, loss=0.07101, avg_loss=0.07361]\n",
      "Step 530708  [5.417 sec/step, loss=0.07603, avg_loss=0.07370]\n",
      "Step 530709  [5.417 sec/step, loss=0.07280, avg_loss=0.07368]\n",
      "Step 530710  [5.411 sec/step, loss=0.07395, avg_loss=0.07368]\n",
      "Step 530711  [5.402 sec/step, loss=0.07444, avg_loss=0.07369]\n",
      "Step 530712  [5.391 sec/step, loss=0.07397, avg_loss=0.07368]\n",
      "Step 530713  [5.408 sec/step, loss=0.07609, avg_loss=0.07370]\n",
      "Step 530714  [5.431 sec/step, loss=0.07303, avg_loss=0.07368]\n",
      "Step 530715  [5.427 sec/step, loss=0.07368, avg_loss=0.07369]\n",
      "Step 530716  [5.407 sec/step, loss=0.06731, avg_loss=0.07361]\n",
      "Step 530717  [5.419 sec/step, loss=0.07590, avg_loss=0.07363]\n",
      "Step 530718  [5.434 sec/step, loss=0.07487, avg_loss=0.07366]\n",
      "Step 530719  [5.418 sec/step, loss=0.07213, avg_loss=0.07362]\n",
      "Step 530720  [5.423 sec/step, loss=0.07526, avg_loss=0.07361]\n",
      "Step 530721  [5.403 sec/step, loss=0.07263, avg_loss=0.07358]\n",
      "Step 530722  [5.415 sec/step, loss=0.07520, avg_loss=0.07359]\n",
      "Step 530723  [5.468 sec/step, loss=0.06618, avg_loss=0.07350]\n",
      "Step 530724  [5.462 sec/step, loss=0.07161, avg_loss=0.07348]\n",
      "Step 530725  [5.468 sec/step, loss=0.07472, avg_loss=0.07349]\n",
      "Step 530726  [5.469 sec/step, loss=0.07526, avg_loss=0.07349]\n",
      "Generated 32 batches of size 32 in 2.332 sec\n",
      "Step 530727  [5.492 sec/step, loss=0.07393, avg_loss=0.07358]\n",
      "Step 530728  [5.513 sec/step, loss=0.07293, avg_loss=0.07358]\n",
      "Step 530729  [5.528 sec/step, loss=0.07544, avg_loss=0.07362]\n",
      "Step 530730  [5.523 sec/step, loss=0.07570, avg_loss=0.07362]\n",
      "Step 530731  [5.484 sec/step, loss=0.07568, avg_loss=0.07371]\n",
      "Step 530732  [5.471 sec/step, loss=0.07394, avg_loss=0.07372]\n",
      "Step 530733  [5.482 sec/step, loss=0.07543, avg_loss=0.07373]\n",
      "Step 530734  [5.486 sec/step, loss=0.07438, avg_loss=0.07371]\n",
      "Step 530735  [5.471 sec/step, loss=0.07421, avg_loss=0.07369]\n",
      "Step 530736  [5.490 sec/step, loss=0.07320, avg_loss=0.07367]\n",
      "Step 530737  [5.486 sec/step, loss=0.07549, avg_loss=0.07368]\n",
      "Step 530738  [5.488 sec/step, loss=0.07400, avg_loss=0.07367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 530739  [5.458 sec/step, loss=0.07216, avg_loss=0.07366]\n",
      "Step 530740  [5.458 sec/step, loss=0.07507, avg_loss=0.07366]\n",
      "Step 530741  [5.452 sec/step, loss=0.07069, avg_loss=0.07361]\n",
      "Step 530742  [5.455 sec/step, loss=0.07327, avg_loss=0.07361]\n",
      "Step 530743  [5.444 sec/step, loss=0.07426, avg_loss=0.07359]\n",
      "Step 530744  [5.449 sec/step, loss=0.07461, avg_loss=0.07359]\n",
      "Step 530745  [5.450 sec/step, loss=0.07543, avg_loss=0.07359]\n",
      "Step 530746  [5.436 sec/step, loss=0.07543, avg_loss=0.07359]\n",
      "Step 530747  [5.401 sec/step, loss=0.07231, avg_loss=0.07358]\n",
      "Step 530748  [5.429 sec/step, loss=0.07368, avg_loss=0.07360]\n",
      "Step 530749  [5.411 sec/step, loss=0.07358, avg_loss=0.07358]\n",
      "Step 530750  [5.434 sec/step, loss=0.07627, avg_loss=0.07364]\n",
      "Step 530751  [5.442 sec/step, loss=0.07579, avg_loss=0.07366]\n",
      "Step 530752  [5.427 sec/step, loss=0.07511, avg_loss=0.07366]\n",
      "Step 530753  [5.434 sec/step, loss=0.07542, avg_loss=0.07367]\n",
      "Step 530754  [5.446 sec/step, loss=0.07340, avg_loss=0.07367]\n",
      "Step 530755  [5.431 sec/step, loss=0.07030, avg_loss=0.07362]\n",
      "Step 530756  [5.454 sec/step, loss=0.07498, avg_loss=0.07373]\n",
      "Step 530757  [5.466 sec/step, loss=0.07530, avg_loss=0.07373]\n",
      "Step 530758  [5.464 sec/step, loss=0.07494, avg_loss=0.07374]\n",
      "Generated 32 batches of size 32 in 2.488 sec\n",
      "Step 530759  [5.409 sec/step, loss=0.07060, avg_loss=0.07377]\n",
      "Step 530760  [5.409 sec/step, loss=0.07384, avg_loss=0.07377]\n",
      "Step 530761  [5.399 sec/step, loss=0.07408, avg_loss=0.07377]\n",
      "Step 530762  [5.398 sec/step, loss=0.07482, avg_loss=0.07378]\n",
      "Step 530763  [5.412 sec/step, loss=0.07341, avg_loss=0.07376]\n",
      "Step 530764  [5.408 sec/step, loss=0.07451, avg_loss=0.07380]\n",
      "Step 530765  [5.466 sec/step, loss=0.06597, avg_loss=0.07374]\n",
      "Step 530766  [5.430 sec/step, loss=0.06426, avg_loss=0.07365]\n",
      "Step 530767  [5.434 sec/step, loss=0.07630, avg_loss=0.07367]\n",
      "Step 530768  [5.434 sec/step, loss=0.07588, avg_loss=0.07368]\n",
      "Step 530769  [5.451 sec/step, loss=0.07167, avg_loss=0.07370]\n",
      "Step 530770  [5.448 sec/step, loss=0.07497, avg_loss=0.07371]\n",
      "Step 530771  [5.441 sec/step, loss=0.07498, avg_loss=0.07369]\n",
      "Step 530772  [5.439 sec/step, loss=0.07468, avg_loss=0.07368]\n",
      "Step 530773  [5.425 sec/step, loss=0.07302, avg_loss=0.07364]\n",
      "Step 530774  [5.453 sec/step, loss=0.07472, avg_loss=0.07365]\n",
      "Step 530775  [5.441 sec/step, loss=0.07356, avg_loss=0.07365]\n",
      "Step 530776  [5.449 sec/step, loss=0.07559, avg_loss=0.07366]\n",
      "Step 530777  [5.420 sec/step, loss=0.07425, avg_loss=0.07365]\n",
      "Step 530778  [5.435 sec/step, loss=0.07609, avg_loss=0.07369]\n",
      "Step 530779  [5.435 sec/step, loss=0.07417, avg_loss=0.07367]\n",
      "Step 530780  [5.407 sec/step, loss=0.06511, avg_loss=0.07356]\n",
      "Step 530781  [5.400 sec/step, loss=0.07015, avg_loss=0.07354]\n",
      "Step 530782  [5.402 sec/step, loss=0.07254, avg_loss=0.07352]\n",
      "Step 530783  [5.422 sec/step, loss=0.07580, avg_loss=0.07356]\n",
      "Step 530784  [5.448 sec/step, loss=0.06533, avg_loss=0.07347]\n",
      "Step 530785  [5.467 sec/step, loss=0.07413, avg_loss=0.07349]\n",
      "Step 530786  [5.464 sec/step, loss=0.07557, avg_loss=0.07350]\n",
      "Step 530787  [5.460 sec/step, loss=0.07599, avg_loss=0.07353]\n",
      "Step 530788  [5.441 sec/step, loss=0.07045, avg_loss=0.07348]\n",
      "Step 530789  [5.438 sec/step, loss=0.07579, avg_loss=0.07348]\n",
      "Step 530790  [5.443 sec/step, loss=0.07415, avg_loss=0.07352]\n",
      "Generated 32 batches of size 32 in 2.547 sec\n",
      "Step 530791  [5.446 sec/step, loss=0.07388, avg_loss=0.07351]\n",
      "Step 530792  [5.463 sec/step, loss=0.07235, avg_loss=0.07356]\n",
      "Step 530793  [5.431 sec/step, loss=0.07572, avg_loss=0.07366]\n",
      "Step 530794  [5.435 sec/step, loss=0.07511, avg_loss=0.07367]\n",
      "Step 530795  [5.447 sec/step, loss=0.07499, avg_loss=0.07368]\n",
      "Step 530796  [5.441 sec/step, loss=0.07457, avg_loss=0.07369]\n",
      "Step 530797  [5.435 sec/step, loss=0.07476, avg_loss=0.07368]\n",
      "Step 530798  [5.429 sec/step, loss=0.07308, avg_loss=0.07367]\n",
      "Step 530799  [5.419 sec/step, loss=0.07387, avg_loss=0.07365]\n",
      "Step 530800  [5.436 sec/step, loss=0.07240, avg_loss=0.07365]\n",
      "Writing summary at step: 530800\n",
      "Step 530801  [5.436 sec/step, loss=0.07470, avg_loss=0.07364]\n",
      "Step 530802  [5.446 sec/step, loss=0.07514, avg_loss=0.07366]\n",
      "Step 530803  [5.434 sec/step, loss=0.07399, avg_loss=0.07367]\n",
      "Step 530804  [5.432 sec/step, loss=0.07273, avg_loss=0.07365]\n",
      "Step 530805  [5.432 sec/step, loss=0.07499, avg_loss=0.07365]\n",
      "Step 530806  [5.447 sec/step, loss=0.07613, avg_loss=0.07367]\n",
      "Step 530807  [5.450 sec/step, loss=0.07426, avg_loss=0.07370]\n",
      "Step 530808  [5.450 sec/step, loss=0.07554, avg_loss=0.07370]\n",
      "Step 530809  [5.454 sec/step, loss=0.07405, avg_loss=0.07371]\n",
      "Step 530810  [5.470 sec/step, loss=0.07439, avg_loss=0.07372]\n",
      "Step 530811  [5.465 sec/step, loss=0.07091, avg_loss=0.07368]\n",
      "Step 530812  [5.474 sec/step, loss=0.07282, avg_loss=0.07367]\n",
      "Step 530813  [5.462 sec/step, loss=0.07404, avg_loss=0.07365]\n",
      "Step 530814  [5.444 sec/step, loss=0.07477, avg_loss=0.07367]\n",
      "Step 530815  [5.452 sec/step, loss=0.07465, avg_loss=0.07368]\n",
      "Step 530816  [5.458 sec/step, loss=0.07120, avg_loss=0.07371]\n",
      "Step 530817  [5.458 sec/step, loss=0.07595, avg_loss=0.07372]\n",
      "Step 530818  [5.436 sec/step, loss=0.06667, avg_loss=0.07363]\n",
      "Step 530819  [5.426 sec/step, loss=0.07202, avg_loss=0.07363]\n",
      "Step 530820  [5.429 sec/step, loss=0.07582, avg_loss=0.07364]\n",
      "Step 530821  [5.434 sec/step, loss=0.07013, avg_loss=0.07361]\n",
      "Generated 32 batches of size 32 in 2.568 sec\n",
      "Step 530822  [5.430 sec/step, loss=0.07340, avg_loss=0.07359]\n",
      "Step 530823  [5.377 sec/step, loss=0.07495, avg_loss=0.07368]\n",
      "Step 530824  [5.397 sec/step, loss=0.07381, avg_loss=0.07370]\n",
      "Step 530825  [5.406 sec/step, loss=0.07583, avg_loss=0.07372]\n",
      "Step 530826  [5.411 sec/step, loss=0.07527, avg_loss=0.07372]\n",
      "Step 530827  [5.405 sec/step, loss=0.07468, avg_loss=0.07372]\n",
      "Step 530828  [5.399 sec/step, loss=0.07581, avg_loss=0.07375]\n",
      "Step 530829  [5.395 sec/step, loss=0.07540, avg_loss=0.07375]\n",
      "Step 530830  [5.442 sec/step, loss=0.06779, avg_loss=0.07367]\n",
      "Step 530831  [5.431 sec/step, loss=0.07364, avg_loss=0.07365]\n",
      "Step 530832  [5.411 sec/step, loss=0.07368, avg_loss=0.07365]\n",
      "Step 530833  [5.400 sec/step, loss=0.07467, avg_loss=0.07364]\n",
      "Step 530834  [5.386 sec/step, loss=0.07529, avg_loss=0.07365]\n",
      "Step 530835  [5.393 sec/step, loss=0.07349, avg_loss=0.07364]\n",
      "Step 530836  [5.367 sec/step, loss=0.07444, avg_loss=0.07366]\n",
      "Step 530837  [5.362 sec/step, loss=0.07367, avg_loss=0.07364]\n",
      "Step 530838  [5.357 sec/step, loss=0.07493, avg_loss=0.07365]\n",
      "Step 530839  [5.360 sec/step, loss=0.07415, avg_loss=0.07367]\n",
      "Step 530840  [5.348 sec/step, loss=0.07394, avg_loss=0.07366]\n",
      "Step 530841  [5.376 sec/step, loss=0.07319, avg_loss=0.07368]\n",
      "Step 530842  [5.373 sec/step, loss=0.07378, avg_loss=0.07369]\n",
      "Step 530843  [5.372 sec/step, loss=0.07205, avg_loss=0.07366]\n",
      "Step 530844  [5.384 sec/step, loss=0.07603, avg_loss=0.07368]\n",
      "Step 530845  [5.394 sec/step, loss=0.07587, avg_loss=0.07368]\n",
      "Step 530846  [5.416 sec/step, loss=0.07308, avg_loss=0.07366]\n",
      "Step 530847  [5.412 sec/step, loss=0.07139, avg_loss=0.07365]\n",
      "Step 530848  [5.399 sec/step, loss=0.07618, avg_loss=0.07368]\n",
      "Step 530849  [5.414 sec/step, loss=0.07615, avg_loss=0.07370]\n",
      "Step 530850  [5.452 sec/step, loss=0.06535, avg_loss=0.07359]\n",
      "Step 530851  [5.427 sec/step, loss=0.07182, avg_loss=0.07355]\n",
      "Step 530852  [5.427 sec/step, loss=0.07014, avg_loss=0.07350]\n",
      "Step 530853  [5.428 sec/step, loss=0.07494, avg_loss=0.07350]\n",
      "Generated 32 batches of size 32 in 2.437 sec\n",
      "Step 530854  [5.437 sec/step, loss=0.07620, avg_loss=0.07353]\n",
      "Step 530855  [5.460 sec/step, loss=0.07406, avg_loss=0.07356]\n",
      "Step 530856  [5.439 sec/step, loss=0.06557, avg_loss=0.07347]\n",
      "Step 530857  [5.438 sec/step, loss=0.07552, avg_loss=0.07347]\n",
      "Step 530858  [5.435 sec/step, loss=0.07551, avg_loss=0.07348]\n",
      "Step 530859  [5.444 sec/step, loss=0.07525, avg_loss=0.07352]\n",
      "Step 530860  [5.446 sec/step, loss=0.07461, avg_loss=0.07353]\n",
      "Step 530861  [5.466 sec/step, loss=0.07590, avg_loss=0.07355]\n",
      "Step 530862  [5.449 sec/step, loss=0.07361, avg_loss=0.07354]\n",
      "Step 530863  [5.438 sec/step, loss=0.07508, avg_loss=0.07355]\n",
      "Step 530864  [5.424 sec/step, loss=0.07175, avg_loss=0.07353]\n",
      "Step 530865  [5.365 sec/step, loss=0.07383, avg_loss=0.07361]\n",
      "Step 530866  [5.385 sec/step, loss=0.07508, avg_loss=0.07371]\n",
      "Step 530867  [5.376 sec/step, loss=0.07207, avg_loss=0.07367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 530868  [5.375 sec/step, loss=0.07618, avg_loss=0.07367]\n",
      "Step 530869  [5.371 sec/step, loss=0.07138, avg_loss=0.07367]\n",
      "Step 530870  [5.380 sec/step, loss=0.07465, avg_loss=0.07367]\n",
      "Step 530871  [5.393 sec/step, loss=0.07526, avg_loss=0.07367]\n",
      "Step 530872  [5.402 sec/step, loss=0.07555, avg_loss=0.07368]\n",
      "Step 530873  [5.401 sec/step, loss=0.07283, avg_loss=0.07368]\n",
      "Step 530874  [5.366 sec/step, loss=0.07073, avg_loss=0.07364]\n",
      "Step 530875  [5.396 sec/step, loss=0.07296, avg_loss=0.07363]\n",
      "Step 530876  [5.399 sec/step, loss=0.07413, avg_loss=0.07362]\n",
      "Step 530877  [5.421 sec/step, loss=0.07587, avg_loss=0.07363]\n",
      "Step 530878  [5.408 sec/step, loss=0.07375, avg_loss=0.07361]\n",
      "Step 530879  [5.391 sec/step, loss=0.07478, avg_loss=0.07362]\n",
      "Step 530880  [5.410 sec/step, loss=0.07541, avg_loss=0.07372]\n",
      "Step 530881  [5.420 sec/step, loss=0.07389, avg_loss=0.07376]\n",
      "Step 530882  [5.428 sec/step, loss=0.07461, avg_loss=0.07378]\n",
      "Step 530883  [5.412 sec/step, loss=0.07251, avg_loss=0.07374]\n",
      "Step 530884  [5.412 sec/step, loss=0.06641, avg_loss=0.07376]\n",
      "Step 530885  [5.388 sec/step, loss=0.06646, avg_loss=0.07368]\n",
      "Generated 32 batches of size 32 in 2.415 sec\n",
      "Step 530886  [5.401 sec/step, loss=0.07530, avg_loss=0.07368]\n",
      "Step 530887  [5.401 sec/step, loss=0.07561, avg_loss=0.07367]\n",
      "Step 530888  [5.415 sec/step, loss=0.07207, avg_loss=0.07369]\n",
      "Step 530889  [5.404 sec/step, loss=0.07482, avg_loss=0.07368]\n",
      "Step 530890  [5.404 sec/step, loss=0.07259, avg_loss=0.07366]\n",
      "Step 530891  [5.407 sec/step, loss=0.07578, avg_loss=0.07368]\n",
      "Step 530892  [5.420 sec/step, loss=0.07366, avg_loss=0.07369]\n",
      "Step 530893  [5.418 sec/step, loss=0.07568, avg_loss=0.07369]\n",
      "Step 530894  [5.407 sec/step, loss=0.07390, avg_loss=0.07368]\n",
      "Step 530895  [5.423 sec/step, loss=0.07607, avg_loss=0.07369]\n",
      "Step 530896  [5.401 sec/step, loss=0.07092, avg_loss=0.07366]\n",
      "Step 530897  [5.414 sec/step, loss=0.07577, avg_loss=0.07367]\n",
      "Step 530898  [5.411 sec/step, loss=0.07408, avg_loss=0.07368]\n",
      "Step 530899  [5.420 sec/step, loss=0.07532, avg_loss=0.07369]\n",
      "Step 530900  [5.428 sec/step, loss=0.07315, avg_loss=0.07370]\n",
      "Writing summary at step: 530900\n",
      "Step 530901  [5.424 sec/step, loss=0.07280, avg_loss=0.07368]\n",
      "Step 530902  [5.411 sec/step, loss=0.07364, avg_loss=0.07366]\n",
      "Step 530903  [5.430 sec/step, loss=0.07583, avg_loss=0.07368]\n",
      "Step 530904  [5.443 sec/step, loss=0.07563, avg_loss=0.07371]\n",
      "Step 530905  [5.449 sec/step, loss=0.07390, avg_loss=0.07370]\n",
      "Step 530906  [5.448 sec/step, loss=0.07608, avg_loss=0.07370]\n",
      "Step 530907  [5.430 sec/step, loss=0.06546, avg_loss=0.07361]\n",
      "Step 530908  [5.416 sec/step, loss=0.07504, avg_loss=0.07361]\n",
      "Step 530909  [5.408 sec/step, loss=0.07536, avg_loss=0.07362]\n",
      "Step 530910  [5.403 sec/step, loss=0.07521, avg_loss=0.07363]\n",
      "Step 530911  [5.405 sec/step, loss=0.07453, avg_loss=0.07367]\n",
      "Step 530912  [5.408 sec/step, loss=0.07516, avg_loss=0.07369]\n",
      "Step 530913  [5.398 sec/step, loss=0.07163, avg_loss=0.07366]\n",
      "Step 530914  [5.385 sec/step, loss=0.07291, avg_loss=0.07365]\n",
      "Step 530915  [5.403 sec/step, loss=0.07296, avg_loss=0.07363]\n",
      "Step 530916  [5.414 sec/step, loss=0.07200, avg_loss=0.07364]\n",
      "Generated 32 batches of size 32 in 2.332 sec\n",
      "Step 530917  [5.407 sec/step, loss=0.07490, avg_loss=0.07363]\n",
      "Step 530918  [5.473 sec/step, loss=0.06537, avg_loss=0.07361]\n",
      "Step 530919  [5.479 sec/step, loss=0.07361, avg_loss=0.07363]\n",
      "Step 530920  [5.479 sec/step, loss=0.07321, avg_loss=0.07360]\n",
      "Step 530921  [5.493 sec/step, loss=0.07561, avg_loss=0.07366]\n",
      "Step 530922  [5.487 sec/step, loss=0.07406, avg_loss=0.07367]\n",
      "Step 530923  [5.489 sec/step, loss=0.07091, avg_loss=0.07362]\n",
      "Step 530924  [5.483 sec/step, loss=0.07486, avg_loss=0.07364]\n",
      "Step 530925  [5.480 sec/step, loss=0.07447, avg_loss=0.07362]\n",
      "Step 530926  [5.483 sec/step, loss=0.07368, avg_loss=0.07361]\n",
      "Step 530927  [5.498 sec/step, loss=0.07546, avg_loss=0.07361]\n",
      "Step 530928  [5.492 sec/step, loss=0.07613, avg_loss=0.07362]\n",
      "Step 530929  [5.493 sec/step, loss=0.07263, avg_loss=0.07359]\n",
      "Step 530930  [5.440 sec/step, loss=0.07399, avg_loss=0.07365]\n",
      "Step 530931  [5.444 sec/step, loss=0.07470, avg_loss=0.07366]\n",
      "Step 530932  [5.504 sec/step, loss=0.06657, avg_loss=0.07359]\n",
      "Step 530933  [5.492 sec/step, loss=0.07317, avg_loss=0.07358]\n",
      "Step 530934  [5.488 sec/step, loss=0.07404, avg_loss=0.07356]\n",
      "Step 530935  [5.483 sec/step, loss=0.07497, avg_loss=0.07358]\n",
      "Step 530936  [5.493 sec/step, loss=0.07328, avg_loss=0.07357]\n",
      "Step 530937  [5.486 sec/step, loss=0.07375, avg_loss=0.07357]\n",
      "Step 530938  [5.482 sec/step, loss=0.07441, avg_loss=0.07356]\n",
      "Step 530939  [5.489 sec/step, loss=0.07370, avg_loss=0.07356]\n",
      "Step 530940  [5.501 sec/step, loss=0.07464, avg_loss=0.07356]\n",
      "Step 530941  [5.479 sec/step, loss=0.07367, avg_loss=0.07357]\n",
      "Step 530942  [5.491 sec/step, loss=0.07615, avg_loss=0.07359]\n",
      "Step 530943  [5.504 sec/step, loss=0.07415, avg_loss=0.07361]\n",
      "Step 530944  [5.481 sec/step, loss=0.07370, avg_loss=0.07359]\n",
      "Step 530945  [5.468 sec/step, loss=0.07523, avg_loss=0.07358]\n",
      "Step 530946  [5.433 sec/step, loss=0.07061, avg_loss=0.07356]\n",
      "Step 530947  [5.440 sec/step, loss=0.07348, avg_loss=0.07358]\n",
      "Step 530948  [5.426 sec/step, loss=0.07479, avg_loss=0.07357]\n",
      "Generated 32 batches of size 32 in 2.579 sec\n",
      "Step 530949  [5.416 sec/step, loss=0.07185, avg_loss=0.07352]\n",
      "Step 530950  [5.347 sec/step, loss=0.06640, avg_loss=0.07353]\n",
      "Step 530951  [5.363 sec/step, loss=0.07536, avg_loss=0.07357]\n",
      "Step 530952  [5.392 sec/step, loss=0.07300, avg_loss=0.07360]\n",
      "Step 530953  [5.406 sec/step, loss=0.07547, avg_loss=0.07360]\n",
      "Step 530954  [5.378 sec/step, loss=0.07043, avg_loss=0.07355]\n",
      "Step 530955  [5.386 sec/step, loss=0.07497, avg_loss=0.07355]\n",
      "Step 530956  [5.414 sec/step, loss=0.07596, avg_loss=0.07366]\n",
      "Step 530957  [5.412 sec/step, loss=0.07601, avg_loss=0.07366]\n",
      "Step 530958  [5.402 sec/step, loss=0.07376, avg_loss=0.07365]\n",
      "Step 530959  [5.400 sec/step, loss=0.07259, avg_loss=0.07362]\n",
      "Step 530960  [5.382 sec/step, loss=0.06520, avg_loss=0.07352]\n",
      "Step 530961  [5.372 sec/step, loss=0.07413, avg_loss=0.07351]\n",
      "Step 530962  [5.431 sec/step, loss=0.06538, avg_loss=0.07342]\n",
      "Step 530963  [5.426 sec/step, loss=0.07408, avg_loss=0.07341]\n",
      "Step 530964  [5.444 sec/step, loss=0.07248, avg_loss=0.07342]\n",
      "Step 530965  [5.459 sec/step, loss=0.07464, avg_loss=0.07343]\n",
      "Step 530966  [5.463 sec/step, loss=0.07597, avg_loss=0.07344]\n",
      "Step 530967  [5.472 sec/step, loss=0.07403, avg_loss=0.07346]\n",
      "Step 530968  [5.469 sec/step, loss=0.07471, avg_loss=0.07344]\n",
      "Step 530969  [5.463 sec/step, loss=0.07392, avg_loss=0.07347]\n",
      "Step 530970  [5.446 sec/step, loss=0.07164, avg_loss=0.07344]\n",
      "Step 530971  [5.442 sec/step, loss=0.07377, avg_loss=0.07342]\n",
      "Step 530972  [5.439 sec/step, loss=0.07446, avg_loss=0.07341]\n",
      "Step 530973  [5.436 sec/step, loss=0.07382, avg_loss=0.07342]\n",
      "Step 530974  [5.448 sec/step, loss=0.07527, avg_loss=0.07347]\n",
      "Step 530975  [5.435 sec/step, loss=0.07599, avg_loss=0.07350]\n",
      "Step 530976  [5.433 sec/step, loss=0.07575, avg_loss=0.07352]\n",
      "Step 530977  [5.422 sec/step, loss=0.07529, avg_loss=0.07351]\n",
      "Step 530978  [5.426 sec/step, loss=0.07547, avg_loss=0.07353]\n",
      "Step 530979  [5.457 sec/step, loss=0.07293, avg_loss=0.07351]\n",
      "Step 530980  [5.456 sec/step, loss=0.07428, avg_loss=0.07350]\n",
      "Generated 32 batches of size 32 in 2.581 sec\n",
      "Step 530981  [5.462 sec/step, loss=0.07492, avg_loss=0.07351]\n",
      "Step 530982  [5.472 sec/step, loss=0.07568, avg_loss=0.07352]\n",
      "Step 530983  [5.482 sec/step, loss=0.07552, avg_loss=0.07355]\n",
      "Step 530984  [5.433 sec/step, loss=0.07180, avg_loss=0.07360]\n",
      "Step 530985  [5.436 sec/step, loss=0.07251, avg_loss=0.07366]\n",
      "Step 530986  [5.419 sec/step, loss=0.07348, avg_loss=0.07364]\n",
      "Step 530987  [5.409 sec/step, loss=0.07404, avg_loss=0.07363]\n",
      "Step 530988  [5.424 sec/step, loss=0.07561, avg_loss=0.07366]\n",
      "Step 530989  [5.435 sec/step, loss=0.07557, avg_loss=0.07367]\n",
      "Step 530990  [5.435 sec/step, loss=0.07099, avg_loss=0.07366]\n",
      "Step 530991  [5.418 sec/step, loss=0.07362, avg_loss=0.07363]\n",
      "Step 530992  [5.403 sec/step, loss=0.07460, avg_loss=0.07364]\n",
      "Step 530993  [5.384 sec/step, loss=0.07474, avg_loss=0.07363]\n",
      "Step 530994  [5.403 sec/step, loss=0.07597, avg_loss=0.07365]\n",
      "Step 530995  [5.385 sec/step, loss=0.07309, avg_loss=0.07362]\n",
      "Step 530996  [5.406 sec/step, loss=0.07553, avg_loss=0.07367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 530997  [5.398 sec/step, loss=0.07524, avg_loss=0.07367]\n",
      "Step 530998  [5.413 sec/step, loss=0.07568, avg_loss=0.07368]\n",
      "Step 530999  [5.422 sec/step, loss=0.07488, avg_loss=0.07368]\n",
      "Step 531000  [5.393 sec/step, loss=0.07402, avg_loss=0.07369]\n",
      "Writing summary at step: 531000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-531000\n",
      "Saving audio and alignment...\n",
      "Input: vaaqaatdan laahvarii tdanzoomazaa kii botshaarr sae gunaahoon miin izaafaa or tdabiijatd miin anddhaaddhandd ratsaaoo aa saktdaa hae~______________________________________________________________________________________\n",
      "Step 531001  [5.417 sec/step, loss=0.07543, avg_loss=0.07371]\n",
      "Step 531002  [5.430 sec/step, loss=0.07521, avg_loss=0.07373]\n",
      "Step 531003  [5.424 sec/step, loss=0.07564, avg_loss=0.07373]\n",
      "Step 531004  [5.413 sec/step, loss=0.07531, avg_loss=0.07372]\n",
      "Step 531005  [5.387 sec/step, loss=0.06633, avg_loss=0.07365]\n",
      "Step 531006  [5.376 sec/step, loss=0.07319, avg_loss=0.07362]\n",
      "Step 531007  [5.398 sec/step, loss=0.07422, avg_loss=0.07371]\n",
      "Step 531008  [5.402 sec/step, loss=0.07408, avg_loss=0.07370]\n",
      "Step 531009  [5.390 sec/step, loss=0.07142, avg_loss=0.07366]\n",
      "Step 531010  [5.384 sec/step, loss=0.07328, avg_loss=0.07364]\n",
      "Generated 32 batches of size 32 in 2.405 sec\n",
      "Step 531011  [5.401 sec/step, loss=0.07656, avg_loss=0.07366]\n",
      "Step 531012  [5.410 sec/step, loss=0.07370, avg_loss=0.07364]\n",
      "Step 531013  [5.416 sec/step, loss=0.07062, avg_loss=0.07363]\n",
      "Step 531014  [5.425 sec/step, loss=0.07435, avg_loss=0.07365]\n",
      "Step 531015  [5.425 sec/step, loss=0.07292, avg_loss=0.07365]\n",
      "Step 531016  [5.417 sec/step, loss=0.07374, avg_loss=0.07366]\n",
      "Step 531017  [5.400 sec/step, loss=0.07144, avg_loss=0.07363]\n",
      "Step 531018  [5.355 sec/step, loss=0.07498, avg_loss=0.07373]\n",
      "Step 531019  [5.362 sec/step, loss=0.07539, avg_loss=0.07374]\n",
      "Step 531020  [5.341 sec/step, loss=0.07396, avg_loss=0.07375]\n",
      "Step 531021  [5.342 sec/step, loss=0.07599, avg_loss=0.07375]\n",
      "Step 531022  [5.350 sec/step, loss=0.07413, avg_loss=0.07376]\n",
      "Step 531023  [5.348 sec/step, loss=0.07537, avg_loss=0.07380]\n",
      "Step 531024  [5.338 sec/step, loss=0.07197, avg_loss=0.07377]\n",
      "Step 531025  [5.322 sec/step, loss=0.07006, avg_loss=0.07373]\n",
      "Step 531026  [5.327 sec/step, loss=0.07257, avg_loss=0.07372]\n",
      "Step 531027  [5.315 sec/step, loss=0.07547, avg_loss=0.07372]\n",
      "Step 531028  [5.357 sec/step, loss=0.06641, avg_loss=0.07362]\n",
      "Step 531029  [5.360 sec/step, loss=0.07508, avg_loss=0.07364]\n",
      "Step 531030  [5.358 sec/step, loss=0.07480, avg_loss=0.07365]\n",
      "Step 531031  [5.354 sec/step, loss=0.07174, avg_loss=0.07362]\n",
      "Step 531032  [5.314 sec/step, loss=0.07386, avg_loss=0.07369]\n",
      "Step 531033  [5.306 sec/step, loss=0.06640, avg_loss=0.07363]\n",
      "Step 531034  [5.313 sec/step, loss=0.07468, avg_loss=0.07363]\n",
      "Step 531035  [5.315 sec/step, loss=0.07469, avg_loss=0.07363]\n",
      "Step 531036  [5.309 sec/step, loss=0.07414, avg_loss=0.07364]\n",
      "Step 531037  [5.309 sec/step, loss=0.07144, avg_loss=0.07362]\n",
      "Step 531038  [5.313 sec/step, loss=0.07375, avg_loss=0.07361]\n",
      "Step 531039  [5.300 sec/step, loss=0.07373, avg_loss=0.07361]\n",
      "Step 531040  [5.287 sec/step, loss=0.07274, avg_loss=0.07359]\n",
      "Step 531041  [5.294 sec/step, loss=0.07613, avg_loss=0.07362]\n",
      "Step 531042  [5.299 sec/step, loss=0.07601, avg_loss=0.07361]\n",
      "Generated 32 batches of size 32 in 2.419 sec\n",
      "Step 531043  [5.320 sec/step, loss=0.07225, avg_loss=0.07359]\n",
      "Step 531044  [5.342 sec/step, loss=0.07578, avg_loss=0.07362]\n",
      "Step 531045  [5.359 sec/step, loss=0.07521, avg_loss=0.07362]\n",
      "Step 531046  [5.373 sec/step, loss=0.07563, avg_loss=0.07367]\n",
      "Step 531047  [5.395 sec/step, loss=0.07564, avg_loss=0.07369]\n",
      "Step 531048  [5.399 sec/step, loss=0.07287, avg_loss=0.07367]\n",
      "Step 531049  [5.385 sec/step, loss=0.07080, avg_loss=0.07366]\n",
      "Step 531050  [5.396 sec/step, loss=0.07355, avg_loss=0.07373]\n",
      "Step 531051  [5.402 sec/step, loss=0.07586, avg_loss=0.07373]\n",
      "Step 531052  [5.381 sec/step, loss=0.07597, avg_loss=0.07376]\n",
      "Step 531053  [5.374 sec/step, loss=0.07398, avg_loss=0.07375]\n",
      "Step 531054  [5.380 sec/step, loss=0.07384, avg_loss=0.07378]\n",
      "Step 531055  [5.370 sec/step, loss=0.07445, avg_loss=0.07378]\n",
      "Step 531056  [5.362 sec/step, loss=0.07440, avg_loss=0.07376]\n",
      "Step 531057  [5.357 sec/step, loss=0.07580, avg_loss=0.07376]\n",
      "Step 531058  [5.366 sec/step, loss=0.07487, avg_loss=0.07377]\n",
      "Step 531059  [5.366 sec/step, loss=0.07495, avg_loss=0.07380]\n",
      "Step 531060  [5.390 sec/step, loss=0.07469, avg_loss=0.07389]\n",
      "Step 531061  [5.386 sec/step, loss=0.07473, avg_loss=0.07390]\n",
      "Step 531062  [5.334 sec/step, loss=0.07474, avg_loss=0.07399]\n",
      "Step 531063  [5.354 sec/step, loss=0.07452, avg_loss=0.07399]\n",
      "Step 531064  [5.360 sec/step, loss=0.07341, avg_loss=0.07400]\n",
      "Step 531065  [5.382 sec/step, loss=0.07272, avg_loss=0.07398]\n",
      "Step 531066  [5.373 sec/step, loss=0.07161, avg_loss=0.07394]\n",
      "Step 531067  [5.375 sec/step, loss=0.07574, avg_loss=0.07396]\n",
      "Step 531068  [5.371 sec/step, loss=0.07148, avg_loss=0.07393]\n",
      "Step 531069  [5.373 sec/step, loss=0.07280, avg_loss=0.07391]\n",
      "Step 531070  [5.380 sec/step, loss=0.07516, avg_loss=0.07395]\n",
      "Step 531071  [5.357 sec/step, loss=0.07075, avg_loss=0.07392]\n",
      "Step 531072  [5.357 sec/step, loss=0.07655, avg_loss=0.07394]\n",
      "Step 531073  [5.362 sec/step, loss=0.07400, avg_loss=0.07394]\n",
      "Step 531074  [5.349 sec/step, loss=0.07125, avg_loss=0.07390]\n",
      "Generated 32 batches of size 32 in 2.422 sec\n",
      "Step 531075  [5.351 sec/step, loss=0.07539, avg_loss=0.07390]\n",
      "Step 531076  [5.393 sec/step, loss=0.06584, avg_loss=0.07380]\n",
      "Step 531077  [5.375 sec/step, loss=0.06564, avg_loss=0.07370]\n",
      "Step 531078  [5.385 sec/step, loss=0.07552, avg_loss=0.07370]\n",
      "Step 531079  [5.359 sec/step, loss=0.07543, avg_loss=0.07373]\n",
      "Step 531080  [5.345 sec/step, loss=0.07228, avg_loss=0.07371]\n",
      "Step 531081  [5.345 sec/step, loss=0.07536, avg_loss=0.07371]\n",
      "Step 531082  [5.330 sec/step, loss=0.07322, avg_loss=0.07369]\n",
      "Step 531083  [5.316 sec/step, loss=0.07386, avg_loss=0.07367]\n",
      "Step 531084  [5.316 sec/step, loss=0.07521, avg_loss=0.07370]\n",
      "Step 531085  [5.322 sec/step, loss=0.07365, avg_loss=0.07371]\n",
      "Step 531086  [5.313 sec/step, loss=0.07127, avg_loss=0.07369]\n",
      "Step 531087  [5.325 sec/step, loss=0.07611, avg_loss=0.07371]\n",
      "Step 531088  [5.319 sec/step, loss=0.07574, avg_loss=0.07371]\n",
      "Step 531089  [5.302 sec/step, loss=0.07286, avg_loss=0.07369]\n",
      "Step 531090  [5.300 sec/step, loss=0.07103, avg_loss=0.07369]\n",
      "Step 531091  [5.320 sec/step, loss=0.07523, avg_loss=0.07370]\n",
      "Step 531092  [5.363 sec/step, loss=0.07017, avg_loss=0.07366]\n",
      "Step 531093  [5.355 sec/step, loss=0.07405, avg_loss=0.07365]\n",
      "Step 531094  [5.347 sec/step, loss=0.07510, avg_loss=0.07364]\n",
      "Step 531095  [5.353 sec/step, loss=0.07220, avg_loss=0.07363]\n",
      "Step 531096  [5.346 sec/step, loss=0.07153, avg_loss=0.07359]\n",
      "Step 531097  [5.325 sec/step, loss=0.07042, avg_loss=0.07355]\n",
      "Step 531098  [5.320 sec/step, loss=0.07504, avg_loss=0.07354]\n",
      "Step 531099  [5.309 sec/step, loss=0.07426, avg_loss=0.07353]\n",
      "Step 531100  [5.336 sec/step, loss=0.07251, avg_loss=0.07352]\n",
      "Writing summary at step: 531100\n",
      "Step 531101  [5.309 sec/step, loss=0.07520, avg_loss=0.07352]\n",
      "Step 531102  [5.304 sec/step, loss=0.07480, avg_loss=0.07351]\n",
      "Step 531103  [5.307 sec/step, loss=0.07610, avg_loss=0.07352]\n",
      "Step 531104  [5.310 sec/step, loss=0.07503, avg_loss=0.07351]\n",
      "Step 531105  [5.324 sec/step, loss=0.07355, avg_loss=0.07359]\n",
      "Generated 32 batches of size 32 in 2.856 sec\n",
      "Step 531106  [5.314 sec/step, loss=0.06607, avg_loss=0.07351]\n",
      "Step 531107  [5.320 sec/step, loss=0.07576, avg_loss=0.07353]\n",
      "Step 531108  [5.327 sec/step, loss=0.07563, avg_loss=0.07355]\n",
      "Step 531109  [5.340 sec/step, loss=0.07465, avg_loss=0.07358]\n",
      "Step 531110  [5.355 sec/step, loss=0.07635, avg_loss=0.07361]\n",
      "Step 531111  [5.356 sec/step, loss=0.07555, avg_loss=0.07360]\n",
      "Step 531112  [5.340 sec/step, loss=0.07507, avg_loss=0.07361]\n",
      "Step 531113  [5.357 sec/step, loss=0.07540, avg_loss=0.07366]\n",
      "Step 531114  [5.360 sec/step, loss=0.07488, avg_loss=0.07367]\n",
      "Step 531115  [5.337 sec/step, loss=0.07501, avg_loss=0.07369]\n",
      "Step 531116  [5.353 sec/step, loss=0.07637, avg_loss=0.07371]\n",
      "Step 531117  [5.354 sec/step, loss=0.07048, avg_loss=0.07370]\n",
      "Step 531118  [5.357 sec/step, loss=0.07576, avg_loss=0.07371]\n",
      "Step 531119  [5.349 sec/step, loss=0.07298, avg_loss=0.07369]\n",
      "Step 531120  [5.368 sec/step, loss=0.07611, avg_loss=0.07371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 531121  [5.355 sec/step, loss=0.07212, avg_loss=0.07367]\n",
      "Step 531122  [5.355 sec/step, loss=0.07572, avg_loss=0.07369]\n",
      "Step 531123  [5.363 sec/step, loss=0.07484, avg_loss=0.07368]\n",
      "Step 531124  [5.418 sec/step, loss=0.06607, avg_loss=0.07362]\n",
      "Step 531125  [5.425 sec/step, loss=0.07360, avg_loss=0.07366]\n",
      "Step 531126  [5.405 sec/step, loss=0.07398, avg_loss=0.07367]\n",
      "Step 531127  [5.396 sec/step, loss=0.07253, avg_loss=0.07364]\n",
      "Step 531128  [5.344 sec/step, loss=0.07405, avg_loss=0.07372]\n",
      "Step 531129  [5.345 sec/step, loss=0.07220, avg_loss=0.07369]\n",
      "Step 531130  [5.360 sec/step, loss=0.07418, avg_loss=0.07368]\n",
      "Step 531131  [5.376 sec/step, loss=0.07511, avg_loss=0.07372]\n",
      "Step 531132  [5.357 sec/step, loss=0.07358, avg_loss=0.07371]\n",
      "Step 531133  [5.396 sec/step, loss=0.07401, avg_loss=0.07379]\n",
      "Step 531134  [5.380 sec/step, loss=0.06635, avg_loss=0.07371]\n",
      "Step 531135  [5.379 sec/step, loss=0.07400, avg_loss=0.07370]\n",
      "Step 531136  [5.375 sec/step, loss=0.07169, avg_loss=0.07367]\n",
      "Step 531137  [5.402 sec/step, loss=0.07428, avg_loss=0.07370]\n",
      "Generated 32 batches of size 32 in 2.355 sec\n",
      "Step 531138  [5.406 sec/step, loss=0.07587, avg_loss=0.07372]\n",
      "Step 531139  [5.413 sec/step, loss=0.07525, avg_loss=0.07374]\n",
      "Step 531140  [5.435 sec/step, loss=0.07290, avg_loss=0.07374]\n",
      "Step 531141  [5.429 sec/step, loss=0.07513, avg_loss=0.07373]\n",
      "Step 531142  [5.426 sec/step, loss=0.07608, avg_loss=0.07373]\n",
      "Step 531143  [5.382 sec/step, loss=0.07110, avg_loss=0.07372]\n",
      "Step 531144  [5.381 sec/step, loss=0.07489, avg_loss=0.07371]\n",
      "Step 531145  [5.362 sec/step, loss=0.07469, avg_loss=0.07371]\n",
      "Step 531146  [5.357 sec/step, loss=0.07506, avg_loss=0.07370]\n",
      "Step 531147  [5.364 sec/step, loss=0.07268, avg_loss=0.07367]\n",
      "Step 531148  [5.372 sec/step, loss=0.07550, avg_loss=0.07370]\n",
      "Step 531149  [5.437 sec/step, loss=0.06693, avg_loss=0.07366]\n",
      "Step 531150  [5.444 sec/step, loss=0.07435, avg_loss=0.07367]\n",
      "Step 531151  [5.436 sec/step, loss=0.07496, avg_loss=0.07366]\n",
      "Step 531152  [5.431 sec/step, loss=0.07504, avg_loss=0.07365]\n",
      "Step 531153  [5.431 sec/step, loss=0.07525, avg_loss=0.07366]\n",
      "Step 531154  [5.428 sec/step, loss=0.07236, avg_loss=0.07365]\n",
      "Step 531155  [5.437 sec/step, loss=0.07545, avg_loss=0.07366]\n",
      "Step 531156  [5.419 sec/step, loss=0.07221, avg_loss=0.07363]\n",
      "Step 531157  [5.418 sec/step, loss=0.07562, avg_loss=0.07363]\n",
      "Step 531158  [5.411 sec/step, loss=0.07322, avg_loss=0.07362]\n",
      "Step 531159  [5.407 sec/step, loss=0.07374, avg_loss=0.07360]\n",
      "Step 531160  [5.408 sec/step, loss=0.07595, avg_loss=0.07362]\n",
      "Step 531161  [5.412 sec/step, loss=0.07480, avg_loss=0.07362]\n",
      "Step 531162  [5.425 sec/step, loss=0.07446, avg_loss=0.07361]\n",
      "Step 531163  [5.419 sec/step, loss=0.07599, avg_loss=0.07363]\n",
      "Step 531164  [5.414 sec/step, loss=0.07546, avg_loss=0.07365]\n",
      "Step 531165  [5.385 sec/step, loss=0.07482, avg_loss=0.07367]\n",
      "Step 531166  [5.387 sec/step, loss=0.07371, avg_loss=0.07369]\n",
      "Step 531167  [5.367 sec/step, loss=0.07457, avg_loss=0.07368]\n",
      "Step 531168  [5.358 sec/step, loss=0.07114, avg_loss=0.07368]\n",
      "Step 531169  [5.360 sec/step, loss=0.07138, avg_loss=0.07366]\n",
      "Generated 32 batches of size 32 in 2.560 sec\n",
      "Step 531170  [5.361 sec/step, loss=0.07331, avg_loss=0.07364]\n",
      "Step 531171  [5.382 sec/step, loss=0.07554, avg_loss=0.07369]\n",
      "Step 531172  [5.368 sec/step, loss=0.07546, avg_loss=0.07368]\n",
      "Step 531173  [5.379 sec/step, loss=0.07558, avg_loss=0.07370]\n",
      "Step 531174  [5.372 sec/step, loss=0.06550, avg_loss=0.07364]\n",
      "Step 531175  [5.362 sec/step, loss=0.07443, avg_loss=0.07363]\n",
      "Step 531176  [5.322 sec/step, loss=0.07310, avg_loss=0.07370]\n",
      "Step 531177  [5.339 sec/step, loss=0.07529, avg_loss=0.07380]\n",
      "Step 531178  [5.338 sec/step, loss=0.07577, avg_loss=0.07380]\n",
      "Step 531179  [5.333 sec/step, loss=0.07106, avg_loss=0.07376]\n",
      "Step 531180  [5.373 sec/step, loss=0.07264, avg_loss=0.07376]\n",
      "Step 531181  [5.381 sec/step, loss=0.07411, avg_loss=0.07375]\n",
      "Step 531182  [5.380 sec/step, loss=0.07436, avg_loss=0.07376]\n",
      "Step 531183  [5.374 sec/step, loss=0.07131, avg_loss=0.07373]\n",
      "Step 531184  [5.381 sec/step, loss=0.07607, avg_loss=0.07374]\n",
      "Step 531185  [5.385 sec/step, loss=0.07181, avg_loss=0.07372]\n",
      "Step 531186  [5.406 sec/step, loss=0.07506, avg_loss=0.07376]\n",
      "Step 531187  [5.398 sec/step, loss=0.07255, avg_loss=0.07373]\n",
      "Step 531188  [5.376 sec/step, loss=0.07425, avg_loss=0.07371]\n",
      "Step 531189  [5.398 sec/step, loss=0.07498, avg_loss=0.07373]\n",
      "Step 531190  [5.417 sec/step, loss=0.07541, avg_loss=0.07378]\n",
      "Step 531191  [5.394 sec/step, loss=0.07014, avg_loss=0.07373]\n",
      "Step 531192  [5.354 sec/step, loss=0.07474, avg_loss=0.07377]\n",
      "Step 531193  [5.369 sec/step, loss=0.07560, avg_loss=0.07379]\n",
      "Step 531194  [5.357 sec/step, loss=0.07230, avg_loss=0.07376]\n",
      "Step 531195  [5.339 sec/step, loss=0.06564, avg_loss=0.07369]\n",
      "Step 531196  [5.348 sec/step, loss=0.07557, avg_loss=0.07373]\n",
      "Step 531197  [5.359 sec/step, loss=0.07533, avg_loss=0.07378]\n",
      "Step 531198  [5.357 sec/step, loss=0.07431, avg_loss=0.07378]\n",
      "Step 531199  [5.368 sec/step, loss=0.07577, avg_loss=0.07379]\n",
      "Step 531200  [5.392 sec/step, loss=0.06543, avg_loss=0.07372]\n",
      "Writing summary at step: 531200\n",
      "Generated 32 batches of size 32 in 2.550 sec\n",
      "Step 531201  [5.391 sec/step, loss=0.07342, avg_loss=0.07370]\n",
      "Step 531202  [5.383 sec/step, loss=0.07476, avg_loss=0.07370]\n",
      "Step 531203  [5.378 sec/step, loss=0.07473, avg_loss=0.07369]\n",
      "Step 531204  [5.384 sec/step, loss=0.07624, avg_loss=0.07370]\n",
      "Step 531205  [5.393 sec/step, loss=0.07620, avg_loss=0.07373]\n",
      "Step 531206  [5.407 sec/step, loss=0.07363, avg_loss=0.07380]\n",
      "Step 531207  [5.396 sec/step, loss=0.07460, avg_loss=0.07379]\n",
      "Step 531208  [5.389 sec/step, loss=0.07579, avg_loss=0.07379]\n",
      "Step 531209  [5.396 sec/step, loss=0.07313, avg_loss=0.07378]\n",
      "Step 531210  [5.383 sec/step, loss=0.07483, avg_loss=0.07376]\n",
      "Step 531211  [5.376 sec/step, loss=0.07451, avg_loss=0.07375]\n",
      "Step 531212  [5.369 sec/step, loss=0.07360, avg_loss=0.07374]\n",
      "Step 531213  [5.373 sec/step, loss=0.07586, avg_loss=0.07374]\n",
      "Step 531214  [5.349 sec/step, loss=0.06569, avg_loss=0.07365]\n",
      "Step 531215  [5.363 sec/step, loss=0.07590, avg_loss=0.07366]\n",
      "Step 531216  [5.347 sec/step, loss=0.07408, avg_loss=0.07364]\n",
      "Step 531217  [5.362 sec/step, loss=0.07333, avg_loss=0.07366]\n",
      "Step 531218  [5.361 sec/step, loss=0.07608, avg_loss=0.07367]\n",
      "Step 531219  [5.381 sec/step, loss=0.07323, avg_loss=0.07367]\n",
      "Step 531220  [5.373 sec/step, loss=0.07454, avg_loss=0.07365]\n",
      "Step 531221  [5.393 sec/step, loss=0.07529, avg_loss=0.07369]\n",
      "Step 531222  [5.401 sec/step, loss=0.07581, avg_loss=0.07369]\n",
      "Step 531223  [5.396 sec/step, loss=0.07408, avg_loss=0.07368]\n",
      "Step 531224  [5.338 sec/step, loss=0.07317, avg_loss=0.07375]\n",
      "Step 531225  [5.345 sec/step, loss=0.07498, avg_loss=0.07376]\n",
      "Step 531226  [5.354 sec/step, loss=0.07384, avg_loss=0.07376]\n",
      "Step 531227  [5.408 sec/step, loss=0.06623, avg_loss=0.07370]\n",
      "Step 531228  [5.414 sec/step, loss=0.07471, avg_loss=0.07371]\n",
      "Step 531229  [5.412 sec/step, loss=0.07528, avg_loss=0.07374]\n",
      "Step 531230  [5.396 sec/step, loss=0.07527, avg_loss=0.07375]\n",
      "Step 531231  [5.365 sec/step, loss=0.07191, avg_loss=0.07372]\n",
      "Step 531232  [5.399 sec/step, loss=0.07223, avg_loss=0.07370]\n",
      "Generated 32 batches of size 32 in 2.427 sec\n",
      "Step 531233  [5.390 sec/step, loss=0.07308, avg_loss=0.07369]\n",
      "Step 531234  [5.408 sec/step, loss=0.07495, avg_loss=0.07378]\n",
      "Step 531235  [5.403 sec/step, loss=0.07444, avg_loss=0.07378]\n",
      "Step 531236  [5.410 sec/step, loss=0.07465, avg_loss=0.07381]\n",
      "Step 531237  [5.378 sec/step, loss=0.07143, avg_loss=0.07378]\n",
      "Step 531238  [5.363 sec/step, loss=0.07187, avg_loss=0.07374]\n",
      "Step 531239  [5.379 sec/step, loss=0.07592, avg_loss=0.07375]\n",
      "Step 531240  [5.361 sec/step, loss=0.07406, avg_loss=0.07376]\n",
      "Step 531241  [5.352 sec/step, loss=0.07342, avg_loss=0.07375]\n",
      "Step 531242  [5.338 sec/step, loss=0.07571, avg_loss=0.07374]\n",
      "Step 531243  [5.361 sec/step, loss=0.07314, avg_loss=0.07376]\n",
      "Step 531244  [5.352 sec/step, loss=0.07561, avg_loss=0.07377]\n",
      "Step 531245  [5.354 sec/step, loss=0.07389, avg_loss=0.07376]\n",
      "Step 531246  [5.380 sec/step, loss=0.07573, avg_loss=0.07377]\n",
      "Step 531247  [5.373 sec/step, loss=0.07377, avg_loss=0.07378]\n",
      "Step 531248  [5.361 sec/step, loss=0.07184, avg_loss=0.07374]\n",
      "Step 531249  [5.316 sec/step, loss=0.07614, avg_loss=0.07384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 531250  [5.328 sec/step, loss=0.07405, avg_loss=0.07383]\n",
      "Step 531251  [5.329 sec/step, loss=0.07607, avg_loss=0.07384]\n",
      "Step 531252  [5.331 sec/step, loss=0.07527, avg_loss=0.07385]\n",
      "Step 531253  [5.339 sec/step, loss=0.07593, avg_loss=0.07385]\n",
      "Step 531254  [5.345 sec/step, loss=0.07283, avg_loss=0.07386]\n",
      "Step 531255  [5.330 sec/step, loss=0.07204, avg_loss=0.07382]\n",
      "Step 531256  [5.339 sec/step, loss=0.07391, avg_loss=0.07384]\n",
      "Step 531257  [5.324 sec/step, loss=0.07351, avg_loss=0.07382]\n",
      "Step 531258  [5.336 sec/step, loss=0.07674, avg_loss=0.07385]\n",
      "Step 531259  [5.337 sec/step, loss=0.07529, avg_loss=0.07387]\n",
      "Step 531260  [5.335 sec/step, loss=0.07508, avg_loss=0.07386]\n",
      "Step 531261  [5.343 sec/step, loss=0.07606, avg_loss=0.07387]\n",
      "Step 531262  [5.329 sec/step, loss=0.07485, avg_loss=0.07388]\n",
      "Step 531263  [5.368 sec/step, loss=0.06623, avg_loss=0.07378]\n",
      "Step 531264  [5.368 sec/step, loss=0.07474, avg_loss=0.07377]\n",
      "Generated 32 batches of size 32 in 2.715 sec\n",
      "Step 531265  [5.362 sec/step, loss=0.07188, avg_loss=0.07374]\n",
      "Step 531266  [5.377 sec/step, loss=0.07597, avg_loss=0.07377]\n",
      "Step 531267  [5.388 sec/step, loss=0.07447, avg_loss=0.07376]\n",
      "Step 531268  [5.388 sec/step, loss=0.07403, avg_loss=0.07379]\n",
      "Step 531269  [5.395 sec/step, loss=0.07164, avg_loss=0.07380]\n",
      "Step 531270  [5.408 sec/step, loss=0.07629, avg_loss=0.07383]\n",
      "Step 531271  [5.387 sec/step, loss=0.07051, avg_loss=0.07378]\n",
      "Step 531272  [5.375 sec/step, loss=0.06684, avg_loss=0.07369]\n",
      "Step 531273  [5.359 sec/step, loss=0.07150, avg_loss=0.07365]\n",
      "Step 531274  [5.426 sec/step, loss=0.06650, avg_loss=0.07366]\n",
      "Step 531275  [5.418 sec/step, loss=0.07527, avg_loss=0.07367]\n",
      "Step 531276  [5.427 sec/step, loss=0.07382, avg_loss=0.07367]\n",
      "Step 531277  [5.434 sec/step, loss=0.07488, avg_loss=0.07367]\n",
      "Step 531278  [5.447 sec/step, loss=0.07367, avg_loss=0.07365]\n",
      "Step 531279  [5.434 sec/step, loss=0.06494, avg_loss=0.07359]\n",
      "Step 531280  [5.397 sec/step, loss=0.07141, avg_loss=0.07358]\n",
      "Step 531281  [5.373 sec/step, loss=0.07138, avg_loss=0.07355]\n",
      "Step 531282  [5.382 sec/step, loss=0.07597, avg_loss=0.07356]\n",
      "Step 531283  [5.409 sec/step, loss=0.07587, avg_loss=0.07361]\n",
      "Step 531284  [5.393 sec/step, loss=0.07318, avg_loss=0.07358]\n",
      "Step 531285  [5.399 sec/step, loss=0.07419, avg_loss=0.07360]\n",
      "Step 531286  [5.401 sec/step, loss=0.07590, avg_loss=0.07361]\n",
      "Step 531287  [5.413 sec/step, loss=0.07590, avg_loss=0.07365]\n",
      "Step 531288  [5.435 sec/step, loss=0.07648, avg_loss=0.07367]\n",
      "Step 531289  [5.419 sec/step, loss=0.07586, avg_loss=0.07368]\n",
      "Step 531290  [5.399 sec/step, loss=0.07508, avg_loss=0.07367]\n",
      "Step 531291  [5.411 sec/step, loss=0.07465, avg_loss=0.07372]\n",
      "Step 531292  [5.404 sec/step, loss=0.07310, avg_loss=0.07370]\n",
      "Step 531293  [5.399 sec/step, loss=0.07272, avg_loss=0.07367]\n",
      "Step 531294  [5.415 sec/step, loss=0.07603, avg_loss=0.07371]\n",
      "Step 531295  [5.427 sec/step, loss=0.07338, avg_loss=0.07379]\n",
      "Step 531296  [5.419 sec/step, loss=0.07419, avg_loss=0.07378]\n",
      "Generated 32 batches of size 32 in 2.446 sec\n",
      "Step 531297  [5.428 sec/step, loss=0.07439, avg_loss=0.07377]\n",
      "Step 531298  [5.430 sec/step, loss=0.07503, avg_loss=0.07377]\n",
      "Step 531299  [5.430 sec/step, loss=0.07290, avg_loss=0.07374]\n",
      "Step 531300  [5.387 sec/step, loss=0.07437, avg_loss=0.07383]\n",
      "Writing summary at step: 531300\n",
      "Step 531301  [5.384 sec/step, loss=0.07395, avg_loss=0.07384]\n",
      "Step 531302  [5.387 sec/step, loss=0.07425, avg_loss=0.07383]\n",
      "Step 531303  [5.386 sec/step, loss=0.07435, avg_loss=0.07383]\n",
      "Step 531304  [5.369 sec/step, loss=0.07362, avg_loss=0.07380]\n",
      "Step 531305  [5.354 sec/step, loss=0.07089, avg_loss=0.07375]\n",
      "Step 531306  [5.333 sec/step, loss=0.06634, avg_loss=0.07368]\n",
      "Step 531307  [5.346 sec/step, loss=0.07337, avg_loss=0.07367]\n",
      "Step 531308  [5.353 sec/step, loss=0.07663, avg_loss=0.07367]\n",
      "Step 531309  [5.333 sec/step, loss=0.07185, avg_loss=0.07366]\n",
      "Step 531310  [5.354 sec/step, loss=0.07491, avg_loss=0.07366]\n",
      "Step 531311  [5.351 sec/step, loss=0.07465, avg_loss=0.07366]\n",
      "Step 531312  [5.385 sec/step, loss=0.07485, avg_loss=0.07368]\n",
      "Step 531313  [5.376 sec/step, loss=0.07527, avg_loss=0.07367]\n",
      "Step 531314  [5.392 sec/step, loss=0.07549, avg_loss=0.07377]\n",
      "Step 531315  [5.385 sec/step, loss=0.07536, avg_loss=0.07376]\n",
      "Step 531316  [5.387 sec/step, loss=0.07301, avg_loss=0.07375]\n",
      "Step 531317  [5.388 sec/step, loss=0.07596, avg_loss=0.07378]\n",
      "Step 531318  [5.431 sec/step, loss=0.06543, avg_loss=0.07367]\n",
      "Step 531319  [5.424 sec/step, loss=0.07146, avg_loss=0.07365]\n",
      "Step 531320  [5.426 sec/step, loss=0.07556, avg_loss=0.07366]\n",
      "Step 531321  [5.425 sec/step, loss=0.07531, avg_loss=0.07366]\n",
      "Step 531322  [5.417 sec/step, loss=0.07115, avg_loss=0.07362]\n",
      "Step 531323  [5.418 sec/step, loss=0.07415, avg_loss=0.07362]\n",
      "Step 531324  [5.424 sec/step, loss=0.07211, avg_loss=0.07361]\n",
      "Step 531325  [5.420 sec/step, loss=0.07449, avg_loss=0.07360]\n",
      "Step 531326  [5.410 sec/step, loss=0.07386, avg_loss=0.07360]\n",
      "Step 531327  [5.354 sec/step, loss=0.07427, avg_loss=0.07368]\n",
      "Generated 32 batches of size 32 in 2.391 sec\n",
      "Step 531328  [5.355 sec/step, loss=0.07296, avg_loss=0.07367]\n",
      "Step 531329  [5.345 sec/step, loss=0.07301, avg_loss=0.07364]\n",
      "Step 531330  [5.345 sec/step, loss=0.07324, avg_loss=0.07362]\n",
      "Step 531331  [5.354 sec/step, loss=0.07508, avg_loss=0.07366]\n",
      "Step 531332  [5.338 sec/step, loss=0.07412, avg_loss=0.07367]\n",
      "Step 531333  [5.333 sec/step, loss=0.07567, avg_loss=0.07370]\n",
      "Step 531334  [5.335 sec/step, loss=0.07449, avg_loss=0.07370]\n",
      "Step 531335  [5.346 sec/step, loss=0.07608, avg_loss=0.07371]\n",
      "Step 531336  [5.351 sec/step, loss=0.07672, avg_loss=0.07373]\n",
      "Step 531337  [5.369 sec/step, loss=0.07526, avg_loss=0.07377]\n",
      "Step 531338  [5.424 sec/step, loss=0.06747, avg_loss=0.07373]\n",
      "Step 531339  [5.399 sec/step, loss=0.07277, avg_loss=0.07370]\n",
      "Step 531340  [5.395 sec/step, loss=0.07091, avg_loss=0.07366]\n",
      "Step 531341  [5.412 sec/step, loss=0.07434, avg_loss=0.07367]\n",
      "Step 531342  [5.414 sec/step, loss=0.07437, avg_loss=0.07366]\n",
      "Step 531343  [5.402 sec/step, loss=0.07196, avg_loss=0.07365]\n",
      "Step 531344  [5.401 sec/step, loss=0.07229, avg_loss=0.07361]\n",
      "Step 531345  [5.391 sec/step, loss=0.07134, avg_loss=0.07359]\n",
      "Step 531346  [5.371 sec/step, loss=0.07423, avg_loss=0.07357]\n",
      "Step 531347  [5.379 sec/step, loss=0.07215, avg_loss=0.07356]\n",
      "Step 531348  [5.387 sec/step, loss=0.07470, avg_loss=0.07359]\n",
      "Step 531349  [5.390 sec/step, loss=0.07609, avg_loss=0.07359]\n",
      "Step 531350  [5.387 sec/step, loss=0.07599, avg_loss=0.07361]\n",
      "Step 531351  [5.394 sec/step, loss=0.07598, avg_loss=0.07360]\n",
      "Step 531352  [5.392 sec/step, loss=0.07564, avg_loss=0.07361]\n",
      "Step 531353  [5.395 sec/step, loss=0.07457, avg_loss=0.07359]\n",
      "Step 531354  [5.401 sec/step, loss=0.07494, avg_loss=0.07362]\n",
      "Step 531355  [5.385 sec/step, loss=0.07162, avg_loss=0.07361]\n",
      "Step 531356  [5.386 sec/step, loss=0.07320, avg_loss=0.07360]\n",
      "Step 531357  [5.376 sec/step, loss=0.06636, avg_loss=0.07353]\n",
      "Step 531358  [5.378 sec/step, loss=0.07402, avg_loss=0.07351]\n",
      "Step 531359  [5.386 sec/step, loss=0.07138, avg_loss=0.07347]\n",
      "Generated 32 batches of size 32 in 2.420 sec\n",
      "Step 531360  [5.396 sec/step, loss=0.07585, avg_loss=0.07347]\n",
      "Step 531361  [5.381 sec/step, loss=0.07439, avg_loss=0.07346]\n",
      "Step 531362  [5.394 sec/step, loss=0.07626, avg_loss=0.07347]\n",
      "Step 531363  [5.340 sec/step, loss=0.07562, avg_loss=0.07357]\n",
      "Step 531364  [5.331 sec/step, loss=0.07427, avg_loss=0.07356]\n",
      "Step 531365  [5.343 sec/step, loss=0.07490, avg_loss=0.07359]\n",
      "Step 531366  [5.324 sec/step, loss=0.07465, avg_loss=0.07358]\n",
      "Step 531367  [5.340 sec/step, loss=0.07427, avg_loss=0.07358]\n",
      "Step 531368  [5.348 sec/step, loss=0.07464, avg_loss=0.07358]\n",
      "Step 531369  [5.344 sec/step, loss=0.07452, avg_loss=0.07361]\n",
      "Step 531370  [5.337 sec/step, loss=0.07484, avg_loss=0.07360]\n",
      "Step 531371  [5.370 sec/step, loss=0.07290, avg_loss=0.07362]\n",
      "Step 531372  [5.396 sec/step, loss=0.07573, avg_loss=0.07371]\n",
      "Step 531373  [5.415 sec/step, loss=0.07596, avg_loss=0.07375]\n",
      "Step 531374  [5.361 sec/step, loss=0.07358, avg_loss=0.07382]\n",
      "Step 531375  [5.377 sec/step, loss=0.07475, avg_loss=0.07382]\n",
      "Step 531376  [5.353 sec/step, loss=0.07146, avg_loss=0.07380]\n",
      "Step 531377  [5.356 sec/step, loss=0.07566, avg_loss=0.07380]\n",
      "Step 531378  [5.330 sec/step, loss=0.07211, avg_loss=0.07379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 531379  [5.343 sec/step, loss=0.07470, avg_loss=0.07389]\n",
      "Step 531380  [5.354 sec/step, loss=0.07504, avg_loss=0.07392]\n",
      "Step 531381  [5.354 sec/step, loss=0.07121, avg_loss=0.07392]\n",
      "Step 531382  [5.330 sec/step, loss=0.06591, avg_loss=0.07382]\n",
      "Step 531383  [5.323 sec/step, loss=0.07590, avg_loss=0.07382]\n",
      "Step 531384  [5.381 sec/step, loss=0.06438, avg_loss=0.07373]\n",
      "Step 531385  [5.380 sec/step, loss=0.07506, avg_loss=0.07374]\n",
      "Step 531386  [5.380 sec/step, loss=0.07486, avg_loss=0.07373]\n",
      "Step 531387  [5.362 sec/step, loss=0.07341, avg_loss=0.07370]\n",
      "Step 531388  [5.355 sec/step, loss=0.07593, avg_loss=0.07370]\n",
      "Step 531389  [5.353 sec/step, loss=0.07475, avg_loss=0.07369]\n",
      "Step 531390  [5.354 sec/step, loss=0.07526, avg_loss=0.07369]\n",
      "Step 531391  [5.358 sec/step, loss=0.07519, avg_loss=0.07370]\n",
      "Generated 32 batches of size 32 in 2.575 sec\n",
      "Step 531392  [5.363 sec/step, loss=0.07393, avg_loss=0.07370]\n",
      "Step 531393  [5.381 sec/step, loss=0.07493, avg_loss=0.07373]\n",
      "Step 531394  [5.387 sec/step, loss=0.07455, avg_loss=0.07371]\n",
      "Step 531395  [5.396 sec/step, loss=0.07272, avg_loss=0.07370]\n",
      "Step 531396  [5.393 sec/step, loss=0.07552, avg_loss=0.07372]\n",
      "Step 531397  [5.377 sec/step, loss=0.07410, avg_loss=0.07371]\n",
      "Step 531398  [5.363 sec/step, loss=0.07010, avg_loss=0.07367]\n",
      "Step 531399  [5.359 sec/step, loss=0.07286, avg_loss=0.07367]\n",
      "Step 531400  [5.346 sec/step, loss=0.07338, avg_loss=0.07366]\n",
      "Writing summary at step: 531400\n",
      "Step 531401  [5.354 sec/step, loss=0.07542, avg_loss=0.07367]\n",
      "Step 531402  [5.364 sec/step, loss=0.07599, avg_loss=0.07369]\n",
      "Step 531403  [5.354 sec/step, loss=0.07372, avg_loss=0.07368]\n",
      "Step 531404  [5.368 sec/step, loss=0.07460, avg_loss=0.07369]\n",
      "Step 531405  [5.382 sec/step, loss=0.07405, avg_loss=0.07372]\n",
      "Step 531406  [5.427 sec/step, loss=0.07372, avg_loss=0.07380]\n",
      "Step 531407  [5.402 sec/step, loss=0.07114, avg_loss=0.07377]\n",
      "Step 531408  [5.389 sec/step, loss=0.07373, avg_loss=0.07374]\n",
      "Step 531409  [5.392 sec/step, loss=0.07404, avg_loss=0.07377]\n",
      "Step 531410  [5.374 sec/step, loss=0.07407, avg_loss=0.07376]\n",
      "Step 531411  [5.373 sec/step, loss=0.07613, avg_loss=0.07377]\n",
      "Step 531412  [5.363 sec/step, loss=0.07641, avg_loss=0.07379]\n",
      "Step 531413  [5.366 sec/step, loss=0.07498, avg_loss=0.07379]\n",
      "Step 531414  [5.376 sec/step, loss=0.07607, avg_loss=0.07379]\n",
      "Step 531415  [5.369 sec/step, loss=0.07190, avg_loss=0.07376]\n",
      "Step 531416  [5.384 sec/step, loss=0.07539, avg_loss=0.07378]\n",
      "Step 531417  [5.377 sec/step, loss=0.07450, avg_loss=0.07377]\n",
      "Step 531418  [5.332 sec/step, loss=0.07511, avg_loss=0.07386]\n",
      "Step 531419  [5.328 sec/step, loss=0.07498, avg_loss=0.07390]\n",
      "Step 531420  [5.345 sec/step, loss=0.07390, avg_loss=0.07388]\n",
      "Step 531421  [5.324 sec/step, loss=0.07263, avg_loss=0.07386]\n",
      "Step 531422  [5.322 sec/step, loss=0.07363, avg_loss=0.07388]\n",
      "Generated 32 batches of size 32 in 2.407 sec\n",
      "Step 531423  [5.340 sec/step, loss=0.07366, avg_loss=0.07387]\n",
      "Step 531424  [5.326 sec/step, loss=0.06685, avg_loss=0.07382]\n",
      "Step 531425  [5.316 sec/step, loss=0.07119, avg_loss=0.07379]\n",
      "Step 531426  [5.338 sec/step, loss=0.07605, avg_loss=0.07381]\n",
      "Step 531427  [5.342 sec/step, loss=0.07527, avg_loss=0.07382]\n",
      "Step 531428  [5.335 sec/step, loss=0.07530, avg_loss=0.07384]\n",
      "Step 531429  [5.344 sec/step, loss=0.07495, avg_loss=0.07386]\n",
      "Step 531430  [5.399 sec/step, loss=0.06683, avg_loss=0.07380]\n",
      "Step 531431  [5.414 sec/step, loss=0.07353, avg_loss=0.07378]\n",
      "Step 531432  [5.411 sec/step, loss=0.07641, avg_loss=0.07381]\n",
      "Step 531433  [5.420 sec/step, loss=0.07520, avg_loss=0.07380]\n",
      "Step 531434  [5.413 sec/step, loss=0.06985, avg_loss=0.07376]\n",
      "Step 531435  [5.403 sec/step, loss=0.07506, avg_loss=0.07375]\n",
      "Step 531436  [5.387 sec/step, loss=0.07348, avg_loss=0.07371]\n",
      "Step 531437  [5.376 sec/step, loss=0.07500, avg_loss=0.07371]\n",
      "Step 531438  [5.340 sec/step, loss=0.07548, avg_loss=0.07379]\n",
      "Step 531439  [5.347 sec/step, loss=0.07446, avg_loss=0.07381]\n",
      "Step 531440  [5.383 sec/step, loss=0.07198, avg_loss=0.07382]\n",
      "Step 531441  [5.368 sec/step, loss=0.07389, avg_loss=0.07381]\n",
      "Step 531442  [5.375 sec/step, loss=0.07440, avg_loss=0.07381]\n",
      "Step 531443  [5.386 sec/step, loss=0.07585, avg_loss=0.07385]\n",
      "Step 531444  [5.395 sec/step, loss=0.07420, avg_loss=0.07387]\n",
      "Step 531445  [5.412 sec/step, loss=0.07174, avg_loss=0.07388]\n",
      "Step 531446  [5.398 sec/step, loss=0.07177, avg_loss=0.07385]\n",
      "Step 531447  [5.382 sec/step, loss=0.07595, avg_loss=0.07389]\n",
      "Step 531448  [5.382 sec/step, loss=0.07444, avg_loss=0.07389]\n",
      "Step 531449  [5.358 sec/step, loss=0.06602, avg_loss=0.07379]\n",
      "Step 531450  [5.362 sec/step, loss=0.07627, avg_loss=0.07379]\n",
      "Step 531451  [5.352 sec/step, loss=0.07469, avg_loss=0.07378]\n",
      "Step 531452  [5.351 sec/step, loss=0.07511, avg_loss=0.07377]\n",
      "Step 531453  [5.334 sec/step, loss=0.07168, avg_loss=0.07374]\n",
      "Step 531454  [5.326 sec/step, loss=0.07112, avg_loss=0.07370]\n",
      "Generated 32 batches of size 32 in 2.391 sec\n",
      "Step 531455  [5.355 sec/step, loss=0.07456, avg_loss=0.07373]\n",
      "Step 531456  [5.402 sec/step, loss=0.06847, avg_loss=0.07369]\n",
      "Step 531457  [5.412 sec/step, loss=0.07387, avg_loss=0.07376]\n",
      "Step 531458  [5.407 sec/step, loss=0.07553, avg_loss=0.07378]\n",
      "Step 531459  [5.410 sec/step, loss=0.07527, avg_loss=0.07382]\n",
      "Step 531460  [5.392 sec/step, loss=0.07275, avg_loss=0.07378]\n",
      "Step 531461  [5.413 sec/step, loss=0.07327, avg_loss=0.07377]\n",
      "Step 531462  [5.404 sec/step, loss=0.07266, avg_loss=0.07374]\n",
      "Step 531463  [5.394 sec/step, loss=0.07252, avg_loss=0.07371]\n",
      "Step 531464  [5.411 sec/step, loss=0.07534, avg_loss=0.07372]\n",
      "Step 531465  [5.412 sec/step, loss=0.07471, avg_loss=0.07372]\n",
      "Step 531466  [5.428 sec/step, loss=0.07631, avg_loss=0.07373]\n",
      "Step 531467  [5.397 sec/step, loss=0.07001, avg_loss=0.07369]\n",
      "Step 531468  [5.398 sec/step, loss=0.07494, avg_loss=0.07369]\n",
      "Step 531469  [5.392 sec/step, loss=0.07350, avg_loss=0.07368]\n",
      "Step 531470  [5.381 sec/step, loss=0.07283, avg_loss=0.07366]\n",
      "Step 531471  [5.364 sec/step, loss=0.07423, avg_loss=0.07368]\n",
      "Step 531472  [5.355 sec/step, loss=0.07453, avg_loss=0.07366]\n",
      "Step 531473  [5.343 sec/step, loss=0.07490, avg_loss=0.07365]\n",
      "Step 531474  [5.373 sec/step, loss=0.07367, avg_loss=0.07365]\n",
      "Step 531475  [5.348 sec/step, loss=0.07059, avg_loss=0.07361]\n",
      "Step 531476  [5.357 sec/step, loss=0.07500, avg_loss=0.07365]\n",
      "Step 531477  [5.350 sec/step, loss=0.07427, avg_loss=0.07363]\n",
      "Step 531478  [5.341 sec/step, loss=0.07432, avg_loss=0.07366]\n",
      "Step 531479  [5.352 sec/step, loss=0.07592, avg_loss=0.07367]\n",
      "Step 531480  [5.358 sec/step, loss=0.07565, avg_loss=0.07367]\n",
      "Step 531481  [5.368 sec/step, loss=0.07339, avg_loss=0.07370]\n",
      "Step 531482  [5.401 sec/step, loss=0.07372, avg_loss=0.07377]\n",
      "Step 531483  [5.419 sec/step, loss=0.07436, avg_loss=0.07376]\n",
      "Step 531484  [5.382 sec/step, loss=0.07331, avg_loss=0.07385]\n",
      "Step 531485  [5.393 sec/step, loss=0.07376, avg_loss=0.07383]\n",
      "Step 531486  [5.377 sec/step, loss=0.07185, avg_loss=0.07380]\n",
      "Generated 32 batches of size 32 in 2.399 sec\n",
      "Step 531487  [5.392 sec/step, loss=0.07568, avg_loss=0.07383]\n",
      "Step 531488  [5.369 sec/step, loss=0.06457, avg_loss=0.07371]\n",
      "Step 531489  [5.369 sec/step, loss=0.07388, avg_loss=0.07370]\n",
      "Step 531490  [5.381 sec/step, loss=0.07626, avg_loss=0.07371]\n",
      "Step 531491  [5.377 sec/step, loss=0.07513, avg_loss=0.07371]\n",
      "Step 531492  [5.378 sec/step, loss=0.07322, avg_loss=0.07371]\n",
      "Step 531493  [5.357 sec/step, loss=0.07382, avg_loss=0.07370]\n",
      "Step 531494  [5.340 sec/step, loss=0.07500, avg_loss=0.07370]\n",
      "Step 531495  [5.384 sec/step, loss=0.06739, avg_loss=0.07365]\n",
      "Step 531496  [5.382 sec/step, loss=0.07137, avg_loss=0.07361]\n",
      "Step 531497  [5.396 sec/step, loss=0.07444, avg_loss=0.07361]\n",
      "Step 531498  [5.413 sec/step, loss=0.07236, avg_loss=0.07363]\n",
      "Step 531499  [5.401 sec/step, loss=0.07360, avg_loss=0.07364]\n",
      "Step 531500  [5.419 sec/step, loss=0.07623, avg_loss=0.07367]\n",
      "Writing summary at step: 531500\n",
      "Step 531501  [5.414 sec/step, loss=0.07490, avg_loss=0.07366]\n",
      "Step 531502  [5.403 sec/step, loss=0.07284, avg_loss=0.07363]\n",
      "Step 531503  [5.403 sec/step, loss=0.07341, avg_loss=0.07363]\n",
      "Step 531504  [5.449 sec/step, loss=0.06623, avg_loss=0.07354]\n",
      "Step 531505  [5.451 sec/step, loss=0.07482, avg_loss=0.07355]\n",
      "Step 531506  [5.406 sec/step, loss=0.06576, avg_loss=0.07347]\n",
      "Step 531507  [5.409 sec/step, loss=0.07414, avg_loss=0.07350]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 531508  [5.416 sec/step, loss=0.07533, avg_loss=0.07352]\n",
      "Step 531509  [5.424 sec/step, loss=0.07260, avg_loss=0.07350]\n",
      "Step 531510  [5.425 sec/step, loss=0.07430, avg_loss=0.07351]\n",
      "Step 531511  [5.428 sec/step, loss=0.07620, avg_loss=0.07351]\n",
      "Step 531512  [5.416 sec/step, loss=0.07483, avg_loss=0.07349]\n",
      "Step 531513  [5.396 sec/step, loss=0.07189, avg_loss=0.07346]\n",
      "Step 531514  [5.402 sec/step, loss=0.07519, avg_loss=0.07345]\n",
      "Step 531515  [5.430 sec/step, loss=0.07520, avg_loss=0.07348]\n",
      "Step 531516  [5.429 sec/step, loss=0.07542, avg_loss=0.07348]\n",
      "Step 531517  [5.423 sec/step, loss=0.07404, avg_loss=0.07348]\n",
      "Generated 32 batches of size 32 in 2.386 sec\n",
      "Step 531518  [5.430 sec/step, loss=0.07659, avg_loss=0.07349]\n",
      "Step 531519  [5.441 sec/step, loss=0.07420, avg_loss=0.07349]\n",
      "Step 531520  [5.424 sec/step, loss=0.07290, avg_loss=0.07348]\n",
      "Step 531521  [5.445 sec/step, loss=0.07614, avg_loss=0.07351]\n",
      "Step 531522  [5.448 sec/step, loss=0.07392, avg_loss=0.07351]\n",
      "Step 531523  [5.433 sec/step, loss=0.07507, avg_loss=0.07353]\n",
      "Step 531524  [5.464 sec/step, loss=0.07551, avg_loss=0.07362]\n",
      "Step 531525  [5.465 sec/step, loss=0.07140, avg_loss=0.07362]\n",
      "Step 531526  [5.463 sec/step, loss=0.07625, avg_loss=0.07362]\n",
      "Step 531527  [5.468 sec/step, loss=0.07424, avg_loss=0.07361]\n",
      "Step 531528  [5.468 sec/step, loss=0.07507, avg_loss=0.07361]\n",
      "Step 531529  [5.473 sec/step, loss=0.07234, avg_loss=0.07358]\n",
      "Step 531530  [5.442 sec/step, loss=0.07545, avg_loss=0.07367]\n",
      "Step 531531  [5.428 sec/step, loss=0.07030, avg_loss=0.07364]\n",
      "Step 531532  [5.431 sec/step, loss=0.07450, avg_loss=0.07362]\n",
      "Step 531533  [5.409 sec/step, loss=0.07256, avg_loss=0.07359]\n",
      "Step 531534  [5.399 sec/step, loss=0.07164, avg_loss=0.07361]\n",
      "Step 531535  [5.407 sec/step, loss=0.07635, avg_loss=0.07362]\n",
      "Step 531536  [5.411 sec/step, loss=0.07393, avg_loss=0.07362]\n",
      "Step 531537  [5.425 sec/step, loss=0.07633, avg_loss=0.07364]\n",
      "Step 531538  [5.420 sec/step, loss=0.07563, avg_loss=0.07364]\n",
      "Step 531539  [5.422 sec/step, loss=0.07575, avg_loss=0.07365]\n",
      "Step 531540  [5.401 sec/step, loss=0.07315, avg_loss=0.07366]\n",
      "Step 531541  [5.393 sec/step, loss=0.07373, avg_loss=0.07366]\n",
      "Step 531542  [5.416 sec/step, loss=0.07312, avg_loss=0.07365]\n",
      "Step 531543  [5.412 sec/step, loss=0.07483, avg_loss=0.07364]\n",
      "Step 531544  [5.412 sec/step, loss=0.07335, avg_loss=0.07363]\n",
      "Step 531545  [5.424 sec/step, loss=0.07514, avg_loss=0.07367]\n",
      "Step 531546  [5.440 sec/step, loss=0.07483, avg_loss=0.07370]\n",
      "Step 531547  [5.412 sec/step, loss=0.06714, avg_loss=0.07361]\n",
      "Step 531548  [5.409 sec/step, loss=0.07440, avg_loss=0.07361]\n",
      "Step 531549  [5.418 sec/step, loss=0.07390, avg_loss=0.07369]\n",
      "Generated 32 batches of size 32 in 2.524 sec\n",
      "Step 531550  [5.399 sec/step, loss=0.07147, avg_loss=0.07364]\n",
      "Step 531551  [5.400 sec/step, loss=0.07591, avg_loss=0.07365]\n",
      "Step 531552  [5.407 sec/step, loss=0.07570, avg_loss=0.07366]\n",
      "Step 531553  [5.460 sec/step, loss=0.06559, avg_loss=0.07360]\n",
      "Step 531554  [5.464 sec/step, loss=0.07532, avg_loss=0.07364]\n",
      "Step 531555  [5.443 sec/step, loss=0.07382, avg_loss=0.07363]\n",
      "Step 531556  [5.400 sec/step, loss=0.07243, avg_loss=0.07367]\n",
      "Step 531557  [5.405 sec/step, loss=0.07398, avg_loss=0.07367]\n",
      "Step 531558  [5.418 sec/step, loss=0.07532, avg_loss=0.07367]\n",
      "Step 531559  [5.403 sec/step, loss=0.07391, avg_loss=0.07365]\n",
      "Step 531560  [5.411 sec/step, loss=0.07415, avg_loss=0.07367]\n",
      "Step 531561  [5.409 sec/step, loss=0.07603, avg_loss=0.07370]\n",
      "Step 531562  [5.404 sec/step, loss=0.07354, avg_loss=0.07371]\n",
      "Step 531563  [5.418 sec/step, loss=0.07550, avg_loss=0.07373]\n",
      "Step 531564  [5.407 sec/step, loss=0.07511, avg_loss=0.07373]\n",
      "Step 531565  [5.406 sec/step, loss=0.07528, avg_loss=0.07374]\n",
      "Step 531566  [5.412 sec/step, loss=0.07516, avg_loss=0.07373]\n",
      "Step 531567  [5.431 sec/step, loss=0.07598, avg_loss=0.07379]\n",
      "Step 531568  [5.429 sec/step, loss=0.07492, avg_loss=0.07379]\n",
      "Step 531569  [5.450 sec/step, loss=0.07539, avg_loss=0.07381]\n",
      "Step 531570  [5.454 sec/step, loss=0.07488, avg_loss=0.07383]\n",
      "Step 531571  [5.458 sec/step, loss=0.07371, avg_loss=0.07382]\n",
      "Step 531572  [5.454 sec/step, loss=0.07327, avg_loss=0.07381]\n",
      "Step 531573  [5.462 sec/step, loss=0.07495, avg_loss=0.07381]\n",
      "Step 531574  [5.449 sec/step, loss=0.07582, avg_loss=0.07383]\n",
      "Step 531575  [5.459 sec/step, loss=0.06998, avg_loss=0.07382]\n",
      "Step 531576  [5.456 sec/step, loss=0.07200, avg_loss=0.07379]\n",
      "Step 531577  [5.442 sec/step, loss=0.07169, avg_loss=0.07377]\n",
      "Step 531578  [5.503 sec/step, loss=0.06589, avg_loss=0.07368]\n",
      "Step 531579  [5.503 sec/step, loss=0.07551, avg_loss=0.07368]\n",
      "Step 531580  [5.497 sec/step, loss=0.07446, avg_loss=0.07367]\n",
      "Step 531581  [5.502 sec/step, loss=0.07405, avg_loss=0.07367]\n",
      "Generated 32 batches of size 32 in 2.926 sec\n",
      "Step 531582  [5.474 sec/step, loss=0.06689, avg_loss=0.07361]\n",
      "Step 531583  [5.451 sec/step, loss=0.07557, avg_loss=0.07362]\n",
      "Step 531584  [5.449 sec/step, loss=0.07592, avg_loss=0.07364]\n",
      "Step 531585  [5.446 sec/step, loss=0.07657, avg_loss=0.07367]\n",
      "Step 531586  [5.457 sec/step, loss=0.07611, avg_loss=0.07372]\n",
      "Step 531587  [5.475 sec/step, loss=0.07510, avg_loss=0.07371]\n",
      "Step 531588  [5.479 sec/step, loss=0.07256, avg_loss=0.07379]\n",
      "Step 531589  [5.471 sec/step, loss=0.07133, avg_loss=0.07376]\n",
      "Step 531590  [5.458 sec/step, loss=0.07368, avg_loss=0.07374]\n",
      "Step 531591  [5.457 sec/step, loss=0.07367, avg_loss=0.07372]\n",
      "Step 531592  [5.475 sec/step, loss=0.07429, avg_loss=0.07373]\n",
      "Step 531593  [5.484 sec/step, loss=0.07455, avg_loss=0.07374]\n",
      "Step 531594  [5.477 sec/step, loss=0.07397, avg_loss=0.07373]\n",
      "Step 531595  [5.423 sec/step, loss=0.07370, avg_loss=0.07379]\n",
      "Step 531596  [5.428 sec/step, loss=0.07451, avg_loss=0.07383]\n",
      "Step 531597  [5.473 sec/step, loss=0.06667, avg_loss=0.07375]\n",
      "Step 531598  [5.481 sec/step, loss=0.07417, avg_loss=0.07377]\n",
      "Step 531599  [5.499 sec/step, loss=0.07642, avg_loss=0.07379]\n",
      "Step 531600  [5.487 sec/step, loss=0.07084, avg_loss=0.07374]\n",
      "Writing summary at step: 531600\n",
      "Step 531601  [5.500 sec/step, loss=0.07397, avg_loss=0.07373]\n",
      "Step 531602  [5.510 sec/step, loss=0.07601, avg_loss=0.07376]\n",
      "Step 531603  [5.542 sec/step, loss=0.07339, avg_loss=0.07376]\n",
      "Step 531604  [5.489 sec/step, loss=0.07517, avg_loss=0.07385]\n",
      "Step 531605  [5.496 sec/step, loss=0.07514, avg_loss=0.07385]\n",
      "Step 531606  [5.521 sec/step, loss=0.07590, avg_loss=0.07396]\n",
      "Step 531607  [5.521 sec/step, loss=0.07056, avg_loss=0.07392]\n",
      "Step 531608  [5.521 sec/step, loss=0.07457, avg_loss=0.07391]\n",
      "Step 531609  [5.533 sec/step, loss=0.07570, avg_loss=0.07394]\n",
      "Step 531610  [5.549 sec/step, loss=0.07645, avg_loss=0.07397]\n",
      "Step 531611  [5.543 sec/step, loss=0.07522, avg_loss=0.07396]\n",
      "Step 531612  [5.543 sec/step, loss=0.07436, avg_loss=0.07395]\n",
      "Generated 32 batches of size 32 in 2.800 sec\n",
      "Step 531613  [5.544 sec/step, loss=0.06462, avg_loss=0.07388]\n",
      "Step 531614  [5.530 sec/step, loss=0.07466, avg_loss=0.07387]\n",
      "Step 531615  [5.507 sec/step, loss=0.07350, avg_loss=0.07386]\n",
      "Step 531616  [5.504 sec/step, loss=0.07242, avg_loss=0.07383]\n",
      "Step 531617  [5.510 sec/step, loss=0.07489, avg_loss=0.07383]\n",
      "Step 531618  [5.490 sec/step, loss=0.07394, avg_loss=0.07381]\n",
      "Step 531619  [5.465 sec/step, loss=0.07075, avg_loss=0.07377]\n",
      "Step 531620  [5.474 sec/step, loss=0.07644, avg_loss=0.07381]\n",
      "Step 531621  [5.453 sec/step, loss=0.07242, avg_loss=0.07377]\n",
      "Step 531622  [5.452 sec/step, loss=0.07488, avg_loss=0.07378]\n",
      "Step 531623  [5.455 sec/step, loss=0.07495, avg_loss=0.07378]\n",
      "Step 531624  [5.448 sec/step, loss=0.07568, avg_loss=0.07378]\n",
      "Step 531625  [5.468 sec/step, loss=0.07359, avg_loss=0.07380]\n",
      "Step 531626  [5.460 sec/step, loss=0.07374, avg_loss=0.07378]\n",
      "Step 531627  [5.442 sec/step, loss=0.06533, avg_loss=0.07369]\n",
      "Step 531628  [5.438 sec/step, loss=0.07407, avg_loss=0.07368]\n",
      "Step 531629  [5.483 sec/step, loss=0.06630, avg_loss=0.07362]\n",
      "Step 531630  [5.460 sec/step, loss=0.07196, avg_loss=0.07358]\n",
      "Step 531631  [5.468 sec/step, loss=0.07431, avg_loss=0.07362]\n",
      "Step 531632  [5.464 sec/step, loss=0.07347, avg_loss=0.07361]\n",
      "Step 531633  [5.466 sec/step, loss=0.07463, avg_loss=0.07363]\n",
      "Step 531634  [5.482 sec/step, loss=0.07461, avg_loss=0.07366]\n",
      "Step 531635  [5.482 sec/step, loss=0.07610, avg_loss=0.07366]\n",
      "Step 531636  [5.482 sec/step, loss=0.07415, avg_loss=0.07366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 531637  [5.460 sec/step, loss=0.07423, avg_loss=0.07364]\n",
      "Step 531638  [5.448 sec/step, loss=0.07305, avg_loss=0.07362]\n",
      "Step 531639  [5.474 sec/step, loss=0.07274, avg_loss=0.07359]\n",
      "Step 531640  [5.467 sec/step, loss=0.07492, avg_loss=0.07360]\n",
      "Step 531641  [5.482 sec/step, loss=0.07330, avg_loss=0.07360]\n",
      "Step 531642  [5.473 sec/step, loss=0.07479, avg_loss=0.07362]\n",
      "Step 531643  [5.460 sec/step, loss=0.07289, avg_loss=0.07360]\n",
      "Step 531644  [5.437 sec/step, loss=0.07106, avg_loss=0.07357]\n",
      "Generated 32 batches of size 32 in 2.404 sec\n",
      "Step 531645  [5.437 sec/step, loss=0.07646, avg_loss=0.07359]\n",
      "Step 531646  [5.435 sec/step, loss=0.07553, avg_loss=0.07360]\n",
      "Step 531647  [5.440 sec/step, loss=0.07054, avg_loss=0.07363]\n",
      "Step 531648  [5.442 sec/step, loss=0.07209, avg_loss=0.07361]\n",
      "Step 531649  [5.462 sec/step, loss=0.07517, avg_loss=0.07362]\n",
      "Step 531650  [5.479 sec/step, loss=0.07516, avg_loss=0.07366]\n",
      "Step 531651  [5.487 sec/step, loss=0.07628, avg_loss=0.07366]\n",
      "Step 531652  [5.475 sec/step, loss=0.07341, avg_loss=0.07364]\n",
      "Step 531653  [5.439 sec/step, loss=0.07473, avg_loss=0.07373]\n",
      "Step 531654  [5.452 sec/step, loss=0.07364, avg_loss=0.07371]\n",
      "Step 531655  [5.451 sec/step, loss=0.07324, avg_loss=0.07371]\n",
      "Step 531656  [5.460 sec/step, loss=0.07588, avg_loss=0.07374]\n",
      "Step 531657  [5.466 sec/step, loss=0.07403, avg_loss=0.07374]\n",
      "Step 531658  [5.454 sec/step, loss=0.07492, avg_loss=0.07374]\n",
      "Step 531659  [5.473 sec/step, loss=0.07587, avg_loss=0.07376]\n",
      "Step 531660  [5.469 sec/step, loss=0.07565, avg_loss=0.07377]\n",
      "Step 531661  [5.460 sec/step, loss=0.07431, avg_loss=0.07375]\n",
      "Step 531662  [5.476 sec/step, loss=0.07558, avg_loss=0.07377]\n",
      "Step 531663  [5.489 sec/step, loss=0.07628, avg_loss=0.07378]\n",
      "Step 531664  [5.471 sec/step, loss=0.06661, avg_loss=0.07370]\n",
      "Step 531665  [5.464 sec/step, loss=0.07375, avg_loss=0.07368]\n",
      "Step 531666  [5.456 sec/step, loss=0.07473, avg_loss=0.07368]\n",
      "Step 531667  [5.437 sec/step, loss=0.07380, avg_loss=0.07366]\n",
      "Step 531668  [5.437 sec/step, loss=0.07528, avg_loss=0.07366]\n",
      "Step 531669  [5.423 sec/step, loss=0.07471, avg_loss=0.07365]\n",
      "Step 531670  [5.428 sec/step, loss=0.07531, avg_loss=0.07366]\n",
      "Step 531671  [5.414 sec/step, loss=0.07370, avg_loss=0.07366]\n",
      "Step 531672  [5.425 sec/step, loss=0.07410, avg_loss=0.07366]\n",
      "Step 531673  [5.414 sec/step, loss=0.07323, avg_loss=0.07365]\n",
      "Step 531674  [5.410 sec/step, loss=0.07616, avg_loss=0.07365]\n",
      "Step 531675  [5.464 sec/step, loss=0.06562, avg_loss=0.07361]\n",
      "Step 531676  [5.453 sec/step, loss=0.07151, avg_loss=0.07360]\n",
      "Generated 32 batches of size 32 in 2.517 sec\n",
      "Step 531677  [5.469 sec/step, loss=0.07375, avg_loss=0.07362]\n",
      "Step 531678  [5.419 sec/step, loss=0.07277, avg_loss=0.07369]\n",
      "Step 531679  [5.426 sec/step, loss=0.07520, avg_loss=0.07369]\n",
      "Step 531680  [5.411 sec/step, loss=0.07285, avg_loss=0.07367]\n",
      "Step 531681  [5.407 sec/step, loss=0.07478, avg_loss=0.07368]\n",
      "Step 531682  [5.425 sec/step, loss=0.07432, avg_loss=0.07375]\n",
      "Step 531683  [5.442 sec/step, loss=0.07582, avg_loss=0.07376]\n",
      "Step 531684  [5.428 sec/step, loss=0.07079, avg_loss=0.07371]\n",
      "Step 531685  [5.446 sec/step, loss=0.07302, avg_loss=0.07367]\n",
      "Step 531686  [5.489 sec/step, loss=0.06559, avg_loss=0.07356]\n",
      "Step 531687  [5.459 sec/step, loss=0.06962, avg_loss=0.07351]\n",
      "Step 531688  [5.471 sec/step, loss=0.07383, avg_loss=0.07352]\n",
      "Step 531689  [5.477 sec/step, loss=0.07358, avg_loss=0.07355]\n",
      "Step 531690  [5.479 sec/step, loss=0.07509, avg_loss=0.07356]\n",
      "Step 531691  [5.475 sec/step, loss=0.07373, avg_loss=0.07356]\n",
      "Step 531692  [5.468 sec/step, loss=0.07342, avg_loss=0.07355]\n",
      "Step 531693  [5.476 sec/step, loss=0.07600, avg_loss=0.07357]\n",
      "Step 531694  [5.513 sec/step, loss=0.07331, avg_loss=0.07356]\n",
      "Step 531695  [5.512 sec/step, loss=0.07083, avg_loss=0.07353]\n",
      "Step 531696  [5.500 sec/step, loss=0.07382, avg_loss=0.07352]\n",
      "Step 531697  [5.451 sec/step, loss=0.07529, avg_loss=0.07361]\n",
      "Step 531698  [5.444 sec/step, loss=0.07544, avg_loss=0.07362]\n",
      "Step 531699  [5.432 sec/step, loss=0.07532, avg_loss=0.07361]\n",
      "Step 531700  [5.437 sec/step, loss=0.07245, avg_loss=0.07363]\n",
      "Writing summary at step: 531700\n",
      "Step 531701  [5.436 sec/step, loss=0.07500, avg_loss=0.07364]\n",
      "Step 531702  [5.433 sec/step, loss=0.07485, avg_loss=0.07363]\n",
      "Step 531703  [5.408 sec/step, loss=0.07239, avg_loss=0.07362]\n",
      "Step 531704  [5.397 sec/step, loss=0.07114, avg_loss=0.07358]\n",
      "Step 531705  [5.393 sec/step, loss=0.07566, avg_loss=0.07358]\n",
      "Step 531706  [5.376 sec/step, loss=0.07115, avg_loss=0.07353]\n",
      "Step 531707  [5.399 sec/step, loss=0.07336, avg_loss=0.07356]\n",
      "Generated 32 batches of size 32 in 2.407 sec\n",
      "Step 531708  [5.404 sec/step, loss=0.07316, avg_loss=0.07355]\n",
      "Step 531709  [5.391 sec/step, loss=0.07314, avg_loss=0.07352]\n",
      "Step 531710  [5.388 sec/step, loss=0.07641, avg_loss=0.07352]\n",
      "Step 531711  [5.403 sec/step, loss=0.07234, avg_loss=0.07349]\n",
      "Step 531712  [5.386 sec/step, loss=0.06620, avg_loss=0.07341]\n",
      "Step 531713  [5.404 sec/step, loss=0.07469, avg_loss=0.07351]\n",
      "Step 531714  [5.416 sec/step, loss=0.07570, avg_loss=0.07352]\n",
      "Step 531715  [5.424 sec/step, loss=0.07624, avg_loss=0.07355]\n",
      "Step 531716  [5.420 sec/step, loss=0.07482, avg_loss=0.07357]\n",
      "Step 531717  [5.409 sec/step, loss=0.07057, avg_loss=0.07353]\n",
      "Step 531718  [5.416 sec/step, loss=0.07310, avg_loss=0.07352]\n",
      "Step 531719  [5.432 sec/step, loss=0.07173, avg_loss=0.07353]\n",
      "Step 531720  [5.429 sec/step, loss=0.07574, avg_loss=0.07352]\n",
      "Step 531721  [5.431 sec/step, loss=0.07161, avg_loss=0.07352]\n",
      "Step 531722  [5.433 sec/step, loss=0.07514, avg_loss=0.07352]\n",
      "Step 531723  [5.412 sec/step, loss=0.06522, avg_loss=0.07342]\n",
      "Step 531724  [5.398 sec/step, loss=0.07355, avg_loss=0.07340]\n",
      "Step 531725  [5.395 sec/step, loss=0.07482, avg_loss=0.07341]\n",
      "Step 531726  [5.401 sec/step, loss=0.07567, avg_loss=0.07343]\n",
      "Step 531727  [5.423 sec/step, loss=0.07348, avg_loss=0.07351]\n",
      "Step 531728  [5.446 sec/step, loss=0.07581, avg_loss=0.07353]\n",
      "Step 531729  [5.396 sec/step, loss=0.07381, avg_loss=0.07361]\n",
      "Step 531730  [5.418 sec/step, loss=0.07543, avg_loss=0.07364]\n",
      "Step 531731  [5.417 sec/step, loss=0.07435, avg_loss=0.07364]\n",
      "Step 531732  [5.399 sec/step, loss=0.06926, avg_loss=0.07360]\n",
      "Step 531733  [5.400 sec/step, loss=0.07478, avg_loss=0.07360]\n",
      "Step 531734  [5.409 sec/step, loss=0.07595, avg_loss=0.07361]\n",
      "Step 531735  [5.394 sec/step, loss=0.07399, avg_loss=0.07359]\n",
      "Step 531736  [5.406 sec/step, loss=0.07590, avg_loss=0.07361]\n",
      "Step 531737  [5.417 sec/step, loss=0.07505, avg_loss=0.07362]\n",
      "Step 531738  [5.413 sec/step, loss=0.07233, avg_loss=0.07361]\n",
      "Step 531739  [5.395 sec/step, loss=0.07403, avg_loss=0.07362]\n",
      "Generated 32 batches of size 32 in 2.418 sec\n",
      "Step 531740  [5.411 sec/step, loss=0.07534, avg_loss=0.07363]\n",
      "Step 531741  [5.402 sec/step, loss=0.07341, avg_loss=0.07363]\n",
      "Step 531742  [5.381 sec/step, loss=0.07508, avg_loss=0.07363]\n",
      "Step 531743  [5.438 sec/step, loss=0.06657, avg_loss=0.07357]\n",
      "Step 531744  [5.482 sec/step, loss=0.07333, avg_loss=0.07359]\n",
      "Step 531745  [5.476 sec/step, loss=0.07609, avg_loss=0.07359]\n",
      "Step 531746  [5.475 sec/step, loss=0.07529, avg_loss=0.07359]\n",
      "Step 531747  [5.491 sec/step, loss=0.07471, avg_loss=0.07363]\n",
      "Step 531748  [5.479 sec/step, loss=0.07414, avg_loss=0.07365]\n",
      "Step 531749  [5.468 sec/step, loss=0.07389, avg_loss=0.07364]\n",
      "Step 531750  [5.460 sec/step, loss=0.07532, avg_loss=0.07364]\n",
      "Step 531751  [5.460 sec/step, loss=0.07484, avg_loss=0.07362]\n",
      "Step 531752  [5.491 sec/step, loss=0.07371, avg_loss=0.07363]\n",
      "Step 531753  [5.467 sec/step, loss=0.07383, avg_loss=0.07362]\n",
      "Step 531754  [5.455 sec/step, loss=0.07498, avg_loss=0.07363]\n",
      "Step 531755  [5.476 sec/step, loss=0.07544, avg_loss=0.07365]\n",
      "Step 531756  [5.477 sec/step, loss=0.07660, avg_loss=0.07366]\n",
      "Step 531757  [5.469 sec/step, loss=0.07497, avg_loss=0.07367]\n",
      "Step 531758  [5.463 sec/step, loss=0.07126, avg_loss=0.07363]\n",
      "Step 531759  [5.465 sec/step, loss=0.07371, avg_loss=0.07361]\n",
      "Step 531760  [5.445 sec/step, loss=0.06685, avg_loss=0.07352]\n",
      "Step 531761  [5.447 sec/step, loss=0.07430, avg_loss=0.07352]\n",
      "Step 531762  [5.428 sec/step, loss=0.07361, avg_loss=0.07350]\n",
      "Step 531763  [5.404 sec/step, loss=0.07403, avg_loss=0.07348]\n",
      "Step 531764  [5.426 sec/step, loss=0.07609, avg_loss=0.07358]\n",
      "Step 531765  [5.439 sec/step, loss=0.07374, avg_loss=0.07357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 531766  [5.435 sec/step, loss=0.07289, avg_loss=0.07356]\n",
      "Step 531767  [5.447 sec/step, loss=0.07516, avg_loss=0.07357]\n",
      "Step 531768  [5.460 sec/step, loss=0.07634, avg_loss=0.07358]\n",
      "Step 531769  [5.450 sec/step, loss=0.07214, avg_loss=0.07355]\n",
      "Step 531770  [5.463 sec/step, loss=0.07569, avg_loss=0.07356]\n",
      "Step 531771  [5.466 sec/step, loss=0.07445, avg_loss=0.07357]\n",
      "Generated 32 batches of size 32 in 2.622 sec\n",
      "Step 531772  [5.457 sec/step, loss=0.07205, avg_loss=0.07355]\n",
      "Step 531773  [5.464 sec/step, loss=0.07429, avg_loss=0.07356]\n",
      "Step 531774  [5.506 sec/step, loss=0.06624, avg_loss=0.07346]\n",
      "Step 531775  [5.441 sec/step, loss=0.07253, avg_loss=0.07353]\n",
      "Step 531776  [5.471 sec/step, loss=0.07298, avg_loss=0.07354]\n",
      "Step 531777  [5.468 sec/step, loss=0.07519, avg_loss=0.07356]\n",
      "Step 531778  [5.477 sec/step, loss=0.07546, avg_loss=0.07358]\n",
      "Step 531779  [5.462 sec/step, loss=0.07328, avg_loss=0.07356]\n",
      "Step 531780  [5.472 sec/step, loss=0.07342, avg_loss=0.07357]\n",
      "Step 531781  [5.479 sec/step, loss=0.07264, avg_loss=0.07355]\n",
      "Step 531782  [5.466 sec/step, loss=0.07349, avg_loss=0.07354]\n",
      "Step 531783  [5.453 sec/step, loss=0.07441, avg_loss=0.07352]\n",
      "Step 531784  [5.458 sec/step, loss=0.07151, avg_loss=0.07353]\n",
      "Step 531785  [5.422 sec/step, loss=0.07426, avg_loss=0.07354]\n",
      "Step 531786  [5.378 sec/step, loss=0.07620, avg_loss=0.07365]\n",
      "Step 531787  [5.379 sec/step, loss=0.07310, avg_loss=0.07369]\n",
      "Step 531788  [5.396 sec/step, loss=0.07508, avg_loss=0.07370]\n",
      "Step 531789  [5.411 sec/step, loss=0.07564, avg_loss=0.07372]\n",
      "Step 531790  [5.425 sec/step, loss=0.07417, avg_loss=0.07371]\n",
      "Step 531791  [5.430 sec/step, loss=0.07332, avg_loss=0.07371]\n",
      "Step 531792  [5.417 sec/step, loss=0.07180, avg_loss=0.07369]\n",
      "Step 531793  [5.409 sec/step, loss=0.07499, avg_loss=0.07368]\n",
      "Step 531794  [5.390 sec/step, loss=0.07584, avg_loss=0.07370]\n",
      "Step 531795  [5.406 sec/step, loss=0.07535, avg_loss=0.07375]\n",
      "Step 531796  [5.430 sec/step, loss=0.07537, avg_loss=0.07376]\n",
      "Step 531797  [5.413 sec/step, loss=0.06662, avg_loss=0.07368]\n",
      "Step 531798  [5.407 sec/step, loss=0.07419, avg_loss=0.07367]\n",
      "Step 531799  [5.413 sec/step, loss=0.07461, avg_loss=0.07366]\n",
      "Step 531800  [5.404 sec/step, loss=0.07387, avg_loss=0.07367]\n",
      "Writing summary at step: 531800\n",
      "Step 531801  [5.407 sec/step, loss=0.07599, avg_loss=0.07368]\n",
      "Step 531802  [5.406 sec/step, loss=0.07482, avg_loss=0.07368]\n",
      "Generated 32 batches of size 32 in 2.438 sec\n",
      "Step 531803  [5.424 sec/step, loss=0.07336, avg_loss=0.07369]\n",
      "Step 531804  [5.489 sec/step, loss=0.06503, avg_loss=0.07363]\n",
      "Step 531805  [5.484 sec/step, loss=0.07558, avg_loss=0.07363]\n",
      "Step 531806  [5.484 sec/step, loss=0.07143, avg_loss=0.07363]\n",
      "Step 531807  [5.461 sec/step, loss=0.07386, avg_loss=0.07364]\n",
      "Step 531808  [5.481 sec/step, loss=0.07487, avg_loss=0.07366]\n",
      "Step 531809  [5.482 sec/step, loss=0.07538, avg_loss=0.07368]\n",
      "Step 531810  [5.456 sec/step, loss=0.07106, avg_loss=0.07362]\n",
      "Step 531811  [5.432 sec/step, loss=0.07498, avg_loss=0.07365]\n",
      "Step 531812  [5.448 sec/step, loss=0.07470, avg_loss=0.07374]\n",
      "Step 531813  [5.436 sec/step, loss=0.07350, avg_loss=0.07372]\n",
      "Step 531814  [5.424 sec/step, loss=0.07530, avg_loss=0.07372]\n",
      "Step 531815  [5.398 sec/step, loss=0.06475, avg_loss=0.07360]\n",
      "Step 531816  [5.412 sec/step, loss=0.07576, avg_loss=0.07361]\n",
      "Step 531817  [5.434 sec/step, loss=0.07573, avg_loss=0.07367]\n",
      "Step 531818  [5.432 sec/step, loss=0.07140, avg_loss=0.07365]\n",
      "Step 531819  [5.435 sec/step, loss=0.07450, avg_loss=0.07368]\n",
      "Step 531820  [5.426 sec/step, loss=0.07394, avg_loss=0.07366]\n",
      "Step 531821  [5.437 sec/step, loss=0.07402, avg_loss=0.07368]\n",
      "Step 531822  [5.425 sec/step, loss=0.07380, avg_loss=0.07367]\n",
      "Step 531823  [5.438 sec/step, loss=0.07508, avg_loss=0.07377]\n",
      "Step 531824  [5.492 sec/step, loss=0.06642, avg_loss=0.07370]\n",
      "Step 531825  [5.480 sec/step, loss=0.07283, avg_loss=0.07368]\n",
      "Step 531826  [5.467 sec/step, loss=0.07327, avg_loss=0.07365]\n",
      "Step 531827  [5.460 sec/step, loss=0.07298, avg_loss=0.07365]\n",
      "Step 531828  [5.453 sec/step, loss=0.07564, avg_loss=0.07365]\n",
      "Step 531829  [5.464 sec/step, loss=0.07647, avg_loss=0.07367]\n",
      "Step 531830  [5.442 sec/step, loss=0.07464, avg_loss=0.07366]\n",
      "Step 531831  [5.439 sec/step, loss=0.07537, avg_loss=0.07367]\n",
      "Step 531832  [5.455 sec/step, loss=0.07528, avg_loss=0.07373]\n",
      "Step 531833  [5.468 sec/step, loss=0.07523, avg_loss=0.07374]\n",
      "Step 531834  [5.470 sec/step, loss=0.07625, avg_loss=0.07374]\n",
      "Generated 32 batches of size 32 in 2.448 sec\n",
      "Step 531835  [5.490 sec/step, loss=0.07462, avg_loss=0.07375]\n",
      "Step 531836  [5.487 sec/step, loss=0.07433, avg_loss=0.07373]\n",
      "Step 531837  [5.482 sec/step, loss=0.07260, avg_loss=0.07371]\n",
      "Step 531838  [5.518 sec/step, loss=0.07341, avg_loss=0.07372]\n",
      "Step 531839  [5.494 sec/step, loss=0.07161, avg_loss=0.07369]\n",
      "Step 531840  [5.481 sec/step, loss=0.07501, avg_loss=0.07369]\n",
      "Step 531841  [5.505 sec/step, loss=0.07290, avg_loss=0.07369]\n",
      "Step 531842  [5.517 sec/step, loss=0.07594, avg_loss=0.07370]\n",
      "Step 531843  [5.457 sec/step, loss=0.07090, avg_loss=0.07374]\n",
      "Step 531844  [5.433 sec/step, loss=0.07446, avg_loss=0.07375]\n",
      "Step 531845  [5.432 sec/step, loss=0.07669, avg_loss=0.07376]\n",
      "Step 531846  [5.419 sec/step, loss=0.07204, avg_loss=0.07372]\n",
      "Step 531847  [5.400 sec/step, loss=0.07120, avg_loss=0.07369]\n",
      "Step 531848  [5.410 sec/step, loss=0.07378, avg_loss=0.07368]\n",
      "Step 531849  [5.458 sec/step, loss=0.06568, avg_loss=0.07360]\n",
      "Step 531850  [5.452 sec/step, loss=0.07437, avg_loss=0.07359]\n",
      "Step 531851  [5.442 sec/step, loss=0.07497, avg_loss=0.07359]\n",
      "Step 531852  [5.434 sec/step, loss=0.07308, avg_loss=0.07359]\n",
      "Step 531853  [5.448 sec/step, loss=0.07446, avg_loss=0.07359]\n",
      "Step 531854  [5.459 sec/step, loss=0.07393, avg_loss=0.07358]\n",
      "Step 531855  [5.444 sec/step, loss=0.07073, avg_loss=0.07354]\n",
      "Step 531856  [5.444 sec/step, loss=0.07590, avg_loss=0.07353]\n",
      "Step 531857  [5.444 sec/step, loss=0.07313, avg_loss=0.07351]\n",
      "Step 531858  [5.456 sec/step, loss=0.07389, avg_loss=0.07354]\n",
      "Step 531859  [5.445 sec/step, loss=0.07543, avg_loss=0.07355]\n",
      "Step 531860  [5.461 sec/step, loss=0.07471, avg_loss=0.07363]\n",
      "Step 531861  [5.449 sec/step, loss=0.07024, avg_loss=0.07359]\n",
      "Step 531862  [5.451 sec/step, loss=0.07250, avg_loss=0.07358]\n",
      "Step 531863  [5.463 sec/step, loss=0.07569, avg_loss=0.07360]\n",
      "Step 531864  [5.450 sec/step, loss=0.07389, avg_loss=0.07358]\n",
      "Step 531865  [5.441 sec/step, loss=0.07399, avg_loss=0.07358]\n",
      "Step 531866  [5.464 sec/step, loss=0.07346, avg_loss=0.07358]\n",
      "Generated 32 batches of size 32 in 2.432 sec\n",
      "Step 531867  [5.478 sec/step, loss=0.07577, avg_loss=0.07359]\n",
      "Step 531868  [5.480 sec/step, loss=0.07360, avg_loss=0.07356]\n",
      "Step 531869  [5.472 sec/step, loss=0.06592, avg_loss=0.07350]\n",
      "Step 531870  [5.458 sec/step, loss=0.07314, avg_loss=0.07348]\n",
      "Step 531871  [5.471 sec/step, loss=0.07606, avg_loss=0.07349]\n",
      "Step 531872  [5.478 sec/step, loss=0.07537, avg_loss=0.07352]\n",
      "Step 531873  [5.483 sec/step, loss=0.07445, avg_loss=0.07353]\n",
      "Step 531874  [5.425 sec/step, loss=0.07320, avg_loss=0.07360]\n",
      "Step 531875  [5.454 sec/step, loss=0.07526, avg_loss=0.07362]\n",
      "Step 531876  [5.453 sec/step, loss=0.07550, avg_loss=0.07365]\n",
      "Step 531877  [5.433 sec/step, loss=0.06541, avg_loss=0.07355]\n",
      "Step 531878  [5.429 sec/step, loss=0.07517, avg_loss=0.07355]\n",
      "Step 531879  [5.455 sec/step, loss=0.07269, avg_loss=0.07354]\n",
      "Step 531880  [5.468 sec/step, loss=0.07684, avg_loss=0.07358]\n",
      "Step 531881  [5.473 sec/step, loss=0.07691, avg_loss=0.07362]\n",
      "Step 531882  [5.485 sec/step, loss=0.07416, avg_loss=0.07363]\n",
      "Step 531883  [5.484 sec/step, loss=0.07418, avg_loss=0.07362]\n",
      "Step 531884  [5.471 sec/step, loss=0.07035, avg_loss=0.07361]\n",
      "Step 531885  [5.481 sec/step, loss=0.07201, avg_loss=0.07359]\n",
      "Step 531886  [5.478 sec/step, loss=0.07496, avg_loss=0.07358]\n",
      "Step 531887  [5.488 sec/step, loss=0.07526, avg_loss=0.07360]\n",
      "Step 531888  [5.469 sec/step, loss=0.07243, avg_loss=0.07357]\n",
      "Step 531889  [5.454 sec/step, loss=0.07511, avg_loss=0.07357]\n",
      "Step 531890  [5.445 sec/step, loss=0.07575, avg_loss=0.07358]\n",
      "Step 531891  [5.446 sec/step, loss=0.07488, avg_loss=0.07360]\n",
      "Step 531892  [5.444 sec/step, loss=0.07346, avg_loss=0.07361]\n",
      "Step 531893  [5.439 sec/step, loss=0.07107, avg_loss=0.07358]\n",
      "Step 531894  [5.444 sec/step, loss=0.07368, avg_loss=0.07355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 531895  [5.433 sec/step, loss=0.07488, avg_loss=0.07355]\n",
      "Step 531896  [5.409 sec/step, loss=0.07362, avg_loss=0.07353]\n",
      "Step 531897  [5.415 sec/step, loss=0.07378, avg_loss=0.07360]\n",
      "Step 531898  [5.425 sec/step, loss=0.07630, avg_loss=0.07362]\n",
      "Generated 32 batches of size 32 in 2.862 sec\n",
      "Step 531899  [5.408 sec/step, loss=0.07168, avg_loss=0.07359]\n",
      "Step 531900  [5.425 sec/step, loss=0.07601, avg_loss=0.07362]\n",
      "Writing summary at step: 531900\n",
      "Step 531901  [5.421 sec/step, loss=0.07562, avg_loss=0.07361]\n",
      "Step 531902  [5.468 sec/step, loss=0.06642, avg_loss=0.07353]\n",
      "Step 531903  [5.462 sec/step, loss=0.07290, avg_loss=0.07352]\n",
      "Step 531904  [5.426 sec/step, loss=0.07661, avg_loss=0.07364]\n",
      "Step 531905  [5.424 sec/step, loss=0.07406, avg_loss=0.07362]\n",
      "Step 531906  [5.427 sec/step, loss=0.07399, avg_loss=0.07365]\n",
      "Step 531907  [5.434 sec/step, loss=0.07542, avg_loss=0.07367]\n",
      "Step 531908  [5.393 sec/step, loss=0.07137, avg_loss=0.07363]\n",
      "Step 531909  [5.392 sec/step, loss=0.07358, avg_loss=0.07361]\n",
      "Step 531910  [5.419 sec/step, loss=0.07619, avg_loss=0.07366]\n",
      "Step 531911  [5.427 sec/step, loss=0.07555, avg_loss=0.07367]\n",
      "Step 531912  [5.438 sec/step, loss=0.07615, avg_loss=0.07368]\n",
      "Step 531913  [5.439 sec/step, loss=0.07389, avg_loss=0.07369]\n",
      "Step 531914  [5.432 sec/step, loss=0.07480, avg_loss=0.07368]\n",
      "Step 531915  [5.447 sec/step, loss=0.07420, avg_loss=0.07378]\n",
      "Step 531916  [5.433 sec/step, loss=0.07579, avg_loss=0.07378]\n",
      "Step 531917  [5.413 sec/step, loss=0.07132, avg_loss=0.07373]\n",
      "Step 531918  [5.418 sec/step, loss=0.07310, avg_loss=0.07375]\n",
      "Step 531919  [5.463 sec/step, loss=0.06787, avg_loss=0.07368]\n",
      "Step 531920  [5.454 sec/step, loss=0.07397, avg_loss=0.07368]\n",
      "Step 531921  [5.453 sec/step, loss=0.07456, avg_loss=0.07369]\n",
      "Step 531922  [5.464 sec/step, loss=0.07431, avg_loss=0.07370]\n",
      "Step 531923  [5.461 sec/step, loss=0.07317, avg_loss=0.07368]\n",
      "Step 531924  [5.420 sec/step, loss=0.07635, avg_loss=0.07378]\n",
      "Step 531925  [5.423 sec/step, loss=0.07320, avg_loss=0.07378]\n",
      "Step 531926  [5.412 sec/step, loss=0.06621, avg_loss=0.07371]\n",
      "Step 531927  [5.419 sec/step, loss=0.07591, avg_loss=0.07374]\n",
      "Step 531928  [5.412 sec/step, loss=0.07203, avg_loss=0.07370]\n",
      "Step 531929  [5.427 sec/step, loss=0.07351, avg_loss=0.07367]\n",
      "Generated 32 batches of size 32 in 2.328 sec\n",
      "Step 531930  [5.445 sec/step, loss=0.07363, avg_loss=0.07366]\n",
      "Step 531931  [5.441 sec/step, loss=0.07178, avg_loss=0.07363]\n",
      "Step 531932  [5.444 sec/step, loss=0.07286, avg_loss=0.07360]\n",
      "Step 531933  [5.444 sec/step, loss=0.07690, avg_loss=0.07362]\n",
      "Step 531934  [5.440 sec/step, loss=0.07625, avg_loss=0.07362]\n",
      "Step 531935  [5.447 sec/step, loss=0.07332, avg_loss=0.07361]\n",
      "Step 531936  [5.440 sec/step, loss=0.07523, avg_loss=0.07361]\n",
      "Step 531937  [5.453 sec/step, loss=0.07479, avg_loss=0.07364]\n",
      "Step 531938  [5.440 sec/step, loss=0.07607, avg_loss=0.07366]\n",
      "Step 531939  [5.468 sec/step, loss=0.07559, avg_loss=0.07370]\n",
      "Step 531940  [5.460 sec/step, loss=0.07260, avg_loss=0.07368]\n",
      "Step 531941  [5.445 sec/step, loss=0.07444, avg_loss=0.07369]\n",
      "Step 531942  [5.438 sec/step, loss=0.07515, avg_loss=0.07369]\n",
      "Step 531943  [5.456 sec/step, loss=0.07357, avg_loss=0.07371]\n",
      "Step 531944  [5.440 sec/step, loss=0.07053, avg_loss=0.07367]\n",
      "Step 531945  [5.415 sec/step, loss=0.06627, avg_loss=0.07357]\n",
      "Step 531946  [5.423 sec/step, loss=0.07277, avg_loss=0.07358]\n",
      "Step 531947  [5.442 sec/step, loss=0.07587, avg_loss=0.07362]\n",
      "Step 531948  [5.467 sec/step, loss=0.07471, avg_loss=0.07363]\n",
      "Step 531949  [5.432 sec/step, loss=0.07448, avg_loss=0.07372]\n",
      "Step 531950  [5.432 sec/step, loss=0.07519, avg_loss=0.07373]\n",
      "Step 531951  [5.454 sec/step, loss=0.07476, avg_loss=0.07373]\n",
      "Step 531952  [5.430 sec/step, loss=0.07359, avg_loss=0.07373]\n",
      "Step 531953  [5.427 sec/step, loss=0.07397, avg_loss=0.07373]\n",
      "Step 531954  [5.418 sec/step, loss=0.07463, avg_loss=0.07373]\n",
      "Step 531955  [5.425 sec/step, loss=0.07433, avg_loss=0.07377]\n",
      "Step 531956  [5.402 sec/step, loss=0.07167, avg_loss=0.07373]\n",
      "Step 531957  [5.415 sec/step, loss=0.07301, avg_loss=0.07373]\n",
      "Step 531958  [5.401 sec/step, loss=0.07301, avg_loss=0.07372]\n",
      "Step 531959  [5.392 sec/step, loss=0.07395, avg_loss=0.07370]\n",
      "Step 531960  [5.389 sec/step, loss=0.07450, avg_loss=0.07370]\n",
      "Step 531961  [5.397 sec/step, loss=0.07448, avg_loss=0.07374]\n",
      "Generated 32 batches of size 32 in 2.344 sec\n",
      "Step 531962  [5.416 sec/step, loss=0.07583, avg_loss=0.07378]\n",
      "Step 531963  [5.416 sec/step, loss=0.07523, avg_loss=0.07377]\n",
      "Step 531964  [5.435 sec/step, loss=0.07556, avg_loss=0.07379]\n",
      "Step 531965  [5.438 sec/step, loss=0.07360, avg_loss=0.07378]\n",
      "Step 531966  [5.409 sec/step, loss=0.07455, avg_loss=0.07380]\n",
      "Step 531967  [5.383 sec/step, loss=0.07409, avg_loss=0.07378]\n",
      "Step 531968  [5.379 sec/step, loss=0.07351, avg_loss=0.07378]\n",
      "Step 531969  [5.448 sec/step, loss=0.06474, avg_loss=0.07377]\n",
      "Step 531970  [5.456 sec/step, loss=0.07537, avg_loss=0.07379]\n",
      "Step 531971  [5.426 sec/step, loss=0.06459, avg_loss=0.07367]\n",
      "Step 531972  [5.428 sec/step, loss=0.07368, avg_loss=0.07366]\n",
      "Step 531973  [5.432 sec/step, loss=0.07581, avg_loss=0.07367]\n",
      "Step 531974  [5.436 sec/step, loss=0.07510, avg_loss=0.07369]\n",
      "Step 531975  [5.425 sec/step, loss=0.07375, avg_loss=0.07367]\n",
      "Step 531976  [5.396 sec/step, loss=0.07390, avg_loss=0.07366]\n",
      "Step 531977  [5.437 sec/step, loss=0.07525, avg_loss=0.07376]\n",
      "Step 531978  [5.440 sec/step, loss=0.07595, avg_loss=0.07376]\n",
      "Step 531979  [5.436 sec/step, loss=0.07354, avg_loss=0.07377]\n",
      "Step 531980  [5.424 sec/step, loss=0.07103, avg_loss=0.07371]\n",
      "Step 531981  [5.427 sec/step, loss=0.07569, avg_loss=0.07370]\n",
      "Step 531982  [5.431 sec/step, loss=0.07586, avg_loss=0.07372]\n",
      "Step 531983  [5.419 sec/step, loss=0.07352, avg_loss=0.07371]\n",
      "Step 531984  [5.426 sec/step, loss=0.07336, avg_loss=0.07374]\n",
      "Step 531985  [5.426 sec/step, loss=0.07420, avg_loss=0.07377]\n",
      "Step 531986  [5.429 sec/step, loss=0.07402, avg_loss=0.07376]\n",
      "Step 531987  [5.423 sec/step, loss=0.07385, avg_loss=0.07374]\n",
      "Step 531988  [5.428 sec/step, loss=0.07506, avg_loss=0.07377]\n",
      "Step 531989  [5.426 sec/step, loss=0.07317, avg_loss=0.07375]\n",
      "Step 531990  [5.424 sec/step, loss=0.07482, avg_loss=0.07374]\n",
      "Step 531991  [5.418 sec/step, loss=0.07374, avg_loss=0.07373]\n",
      "Step 531992  [5.432 sec/step, loss=0.07573, avg_loss=0.07375]\n",
      "Step 531993  [5.422 sec/step, loss=0.07094, avg_loss=0.07375]\n",
      "Generated 32 batches of size 32 in 2.528 sec\n",
      "Step 531994  [5.405 sec/step, loss=0.07246, avg_loss=0.07374]\n",
      "Step 531995  [5.421 sec/step, loss=0.07593, avg_loss=0.07375]\n",
      "Step 531996  [5.426 sec/step, loss=0.07151, avg_loss=0.07373]\n",
      "Step 531997  [5.439 sec/step, loss=0.07530, avg_loss=0.07374]\n",
      "Step 531998  [5.436 sec/step, loss=0.07382, avg_loss=0.07372]\n",
      "Step 531999  [5.431 sec/step, loss=0.07124, avg_loss=0.07371]\n",
      "Step 532000  [5.469 sec/step, loss=0.06556, avg_loss=0.07361]\n",
      "Writing summary at step: 532000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-532000\n",
      "Saving audio and alignment...\n",
      "Input: imiigarayshan kay liiay toofal paroogaraam kii azsarayno tdarviidz miin baalvaastdaa ggor v fikr moodzizan hae~_______\n",
      "Step 532001  [5.461 sec/step, loss=0.07568, avg_loss=0.07361]\n",
      "Step 532002  [5.422 sec/step, loss=0.07363, avg_loss=0.07368]\n",
      "Step 532003  [5.460 sec/step, loss=0.06721, avg_loss=0.07362]\n",
      "Step 532004  [5.439 sec/step, loss=0.07393, avg_loss=0.07360]\n",
      "Step 532005  [5.427 sec/step, loss=0.07136, avg_loss=0.07357]\n",
      "Step 532006  [5.442 sec/step, loss=0.07513, avg_loss=0.07358]\n",
      "Step 532007  [5.445 sec/step, loss=0.07499, avg_loss=0.07358]\n",
      "Step 532008  [5.445 sec/step, loss=0.07222, avg_loss=0.07359]\n",
      "Step 532009  [5.472 sec/step, loss=0.07416, avg_loss=0.07359]\n",
      "Step 532010  [5.476 sec/step, loss=0.07488, avg_loss=0.07358]\n",
      "Step 532011  [5.470 sec/step, loss=0.07427, avg_loss=0.07357]\n",
      "Step 532012  [5.457 sec/step, loss=0.07345, avg_loss=0.07354]\n",
      "Step 532013  [5.456 sec/step, loss=0.07344, avg_loss=0.07353]\n",
      "Step 532014  [5.471 sec/step, loss=0.07581, avg_loss=0.07354]\n",
      "Step 532015  [5.478 sec/step, loss=0.07242, avg_loss=0.07353]\n",
      "Step 532016  [5.484 sec/step, loss=0.07581, avg_loss=0.07353]\n",
      "Step 532017  [5.485 sec/step, loss=0.07399, avg_loss=0.07355]\n",
      "Step 532018  [5.495 sec/step, loss=0.07557, avg_loss=0.07358]\n",
      "Step 532019  [5.443 sec/step, loss=0.07199, avg_loss=0.07362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 532020  [5.457 sec/step, loss=0.07499, avg_loss=0.07363]\n",
      "Step 532021  [5.435 sec/step, loss=0.06554, avg_loss=0.07354]\n",
      "Step 532022  [5.446 sec/step, loss=0.07605, avg_loss=0.07356]\n",
      "Step 532023  [5.455 sec/step, loss=0.07504, avg_loss=0.07358]\n",
      "Generated 32 batches of size 32 in 2.389 sec\n",
      "Step 532024  [5.459 sec/step, loss=0.07624, avg_loss=0.07357]\n",
      "Step 532025  [5.474 sec/step, loss=0.07421, avg_loss=0.07358]\n",
      "Step 532026  [5.490 sec/step, loss=0.07289, avg_loss=0.07365]\n",
      "Step 532027  [5.482 sec/step, loss=0.07497, avg_loss=0.07364]\n",
      "Step 532028  [5.476 sec/step, loss=0.06959, avg_loss=0.07362]\n",
      "Step 532029  [5.452 sec/step, loss=0.07204, avg_loss=0.07360]\n",
      "Step 532030  [5.434 sec/step, loss=0.07375, avg_loss=0.07360]\n",
      "Step 532031  [5.435 sec/step, loss=0.07404, avg_loss=0.07363]\n",
      "Step 532032  [5.436 sec/step, loss=0.07607, avg_loss=0.07366]\n",
      "Step 532033  [5.432 sec/step, loss=0.07507, avg_loss=0.07364]\n",
      "Step 532034  [5.428 sec/step, loss=0.07459, avg_loss=0.07362]\n",
      "Step 532035  [5.456 sec/step, loss=0.06524, avg_loss=0.07354]\n",
      "Step 532036  [5.444 sec/step, loss=0.07098, avg_loss=0.07350]\n",
      "Step 532037  [5.431 sec/step, loss=0.07494, avg_loss=0.07350]\n",
      "Step 532038  [5.397 sec/step, loss=0.06613, avg_loss=0.07340]\n",
      "Step 532039  [5.396 sec/step, loss=0.07375, avg_loss=0.07338]\n",
      "Step 532040  [5.422 sec/step, loss=0.07293, avg_loss=0.07339]\n",
      "Step 532041  [5.424 sec/step, loss=0.07551, avg_loss=0.07340]\n",
      "Step 532042  [5.435 sec/step, loss=0.07625, avg_loss=0.07341]\n",
      "Step 532043  [5.412 sec/step, loss=0.07163, avg_loss=0.07339]\n",
      "Step 532044  [5.413 sec/step, loss=0.07390, avg_loss=0.07342]\n",
      "Step 532045  [5.424 sec/step, loss=0.07527, avg_loss=0.07351]\n",
      "Step 532046  [5.440 sec/step, loss=0.07611, avg_loss=0.07355]\n",
      "Step 532047  [5.439 sec/step, loss=0.07461, avg_loss=0.07353]\n",
      "Step 532048  [5.418 sec/step, loss=0.07450, avg_loss=0.07353]\n",
      "Step 532049  [5.402 sec/step, loss=0.07345, avg_loss=0.07352]\n",
      "Step 532050  [5.404 sec/step, loss=0.07520, avg_loss=0.07352]\n",
      "Step 532051  [5.411 sec/step, loss=0.07297, avg_loss=0.07350]\n",
      "Step 532052  [5.421 sec/step, loss=0.07484, avg_loss=0.07352]\n",
      "Step 532053  [5.420 sec/step, loss=0.07433, avg_loss=0.07352]\n",
      "Step 532054  [5.411 sec/step, loss=0.07385, avg_loss=0.07351]\n",
      "Step 532055  [5.407 sec/step, loss=0.07453, avg_loss=0.07351]\n",
      "Generated 32 batches of size 32 in 2.358 sec\n",
      "Step 532056  [5.425 sec/step, loss=0.07426, avg_loss=0.07354]\n",
      "Step 532057  [5.408 sec/step, loss=0.07352, avg_loss=0.07355]\n",
      "Step 532058  [5.407 sec/step, loss=0.07381, avg_loss=0.07355]\n",
      "Step 532059  [5.426 sec/step, loss=0.07614, avg_loss=0.07358]\n",
      "Step 532060  [5.426 sec/step, loss=0.07223, avg_loss=0.07355]\n",
      "Step 532061  [5.436 sec/step, loss=0.07484, avg_loss=0.07356]\n",
      "Step 532062  [5.431 sec/step, loss=0.07585, avg_loss=0.07356]\n",
      "Step 532063  [5.432 sec/step, loss=0.07524, avg_loss=0.07356]\n",
      "Step 532064  [5.416 sec/step, loss=0.07052, avg_loss=0.07351]\n",
      "Step 532065  [5.422 sec/step, loss=0.07370, avg_loss=0.07351]\n",
      "Step 532066  [5.416 sec/step, loss=0.07358, avg_loss=0.07350]\n",
      "Step 532067  [5.408 sec/step, loss=0.06701, avg_loss=0.07343]\n",
      "Step 532068  [5.415 sec/step, loss=0.07520, avg_loss=0.07344]\n",
      "Step 532069  [5.361 sec/step, loss=0.07512, avg_loss=0.07355]\n",
      "Step 532070  [5.363 sec/step, loss=0.07529, avg_loss=0.07355]\n",
      "Step 532071  [5.402 sec/step, loss=0.07401, avg_loss=0.07364]\n",
      "Step 532072  [5.400 sec/step, loss=0.07350, avg_loss=0.07364]\n",
      "Step 532073  [5.400 sec/step, loss=0.07608, avg_loss=0.07364]\n",
      "Step 532074  [5.393 sec/step, loss=0.07021, avg_loss=0.07359]\n",
      "Step 532075  [5.388 sec/step, loss=0.07421, avg_loss=0.07360]\n",
      "Step 532076  [5.412 sec/step, loss=0.07550, avg_loss=0.07361]\n",
      "Step 532077  [5.383 sec/step, loss=0.07412, avg_loss=0.07360]\n",
      "Step 532078  [5.376 sec/step, loss=0.07334, avg_loss=0.07358]\n",
      "Step 532079  [5.357 sec/step, loss=0.07404, avg_loss=0.07358]\n",
      "Step 532080  [5.359 sec/step, loss=0.07430, avg_loss=0.07361]\n",
      "Step 532081  [5.348 sec/step, loss=0.07548, avg_loss=0.07361]\n",
      "Step 532082  [5.334 sec/step, loss=0.07173, avg_loss=0.07357]\n",
      "Step 532083  [5.327 sec/step, loss=0.07136, avg_loss=0.07355]\n",
      "Step 532084  [5.326 sec/step, loss=0.07352, avg_loss=0.07355]\n",
      "Step 532085  [5.321 sec/step, loss=0.07118, avg_loss=0.07352]\n",
      "Step 532086  [5.330 sec/step, loss=0.07546, avg_loss=0.07353]\n",
      "Step 532087  [5.342 sec/step, loss=0.07555, avg_loss=0.07355]\n",
      "Generated 32 batches of size 32 in 2.326 sec\n",
      "Step 532088  [5.348 sec/step, loss=0.07193, avg_loss=0.07352]\n",
      "Step 532089  [5.352 sec/step, loss=0.07496, avg_loss=0.07354]\n",
      "Step 532090  [5.355 sec/step, loss=0.07475, avg_loss=0.07354]\n",
      "Step 532091  [5.350 sec/step, loss=0.07311, avg_loss=0.07353]\n",
      "Step 532092  [5.346 sec/step, loss=0.07569, avg_loss=0.07353]\n",
      "Step 532093  [5.346 sec/step, loss=0.07379, avg_loss=0.07356]\n",
      "Step 532094  [5.400 sec/step, loss=0.06534, avg_loss=0.07349]\n",
      "Step 532095  [5.382 sec/step, loss=0.07457, avg_loss=0.07347]\n",
      "Step 532096  [5.392 sec/step, loss=0.07585, avg_loss=0.07352]\n",
      "Step 532097  [5.439 sec/step, loss=0.06692, avg_loss=0.07343]\n",
      "Step 532098  [5.447 sec/step, loss=0.07620, avg_loss=0.07346]\n",
      "Step 532099  [5.469 sec/step, loss=0.07602, avg_loss=0.07351]\n",
      "Step 532100  [5.421 sec/step, loss=0.07522, avg_loss=0.07360]\n",
      "Writing summary at step: 532100\n",
      "Step 532101  [5.426 sec/step, loss=0.07518, avg_loss=0.07360]\n",
      "Step 532102  [5.415 sec/step, loss=0.07553, avg_loss=0.07362]\n",
      "Step 532103  [5.378 sec/step, loss=0.07553, avg_loss=0.07370]\n",
      "Step 532104  [5.373 sec/step, loss=0.07364, avg_loss=0.07370]\n",
      "Step 532105  [5.392 sec/step, loss=0.07557, avg_loss=0.07374]\n",
      "Step 532106  [5.386 sec/step, loss=0.07237, avg_loss=0.07371]\n",
      "Step 532107  [5.384 sec/step, loss=0.07348, avg_loss=0.07370]\n",
      "Step 532108  [5.402 sec/step, loss=0.07262, avg_loss=0.07370]\n",
      "Step 532109  [5.390 sec/step, loss=0.07677, avg_loss=0.07373]\n",
      "Step 532110  [5.374 sec/step, loss=0.07451, avg_loss=0.07372]\n",
      "Step 532111  [5.374 sec/step, loss=0.07467, avg_loss=0.07373]\n",
      "Step 532112  [5.360 sec/step, loss=0.06498, avg_loss=0.07364]\n",
      "Step 532113  [5.381 sec/step, loss=0.07566, avg_loss=0.07366]\n",
      "Step 532114  [5.358 sec/step, loss=0.07124, avg_loss=0.07362]\n",
      "Step 532115  [5.356 sec/step, loss=0.07448, avg_loss=0.07364]\n",
      "Step 532116  [5.335 sec/step, loss=0.07149, avg_loss=0.07360]\n",
      "Step 532117  [5.342 sec/step, loss=0.07296, avg_loss=0.07359]\n",
      "Step 532118  [5.349 sec/step, loss=0.07298, avg_loss=0.07356]\n",
      "Generated 32 batches of size 32 in 2.543 sec\n",
      "Step 532119  [5.348 sec/step, loss=0.07386, avg_loss=0.07358]\n",
      "Step 532120  [5.352 sec/step, loss=0.07455, avg_loss=0.07357]\n",
      "Step 532121  [5.373 sec/step, loss=0.07401, avg_loss=0.07366]\n",
      "Step 532122  [5.361 sec/step, loss=0.07455, avg_loss=0.07364]\n",
      "Step 532123  [5.357 sec/step, loss=0.07134, avg_loss=0.07361]\n",
      "Step 532124  [5.338 sec/step, loss=0.07380, avg_loss=0.07358]\n",
      "Step 532125  [5.332 sec/step, loss=0.07499, avg_loss=0.07359]\n",
      "Step 532126  [5.329 sec/step, loss=0.07314, avg_loss=0.07359]\n",
      "Step 532127  [5.343 sec/step, loss=0.07431, avg_loss=0.07359]\n",
      "Step 532128  [5.350 sec/step, loss=0.07406, avg_loss=0.07363]\n",
      "Step 532129  [5.361 sec/step, loss=0.07601, avg_loss=0.07367]\n",
      "Step 532130  [5.372 sec/step, loss=0.07543, avg_loss=0.07369]\n",
      "Step 532131  [5.366 sec/step, loss=0.07388, avg_loss=0.07368]\n",
      "Step 532132  [5.376 sec/step, loss=0.07537, avg_loss=0.07368]\n",
      "Step 532133  [5.362 sec/step, loss=0.07372, avg_loss=0.07366]\n",
      "Step 532134  [5.347 sec/step, loss=0.07436, avg_loss=0.07366]\n",
      "Step 532135  [5.297 sec/step, loss=0.07162, avg_loss=0.07373]\n",
      "Step 532136  [5.312 sec/step, loss=0.07478, avg_loss=0.07376]\n",
      "Step 532137  [5.310 sec/step, loss=0.07256, avg_loss=0.07374]\n",
      "Step 532138  [5.334 sec/step, loss=0.07198, avg_loss=0.07380]\n",
      "Step 532139  [5.323 sec/step, loss=0.07364, avg_loss=0.07380]\n",
      "Step 532140  [5.305 sec/step, loss=0.07535, avg_loss=0.07382]\n",
      "Step 532141  [5.311 sec/step, loss=0.07631, avg_loss=0.07383]\n",
      "Step 532142  [5.280 sec/step, loss=0.06494, avg_loss=0.07372]\n",
      "Step 532143  [5.319 sec/step, loss=0.07311, avg_loss=0.07373]\n",
      "Step 532144  [5.325 sec/step, loss=0.07375, avg_loss=0.07373]\n",
      "Step 532145  [5.378 sec/step, loss=0.06704, avg_loss=0.07365]\n",
      "Step 532146  [5.375 sec/step, loss=0.07671, avg_loss=0.07365]\n",
      "Step 532147  [5.371 sec/step, loss=0.07468, avg_loss=0.07365]\n",
      "Step 532148  [5.369 sec/step, loss=0.07484, avg_loss=0.07366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 532149  [5.380 sec/step, loss=0.07610, avg_loss=0.07368]\n",
      "Step 532150  [5.392 sec/step, loss=0.07378, avg_loss=0.07367]\n",
      "Generated 32 batches of size 32 in 2.333 sec\n",
      "Step 532151  [5.375 sec/step, loss=0.07269, avg_loss=0.07367]\n",
      "Step 532152  [5.378 sec/step, loss=0.07523, avg_loss=0.07367]\n",
      "Step 532153  [5.387 sec/step, loss=0.07630, avg_loss=0.07369]\n",
      "Step 532154  [5.393 sec/step, loss=0.07228, avg_loss=0.07367]\n",
      "Step 532155  [5.378 sec/step, loss=0.07073, avg_loss=0.07364]\n",
      "Step 532156  [5.372 sec/step, loss=0.07503, avg_loss=0.07364]\n",
      "Step 532157  [5.391 sec/step, loss=0.07457, avg_loss=0.07365]\n",
      "Step 532158  [5.402 sec/step, loss=0.07535, avg_loss=0.07367]\n",
      "Step 532159  [5.379 sec/step, loss=0.07028, avg_loss=0.07361]\n",
      "Step 532160  [5.381 sec/step, loss=0.07485, avg_loss=0.07364]\n",
      "Step 532161  [5.381 sec/step, loss=0.07577, avg_loss=0.07365]\n",
      "Step 532162  [5.357 sec/step, loss=0.07271, avg_loss=0.07362]\n",
      "Step 532163  [5.344 sec/step, loss=0.07386, avg_loss=0.07360]\n",
      "Step 532164  [5.349 sec/step, loss=0.07343, avg_loss=0.07363]\n",
      "Step 532165  [5.341 sec/step, loss=0.07483, avg_loss=0.07364]\n",
      "Step 532166  [5.354 sec/step, loss=0.07574, avg_loss=0.07366]\n",
      "Step 532167  [5.365 sec/step, loss=0.07345, avg_loss=0.07373]\n",
      "Step 532168  [5.364 sec/step, loss=0.07545, avg_loss=0.07373]\n",
      "Step 532169  [5.361 sec/step, loss=0.07325, avg_loss=0.07371]\n",
      "Step 532170  [5.376 sec/step, loss=0.07265, avg_loss=0.07369]\n",
      "Step 532171  [5.362 sec/step, loss=0.07601, avg_loss=0.07371]\n",
      "Step 532172  [5.362 sec/step, loss=0.07316, avg_loss=0.07370]\n",
      "Step 532173  [5.348 sec/step, loss=0.07319, avg_loss=0.07367]\n",
      "Step 532174  [5.362 sec/step, loss=0.07458, avg_loss=0.07372]\n",
      "Step 532175  [5.371 sec/step, loss=0.07583, avg_loss=0.07373]\n",
      "Step 532176  [5.370 sec/step, loss=0.07574, avg_loss=0.07374]\n",
      "Step 532177  [5.375 sec/step, loss=0.07449, avg_loss=0.07374]\n",
      "Step 532178  [5.358 sec/step, loss=0.06769, avg_loss=0.07368]\n",
      "Step 532179  [5.372 sec/step, loss=0.07516, avg_loss=0.07369]\n",
      "Step 532180  [5.373 sec/step, loss=0.07307, avg_loss=0.07368]\n",
      "Step 532181  [5.374 sec/step, loss=0.07510, avg_loss=0.07368]\n",
      "Step 532182  [5.369 sec/step, loss=0.07256, avg_loss=0.07369]\n",
      "Generated 32 batches of size 32 in 2.622 sec\n",
      "Step 532183  [5.438 sec/step, loss=0.06545, avg_loss=0.07363]\n",
      "Step 532184  [5.442 sec/step, loss=0.07490, avg_loss=0.07364]\n",
      "Step 532185  [5.443 sec/step, loss=0.07184, avg_loss=0.07365]\n",
      "Step 532186  [5.426 sec/step, loss=0.07279, avg_loss=0.07362]\n",
      "Step 532187  [5.419 sec/step, loss=0.07333, avg_loss=0.07360]\n",
      "Step 532188  [5.422 sec/step, loss=0.07365, avg_loss=0.07362]\n",
      "Step 532189  [5.437 sec/step, loss=0.07497, avg_loss=0.07362]\n",
      "Step 532190  [5.434 sec/step, loss=0.07392, avg_loss=0.07361]\n",
      "Step 532191  [5.450 sec/step, loss=0.07501, avg_loss=0.07363]\n",
      "Step 532192  [5.449 sec/step, loss=0.07397, avg_loss=0.07361]\n",
      "Step 532193  [5.456 sec/step, loss=0.07310, avg_loss=0.07360]\n",
      "Step 532194  [5.404 sec/step, loss=0.07121, avg_loss=0.07366]\n",
      "Step 532195  [5.421 sec/step, loss=0.07507, avg_loss=0.07367]\n",
      "Step 532196  [5.403 sec/step, loss=0.06970, avg_loss=0.07361]\n",
      "Step 532197  [5.343 sec/step, loss=0.07346, avg_loss=0.07367]\n",
      "Step 532198  [5.322 sec/step, loss=0.07303, avg_loss=0.07364]\n",
      "Step 532199  [5.322 sec/step, loss=0.07510, avg_loss=0.07363]\n",
      "Step 532200  [5.319 sec/step, loss=0.07483, avg_loss=0.07363]\n",
      "Writing summary at step: 532200\n",
      "Step 532201  [5.325 sec/step, loss=0.07449, avg_loss=0.07362]\n",
      "Step 532202  [5.318 sec/step, loss=0.07287, avg_loss=0.07359]\n",
      "Step 532203  [5.319 sec/step, loss=0.07461, avg_loss=0.07358]\n",
      "Step 532204  [5.336 sec/step, loss=0.07525, avg_loss=0.07360]\n",
      "Step 532205  [5.337 sec/step, loss=0.07553, avg_loss=0.07360]\n",
      "Step 532206  [5.336 sec/step, loss=0.07202, avg_loss=0.07360]\n",
      "Step 532207  [5.350 sec/step, loss=0.07556, avg_loss=0.07362]\n",
      "Step 532208  [5.349 sec/step, loss=0.07506, avg_loss=0.07364]\n",
      "Step 532209  [5.337 sec/step, loss=0.07465, avg_loss=0.07362]\n",
      "Step 532210  [5.361 sec/step, loss=0.07269, avg_loss=0.07360]\n",
      "Step 532211  [5.349 sec/step, loss=0.07088, avg_loss=0.07356]\n",
      "Step 532212  [5.373 sec/step, loss=0.07506, avg_loss=0.07366]\n",
      "Step 532213  [5.343 sec/step, loss=0.06543, avg_loss=0.07356]\n",
      "Generated 32 batches of size 32 in 2.776 sec\n",
      "Step 532214  [5.409 sec/step, loss=0.06530, avg_loss=0.07350]\n",
      "Step 532215  [5.404 sec/step, loss=0.07395, avg_loss=0.07350]\n",
      "Step 532216  [5.419 sec/step, loss=0.07476, avg_loss=0.07353]\n",
      "Step 532217  [5.431 sec/step, loss=0.07597, avg_loss=0.07356]\n",
      "Step 532218  [5.412 sec/step, loss=0.07585, avg_loss=0.07359]\n",
      "Step 532219  [5.432 sec/step, loss=0.07557, avg_loss=0.07361]\n",
      "Step 532220  [5.437 sec/step, loss=0.07630, avg_loss=0.07362]\n",
      "Step 532221  [5.424 sec/step, loss=0.07424, avg_loss=0.07363]\n",
      "Step 532222  [5.422 sec/step, loss=0.07523, avg_loss=0.07363]\n",
      "Step 532223  [5.414 sec/step, loss=0.07341, avg_loss=0.07365]\n",
      "Step 532224  [5.417 sec/step, loss=0.07350, avg_loss=0.07365]\n",
      "Step 532225  [5.461 sec/step, loss=0.06594, avg_loss=0.07356]\n",
      "Step 532226  [5.464 sec/step, loss=0.07578, avg_loss=0.07359]\n",
      "Step 532227  [5.470 sec/step, loss=0.07571, avg_loss=0.07360]\n",
      "Step 532228  [5.468 sec/step, loss=0.07308, avg_loss=0.07359]\n",
      "Step 532229  [5.447 sec/step, loss=0.07352, avg_loss=0.07357]\n",
      "Step 532230  [5.444 sec/step, loss=0.07408, avg_loss=0.07355]\n",
      "Step 532231  [5.450 sec/step, loss=0.07426, avg_loss=0.07356]\n",
      "Step 532232  [5.439 sec/step, loss=0.07610, avg_loss=0.07356]\n",
      "Step 532233  [5.455 sec/step, loss=0.07600, avg_loss=0.07359]\n",
      "Step 532234  [5.476 sec/step, loss=0.07326, avg_loss=0.07357]\n",
      "Step 532235  [5.491 sec/step, loss=0.07556, avg_loss=0.07361]\n",
      "Step 532236  [5.513 sec/step, loss=0.07278, avg_loss=0.07359]\n",
      "Step 532237  [5.532 sec/step, loss=0.07360, avg_loss=0.07360]\n",
      "Step 532238  [5.511 sec/step, loss=0.06631, avg_loss=0.07355]\n",
      "Step 532239  [5.516 sec/step, loss=0.07498, avg_loss=0.07356]\n",
      "Step 532240  [5.514 sec/step, loss=0.07174, avg_loss=0.07352]\n",
      "Step 532241  [5.498 sec/step, loss=0.07515, avg_loss=0.07351]\n",
      "Step 532242  [5.517 sec/step, loss=0.07410, avg_loss=0.07360]\n",
      "Step 532243  [5.493 sec/step, loss=0.07409, avg_loss=0.07361]\n",
      "Step 532244  [5.488 sec/step, loss=0.07158, avg_loss=0.07359]\n",
      "Step 532245  [5.447 sec/step, loss=0.07477, avg_loss=0.07367]\n",
      "Generated 32 batches of size 32 in 2.512 sec\n",
      "Step 532246  [5.432 sec/step, loss=0.07049, avg_loss=0.07361]\n",
      "Step 532247  [5.419 sec/step, loss=0.07108, avg_loss=0.07357]\n",
      "Step 532248  [5.421 sec/step, loss=0.07448, avg_loss=0.07357]\n",
      "Step 532249  [5.409 sec/step, loss=0.07453, avg_loss=0.07355]\n",
      "Step 532250  [5.409 sec/step, loss=0.07623, avg_loss=0.07358]\n",
      "Step 532251  [5.402 sec/step, loss=0.07459, avg_loss=0.07360]\n",
      "Step 532252  [5.409 sec/step, loss=0.07628, avg_loss=0.07361]\n",
      "Step 532253  [5.395 sec/step, loss=0.07323, avg_loss=0.07358]\n",
      "Step 532254  [5.402 sec/step, loss=0.07489, avg_loss=0.07360]\n",
      "Step 532255  [5.411 sec/step, loss=0.07170, avg_loss=0.07361]\n",
      "Step 532256  [5.411 sec/step, loss=0.07027, avg_loss=0.07356]\n",
      "Step 532257  [5.403 sec/step, loss=0.07501, avg_loss=0.07357]\n",
      "Step 532258  [5.390 sec/step, loss=0.07355, avg_loss=0.07355]\n",
      "Step 532259  [5.391 sec/step, loss=0.07153, avg_loss=0.07356]\n",
      "Step 532260  [5.389 sec/step, loss=0.07526, avg_loss=0.07357]\n",
      "Step 532261  [5.392 sec/step, loss=0.07587, avg_loss=0.07357]\n",
      "Step 532262  [5.408 sec/step, loss=0.07242, avg_loss=0.07357]\n",
      "Step 532263  [5.416 sec/step, loss=0.07468, avg_loss=0.07357]\n",
      "Step 532264  [5.416 sec/step, loss=0.07490, avg_loss=0.07359]\n",
      "Step 532265  [5.414 sec/step, loss=0.07357, avg_loss=0.07358]\n",
      "Step 532266  [5.394 sec/step, loss=0.07210, avg_loss=0.07354]\n",
      "Step 532267  [5.407 sec/step, loss=0.07603, avg_loss=0.07356]\n",
      "Step 532268  [5.401 sec/step, loss=0.07592, avg_loss=0.07357]\n",
      "Step 532269  [5.408 sec/step, loss=0.07443, avg_loss=0.07358]\n",
      "Step 532270  [5.386 sec/step, loss=0.07409, avg_loss=0.07360]\n",
      "Step 532271  [5.368 sec/step, loss=0.07416, avg_loss=0.07358]\n",
      "Step 532272  [5.358 sec/step, loss=0.07407, avg_loss=0.07359]\n",
      "Step 532273  [5.374 sec/step, loss=0.07594, avg_loss=0.07361]\n",
      "Step 532274  [5.421 sec/step, loss=0.06666, avg_loss=0.07353]\n",
      "Step 532275  [5.415 sec/step, loss=0.07221, avg_loss=0.07350]\n",
      "Step 532276  [5.424 sec/step, loss=0.07252, avg_loss=0.07347]\n",
      "Step 532277  [5.406 sec/step, loss=0.06557, avg_loss=0.07338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 32 batches of size 32 in 2.368 sec\n",
      "Step 532278  [5.431 sec/step, loss=0.07525, avg_loss=0.07345]\n",
      "Step 532279  [5.414 sec/step, loss=0.07548, avg_loss=0.07346]\n",
      "Step 532280  [5.423 sec/step, loss=0.07520, avg_loss=0.07348]\n",
      "Step 532281  [5.430 sec/step, loss=0.07565, avg_loss=0.07348]\n",
      "Step 532282  [5.454 sec/step, loss=0.07390, avg_loss=0.07350]\n",
      "Step 532283  [5.409 sec/step, loss=0.07621, avg_loss=0.07360]\n",
      "Step 532284  [5.438 sec/step, loss=0.07345, avg_loss=0.07359]\n",
      "Step 532285  [5.451 sec/step, loss=0.07397, avg_loss=0.07361]\n",
      "Step 532286  [5.447 sec/step, loss=0.07347, avg_loss=0.07362]\n",
      "Step 532287  [5.438 sec/step, loss=0.07238, avg_loss=0.07361]\n",
      "Step 532288  [5.425 sec/step, loss=0.07506, avg_loss=0.07362]\n",
      "Step 532289  [5.412 sec/step, loss=0.07473, avg_loss=0.07362]\n",
      "Step 532290  [5.403 sec/step, loss=0.07287, avg_loss=0.07361]\n",
      "Step 532291  [5.406 sec/step, loss=0.07547, avg_loss=0.07361]\n",
      "Step 532292  [5.393 sec/step, loss=0.07380, avg_loss=0.07361]\n",
      "Step 532293  [5.386 sec/step, loss=0.07062, avg_loss=0.07359]\n",
      "Step 532294  [5.401 sec/step, loss=0.07546, avg_loss=0.07363]\n",
      "Step 532295  [5.387 sec/step, loss=0.07432, avg_loss=0.07362]\n",
      "Step 532296  [5.406 sec/step, loss=0.07449, avg_loss=0.07367]\n",
      "Step 532297  [5.428 sec/step, loss=0.07260, avg_loss=0.07366]\n",
      "Step 532298  [5.432 sec/step, loss=0.07400, avg_loss=0.07367]\n",
      "Step 532299  [5.428 sec/step, loss=0.07419, avg_loss=0.07366]\n",
      "Step 532300  [5.429 sec/step, loss=0.07438, avg_loss=0.07366]\n",
      "Writing summary at step: 532300\n",
      "Step 532301  [5.409 sec/step, loss=0.07387, avg_loss=0.07365]\n",
      "Step 532302  [5.398 sec/step, loss=0.06738, avg_loss=0.07360]\n",
      "Step 532303  [5.384 sec/step, loss=0.07408, avg_loss=0.07359]\n",
      "Step 532304  [5.379 sec/step, loss=0.07495, avg_loss=0.07359]\n",
      "Step 532305  [5.419 sec/step, loss=0.06747, avg_loss=0.07351]\n",
      "Step 532306  [5.426 sec/step, loss=0.07625, avg_loss=0.07355]\n",
      "Step 532307  [5.419 sec/step, loss=0.07501, avg_loss=0.07354]\n",
      "Step 532308  [5.442 sec/step, loss=0.07322, avg_loss=0.07353]\n",
      "Generated 32 batches of size 32 in 2.426 sec\n",
      "Step 532309  [5.464 sec/step, loss=0.07550, avg_loss=0.07353]\n",
      "Step 532310  [5.443 sec/step, loss=0.07490, avg_loss=0.07356]\n",
      "Step 532311  [5.453 sec/step, loss=0.07482, avg_loss=0.07360]\n",
      "Step 532312  [5.447 sec/step, loss=0.07490, avg_loss=0.07359]\n",
      "Step 532313  [5.474 sec/step, loss=0.07567, avg_loss=0.07370]\n",
      "Step 532314  [5.430 sec/step, loss=0.07594, avg_loss=0.07380]\n",
      "Step 532315  [5.443 sec/step, loss=0.07595, avg_loss=0.07382]\n",
      "Step 532316  [5.428 sec/step, loss=0.07193, avg_loss=0.07380]\n",
      "Step 532317  [5.439 sec/step, loss=0.07334, avg_loss=0.07377]\n",
      "Step 532318  [5.439 sec/step, loss=0.07546, avg_loss=0.07377]\n",
      "Step 532319  [5.426 sec/step, loss=0.07431, avg_loss=0.07375]\n",
      "Step 532320  [5.409 sec/step, loss=0.07263, avg_loss=0.07372]\n",
      "Step 532321  [5.415 sec/step, loss=0.07291, avg_loss=0.07370]\n",
      "Step 532322  [5.408 sec/step, loss=0.07400, avg_loss=0.07369]\n",
      "Step 532323  [5.409 sec/step, loss=0.07376, avg_loss=0.07369]\n",
      "Step 532324  [5.410 sec/step, loss=0.07454, avg_loss=0.07370]\n",
      "Step 532325  [5.410 sec/step, loss=0.06487, avg_loss=0.07369]\n",
      "Step 532326  [5.412 sec/step, loss=0.07474, avg_loss=0.07368]\n",
      "Step 532327  [5.400 sec/step, loss=0.07467, avg_loss=0.07367]\n",
      "Step 532328  [5.401 sec/step, loss=0.07484, avg_loss=0.07369]\n",
      "Step 532329  [5.425 sec/step, loss=0.07471, avg_loss=0.07370]\n",
      "Step 532330  [5.421 sec/step, loss=0.07197, avg_loss=0.07368]\n",
      "Step 532331  [5.435 sec/step, loss=0.07545, avg_loss=0.07369]\n",
      "Step 532332  [5.431 sec/step, loss=0.07433, avg_loss=0.07368]\n",
      "Step 532333  [5.421 sec/step, loss=0.07410, avg_loss=0.07366]\n",
      "Step 532334  [5.422 sec/step, loss=0.07335, avg_loss=0.07366]\n",
      "Step 532335  [5.422 sec/step, loss=0.07574, avg_loss=0.07366]\n",
      "Step 532336  [5.420 sec/step, loss=0.07446, avg_loss=0.07368]\n",
      "Step 532337  [5.419 sec/step, loss=0.07431, avg_loss=0.07368]\n",
      "Step 532338  [5.423 sec/step, loss=0.07030, avg_loss=0.07372]\n",
      "Step 532339  [5.422 sec/step, loss=0.07596, avg_loss=0.07373]\n",
      "Step 532340  [5.421 sec/step, loss=0.07169, avg_loss=0.07373]\n",
      "Generated 32 batches of size 32 in 2.515 sec\n",
      "Step 532341  [5.421 sec/step, loss=0.07364, avg_loss=0.07372]\n",
      "Step 532342  [5.406 sec/step, loss=0.07120, avg_loss=0.07369]\n",
      "Step 532343  [5.402 sec/step, loss=0.07510, avg_loss=0.07370]\n",
      "Step 532344  [5.424 sec/step, loss=0.07489, avg_loss=0.07373]\n",
      "Step 532345  [5.428 sec/step, loss=0.07591, avg_loss=0.07374]\n",
      "Step 532346  [5.434 sec/step, loss=0.07481, avg_loss=0.07379]\n",
      "Step 532347  [5.457 sec/step, loss=0.07583, avg_loss=0.07383]\n",
      "Step 532348  [5.437 sec/step, loss=0.06499, avg_loss=0.07374]\n",
      "Step 532349  [5.435 sec/step, loss=0.07326, avg_loss=0.07373]\n",
      "Step 532350  [5.413 sec/step, loss=0.07411, avg_loss=0.07370]\n",
      "Step 532351  [5.412 sec/step, loss=0.07464, avg_loss=0.07371]\n",
      "Step 532352  [5.408 sec/step, loss=0.07314, avg_loss=0.07367]\n",
      "Step 532353  [5.419 sec/step, loss=0.07527, avg_loss=0.07369]\n",
      "Step 532354  [5.429 sec/step, loss=0.07402, avg_loss=0.07369]\n",
      "Step 532355  [5.431 sec/step, loss=0.07111, avg_loss=0.07368]\n",
      "Step 532356  [5.434 sec/step, loss=0.07519, avg_loss=0.07373]\n",
      "Step 532357  [5.430 sec/step, loss=0.07306, avg_loss=0.07371]\n",
      "Step 532358  [5.437 sec/step, loss=0.07439, avg_loss=0.07372]\n",
      "Step 532359  [5.471 sec/step, loss=0.07353, avg_loss=0.07374]\n",
      "Step 532360  [5.472 sec/step, loss=0.07493, avg_loss=0.07373]\n",
      "Step 532361  [5.467 sec/step, loss=0.07621, avg_loss=0.07374]\n",
      "Step 532362  [5.475 sec/step, loss=0.07659, avg_loss=0.07378]\n",
      "Step 532363  [5.461 sec/step, loss=0.06529, avg_loss=0.07369]\n",
      "Step 532364  [5.451 sec/step, loss=0.07416, avg_loss=0.07368]\n",
      "Step 532365  [5.448 sec/step, loss=0.07378, avg_loss=0.07368]\n",
      "Step 532366  [5.467 sec/step, loss=0.07481, avg_loss=0.07371]\n",
      "Step 532367  [5.459 sec/step, loss=0.07542, avg_loss=0.07370]\n",
      "Step 532368  [5.462 sec/step, loss=0.07575, avg_loss=0.07370]\n",
      "Step 532369  [5.462 sec/step, loss=0.07455, avg_loss=0.07370]\n",
      "Step 532370  [5.444 sec/step, loss=0.07110, avg_loss=0.07367]\n",
      "Step 532371  [5.443 sec/step, loss=0.07121, avg_loss=0.07364]\n",
      "Step 532372  [5.450 sec/step, loss=0.07460, avg_loss=0.07365]\n",
      "Generated 32 batches of size 32 in 2.525 sec\n",
      "Step 532373  [5.438 sec/step, loss=0.07273, avg_loss=0.07361]\n",
      "Step 532374  [5.437 sec/step, loss=0.06654, avg_loss=0.07361]\n",
      "Step 532375  [5.461 sec/step, loss=0.07327, avg_loss=0.07362]\n",
      "Step 532376  [5.434 sec/step, loss=0.07292, avg_loss=0.07363]\n",
      "Step 532377  [5.458 sec/step, loss=0.07406, avg_loss=0.07371]\n",
      "Step 532378  [5.453 sec/step, loss=0.07453, avg_loss=0.07371]\n",
      "Step 532379  [5.453 sec/step, loss=0.07527, avg_loss=0.07370]\n",
      "Step 532380  [5.458 sec/step, loss=0.07386, avg_loss=0.07369]\n",
      "Step 532381  [5.463 sec/step, loss=0.07638, avg_loss=0.07370]\n",
      "Step 532382  [5.462 sec/step, loss=0.07546, avg_loss=0.07371]\n",
      "Step 532383  [5.448 sec/step, loss=0.07251, avg_loss=0.07368]\n",
      "Step 532384  [5.418 sec/step, loss=0.07141, avg_loss=0.07366]\n",
      "Step 532385  [5.415 sec/step, loss=0.07584, avg_loss=0.07367]\n",
      "Step 532386  [5.421 sec/step, loss=0.07398, avg_loss=0.07368]\n",
      "Step 532387  [5.416 sec/step, loss=0.07383, avg_loss=0.07369]\n",
      "Step 532388  [5.428 sec/step, loss=0.07320, avg_loss=0.07368]\n",
      "Step 532389  [5.430 sec/step, loss=0.07513, avg_loss=0.07368]\n",
      "Step 532390  [5.425 sec/step, loss=0.07071, avg_loss=0.07366]\n",
      "Step 532391  [5.409 sec/step, loss=0.07325, avg_loss=0.07364]\n",
      "Step 532392  [5.427 sec/step, loss=0.07563, avg_loss=0.07365]\n",
      "Step 532393  [5.431 sec/step, loss=0.07356, avg_loss=0.07368]\n",
      "Step 532394  [5.468 sec/step, loss=0.06566, avg_loss=0.07359]\n",
      "Step 532395  [5.482 sec/step, loss=0.07451, avg_loss=0.07359]\n",
      "Step 532396  [5.490 sec/step, loss=0.07534, avg_loss=0.07360]\n",
      "Step 532397  [5.505 sec/step, loss=0.07483, avg_loss=0.07362]\n",
      "Step 532398  [5.510 sec/step, loss=0.07531, avg_loss=0.07363]\n",
      "Step 532399  [5.507 sec/step, loss=0.07463, avg_loss=0.07364]\n",
      "Step 532400  [5.504 sec/step, loss=0.07526, avg_loss=0.07364]\n",
      "Writing summary at step: 532400\n",
      "Step 532401  [5.513 sec/step, loss=0.07483, avg_loss=0.07365]\n",
      "Step 532402  [5.531 sec/step, loss=0.07496, avg_loss=0.07373]\n",
      "Step 532403  [5.541 sec/step, loss=0.07398, avg_loss=0.07373]\n",
      "Generated 32 batches of size 32 in 2.398 sec\n",
      "Step 532404  [5.554 sec/step, loss=0.07597, avg_loss=0.07374]\n",
      "Step 532405  [5.490 sec/step, loss=0.07156, avg_loss=0.07378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 532406  [5.483 sec/step, loss=0.07466, avg_loss=0.07376]\n",
      "Step 532407  [5.487 sec/step, loss=0.07387, avg_loss=0.07375]\n",
      "Step 532408  [5.460 sec/step, loss=0.07315, avg_loss=0.07375]\n",
      "Step 532409  [5.446 sec/step, loss=0.07310, avg_loss=0.07373]\n",
      "Step 532410  [5.445 sec/step, loss=0.07438, avg_loss=0.07372]\n",
      "Step 532411  [5.462 sec/step, loss=0.07612, avg_loss=0.07374]\n",
      "Step 532412  [5.445 sec/step, loss=0.06561, avg_loss=0.07364]\n",
      "Step 532413  [5.444 sec/step, loss=0.07543, avg_loss=0.07364]\n",
      "Step 532414  [5.425 sec/step, loss=0.07420, avg_loss=0.07362]\n",
      "Step 532415  [5.414 sec/step, loss=0.07521, avg_loss=0.07362]\n",
      "Step 532416  [5.424 sec/step, loss=0.07293, avg_loss=0.07363]\n",
      "Step 532417  [5.390 sec/step, loss=0.07201, avg_loss=0.07361]\n",
      "Step 532418  [5.439 sec/step, loss=0.06693, avg_loss=0.07353]\n",
      "Step 532419  [5.441 sec/step, loss=0.07525, avg_loss=0.07354]\n",
      "Step 532420  [5.449 sec/step, loss=0.07414, avg_loss=0.07355]\n",
      "Step 532421  [5.448 sec/step, loss=0.07121, avg_loss=0.07353]\n",
      "Step 532422  [5.442 sec/step, loss=0.06722, avg_loss=0.07347]\n",
      "Step 532423  [5.451 sec/step, loss=0.07124, avg_loss=0.07344]\n",
      "Step 532424  [5.465 sec/step, loss=0.07351, avg_loss=0.07343]\n",
      "Step 532425  [5.433 sec/step, loss=0.07518, avg_loss=0.07353]\n",
      "Step 532426  [5.419 sec/step, loss=0.07204, avg_loss=0.07351]\n",
      "Step 532427  [5.414 sec/step, loss=0.07500, avg_loss=0.07351]\n",
      "Step 532428  [5.428 sec/step, loss=0.07637, avg_loss=0.07353]\n",
      "Step 532429  [5.422 sec/step, loss=0.07551, avg_loss=0.07353]\n",
      "Step 532430  [5.414 sec/step, loss=0.07373, avg_loss=0.07355]\n",
      "Step 532431  [5.428 sec/step, loss=0.07342, avg_loss=0.07353]\n",
      "Step 532432  [5.428 sec/step, loss=0.07386, avg_loss=0.07353]\n",
      "Step 532433  [5.429 sec/step, loss=0.07377, avg_loss=0.07352]\n",
      "Step 532434  [5.429 sec/step, loss=0.07564, avg_loss=0.07355]\n",
      "Step 532435  [5.420 sec/step, loss=0.07493, avg_loss=0.07354]\n",
      "Generated 32 batches of size 32 in 2.411 sec\n",
      "Step 532436  [5.408 sec/step, loss=0.07596, avg_loss=0.07355]\n",
      "Step 532437  [5.392 sec/step, loss=0.07504, avg_loss=0.07356]\n",
      "Step 532438  [5.417 sec/step, loss=0.07362, avg_loss=0.07359]\n",
      "Step 532439  [5.409 sec/step, loss=0.07542, avg_loss=0.07359]\n",
      "Step 532440  [5.413 sec/step, loss=0.07539, avg_loss=0.07362]\n",
      "Step 532441  [5.412 sec/step, loss=0.07350, avg_loss=0.07362]\n",
      "Step 532442  [5.437 sec/step, loss=0.07466, avg_loss=0.07366]\n",
      "Step 532443  [5.439 sec/step, loss=0.07412, avg_loss=0.07365]\n",
      "Step 532444  [5.421 sec/step, loss=0.07045, avg_loss=0.07360]\n",
      "Step 532445  [5.405 sec/step, loss=0.07394, avg_loss=0.07358]\n",
      "Step 532446  [5.414 sec/step, loss=0.07612, avg_loss=0.07360]\n",
      "Step 532447  [5.402 sec/step, loss=0.07496, avg_loss=0.07359]\n",
      "Step 532448  [5.435 sec/step, loss=0.07552, avg_loss=0.07369]\n",
      "Step 532449  [5.438 sec/step, loss=0.07370, avg_loss=0.07370]\n",
      "Step 532450  [5.455 sec/step, loss=0.07448, avg_loss=0.07370]\n",
      "Step 532451  [5.464 sec/step, loss=0.07415, avg_loss=0.07370]\n",
      "Step 532452  [5.480 sec/step, loss=0.07215, avg_loss=0.07369]\n",
      "Step 532453  [5.472 sec/step, loss=0.07311, avg_loss=0.07367]\n",
      "Step 532454  [5.454 sec/step, loss=0.07454, avg_loss=0.07367]\n",
      "Step 532455  [5.443 sec/step, loss=0.07025, avg_loss=0.07366]\n",
      "Step 532456  [5.493 sec/step, loss=0.06555, avg_loss=0.07357]\n",
      "Step 532457  [5.506 sec/step, loss=0.07607, avg_loss=0.07360]\n",
      "Step 532458  [5.504 sec/step, loss=0.07102, avg_loss=0.07356]\n",
      "Step 532459  [5.483 sec/step, loss=0.07495, avg_loss=0.07358]\n",
      "Step 532460  [5.470 sec/step, loss=0.06476, avg_loss=0.07347]\n",
      "Step 532461  [5.465 sec/step, loss=0.07493, avg_loss=0.07346]\n",
      "Step 532462  [5.457 sec/step, loss=0.07520, avg_loss=0.07345]\n",
      "Step 532463  [5.473 sec/step, loss=0.07456, avg_loss=0.07354]\n",
      "Step 532464  [5.484 sec/step, loss=0.07443, avg_loss=0.07354]\n",
      "Step 532465  [5.477 sec/step, loss=0.06958, avg_loss=0.07350]\n",
      "Step 532466  [5.466 sec/step, loss=0.07379, avg_loss=0.07349]\n",
      "Step 532467  [5.461 sec/step, loss=0.07220, avg_loss=0.07346]\n",
      "Generated 32 batches of size 32 in 2.634 sec\n",
      "Step 532468  [5.449 sec/step, loss=0.07203, avg_loss=0.07342]\n",
      "Step 532469  [5.439 sec/step, loss=0.07382, avg_loss=0.07341]\n",
      "Step 532470  [5.462 sec/step, loss=0.07591, avg_loss=0.07346]\n",
      "Step 532471  [5.478 sec/step, loss=0.07636, avg_loss=0.07351]\n",
      "Step 532472  [5.484 sec/step, loss=0.07298, avg_loss=0.07350]\n",
      "Step 532473  [5.499 sec/step, loss=0.07529, avg_loss=0.07352]\n",
      "Step 532474  [5.451 sec/step, loss=0.07487, avg_loss=0.07361]\n",
      "Step 532475  [5.429 sec/step, loss=0.07467, avg_loss=0.07362]\n",
      "Step 532476  [5.448 sec/step, loss=0.07567, avg_loss=0.07365]\n",
      "Step 532477  [5.450 sec/step, loss=0.07425, avg_loss=0.07365]\n",
      "Step 532478  [5.455 sec/step, loss=0.07457, avg_loss=0.07365]\n",
      "Step 532479  [5.454 sec/step, loss=0.07512, avg_loss=0.07365]\n",
      "Step 532480  [5.443 sec/step, loss=0.07141, avg_loss=0.07362]\n",
      "Step 532481  [5.418 sec/step, loss=0.07068, avg_loss=0.07357]\n",
      "Step 532482  [5.403 sec/step, loss=0.07472, avg_loss=0.07356]\n",
      "Step 532483  [5.455 sec/step, loss=0.06629, avg_loss=0.07350]\n",
      "Step 532484  [5.463 sec/step, loss=0.07491, avg_loss=0.07353]\n",
      "Step 532485  [5.462 sec/step, loss=0.07629, avg_loss=0.07354]\n",
      "Step 532486  [5.476 sec/step, loss=0.07588, avg_loss=0.07356]\n",
      "Step 532487  [5.476 sec/step, loss=0.07376, avg_loss=0.07356]\n",
      "Step 532488  [5.472 sec/step, loss=0.07463, avg_loss=0.07357]\n",
      "Step 532489  [5.454 sec/step, loss=0.07083, avg_loss=0.07353]\n",
      "Step 532490  [5.459 sec/step, loss=0.07412, avg_loss=0.07356]\n",
      "Step 532491  [5.479 sec/step, loss=0.07300, avg_loss=0.07356]\n",
      "Step 532492  [5.466 sec/step, loss=0.07318, avg_loss=0.07353]\n",
      "Step 532493  [5.477 sec/step, loss=0.07428, avg_loss=0.07354]\n",
      "Step 532494  [5.447 sec/step, loss=0.07243, avg_loss=0.07361]\n",
      "Step 532495  [5.429 sec/step, loss=0.07024, avg_loss=0.07357]\n",
      "Step 532496  [5.421 sec/step, loss=0.07556, avg_loss=0.07357]\n",
      "Step 532497  [5.376 sec/step, loss=0.06527, avg_loss=0.07347]\n",
      "Step 532498  [5.377 sec/step, loss=0.07506, avg_loss=0.07347]\n",
      "Step 532499  [5.380 sec/step, loss=0.07566, avg_loss=0.07348]\n",
      "Generated 32 batches of size 32 in 2.392 sec\n",
      "Step 532500  [5.402 sec/step, loss=0.07643, avg_loss=0.07349]\n",
      "Writing summary at step: 532500\n",
      "Step 532501  [5.431 sec/step, loss=0.07277, avg_loss=0.07347]\n",
      "Step 532502  [5.431 sec/step, loss=0.07500, avg_loss=0.07347]\n",
      "Step 532503  [5.418 sec/step, loss=0.07344, avg_loss=0.07347]\n",
      "Step 532504  [5.402 sec/step, loss=0.07508, avg_loss=0.07346]\n",
      "Step 532505  [5.411 sec/step, loss=0.07223, avg_loss=0.07346]\n",
      "Step 532506  [5.411 sec/step, loss=0.07463, avg_loss=0.07346]\n",
      "Step 532507  [5.396 sec/step, loss=0.07274, avg_loss=0.07345]\n",
      "Step 532508  [5.399 sec/step, loss=0.07399, avg_loss=0.07346]\n",
      "Step 532509  [5.395 sec/step, loss=0.07163, avg_loss=0.07345]\n",
      "Step 532510  [5.398 sec/step, loss=0.07433, avg_loss=0.07345]\n",
      "Step 532511  [5.391 sec/step, loss=0.07581, avg_loss=0.07344]\n",
      "Step 532512  [5.400 sec/step, loss=0.07337, avg_loss=0.07352]\n",
      "Step 532513  [5.387 sec/step, loss=0.07193, avg_loss=0.07348]\n",
      "Step 532514  [5.412 sec/step, loss=0.07553, avg_loss=0.07350]\n",
      "Step 532515  [5.403 sec/step, loss=0.07371, avg_loss=0.07348]\n",
      "Step 532516  [5.417 sec/step, loss=0.07324, avg_loss=0.07349]\n",
      "Step 532517  [5.432 sec/step, loss=0.07481, avg_loss=0.07351]\n",
      "Step 532518  [5.432 sec/step, loss=0.06627, avg_loss=0.07351]\n",
      "Step 532519  [5.453 sec/step, loss=0.07266, avg_loss=0.07348]\n",
      "Step 532520  [5.454 sec/step, loss=0.07540, avg_loss=0.07349]\n",
      "Step 532521  [5.442 sec/step, loss=0.07180, avg_loss=0.07350]\n",
      "Step 532522  [5.443 sec/step, loss=0.06628, avg_loss=0.07349]\n",
      "Step 532523  [5.457 sec/step, loss=0.07507, avg_loss=0.07353]\n",
      "Step 532524  [5.456 sec/step, loss=0.07506, avg_loss=0.07354]\n",
      "Step 532525  [5.440 sec/step, loss=0.07570, avg_loss=0.07355]\n",
      "Step 532526  [5.439 sec/step, loss=0.07092, avg_loss=0.07354]\n",
      "Step 532527  [5.443 sec/step, loss=0.07428, avg_loss=0.07353]\n",
      "Step 532528  [5.426 sec/step, loss=0.07346, avg_loss=0.07350]\n",
      "Step 532529  [5.414 sec/step, loss=0.07382, avg_loss=0.07349]\n",
      "Step 532530  [5.433 sec/step, loss=0.07602, avg_loss=0.07351]\n",
      "Generated 32 batches of size 32 in 2.368 sec\n",
      "Step 532531  [5.421 sec/step, loss=0.07570, avg_loss=0.07353]\n",
      "Step 532532  [5.414 sec/step, loss=0.07368, avg_loss=0.07353]\n",
      "Step 532533  [5.424 sec/step, loss=0.07593, avg_loss=0.07355]\n",
      "Step 532534  [5.410 sec/step, loss=0.07518, avg_loss=0.07355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 532535  [5.408 sec/step, loss=0.07457, avg_loss=0.07354]\n",
      "Step 532536  [5.393 sec/step, loss=0.07509, avg_loss=0.07353]\n",
      "Step 532537  [5.386 sec/step, loss=0.07380, avg_loss=0.07352]\n",
      "Step 532538  [5.387 sec/step, loss=0.07616, avg_loss=0.07355]\n",
      "Step 532539  [5.391 sec/step, loss=0.07477, avg_loss=0.07354]\n",
      "Step 532540  [5.391 sec/step, loss=0.07190, avg_loss=0.07351]\n",
      "Step 532541  [5.390 sec/step, loss=0.07351, avg_loss=0.07351]\n",
      "Step 532542  [5.371 sec/step, loss=0.07411, avg_loss=0.07350]\n",
      "Step 532543  [5.380 sec/step, loss=0.07593, avg_loss=0.07352]\n",
      "Step 532544  [5.374 sec/step, loss=0.07172, avg_loss=0.07353]\n",
      "Step 532545  [5.375 sec/step, loss=0.07520, avg_loss=0.07354]\n",
      "Step 532546  [5.413 sec/step, loss=0.06599, avg_loss=0.07344]\n",
      "Step 532547  [5.416 sec/step, loss=0.07448, avg_loss=0.07344]\n",
      "Step 532548  [5.403 sec/step, loss=0.07492, avg_loss=0.07343]\n",
      "Step 532549  [5.406 sec/step, loss=0.07479, avg_loss=0.07344]\n",
      "Step 532550  [5.411 sec/step, loss=0.07416, avg_loss=0.07344]\n",
      "Step 532551  [5.405 sec/step, loss=0.07359, avg_loss=0.07343]\n",
      "Step 532552  [5.380 sec/step, loss=0.07586, avg_loss=0.07347]\n",
      "Step 532553  [5.379 sec/step, loss=0.07282, avg_loss=0.07347]\n",
      "Step 532554  [5.375 sec/step, loss=0.07362, avg_loss=0.07346]\n",
      "Step 532555  [5.380 sec/step, loss=0.07030, avg_loss=0.07346]\n",
      "Step 532556  [5.343 sec/step, loss=0.07539, avg_loss=0.07356]\n",
      "Step 532557  [5.335 sec/step, loss=0.07491, avg_loss=0.07355]\n",
      "Step 532558  [5.334 sec/step, loss=0.07310, avg_loss=0.07357]\n",
      "Step 532559  [5.339 sec/step, loss=0.07473, avg_loss=0.07356]\n",
      "Step 532560  [5.346 sec/step, loss=0.07168, avg_loss=0.07363]\n",
      "Step 532561  [5.349 sec/step, loss=0.07620, avg_loss=0.07365]\n",
      "Step 532562  [5.351 sec/step, loss=0.07479, avg_loss=0.07364]\n",
      "Generated 32 batches of size 32 in 2.402 sec\n",
      "Step 532563  [5.367 sec/step, loss=0.07582, avg_loss=0.07366]\n",
      "Step 532564  [5.363 sec/step, loss=0.07432, avg_loss=0.07365]\n",
      "Step 532565  [5.374 sec/step, loss=0.07501, avg_loss=0.07371]\n",
      "Step 532566  [5.380 sec/step, loss=0.07478, avg_loss=0.07372]\n",
      "Step 532567  [5.369 sec/step, loss=0.06634, avg_loss=0.07366]\n",
      "Step 532568  [5.389 sec/step, loss=0.07367, avg_loss=0.07368]\n",
      "Step 532569  [5.427 sec/step, loss=0.07321, avg_loss=0.07367]\n",
      "Step 532570  [5.429 sec/step, loss=0.07306, avg_loss=0.07364]\n",
      "Step 532571  [5.438 sec/step, loss=0.07642, avg_loss=0.07364]\n",
      "Step 532572  [5.454 sec/step, loss=0.07263, avg_loss=0.07364]\n",
      "Step 532573  [5.442 sec/step, loss=0.07236, avg_loss=0.07361]\n",
      "Step 532574  [5.451 sec/step, loss=0.07585, avg_loss=0.07362]\n",
      "Step 532575  [5.444 sec/step, loss=0.07368, avg_loss=0.07361]\n",
      "Step 532576  [5.431 sec/step, loss=0.07460, avg_loss=0.07360]\n",
      "Step 532577  [5.410 sec/step, loss=0.07201, avg_loss=0.07358]\n",
      "Step 532578  [5.399 sec/step, loss=0.07195, avg_loss=0.07355]\n",
      "Step 532579  [5.412 sec/step, loss=0.07621, avg_loss=0.07356]\n",
      "Step 532580  [5.409 sec/step, loss=0.07499, avg_loss=0.07360]\n",
      "Step 532581  [5.425 sec/step, loss=0.07585, avg_loss=0.07365]\n",
      "Step 532582  [5.436 sec/step, loss=0.07488, avg_loss=0.07365]\n",
      "Step 532583  [5.387 sec/step, loss=0.07450, avg_loss=0.07373]\n",
      "Step 532584  [5.377 sec/step, loss=0.07322, avg_loss=0.07372]\n",
      "Step 532585  [5.353 sec/step, loss=0.06523, avg_loss=0.07360]\n",
      "Step 532586  [5.353 sec/step, loss=0.07550, avg_loss=0.07360]\n",
      "Step 532587  [5.375 sec/step, loss=0.07584, avg_loss=0.07362]\n",
      "Step 532588  [5.381 sec/step, loss=0.07341, avg_loss=0.07361]\n",
      "Step 532589  [5.395 sec/step, loss=0.07545, avg_loss=0.07366]\n",
      "Step 532590  [5.406 sec/step, loss=0.07495, avg_loss=0.07366]\n",
      "Step 532591  [5.393 sec/step, loss=0.07420, avg_loss=0.07368]\n",
      "Step 532592  [5.396 sec/step, loss=0.07520, avg_loss=0.07370]\n",
      "Step 532593  [5.401 sec/step, loss=0.07669, avg_loss=0.07372]\n",
      "Step 532594  [5.367 sec/step, loss=0.07222, avg_loss=0.07372]\n",
      "Generated 32 batches of size 32 in 2.416 sec\n",
      "Step 532595  [5.379 sec/step, loss=0.07282, avg_loss=0.07374]\n",
      "Step 532596  [5.362 sec/step, loss=0.07365, avg_loss=0.07372]\n",
      "Step 532597  [5.387 sec/step, loss=0.07261, avg_loss=0.07380]\n",
      "Step 532598  [5.395 sec/step, loss=0.07592, avg_loss=0.07381]\n",
      "Step 532599  [5.382 sec/step, loss=0.07396, avg_loss=0.07379]\n",
      "Step 532600  [5.353 sec/step, loss=0.07422, avg_loss=0.07377]\n",
      "Writing summary at step: 532600\n",
      "Step 532601  [5.369 sec/step, loss=0.06859, avg_loss=0.07373]\n",
      "Step 532602  [5.372 sec/step, loss=0.07560, avg_loss=0.07373]\n",
      "Step 532603  [5.384 sec/step, loss=0.07624, avg_loss=0.07376]\n",
      "Step 532604  [5.386 sec/step, loss=0.07411, avg_loss=0.07375]\n",
      "Step 532605  [5.398 sec/step, loss=0.07589, avg_loss=0.07379]\n",
      "Step 532606  [5.401 sec/step, loss=0.07319, avg_loss=0.07377]\n",
      "Step 532607  [5.395 sec/step, loss=0.07122, avg_loss=0.07376]\n",
      "Step 532608  [5.441 sec/step, loss=0.06692, avg_loss=0.07369]\n",
      "Step 532609  [5.449 sec/step, loss=0.07373, avg_loss=0.07371]\n",
      "Step 532610  [5.436 sec/step, loss=0.07364, avg_loss=0.07370]\n",
      "Step 532611  [5.430 sec/step, loss=0.07528, avg_loss=0.07370]\n",
      "Step 532612  [5.439 sec/step, loss=0.07318, avg_loss=0.07369]\n",
      "Step 532613  [5.470 sec/step, loss=0.07212, avg_loss=0.07370]\n",
      "Step 532614  [5.454 sec/step, loss=0.07506, avg_loss=0.07369]\n",
      "Step 532615  [5.466 sec/step, loss=0.07461, avg_loss=0.07370]\n",
      "Step 532616  [5.464 sec/step, loss=0.07580, avg_loss=0.07373]\n",
      "Step 532617  [5.477 sec/step, loss=0.07540, avg_loss=0.07373]\n",
      "Step 532618  [5.439 sec/step, loss=0.07353, avg_loss=0.07380]\n",
      "Step 532619  [5.430 sec/step, loss=0.07493, avg_loss=0.07383]\n",
      "Step 532620  [5.437 sec/step, loss=0.07582, avg_loss=0.07383]\n",
      "Step 532621  [5.445 sec/step, loss=0.07360, avg_loss=0.07385]\n",
      "Step 532622  [5.451 sec/step, loss=0.07384, avg_loss=0.07392]\n",
      "Step 532623  [5.433 sec/step, loss=0.07511, avg_loss=0.07392]\n",
      "Step 532624  [5.406 sec/step, loss=0.06693, avg_loss=0.07384]\n",
      "Step 532625  [5.400 sec/step, loss=0.07151, avg_loss=0.07380]\n",
      "Generated 32 batches of size 32 in 2.501 sec\n",
      "Step 532626  [5.408 sec/step, loss=0.07291, avg_loss=0.07382]\n",
      "Step 532627  [5.403 sec/step, loss=0.07292, avg_loss=0.07381]\n",
      "Step 532628  [5.418 sec/step, loss=0.07609, avg_loss=0.07383]\n",
      "Step 532629  [5.417 sec/step, loss=0.07535, avg_loss=0.07385]\n",
      "Step 532630  [5.406 sec/step, loss=0.07026, avg_loss=0.07379]\n",
      "Step 532631  [5.396 sec/step, loss=0.07522, avg_loss=0.07379]\n",
      "Step 532632  [5.413 sec/step, loss=0.07469, avg_loss=0.07380]\n",
      "Step 532633  [5.405 sec/step, loss=0.07438, avg_loss=0.07378]\n",
      "Step 532634  [5.395 sec/step, loss=0.06912, avg_loss=0.07372]\n",
      "Step 532635  [5.386 sec/step, loss=0.07423, avg_loss=0.07372]\n",
      "Step 532636  [5.393 sec/step, loss=0.07395, avg_loss=0.07371]\n",
      "Step 532637  [5.405 sec/step, loss=0.07473, avg_loss=0.07372]\n",
      "Step 532638  [5.390 sec/step, loss=0.07438, avg_loss=0.07370]\n",
      "Step 532639  [5.387 sec/step, loss=0.07483, avg_loss=0.07370]\n",
      "Step 532640  [5.389 sec/step, loss=0.07478, avg_loss=0.07373]\n",
      "Step 532641  [5.396 sec/step, loss=0.07543, avg_loss=0.07375]\n",
      "Step 532642  [5.416 sec/step, loss=0.07420, avg_loss=0.07375]\n",
      "Step 532643  [5.412 sec/step, loss=0.07284, avg_loss=0.07372]\n",
      "Step 532644  [5.418 sec/step, loss=0.07251, avg_loss=0.07372]\n",
      "Step 532645  [5.411 sec/step, loss=0.07386, avg_loss=0.07371]\n",
      "Step 532646  [5.413 sec/step, loss=0.06670, avg_loss=0.07372]\n",
      "Step 532647  [5.425 sec/step, loss=0.07583, avg_loss=0.07373]\n",
      "Step 532648  [5.421 sec/step, loss=0.07180, avg_loss=0.07370]\n",
      "Step 532649  [5.428 sec/step, loss=0.07601, avg_loss=0.07371]\n",
      "Step 532650  [5.424 sec/step, loss=0.07563, avg_loss=0.07373]\n",
      "Step 532651  [5.447 sec/step, loss=0.07316, avg_loss=0.07372]\n",
      "Step 532652  [5.443 sec/step, loss=0.07221, avg_loss=0.07369]\n",
      "Step 532653  [5.465 sec/step, loss=0.07317, avg_loss=0.07369]\n",
      "Step 532654  [5.486 sec/step, loss=0.07661, avg_loss=0.07372]\n",
      "Step 532655  [5.479 sec/step, loss=0.06443, avg_loss=0.07366]\n",
      "Step 532656  [5.480 sec/step, loss=0.07566, avg_loss=0.07366]\n",
      "Step 532657  [5.472 sec/step, loss=0.07486, avg_loss=0.07366]\n",
      "Generated 32 batches of size 32 in 2.442 sec\n",
      "Step 532658  [5.482 sec/step, loss=0.07526, avg_loss=0.07369]\n",
      "Step 532659  [5.473 sec/step, loss=0.07441, avg_loss=0.07368]\n",
      "Step 532660  [5.479 sec/step, loss=0.07298, avg_loss=0.07370]\n",
      "Step 532661  [5.483 sec/step, loss=0.07341, avg_loss=0.07367]\n",
      "Step 532662  [5.490 sec/step, loss=0.07406, avg_loss=0.07366]\n",
      "Step 532663  [5.471 sec/step, loss=0.07300, avg_loss=0.07363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 532664  [5.478 sec/step, loss=0.07573, avg_loss=0.07365]\n",
      "Step 532665  [5.463 sec/step, loss=0.07087, avg_loss=0.07360]\n",
      "Step 532666  [5.453 sec/step, loss=0.06978, avg_loss=0.07355]\n",
      "Step 532667  [5.468 sec/step, loss=0.07208, avg_loss=0.07361]\n",
      "Step 532668  [5.454 sec/step, loss=0.07289, avg_loss=0.07360]\n",
      "Step 532669  [5.441 sec/step, loss=0.07448, avg_loss=0.07362]\n",
      "Step 532670  [5.432 sec/step, loss=0.07446, avg_loss=0.07363]\n",
      "Step 532671  [5.428 sec/step, loss=0.07574, avg_loss=0.07362]\n",
      "Step 532672  [5.399 sec/step, loss=0.07477, avg_loss=0.07365]\n",
      "Step 532673  [5.387 sec/step, loss=0.07179, avg_loss=0.07364]\n",
      "Step 532674  [5.375 sec/step, loss=0.07383, avg_loss=0.07362]\n",
      "Step 532675  [5.406 sec/step, loss=0.07192, avg_loss=0.07360]\n",
      "Step 532676  [5.407 sec/step, loss=0.07243, avg_loss=0.07358]\n",
      "Step 532677  [5.422 sec/step, loss=0.07467, avg_loss=0.07361]\n",
      "Step 532678  [5.475 sec/step, loss=0.06528, avg_loss=0.07354]\n",
      "Step 532679  [5.464 sec/step, loss=0.07510, avg_loss=0.07353]\n",
      "Step 532680  [5.458 sec/step, loss=0.07422, avg_loss=0.07352]\n",
      "Step 532681  [5.453 sec/step, loss=0.07491, avg_loss=0.07351]\n",
      "Step 532682  [5.442 sec/step, loss=0.07499, avg_loss=0.07351]\n",
      "Step 532683  [5.447 sec/step, loss=0.07353, avg_loss=0.07350]\n",
      "Step 532684  [5.438 sec/step, loss=0.07132, avg_loss=0.07348]\n",
      "Step 532685  [5.456 sec/step, loss=0.07461, avg_loss=0.07358]\n",
      "Step 532686  [5.449 sec/step, loss=0.07592, avg_loss=0.07358]\n",
      "Step 532687  [5.439 sec/step, loss=0.07544, avg_loss=0.07358]\n",
      "Step 532688  [5.424 sec/step, loss=0.07343, avg_loss=0.07358]\n",
      "Step 532689  [5.432 sec/step, loss=0.07590, avg_loss=0.07358]\n",
      "Generated 32 batches of size 32 in 2.487 sec\n",
      "Step 532690  [5.450 sec/step, loss=0.07587, avg_loss=0.07359]\n",
      "Step 532691  [5.459 sec/step, loss=0.07606, avg_loss=0.07361]\n",
      "Step 532692  [5.440 sec/step, loss=0.06716, avg_loss=0.07353]\n",
      "Step 532693  [5.423 sec/step, loss=0.07298, avg_loss=0.07349]\n",
      "Step 532694  [5.448 sec/step, loss=0.07587, avg_loss=0.07353]\n",
      "Step 532695  [5.454 sec/step, loss=0.07584, avg_loss=0.07356]\n",
      "Step 532696  [5.461 sec/step, loss=0.07097, avg_loss=0.07353]\n",
      "Step 532697  [5.464 sec/step, loss=0.07567, avg_loss=0.07356]\n",
      "Step 532698  [5.444 sec/step, loss=0.07390, avg_loss=0.07354]\n",
      "Step 532699  [5.458 sec/step, loss=0.07507, avg_loss=0.07355]\n",
      "Step 532700  [5.474 sec/step, loss=0.07466, avg_loss=0.07356]\n",
      "Writing summary at step: 532700\n",
      "Step 532701  [5.420 sec/step, loss=0.07151, avg_loss=0.07359]\n",
      "Step 532702  [5.401 sec/step, loss=0.07255, avg_loss=0.07356]\n",
      "Step 532703  [5.390 sec/step, loss=0.07458, avg_loss=0.07354]\n",
      "Step 532704  [5.382 sec/step, loss=0.07027, avg_loss=0.07350]\n",
      "Step 532705  [5.390 sec/step, loss=0.07585, avg_loss=0.07350]\n",
      "Step 532706  [5.390 sec/step, loss=0.07475, avg_loss=0.07352]\n",
      "Step 532707  [5.399 sec/step, loss=0.07013, avg_loss=0.07351]\n",
      "Step 532708  [5.360 sec/step, loss=0.07275, avg_loss=0.07357]\n",
      "Step 532709  [5.351 sec/step, loss=0.07467, avg_loss=0.07357]\n",
      "Step 532710  [5.352 sec/step, loss=0.07394, avg_loss=0.07358]\n",
      "Step 532711  [5.363 sec/step, loss=0.07339, avg_loss=0.07356]\n",
      "Step 532712  [5.370 sec/step, loss=0.07621, avg_loss=0.07359]\n",
      "Step 532713  [5.336 sec/step, loss=0.07353, avg_loss=0.07360]\n",
      "Step 532714  [5.337 sec/step, loss=0.07564, avg_loss=0.07361]\n",
      "Step 532715  [5.332 sec/step, loss=0.07441, avg_loss=0.07361]\n",
      "Step 532716  [5.341 sec/step, loss=0.07525, avg_loss=0.07360]\n",
      "Step 532717  [5.326 sec/step, loss=0.07435, avg_loss=0.07359]\n",
      "Step 532718  [5.312 sec/step, loss=0.07289, avg_loss=0.07358]\n",
      "Step 532719  [5.319 sec/step, loss=0.07295, avg_loss=0.07356]\n",
      "Step 532720  [5.306 sec/step, loss=0.07127, avg_loss=0.07352]\n",
      "Generated 32 batches of size 32 in 2.422 sec\n",
      "Step 532721  [5.319 sec/step, loss=0.07262, avg_loss=0.07351]\n",
      "Step 532722  [5.312 sec/step, loss=0.06659, avg_loss=0.07344]\n",
      "Step 532723  [5.325 sec/step, loss=0.07563, avg_loss=0.07344]\n",
      "Step 532724  [5.394 sec/step, loss=0.06576, avg_loss=0.07343]\n",
      "Step 532725  [5.403 sec/step, loss=0.07223, avg_loss=0.07344]\n",
      "Step 532726  [5.420 sec/step, loss=0.07563, avg_loss=0.07346]\n",
      "Step 532727  [5.418 sec/step, loss=0.07337, avg_loss=0.07347]\n",
      "Step 532728  [5.413 sec/step, loss=0.07509, avg_loss=0.07346]\n",
      "Step 532729  [5.425 sec/step, loss=0.07635, avg_loss=0.07347]\n",
      "Step 532730  [5.432 sec/step, loss=0.07427, avg_loss=0.07351]\n",
      "Step 532731  [5.419 sec/step, loss=0.07307, avg_loss=0.07349]\n",
      "Step 532732  [5.432 sec/step, loss=0.07317, avg_loss=0.07347]\n",
      "Step 532733  [5.421 sec/step, loss=0.07408, avg_loss=0.07347]\n",
      "Step 532734  [5.429 sec/step, loss=0.07344, avg_loss=0.07351]\n",
      "Step 532735  [5.442 sec/step, loss=0.07518, avg_loss=0.07352]\n",
      "Step 532736  [5.426 sec/step, loss=0.07045, avg_loss=0.07349]\n",
      "Step 532737  [5.422 sec/step, loss=0.07462, avg_loss=0.07349]\n",
      "Step 532738  [5.438 sec/step, loss=0.07601, avg_loss=0.07350]\n",
      "Step 532739  [5.442 sec/step, loss=0.07504, avg_loss=0.07350]\n",
      "Step 532740  [5.450 sec/step, loss=0.07563, avg_loss=0.07351]\n",
      "Step 532741  [5.432 sec/step, loss=0.06452, avg_loss=0.07340]\n",
      "Step 532742  [5.431 sec/step, loss=0.07454, avg_loss=0.07341]\n",
      "Step 532743  [5.432 sec/step, loss=0.07583, avg_loss=0.07344]\n",
      "Step 532744  [5.446 sec/step, loss=0.07459, avg_loss=0.07346]\n",
      "Step 532745  [5.456 sec/step, loss=0.07269, avg_loss=0.07345]\n",
      "Step 532746  [5.405 sec/step, loss=0.07402, avg_loss=0.07352]\n",
      "Step 532747  [5.392 sec/step, loss=0.07057, avg_loss=0.07347]\n",
      "Step 532748  [5.383 sec/step, loss=0.07388, avg_loss=0.07349]\n",
      "Step 532749  [5.376 sec/step, loss=0.07507, avg_loss=0.07348]\n",
      "Step 532750  [5.373 sec/step, loss=0.07383, avg_loss=0.07346]\n",
      "Step 532751  [5.347 sec/step, loss=0.07499, avg_loss=0.07348]\n",
      "Step 532752  [5.399 sec/step, loss=0.06593, avg_loss=0.07342]\n",
      "Generated 32 batches of size 32 in 2.417 sec\n",
      "Step 532753  [5.394 sec/step, loss=0.07511, avg_loss=0.07343]\n",
      "Step 532754  [5.389 sec/step, loss=0.07581, avg_loss=0.07343]\n",
      "Step 532755  [5.403 sec/step, loss=0.07479, avg_loss=0.07353]\n",
      "Step 532756  [5.386 sec/step, loss=0.07122, avg_loss=0.07349]\n",
      "Step 532757  [5.401 sec/step, loss=0.07594, avg_loss=0.07350]\n",
      "Step 532758  [5.394 sec/step, loss=0.07404, avg_loss=0.07348]\n",
      "Step 532759  [5.407 sec/step, loss=0.07571, avg_loss=0.07350]\n",
      "Step 532760  [5.396 sec/step, loss=0.07064, avg_loss=0.07347]\n",
      "Step 532761  [5.405 sec/step, loss=0.07281, avg_loss=0.07347]\n",
      "Step 532762  [5.392 sec/step, loss=0.07482, avg_loss=0.07348]\n",
      "Step 532763  [5.399 sec/step, loss=0.07477, avg_loss=0.07349]\n",
      "Step 532764  [5.403 sec/step, loss=0.07537, avg_loss=0.07349]\n",
      "Step 532765  [5.415 sec/step, loss=0.07462, avg_loss=0.07353]\n",
      "Step 532766  [5.427 sec/step, loss=0.07461, avg_loss=0.07358]\n",
      "Step 532767  [5.437 sec/step, loss=0.07450, avg_loss=0.07360]\n",
      "Step 532768  [5.432 sec/step, loss=0.07563, avg_loss=0.07363]\n",
      "Step 532769  [5.415 sec/step, loss=0.07288, avg_loss=0.07361]\n",
      "Step 532770  [5.440 sec/step, loss=0.07244, avg_loss=0.07359]\n",
      "Step 532771  [5.422 sec/step, loss=0.07286, avg_loss=0.07356]\n",
      "Step 532772  [5.423 sec/step, loss=0.07387, avg_loss=0.07355]\n",
      "Step 532773  [5.445 sec/step, loss=0.07582, avg_loss=0.07359]\n",
      "Step 532774  [5.454 sec/step, loss=0.07553, avg_loss=0.07361]\n",
      "Step 532775  [5.437 sec/step, loss=0.07368, avg_loss=0.07363]\n",
      "Step 532776  [5.439 sec/step, loss=0.07474, avg_loss=0.07365]\n",
      "Step 532777  [5.485 sec/step, loss=0.06492, avg_loss=0.07355]\n",
      "Step 532778  [5.437 sec/step, loss=0.07136, avg_loss=0.07361]\n",
      "Step 532779  [5.449 sec/step, loss=0.07527, avg_loss=0.07362]\n",
      "Step 532780  [5.452 sec/step, loss=0.07306, avg_loss=0.07360]\n",
      "Step 532781  [5.470 sec/step, loss=0.07532, avg_loss=0.07361]\n",
      "Step 532782  [5.487 sec/step, loss=0.07542, avg_loss=0.07361]\n",
      "Step 532783  [5.468 sec/step, loss=0.07070, avg_loss=0.07358]\n",
      "Step 532784  [5.472 sec/step, loss=0.07074, avg_loss=0.07358]\n",
      "Generated 32 batches of size 32 in 2.384 sec\n",
      "Step 532785  [5.480 sec/step, loss=0.07461, avg_loss=0.07358]\n",
      "Step 532786  [5.463 sec/step, loss=0.07343, avg_loss=0.07355]\n",
      "Step 532787  [5.469 sec/step, loss=0.07592, avg_loss=0.07356]\n",
      "Step 532788  [5.467 sec/step, loss=0.07393, avg_loss=0.07356]\n",
      "Step 532789  [5.459 sec/step, loss=0.07450, avg_loss=0.07355]\n",
      "Step 532790  [5.420 sec/step, loss=0.06651, avg_loss=0.07346]\n",
      "Step 532791  [5.407 sec/step, loss=0.07304, avg_loss=0.07343]\n",
      "Step 532792  [5.429 sec/step, loss=0.07477, avg_loss=0.07350]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 532793  [5.446 sec/step, loss=0.07688, avg_loss=0.07354]\n",
      "Step 532794  [5.439 sec/step, loss=0.07429, avg_loss=0.07353]\n",
      "Step 532795  [5.426 sec/step, loss=0.07535, avg_loss=0.07352]\n",
      "Step 532796  [5.442 sec/step, loss=0.07581, avg_loss=0.07357]\n",
      "Step 532797  [5.432 sec/step, loss=0.07433, avg_loss=0.07356]\n",
      "Step 532798  [5.444 sec/step, loss=0.07512, avg_loss=0.07357]\n",
      "Step 532799  [5.487 sec/step, loss=0.06715, avg_loss=0.07349]\n",
      "Step 532800  [5.466 sec/step, loss=0.06600, avg_loss=0.07340]\n",
      "Writing summary at step: 532800\n",
      "Step 532801  [5.488 sec/step, loss=0.07640, avg_loss=0.07345]\n",
      "Step 532802  [5.507 sec/step, loss=0.07527, avg_loss=0.07348]\n",
      "Step 532803  [5.510 sec/step, loss=0.07456, avg_loss=0.07348]\n",
      "Step 532804  [5.527 sec/step, loss=0.07404, avg_loss=0.07352]\n",
      "Step 532805  [5.505 sec/step, loss=0.07390, avg_loss=0.07350]\n",
      "Step 532806  [5.501 sec/step, loss=0.07256, avg_loss=0.07347]\n",
      "Step 532807  [5.495 sec/step, loss=0.07230, avg_loss=0.07350]\n",
      "Step 532808  [5.483 sec/step, loss=0.07075, avg_loss=0.07348]\n",
      "Step 532809  [5.475 sec/step, loss=0.07403, avg_loss=0.07347]\n",
      "Step 532810  [5.502 sec/step, loss=0.07305, avg_loss=0.07346]\n",
      "Step 532811  [5.516 sec/step, loss=0.07312, avg_loss=0.07346]\n",
      "Step 532812  [5.510 sec/step, loss=0.07567, avg_loss=0.07345]\n",
      "Step 532813  [5.525 sec/step, loss=0.07582, avg_loss=0.07348]\n",
      "Step 532814  [5.528 sec/step, loss=0.07359, avg_loss=0.07346]\n",
      "Step 532815  [5.523 sec/step, loss=0.07499, avg_loss=0.07346]\n",
      "Generated 32 batches of size 32 in 2.423 sec\n",
      "Step 532816  [5.525 sec/step, loss=0.07605, avg_loss=0.07347]\n",
      "Step 532817  [5.532 sec/step, loss=0.07538, avg_loss=0.07348]\n",
      "Step 532818  [5.544 sec/step, loss=0.07621, avg_loss=0.07351]\n",
      "Step 532819  [5.514 sec/step, loss=0.07275, avg_loss=0.07351]\n",
      "Step 532820  [5.512 sec/step, loss=0.07359, avg_loss=0.07353]\n",
      "Step 532821  [5.503 sec/step, loss=0.07498, avg_loss=0.07356]\n",
      "Step 532822  [5.534 sec/step, loss=0.07355, avg_loss=0.07363]\n",
      "Step 532823  [5.509 sec/step, loss=0.07238, avg_loss=0.07359]\n",
      "Step 532824  [5.470 sec/step, loss=0.07388, avg_loss=0.07368]\n",
      "Step 532825  [5.473 sec/step, loss=0.07584, avg_loss=0.07371]\n",
      "Step 532826  [5.449 sec/step, loss=0.07391, avg_loss=0.07369]\n",
      "Step 532827  [5.465 sec/step, loss=0.07573, avg_loss=0.07372]\n",
      "Step 532828  [5.455 sec/step, loss=0.07221, avg_loss=0.07369]\n",
      "Step 532829  [5.455 sec/step, loss=0.07614, avg_loss=0.07369]\n",
      "Step 532830  [5.468 sec/step, loss=0.07521, avg_loss=0.07370]\n",
      "Step 532831  [5.472 sec/step, loss=0.07471, avg_loss=0.07371]\n",
      "Step 532832  [5.433 sec/step, loss=0.07080, avg_loss=0.07369]\n",
      "Step 532833  [5.439 sec/step, loss=0.07395, avg_loss=0.07369]\n",
      "Step 532834  [5.435 sec/step, loss=0.07266, avg_loss=0.07368]\n",
      "Step 532835  [5.431 sec/step, loss=0.07526, avg_loss=0.07368]\n",
      "Step 532836  [5.449 sec/step, loss=0.07483, avg_loss=0.07372]\n",
      "Step 532837  [5.475 sec/step, loss=0.07451, avg_loss=0.07372]\n",
      "Step 532838  [5.446 sec/step, loss=0.07077, avg_loss=0.07367]\n",
      "Step 532839  [5.452 sec/step, loss=0.07409, avg_loss=0.07366]\n",
      "Step 532840  [5.491 sec/step, loss=0.06503, avg_loss=0.07356]\n",
      "Step 532841  [5.509 sec/step, loss=0.07434, avg_loss=0.07365]\n",
      "Step 532842  [5.501 sec/step, loss=0.07535, avg_loss=0.07366]\n",
      "Step 532843  [5.498 sec/step, loss=0.07516, avg_loss=0.07366]\n",
      "Step 532844  [5.502 sec/step, loss=0.07605, avg_loss=0.07367]\n",
      "Step 532845  [5.484 sec/step, loss=0.06456, avg_loss=0.07359]\n",
      "Step 532846  [5.494 sec/step, loss=0.07290, avg_loss=0.07358]\n",
      "Step 532847  [5.496 sec/step, loss=0.07313, avg_loss=0.07360]\n",
      "Generated 32 batches of size 32 in 2.598 sec\n",
      "Step 532848  [5.506 sec/step, loss=0.07466, avg_loss=0.07361]\n",
      "Step 532849  [5.509 sec/step, loss=0.07330, avg_loss=0.07359]\n",
      "Step 532850  [5.506 sec/step, loss=0.07432, avg_loss=0.07360]\n",
      "Step 532851  [5.519 sec/step, loss=0.07525, avg_loss=0.07360]\n",
      "Step 532852  [5.474 sec/step, loss=0.07417, avg_loss=0.07368]\n",
      "Step 532853  [5.471 sec/step, loss=0.07558, avg_loss=0.07369]\n",
      "Step 532854  [5.460 sec/step, loss=0.07280, avg_loss=0.07366]\n",
      "Step 532855  [5.462 sec/step, loss=0.07417, avg_loss=0.07365]\n",
      "Step 532856  [5.459 sec/step, loss=0.07371, avg_loss=0.07368]\n",
      "Step 532857  [5.435 sec/step, loss=0.07199, avg_loss=0.07364]\n",
      "Step 532858  [5.440 sec/step, loss=0.07282, avg_loss=0.07362]\n",
      "Step 532859  [5.425 sec/step, loss=0.07301, avg_loss=0.07360]\n",
      "Step 532860  [5.447 sec/step, loss=0.07572, avg_loss=0.07365]\n",
      "Step 532861  [5.435 sec/step, loss=0.07574, avg_loss=0.07368]\n",
      "Step 532862  [5.433 sec/step, loss=0.07310, avg_loss=0.07366]\n",
      "Step 532863  [5.431 sec/step, loss=0.07504, avg_loss=0.07366]\n",
      "Step 532864  [5.405 sec/step, loss=0.06653, avg_loss=0.07358]\n",
      "Step 532865  [5.434 sec/step, loss=0.07345, avg_loss=0.07356]\n",
      "Step 532866  [5.456 sec/step, loss=0.07376, avg_loss=0.07356]\n",
      "Step 532867  [5.454 sec/step, loss=0.07572, avg_loss=0.07357]\n",
      "Step 532868  [5.453 sec/step, loss=0.07433, avg_loss=0.07355]\n",
      "Step 532869  [5.461 sec/step, loss=0.07505, avg_loss=0.07358]\n",
      "Step 532870  [5.442 sec/step, loss=0.07530, avg_loss=0.07360]\n",
      "Step 532871  [5.447 sec/step, loss=0.07479, avg_loss=0.07362]\n",
      "Step 532872  [5.453 sec/step, loss=0.07465, avg_loss=0.07363]\n",
      "Step 532873  [5.438 sec/step, loss=0.07326, avg_loss=0.07361]\n",
      "Step 532874  [5.420 sec/step, loss=0.07415, avg_loss=0.07359]\n",
      "Step 532875  [5.411 sec/step, loss=0.07418, avg_loss=0.07360]\n",
      "Step 532876  [5.423 sec/step, loss=0.07639, avg_loss=0.07361]\n",
      "Step 532877  [5.371 sec/step, loss=0.07499, avg_loss=0.07371]\n",
      "Step 532878  [5.370 sec/step, loss=0.07180, avg_loss=0.07372]\n",
      "Step 532879  [5.360 sec/step, loss=0.07504, avg_loss=0.07372]\n",
      "Generated 32 batches of size 32 in 2.826 sec\n",
      "Step 532880  [5.354 sec/step, loss=0.07157, avg_loss=0.07370]\n",
      "Step 532881  [5.385 sec/step, loss=0.06679, avg_loss=0.07362]\n",
      "Step 532882  [5.384 sec/step, loss=0.07361, avg_loss=0.07360]\n",
      "Step 532883  [5.390 sec/step, loss=0.07343, avg_loss=0.07363]\n",
      "Step 532884  [5.409 sec/step, loss=0.07365, avg_loss=0.07365]\n",
      "Step 532885  [5.410 sec/step, loss=0.07597, avg_loss=0.07367]\n",
      "Step 532886  [5.432 sec/step, loss=0.07480, avg_loss=0.07368]\n",
      "Step 532887  [5.417 sec/step, loss=0.07383, avg_loss=0.07366]\n",
      "Step 532888  [5.438 sec/step, loss=0.07599, avg_loss=0.07368]\n",
      "Step 532889  [5.445 sec/step, loss=0.07565, avg_loss=0.07369]\n",
      "Step 532890  [5.471 sec/step, loss=0.07585, avg_loss=0.07379]\n",
      "Step 532891  [5.467 sec/step, loss=0.07365, avg_loss=0.07379]\n",
      "Step 532892  [5.466 sec/step, loss=0.07506, avg_loss=0.07380]\n",
      "Step 532893  [5.448 sec/step, loss=0.07366, avg_loss=0.07376]\n",
      "Step 532894  [5.445 sec/step, loss=0.07254, avg_loss=0.07375]\n",
      "Step 532895  [5.454 sec/step, loss=0.07398, avg_loss=0.07373]\n",
      "Step 532896  [5.441 sec/step, loss=0.07523, avg_loss=0.07373]\n",
      "Step 532897  [5.440 sec/step, loss=0.07480, avg_loss=0.07373]\n",
      "Step 532898  [5.431 sec/step, loss=0.07312, avg_loss=0.07371]\n",
      "Step 532899  [5.407 sec/step, loss=0.07283, avg_loss=0.07377]\n",
      "Step 532900  [5.428 sec/step, loss=0.07419, avg_loss=0.07385]\n",
      "Writing summary at step: 532900\n",
      "Step 532901  [5.427 sec/step, loss=0.07538, avg_loss=0.07384]\n",
      "Step 532902  [5.423 sec/step, loss=0.07166, avg_loss=0.07380]\n",
      "Step 532903  [5.418 sec/step, loss=0.07355, avg_loss=0.07379]\n",
      "Step 532904  [5.411 sec/step, loss=0.07530, avg_loss=0.07381]\n",
      "Step 532905  [5.433 sec/step, loss=0.07559, avg_loss=0.07382]\n",
      "Step 532906  [5.430 sec/step, loss=0.07485, avg_loss=0.07385]\n",
      "Step 532907  [5.452 sec/step, loss=0.07585, avg_loss=0.07388]\n",
      "Step 532908  [5.442 sec/step, loss=0.06968, avg_loss=0.07387]\n",
      "Step 532909  [5.448 sec/step, loss=0.07412, avg_loss=0.07387]\n",
      "Step 532910  [5.430 sec/step, loss=0.07250, avg_loss=0.07387]\n",
      "Generated 32 batches of size 32 in 2.544 sec\n",
      "Step 532911  [5.404 sec/step, loss=0.07348, avg_loss=0.07387]\n",
      "Step 532912  [5.403 sec/step, loss=0.07405, avg_loss=0.07385]\n",
      "Step 532913  [5.401 sec/step, loss=0.07570, avg_loss=0.07385]\n",
      "Step 532914  [5.447 sec/step, loss=0.06615, avg_loss=0.07378]\n",
      "Step 532915  [5.438 sec/step, loss=0.07113, avg_loss=0.07374]\n",
      "Step 532916  [5.438 sec/step, loss=0.07537, avg_loss=0.07373]\n",
      "Step 532917  [5.428 sec/step, loss=0.07124, avg_loss=0.07369]\n",
      "Step 532918  [5.428 sec/step, loss=0.07419, avg_loss=0.07367]\n",
      "Step 532919  [5.416 sec/step, loss=0.06543, avg_loss=0.07360]\n",
      "Step 532920  [5.423 sec/step, loss=0.07525, avg_loss=0.07361]\n",
      "Step 532921  [5.440 sec/step, loss=0.07569, avg_loss=0.07362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 532922  [5.428 sec/step, loss=0.07553, avg_loss=0.07364]\n",
      "Step 532923  [5.451 sec/step, loss=0.07438, avg_loss=0.07366]\n",
      "Step 532924  [5.457 sec/step, loss=0.07487, avg_loss=0.07367]\n",
      "Step 532925  [5.433 sec/step, loss=0.06626, avg_loss=0.07357]\n",
      "Step 532926  [5.445 sec/step, loss=0.07445, avg_loss=0.07358]\n",
      "Step 532927  [5.424 sec/step, loss=0.07368, avg_loss=0.07356]\n",
      "Step 532928  [5.418 sec/step, loss=0.07117, avg_loss=0.07355]\n",
      "Step 532929  [5.450 sec/step, loss=0.06839, avg_loss=0.07347]\n",
      "Step 532930  [5.417 sec/step, loss=0.07205, avg_loss=0.07344]\n",
      "Step 532931  [5.419 sec/step, loss=0.07339, avg_loss=0.07343]\n",
      "Step 532932  [5.432 sec/step, loss=0.07142, avg_loss=0.07343]\n",
      "Step 532933  [5.437 sec/step, loss=0.07496, avg_loss=0.07344]\n",
      "Step 532934  [5.455 sec/step, loss=0.07610, avg_loss=0.07348]\n",
      "Step 532935  [5.456 sec/step, loss=0.07474, avg_loss=0.07347]\n",
      "Step 532936  [5.463 sec/step, loss=0.07590, avg_loss=0.07348]\n",
      "Step 532937  [5.437 sec/step, loss=0.07089, avg_loss=0.07345]\n",
      "Step 532938  [5.446 sec/step, loss=0.07344, avg_loss=0.07347]\n",
      "Step 532939  [5.427 sec/step, loss=0.07381, avg_loss=0.07347]\n",
      "Step 532940  [5.383 sec/step, loss=0.07585, avg_loss=0.07358]\n",
      "Step 532941  [5.389 sec/step, loss=0.07619, avg_loss=0.07360]\n",
      "Step 532942  [5.397 sec/step, loss=0.07497, avg_loss=0.07359]\n",
      "Generated 32 batches of size 32 in 2.381 sec\n",
      "Step 532943  [5.409 sec/step, loss=0.07401, avg_loss=0.07358]\n",
      "Step 532944  [5.393 sec/step, loss=0.07360, avg_loss=0.07356]\n",
      "Step 532945  [5.416 sec/step, loss=0.07403, avg_loss=0.07365]\n",
      "Step 532946  [5.410 sec/step, loss=0.07460, avg_loss=0.07367]\n",
      "Step 532947  [5.424 sec/step, loss=0.07350, avg_loss=0.07367]\n",
      "Step 532948  [5.420 sec/step, loss=0.07471, avg_loss=0.07367]\n",
      "Step 532949  [5.410 sec/step, loss=0.07267, avg_loss=0.07367]\n",
      "Step 532950  [5.433 sec/step, loss=0.07530, avg_loss=0.07368]\n",
      "Step 532951  [5.418 sec/step, loss=0.07537, avg_loss=0.07368]\n",
      "Step 532952  [5.399 sec/step, loss=0.07183, avg_loss=0.07365]\n",
      "Step 532953  [5.392 sec/step, loss=0.07465, avg_loss=0.07365]\n",
      "Step 532954  [5.401 sec/step, loss=0.07535, avg_loss=0.07367]\n",
      "Step 532955  [5.419 sec/step, loss=0.07529, avg_loss=0.07368]\n",
      "Step 532956  [5.437 sec/step, loss=0.07562, avg_loss=0.07370]\n",
      "Step 532957  [5.451 sec/step, loss=0.07353, avg_loss=0.07372]\n",
      "Step 532958  [5.439 sec/step, loss=0.07360, avg_loss=0.07372]\n",
      "Step 532959  [5.438 sec/step, loss=0.07267, avg_loss=0.07372]\n",
      "Step 532960  [5.426 sec/step, loss=0.07322, avg_loss=0.07370]\n",
      "Step 532961  [5.408 sec/step, loss=0.07110, avg_loss=0.07365]\n",
      "Step 532962  [5.411 sec/step, loss=0.07454, avg_loss=0.07366]\n",
      "Step 532963  [5.406 sec/step, loss=0.07475, avg_loss=0.07366]\n",
      "Step 532964  [5.423 sec/step, loss=0.07544, avg_loss=0.07375]\n",
      "Step 532965  [5.405 sec/step, loss=0.07612, avg_loss=0.07378]\n",
      "Step 532966  [5.432 sec/step, loss=0.06633, avg_loss=0.07370]\n",
      "Step 532967  [5.441 sec/step, loss=0.07499, avg_loss=0.07370]\n",
      "Step 532968  [5.447 sec/step, loss=0.07452, avg_loss=0.07370]\n",
      "Step 532969  [5.440 sec/step, loss=0.07227, avg_loss=0.07367]\n",
      "Step 532970  [5.422 sec/step, loss=0.07398, avg_loss=0.07366]\n",
      "Step 532971  [5.427 sec/step, loss=0.07477, avg_loss=0.07366]\n",
      "Step 532972  [5.421 sec/step, loss=0.07383, avg_loss=0.07365]\n",
      "Step 532973  [5.435 sec/step, loss=0.07418, avg_loss=0.07366]\n",
      "Step 532974  [5.439 sec/step, loss=0.07338, avg_loss=0.07365]\n",
      "Generated 32 batches of size 32 in 2.971 sec\n",
      "Step 532975  [5.425 sec/step, loss=0.06563, avg_loss=0.07356]\n",
      "Step 532976  [5.408 sec/step, loss=0.07518, avg_loss=0.07355]\n",
      "Step 532977  [5.412 sec/step, loss=0.07473, avg_loss=0.07355]\n",
      "Step 532978  [5.418 sec/step, loss=0.07558, avg_loss=0.07359]\n",
      "Step 532979  [5.425 sec/step, loss=0.07578, avg_loss=0.07359]\n",
      "Step 532980  [5.435 sec/step, loss=0.07553, avg_loss=0.07363]\n",
      "Step 532981  [5.413 sec/step, loss=0.07240, avg_loss=0.07369]\n",
      "Step 532982  [5.414 sec/step, loss=0.07546, avg_loss=0.07371]\n",
      "Step 532983  [5.421 sec/step, loss=0.07397, avg_loss=0.07371]\n",
      "Step 532984  [5.410 sec/step, loss=0.07059, avg_loss=0.07368]\n",
      "Step 532985  [5.411 sec/step, loss=0.07324, avg_loss=0.07366]\n",
      "Step 532986  [5.405 sec/step, loss=0.07430, avg_loss=0.07365]\n",
      "Step 532987  [5.407 sec/step, loss=0.07278, avg_loss=0.07364]\n",
      "Step 532988  [5.392 sec/step, loss=0.07494, avg_loss=0.07363]\n",
      "Step 532989  [5.376 sec/step, loss=0.07363, avg_loss=0.07361]\n",
      "Step 532990  [5.388 sec/step, loss=0.07286, avg_loss=0.07358]\n",
      "Step 532991  [5.380 sec/step, loss=0.07048, avg_loss=0.07355]\n",
      "Step 532992  [5.378 sec/step, loss=0.07463, avg_loss=0.07354]\n",
      "Step 532993  [5.379 sec/step, loss=0.07380, avg_loss=0.07355]\n",
      "Step 532994  [5.374 sec/step, loss=0.07462, avg_loss=0.07357]\n",
      "Step 532995  [5.346 sec/step, loss=0.06521, avg_loss=0.07348]\n",
      "Step 532996  [5.350 sec/step, loss=0.07586, avg_loss=0.07348]\n",
      "Step 532997  [5.363 sec/step, loss=0.07547, avg_loss=0.07349]\n",
      "Step 532998  [5.381 sec/step, loss=0.07580, avg_loss=0.07352]\n",
      "Step 532999  [5.358 sec/step, loss=0.07297, avg_loss=0.07352]\n",
      "Step 533000  [5.366 sec/step, loss=0.07568, avg_loss=0.07353]\n",
      "Writing summary at step: 533000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-533000\n",
      "Saving audio and alignment...\n",
      "Input: barfabaarii miin dhaabay kay tsolhay pir rakkhii tshoolay or guubhii kii maamuulii khitsrrii naaqis qisam kii hae~______\n",
      "Step 533001  [5.380 sec/step, loss=0.07339, avg_loss=0.07351]\n",
      "Step 533002  [5.390 sec/step, loss=0.07437, avg_loss=0.07354]\n",
      "Step 533003  [5.392 sec/step, loss=0.07419, avg_loss=0.07355]\n",
      "Step 533004  [5.380 sec/step, loss=0.07103, avg_loss=0.07351]\n",
      "Generated 32 batches of size 32 in 2.335 sec\n",
      "Step 533005  [5.375 sec/step, loss=0.07553, avg_loss=0.07350]\n",
      "Step 533006  [5.384 sec/step, loss=0.07183, avg_loss=0.07347]\n",
      "Step 533007  [5.422 sec/step, loss=0.06675, avg_loss=0.07338]\n",
      "Step 533008  [5.435 sec/step, loss=0.07557, avg_loss=0.07344]\n",
      "Step 533009  [5.433 sec/step, loss=0.07351, avg_loss=0.07344]\n",
      "Step 533010  [5.437 sec/step, loss=0.07592, avg_loss=0.07347]\n",
      "Step 533011  [5.429 sec/step, loss=0.07411, avg_loss=0.07348]\n",
      "Step 533012  [5.432 sec/step, loss=0.07451, avg_loss=0.07348]\n",
      "Step 533013  [5.426 sec/step, loss=0.07306, avg_loss=0.07346]\n",
      "Step 533014  [5.388 sec/step, loss=0.07346, avg_loss=0.07353]\n",
      "Step 533015  [5.392 sec/step, loss=0.07125, avg_loss=0.07353]\n",
      "Step 533016  [5.382 sec/step, loss=0.07591, avg_loss=0.07353]\n",
      "Step 533017  [5.385 sec/step, loss=0.07469, avg_loss=0.07357]\n",
      "Step 533018  [5.382 sec/step, loss=0.07376, avg_loss=0.07356]\n",
      "Step 533019  [5.388 sec/step, loss=0.07236, avg_loss=0.07363]\n",
      "Step 533020  [5.381 sec/step, loss=0.07399, avg_loss=0.07362]\n",
      "Step 533021  [5.379 sec/step, loss=0.07435, avg_loss=0.07361]\n",
      "Step 533022  [5.394 sec/step, loss=0.07612, avg_loss=0.07361]\n",
      "Step 533023  [5.380 sec/step, loss=0.07253, avg_loss=0.07360]\n",
      "Step 533024  [5.359 sec/step, loss=0.07232, avg_loss=0.07357]\n",
      "Step 533025  [5.381 sec/step, loss=0.07505, avg_loss=0.07366]\n",
      "Step 533026  [5.378 sec/step, loss=0.07452, avg_loss=0.07366]\n",
      "Step 533027  [5.377 sec/step, loss=0.07346, avg_loss=0.07366]\n",
      "Step 533028  [5.388 sec/step, loss=0.07561, avg_loss=0.07370]\n",
      "Step 533029  [5.343 sec/step, loss=0.07102, avg_loss=0.07373]\n",
      "Step 533030  [5.356 sec/step, loss=0.07474, avg_loss=0.07375]\n",
      "Step 533031  [5.367 sec/step, loss=0.07613, avg_loss=0.07378]\n",
      "Step 533032  [5.408 sec/step, loss=0.06857, avg_loss=0.07375]\n",
      "Step 533033  [5.390 sec/step, loss=0.06615, avg_loss=0.07366]\n",
      "Step 533034  [5.369 sec/step, loss=0.07405, avg_loss=0.07364]\n",
      "Step 533035  [5.372 sec/step, loss=0.07489, avg_loss=0.07365]\n",
      "Step 533036  [5.363 sec/step, loss=0.07414, avg_loss=0.07363]\n",
      "Generated 32 batches of size 32 in 2.374 sec\n",
      "Step 533037  [5.386 sec/step, loss=0.07580, avg_loss=0.07368]\n",
      "Step 533038  [5.387 sec/step, loss=0.07488, avg_loss=0.07369]\n",
      "Step 533039  [5.410 sec/step, loss=0.07324, avg_loss=0.07369]\n",
      "Step 533040  [5.404 sec/step, loss=0.07306, avg_loss=0.07366]\n",
      "Step 533041  [5.411 sec/step, loss=0.07516, avg_loss=0.07365]\n",
      "Step 533042  [5.399 sec/step, loss=0.07257, avg_loss=0.07362]\n",
      "Step 533043  [5.382 sec/step, loss=0.07460, avg_loss=0.07363]\n",
      "Step 533044  [5.395 sec/step, loss=0.07546, avg_loss=0.07365]\n",
      "Step 533045  [5.417 sec/step, loss=0.07240, avg_loss=0.07363]\n",
      "Step 533046  [5.423 sec/step, loss=0.07557, avg_loss=0.07364]\n",
      "Step 533047  [5.401 sec/step, loss=0.07344, avg_loss=0.07364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 533048  [5.398 sec/step, loss=0.07350, avg_loss=0.07363]\n",
      "Step 533049  [5.394 sec/step, loss=0.07413, avg_loss=0.07364]\n",
      "Step 533050  [5.368 sec/step, loss=0.07483, avg_loss=0.07364]\n",
      "Step 533051  [5.384 sec/step, loss=0.07500, avg_loss=0.07364]\n",
      "Step 533052  [5.404 sec/step, loss=0.07352, avg_loss=0.07365]\n",
      "Step 533053  [5.399 sec/step, loss=0.07513, avg_loss=0.07366]\n",
      "Step 533054  [5.399 sec/step, loss=0.07583, avg_loss=0.07366]\n",
      "Step 533055  [5.400 sec/step, loss=0.07273, avg_loss=0.07364]\n",
      "Step 533056  [5.393 sec/step, loss=0.07551, avg_loss=0.07364]\n",
      "Step 533057  [5.386 sec/step, loss=0.07441, avg_loss=0.07364]\n",
      "Step 533058  [5.390 sec/step, loss=0.07502, avg_loss=0.07366]\n",
      "Step 533059  [5.383 sec/step, loss=0.07166, avg_loss=0.07365]\n",
      "Step 533060  [5.391 sec/step, loss=0.07382, avg_loss=0.07365]\n",
      "Step 533061  [5.405 sec/step, loss=0.07376, avg_loss=0.07368]\n",
      "Step 533062  [5.413 sec/step, loss=0.07615, avg_loss=0.07370]\n",
      "Step 533063  [5.467 sec/step, loss=0.06724, avg_loss=0.07362]\n",
      "Step 533064  [5.476 sec/step, loss=0.07595, avg_loss=0.07363]\n",
      "Step 533065  [5.470 sec/step, loss=0.07511, avg_loss=0.07362]\n",
      "Step 533066  [5.411 sec/step, loss=0.07048, avg_loss=0.07366]\n",
      "Step 533067  [5.398 sec/step, loss=0.07491, avg_loss=0.07366]\n",
      "Step 533068  [5.378 sec/step, loss=0.07176, avg_loss=0.07363]\n",
      "Generated 32 batches of size 32 in 2.379 sec\n",
      "Step 533069  [5.395 sec/step, loss=0.07482, avg_loss=0.07366]\n",
      "Step 533070  [5.402 sec/step, loss=0.07223, avg_loss=0.07364]\n",
      "Step 533071  [5.384 sec/step, loss=0.06615, avg_loss=0.07355]\n",
      "Step 533072  [5.379 sec/step, loss=0.07337, avg_loss=0.07355]\n",
      "Step 533073  [5.365 sec/step, loss=0.07128, avg_loss=0.07352]\n",
      "Step 533074  [5.380 sec/step, loss=0.07470, avg_loss=0.07353]\n",
      "Step 533075  [5.405 sec/step, loss=0.07542, avg_loss=0.07363]\n",
      "Step 533076  [5.409 sec/step, loss=0.07276, avg_loss=0.07361]\n",
      "Step 533077  [5.432 sec/step, loss=0.07331, avg_loss=0.07359]\n",
      "Step 533078  [5.476 sec/step, loss=0.06498, avg_loss=0.07348]\n",
      "Step 533079  [5.480 sec/step, loss=0.07621, avg_loss=0.07349]\n",
      "Step 533080  [5.469 sec/step, loss=0.07094, avg_loss=0.07344]\n",
      "Step 533081  [5.453 sec/step, loss=0.07612, avg_loss=0.07348]\n",
      "Step 533082  [5.432 sec/step, loss=0.07224, avg_loss=0.07345]\n",
      "Step 533083  [5.433 sec/step, loss=0.07486, avg_loss=0.07346]\n",
      "Step 533084  [5.426 sec/step, loss=0.07354, avg_loss=0.07349]\n",
      "Step 533085  [5.417 sec/step, loss=0.07419, avg_loss=0.07350]\n",
      "Step 533086  [5.397 sec/step, loss=0.07168, avg_loss=0.07347]\n",
      "Step 533087  [5.425 sec/step, loss=0.07262, avg_loss=0.07347]\n",
      "Step 533088  [5.420 sec/step, loss=0.07403, avg_loss=0.07346]\n",
      "Step 533089  [5.431 sec/step, loss=0.07499, avg_loss=0.07347]\n",
      "Step 533090  [5.420 sec/step, loss=0.07582, avg_loss=0.07350]\n",
      "Step 533091  [5.445 sec/step, loss=0.07516, avg_loss=0.07355]\n",
      "Step 533092  [5.440 sec/step, loss=0.07209, avg_loss=0.07352]\n",
      "Step 533093  [5.463 sec/step, loss=0.07603, avg_loss=0.07355]\n",
      "Step 533094  [5.470 sec/step, loss=0.07513, avg_loss=0.07355]\n",
      "Step 533095  [5.471 sec/step, loss=0.06578, avg_loss=0.07356]\n",
      "Step 533096  [5.470 sec/step, loss=0.07558, avg_loss=0.07355]\n",
      "Step 533097  [5.470 sec/step, loss=0.07484, avg_loss=0.07355]\n",
      "Step 533098  [5.456 sec/step, loss=0.07389, avg_loss=0.07353]\n",
      "Step 533099  [5.448 sec/step, loss=0.07511, avg_loss=0.07355]\n",
      "Step 533100  [5.435 sec/step, loss=0.07225, avg_loss=0.07352]\n",
      "Writing summary at step: 533100\n",
      "Generated 32 batches of size 32 in 2.424 sec\n",
      "Step 533101  [5.411 sec/step, loss=0.07330, avg_loss=0.07352]\n",
      "Step 533102  [5.398 sec/step, loss=0.07485, avg_loss=0.07352]\n",
      "Step 533103  [5.398 sec/step, loss=0.07083, avg_loss=0.07349]\n",
      "Step 533104  [5.422 sec/step, loss=0.07379, avg_loss=0.07351]\n",
      "Step 533105  [5.413 sec/step, loss=0.07422, avg_loss=0.07350]\n",
      "Step 533106  [5.411 sec/step, loss=0.07222, avg_loss=0.07350]\n",
      "Step 533107  [5.363 sec/step, loss=0.07516, avg_loss=0.07359]\n",
      "Step 533108  [5.370 sec/step, loss=0.07567, avg_loss=0.07359]\n",
      "Step 533109  [5.391 sec/step, loss=0.07513, avg_loss=0.07361]\n",
      "Step 533110  [5.376 sec/step, loss=0.07349, avg_loss=0.07358]\n",
      "Step 533111  [5.393 sec/step, loss=0.07592, avg_loss=0.07360]\n",
      "Step 533112  [5.385 sec/step, loss=0.07311, avg_loss=0.07359]\n",
      "Step 533113  [5.388 sec/step, loss=0.07456, avg_loss=0.07360]\n",
      "Step 533114  [5.379 sec/step, loss=0.07442, avg_loss=0.07361]\n",
      "Step 533115  [5.374 sec/step, loss=0.06998, avg_loss=0.07360]\n",
      "Step 533116  [5.373 sec/step, loss=0.07528, avg_loss=0.07359]\n",
      "Step 533117  [5.371 sec/step, loss=0.07039, avg_loss=0.07355]\n",
      "Step 533118  [5.367 sec/step, loss=0.07559, avg_loss=0.07357]\n",
      "Step 533119  [5.389 sec/step, loss=0.07623, avg_loss=0.07361]\n",
      "Step 533120  [5.386 sec/step, loss=0.07306, avg_loss=0.07360]\n",
      "Step 533121  [5.373 sec/step, loss=0.07518, avg_loss=0.07360]\n",
      "Step 533122  [5.381 sec/step, loss=0.07517, avg_loss=0.07360]\n",
      "Step 533123  [5.386 sec/step, loss=0.07455, avg_loss=0.07362]\n",
      "Step 533124  [5.388 sec/step, loss=0.07243, avg_loss=0.07362]\n",
      "Step 533125  [5.396 sec/step, loss=0.07625, avg_loss=0.07363]\n",
      "Step 533126  [5.395 sec/step, loss=0.07526, avg_loss=0.07364]\n",
      "Step 533127  [5.398 sec/step, loss=0.07513, avg_loss=0.07365]\n",
      "Step 533128  [5.388 sec/step, loss=0.07403, avg_loss=0.07364]\n",
      "Step 533129  [5.375 sec/step, loss=0.06504, avg_loss=0.07358]\n",
      "Step 533130  [5.426 sec/step, loss=0.06555, avg_loss=0.07349]\n",
      "Step 533131  [5.407 sec/step, loss=0.07392, avg_loss=0.07346]\n",
      "Generated 32 batches of size 32 in 2.434 sec\n",
      "Step 533132  [5.376 sec/step, loss=0.07524, avg_loss=0.07353]\n",
      "Step 533133  [5.397 sec/step, loss=0.07444, avg_loss=0.07361]\n",
      "Step 533134  [5.416 sec/step, loss=0.07594, avg_loss=0.07363]\n",
      "Step 533135  [5.426 sec/step, loss=0.07558, avg_loss=0.07364]\n",
      "Step 533136  [5.433 sec/step, loss=0.07323, avg_loss=0.07363]\n",
      "Step 533137  [5.425 sec/step, loss=0.07379, avg_loss=0.07361]\n",
      "Step 533138  [5.418 sec/step, loss=0.07005, avg_loss=0.07356]\n",
      "Step 533139  [5.406 sec/step, loss=0.07500, avg_loss=0.07358]\n",
      "Step 533140  [5.407 sec/step, loss=0.07518, avg_loss=0.07360]\n",
      "Step 533141  [5.388 sec/step, loss=0.07327, avg_loss=0.07358]\n",
      "Step 533142  [5.388 sec/step, loss=0.07504, avg_loss=0.07361]\n",
      "Step 533143  [5.382 sec/step, loss=0.07407, avg_loss=0.07360]\n",
      "Step 533144  [5.402 sec/step, loss=0.07344, avg_loss=0.07358]\n",
      "Step 533145  [5.380 sec/step, loss=0.07403, avg_loss=0.07360]\n",
      "Step 533146  [5.368 sec/step, loss=0.07355, avg_loss=0.07358]\n",
      "Step 533147  [5.379 sec/step, loss=0.07533, avg_loss=0.07359]\n",
      "Step 533148  [5.392 sec/step, loss=0.07455, avg_loss=0.07361]\n",
      "Step 533149  [5.406 sec/step, loss=0.07488, avg_loss=0.07361]\n",
      "Step 533150  [5.420 sec/step, loss=0.07337, avg_loss=0.07360]\n",
      "Step 533151  [5.396 sec/step, loss=0.07386, avg_loss=0.07359]\n",
      "Step 533152  [5.397 sec/step, loss=0.07535, avg_loss=0.07361]\n",
      "Step 533153  [5.395 sec/step, loss=0.07241, avg_loss=0.07358]\n",
      "Step 533154  [5.380 sec/step, loss=0.07343, avg_loss=0.07355]\n",
      "Step 533155  [5.382 sec/step, loss=0.07321, avg_loss=0.07356]\n",
      "Step 533156  [5.387 sec/step, loss=0.07619, avg_loss=0.07357]\n",
      "Step 533157  [5.379 sec/step, loss=0.07280, avg_loss=0.07355]\n",
      "Step 533158  [5.393 sec/step, loss=0.07585, avg_loss=0.07356]\n",
      "Step 533159  [5.400 sec/step, loss=0.07466, avg_loss=0.07359]\n",
      "Step 533160  [5.400 sec/step, loss=0.07490, avg_loss=0.07360]\n",
      "Step 533161  [5.377 sec/step, loss=0.06722, avg_loss=0.07353]\n",
      "Step 533162  [5.380 sec/step, loss=0.07380, avg_loss=0.07351]\n",
      "Step 533163  [5.342 sec/step, loss=0.07631, avg_loss=0.07360]\n",
      "Generated 32 batches of size 32 in 2.363 sec\n",
      "Step 533164  [5.338 sec/step, loss=0.07535, avg_loss=0.07359]\n",
      "Step 533165  [5.351 sec/step, loss=0.07541, avg_loss=0.07360]\n",
      "Step 533166  [5.357 sec/step, loss=0.07517, avg_loss=0.07364]\n",
      "Step 533167  [5.406 sec/step, loss=0.06637, avg_loss=0.07356]\n",
      "Step 533168  [5.423 sec/step, loss=0.07478, avg_loss=0.07359]\n",
      "Step 533169  [5.418 sec/step, loss=0.07428, avg_loss=0.07358]\n",
      "Step 533170  [5.421 sec/step, loss=0.07376, avg_loss=0.07360]\n",
      "Step 533171  [5.433 sec/step, loss=0.07345, avg_loss=0.07367]\n",
      "Step 533172  [5.425 sec/step, loss=0.07284, avg_loss=0.07367]\n",
      "Step 533173  [5.447 sec/step, loss=0.07301, avg_loss=0.07368]\n",
      "Step 533174  [5.436 sec/step, loss=0.07346, avg_loss=0.07367]\n",
      "Step 533175  [5.417 sec/step, loss=0.07385, avg_loss=0.07366]\n",
      "Step 533176  [5.412 sec/step, loss=0.07075, avg_loss=0.07364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 533177  [5.385 sec/step, loss=0.07389, avg_loss=0.07364]\n",
      "Step 533178  [5.363 sec/step, loss=0.07269, avg_loss=0.07372]\n",
      "Step 533179  [5.357 sec/step, loss=0.07574, avg_loss=0.07371]\n",
      "Step 533180  [5.370 sec/step, loss=0.07514, avg_loss=0.07376]\n",
      "Step 533181  [5.342 sec/step, loss=0.06485, avg_loss=0.07364]\n",
      "Step 533182  [5.358 sec/step, loss=0.07475, avg_loss=0.07367]\n",
      "Step 533183  [5.355 sec/step, loss=0.07450, avg_loss=0.07366]\n",
      "Step 533184  [5.365 sec/step, loss=0.07313, avg_loss=0.07366]\n",
      "Step 533185  [5.362 sec/step, loss=0.07518, avg_loss=0.07367]\n",
      "Step 533186  [5.379 sec/step, loss=0.07497, avg_loss=0.07370]\n",
      "Step 533187  [5.354 sec/step, loss=0.07467, avg_loss=0.07372]\n",
      "Step 533188  [5.347 sec/step, loss=0.07145, avg_loss=0.07370]\n",
      "Step 533189  [5.352 sec/step, loss=0.07623, avg_loss=0.07371]\n",
      "Step 533190  [5.335 sec/step, loss=0.07256, avg_loss=0.07368]\n",
      "Step 533191  [5.337 sec/step, loss=0.07593, avg_loss=0.07369]\n",
      "Step 533192  [5.345 sec/step, loss=0.07388, avg_loss=0.07370]\n",
      "Step 533193  [5.338 sec/step, loss=0.07439, avg_loss=0.07369]\n",
      "Step 533194  [5.345 sec/step, loss=0.07621, avg_loss=0.07370]\n",
      "Step 533195  [5.356 sec/step, loss=0.07335, avg_loss=0.07377]\n",
      "Generated 32 batches of size 32 in 2.599 sec\n",
      "Step 533196  [5.407 sec/step, loss=0.06572, avg_loss=0.07367]\n",
      "Step 533197  [5.411 sec/step, loss=0.07513, avg_loss=0.07368]\n",
      "Step 533198  [5.404 sec/step, loss=0.06972, avg_loss=0.07364]\n",
      "Step 533199  [5.412 sec/step, loss=0.07460, avg_loss=0.07363]\n",
      "Step 533200  [5.423 sec/step, loss=0.07581, avg_loss=0.07367]\n",
      "Writing summary at step: 533200\n",
      "Step 533201  [5.419 sec/step, loss=0.07501, avg_loss=0.07368]\n",
      "Step 533202  [5.427 sec/step, loss=0.07525, avg_loss=0.07369]\n",
      "Step 533203  [5.418 sec/step, loss=0.07392, avg_loss=0.07372]\n",
      "Step 533204  [5.406 sec/step, loss=0.07470, avg_loss=0.07373]\n",
      "Step 533205  [5.418 sec/step, loss=0.07585, avg_loss=0.07374]\n",
      "Step 533206  [5.415 sec/step, loss=0.07529, avg_loss=0.07377]\n",
      "Step 533207  [5.417 sec/step, loss=0.07246, avg_loss=0.07375]\n",
      "Step 533208  [5.413 sec/step, loss=0.07496, avg_loss=0.07374]\n",
      "Step 533209  [5.406 sec/step, loss=0.07378, avg_loss=0.07373]\n",
      "Step 533210  [5.408 sec/step, loss=0.06988, avg_loss=0.07369]\n",
      "Step 533211  [5.388 sec/step, loss=0.07113, avg_loss=0.07364]\n",
      "Step 533212  [5.386 sec/step, loss=0.07363, avg_loss=0.07365]\n",
      "Step 533213  [5.369 sec/step, loss=0.07174, avg_loss=0.07362]\n",
      "Step 533214  [5.373 sec/step, loss=0.07553, avg_loss=0.07363]\n",
      "Step 533215  [5.383 sec/step, loss=0.07370, avg_loss=0.07367]\n",
      "Step 533216  [5.384 sec/step, loss=0.07511, avg_loss=0.07367]\n",
      "Step 533217  [5.392 sec/step, loss=0.07611, avg_loss=0.07372]\n",
      "Step 533218  [5.382 sec/step, loss=0.07371, avg_loss=0.07371]\n",
      "Step 533219  [5.380 sec/step, loss=0.07593, avg_loss=0.07370]\n",
      "Step 533220  [5.399 sec/step, loss=0.07645, avg_loss=0.07374]\n",
      "Step 533221  [5.391 sec/step, loss=0.07079, avg_loss=0.07369]\n",
      "Step 533222  [5.364 sec/step, loss=0.07136, avg_loss=0.07365]\n",
      "Step 533223  [5.415 sec/step, loss=0.06639, avg_loss=0.07357]\n",
      "Step 533224  [5.417 sec/step, loss=0.07410, avg_loss=0.07359]\n",
      "Step 533225  [5.400 sec/step, loss=0.07484, avg_loss=0.07357]\n",
      "Step 533226  [5.415 sec/step, loss=0.07548, avg_loss=0.07358]\n",
      "Generated 32 batches of size 32 in 2.386 sec\n",
      "Step 533227  [5.426 sec/step, loss=0.07468, avg_loss=0.07357]\n",
      "Step 533228  [5.440 sec/step, loss=0.07514, avg_loss=0.07358]\n",
      "Step 533229  [5.475 sec/step, loss=0.07504, avg_loss=0.07368]\n",
      "Step 533230  [5.433 sec/step, loss=0.07522, avg_loss=0.07378]\n",
      "Step 533231  [5.438 sec/step, loss=0.07317, avg_loss=0.07377]\n",
      "Step 533232  [5.411 sec/step, loss=0.06741, avg_loss=0.07369]\n",
      "Step 533233  [5.408 sec/step, loss=0.07415, avg_loss=0.07369]\n",
      "Step 533234  [5.424 sec/step, loss=0.07278, avg_loss=0.07366]\n",
      "Step 533235  [5.413 sec/step, loss=0.07307, avg_loss=0.07363]\n",
      "Step 533236  [5.387 sec/step, loss=0.07128, avg_loss=0.07362]\n",
      "Step 533237  [5.367 sec/step, loss=0.07378, avg_loss=0.07362]\n",
      "Step 533238  [5.387 sec/step, loss=0.07520, avg_loss=0.07367]\n",
      "Step 533239  [5.388 sec/step, loss=0.07476, avg_loss=0.07366]\n",
      "Step 533240  [5.382 sec/step, loss=0.07355, avg_loss=0.07365]\n",
      "Step 533241  [5.385 sec/step, loss=0.07048, avg_loss=0.07362]\n",
      "Step 533242  [5.382 sec/step, loss=0.07497, avg_loss=0.07362]\n",
      "Step 533243  [5.415 sec/step, loss=0.07358, avg_loss=0.07361]\n",
      "Step 533244  [5.391 sec/step, loss=0.07488, avg_loss=0.07363]\n",
      "Step 533245  [5.384 sec/step, loss=0.07465, avg_loss=0.07364]\n",
      "Step 533246  [5.378 sec/step, loss=0.07359, avg_loss=0.07364]\n",
      "Step 533247  [5.390 sec/step, loss=0.07569, avg_loss=0.07364]\n",
      "Step 533248  [5.376 sec/step, loss=0.07529, avg_loss=0.07365]\n",
      "Step 533249  [5.375 sec/step, loss=0.07476, avg_loss=0.07365]\n",
      "Step 533250  [5.353 sec/step, loss=0.07086, avg_loss=0.07362]\n",
      "Step 533251  [5.372 sec/step, loss=0.07483, avg_loss=0.07363]\n",
      "Step 533252  [5.413 sec/step, loss=0.06658, avg_loss=0.07354]\n",
      "Step 533253  [5.424 sec/step, loss=0.07598, avg_loss=0.07358]\n",
      "Step 533254  [5.443 sec/step, loss=0.07614, avg_loss=0.07361]\n",
      "Step 533255  [5.428 sec/step, loss=0.07583, avg_loss=0.07363]\n",
      "Step 533256  [5.419 sec/step, loss=0.07380, avg_loss=0.07361]\n",
      "Step 533257  [5.453 sec/step, loss=0.07382, avg_loss=0.07362]\n",
      "Step 533258  [5.446 sec/step, loss=0.07332, avg_loss=0.07359]\n",
      "Generated 32 batches of size 32 in 2.408 sec\n",
      "Step 533259  [5.466 sec/step, loss=0.07553, avg_loss=0.07360]\n",
      "Step 533260  [5.464 sec/step, loss=0.07530, avg_loss=0.07361]\n",
      "Step 533261  [5.467 sec/step, loss=0.06560, avg_loss=0.07359]\n",
      "Step 533262  [5.462 sec/step, loss=0.07511, avg_loss=0.07360]\n",
      "Step 533263  [5.460 sec/step, loss=0.07433, avg_loss=0.07358]\n",
      "Step 533264  [5.467 sec/step, loss=0.07396, avg_loss=0.07357]\n",
      "Step 533265  [5.449 sec/step, loss=0.07235, avg_loss=0.07354]\n",
      "Step 533266  [5.451 sec/step, loss=0.07271, avg_loss=0.07351]\n",
      "Step 533267  [5.390 sec/step, loss=0.07364, avg_loss=0.07359]\n",
      "Step 533268  [5.386 sec/step, loss=0.07304, avg_loss=0.07357]\n",
      "Step 533269  [5.379 sec/step, loss=0.07518, avg_loss=0.07358]\n",
      "Step 533270  [5.375 sec/step, loss=0.07260, avg_loss=0.07357]\n",
      "Step 533271  [5.383 sec/step, loss=0.07479, avg_loss=0.07358]\n",
      "Step 533272  [5.397 sec/step, loss=0.07422, avg_loss=0.07359]\n",
      "Step 533273  [5.405 sec/step, loss=0.07165, avg_loss=0.07358]\n",
      "Step 533274  [5.421 sec/step, loss=0.07564, avg_loss=0.07360]\n",
      "Step 533275  [5.435 sec/step, loss=0.07556, avg_loss=0.07362]\n",
      "Step 533276  [5.441 sec/step, loss=0.07441, avg_loss=0.07366]\n",
      "Step 533277  [5.439 sec/step, loss=0.07468, avg_loss=0.07366]\n",
      "Step 533278  [5.407 sec/step, loss=0.07341, avg_loss=0.07367]\n",
      "Step 533279  [5.386 sec/step, loss=0.07148, avg_loss=0.07363]\n",
      "Step 533280  [5.367 sec/step, loss=0.06559, avg_loss=0.07353]\n",
      "Step 533281  [5.373 sec/step, loss=0.07411, avg_loss=0.07362]\n",
      "Step 533282  [5.357 sec/step, loss=0.07370, avg_loss=0.07361]\n",
      "Step 533283  [5.367 sec/step, loss=0.07561, avg_loss=0.07363]\n",
      "Step 533284  [5.372 sec/step, loss=0.07520, avg_loss=0.07365]\n",
      "Step 533285  [5.363 sec/step, loss=0.07345, avg_loss=0.07363]\n",
      "Step 533286  [5.373 sec/step, loss=0.07570, avg_loss=0.07364]\n",
      "Step 533287  [5.416 sec/step, loss=0.06929, avg_loss=0.07358]\n",
      "Step 533288  [5.447 sec/step, loss=0.07508, avg_loss=0.07362]\n",
      "Step 533289  [5.428 sec/step, loss=0.07123, avg_loss=0.07357]\n",
      "Step 533290  [5.431 sec/step, loss=0.07512, avg_loss=0.07359]\n",
      "Generated 32 batches of size 32 in 2.378 sec\n",
      "Step 533291  [5.435 sec/step, loss=0.07521, avg_loss=0.07359]\n",
      "Step 533292  [5.428 sec/step, loss=0.07120, avg_loss=0.07356]\n",
      "Step 533293  [5.431 sec/step, loss=0.07544, avg_loss=0.07357]\n",
      "Step 533294  [5.421 sec/step, loss=0.07509, avg_loss=0.07356]\n",
      "Step 533295  [5.434 sec/step, loss=0.07453, avg_loss=0.07357]\n",
      "Step 533296  [5.388 sec/step, loss=0.07269, avg_loss=0.07364]\n",
      "Step 533297  [5.387 sec/step, loss=0.07583, avg_loss=0.07365]\n",
      "Step 533298  [5.399 sec/step, loss=0.07525, avg_loss=0.07370]\n",
      "Step 533299  [5.398 sec/step, loss=0.07379, avg_loss=0.07370]\n",
      "Step 533300  [5.398 sec/step, loss=0.07533, avg_loss=0.07369]\n",
      "Writing summary at step: 533300\n",
      "Step 533301  [5.406 sec/step, loss=0.07437, avg_loss=0.07368]\n",
      "Step 533302  [5.400 sec/step, loss=0.07427, avg_loss=0.07367]\n",
      "Step 533303  [5.460 sec/step, loss=0.06579, avg_loss=0.07359]\n",
      "Step 533304  [5.464 sec/step, loss=0.07519, avg_loss=0.07360]\n",
      "Step 533305  [5.459 sec/step, loss=0.07463, avg_loss=0.07359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 533306  [5.442 sec/step, loss=0.06531, avg_loss=0.07349]\n",
      "Step 533307  [5.431 sec/step, loss=0.07261, avg_loss=0.07349]\n",
      "Step 533308  [5.416 sec/step, loss=0.07393, avg_loss=0.07348]\n",
      "Step 533309  [5.397 sec/step, loss=0.07358, avg_loss=0.07347]\n",
      "Step 533310  [5.411 sec/step, loss=0.07608, avg_loss=0.07354]\n",
      "Step 533311  [5.418 sec/step, loss=0.07257, avg_loss=0.07355]\n",
      "Step 533312  [5.450 sec/step, loss=0.07305, avg_loss=0.07355]\n",
      "Step 533313  [5.469 sec/step, loss=0.07405, avg_loss=0.07357]\n",
      "Step 533314  [5.459 sec/step, loss=0.07527, avg_loss=0.07357]\n",
      "Step 533315  [5.450 sec/step, loss=0.07115, avg_loss=0.07354]\n",
      "Step 533316  [5.451 sec/step, loss=0.07315, avg_loss=0.07352]\n",
      "Step 533317  [5.454 sec/step, loss=0.07464, avg_loss=0.07351]\n",
      "Step 533318  [5.462 sec/step, loss=0.07374, avg_loss=0.07351]\n",
      "Step 533319  [5.442 sec/step, loss=0.07118, avg_loss=0.07346]\n",
      "Step 533320  [5.430 sec/step, loss=0.07169, avg_loss=0.07341]\n",
      "Step 533321  [5.433 sec/step, loss=0.07264, avg_loss=0.07343]\n",
      "Generated 32 batches of size 32 in 2.401 sec\n",
      "Step 533322  [5.454 sec/step, loss=0.07493, avg_loss=0.07347]\n",
      "Step 533323  [5.413 sec/step, loss=0.07584, avg_loss=0.07356]\n",
      "Step 533324  [5.425 sec/step, loss=0.07590, avg_loss=0.07358]\n",
      "Step 533325  [5.431 sec/step, loss=0.07523, avg_loss=0.07358]\n",
      "Step 533326  [5.419 sec/step, loss=0.07463, avg_loss=0.07357]\n",
      "Step 533327  [5.408 sec/step, loss=0.07485, avg_loss=0.07358]\n",
      "Step 533328  [5.404 sec/step, loss=0.07503, avg_loss=0.07357]\n",
      "Step 533329  [5.382 sec/step, loss=0.07318, avg_loss=0.07356]\n",
      "Step 533330  [5.396 sec/step, loss=0.07340, avg_loss=0.07354]\n",
      "Step 533331  [5.388 sec/step, loss=0.07016, avg_loss=0.07351]\n",
      "Step 533332  [5.399 sec/step, loss=0.07345, avg_loss=0.07357]\n",
      "Step 533333  [5.426 sec/step, loss=0.07280, avg_loss=0.07355]\n",
      "Step 533334  [5.397 sec/step, loss=0.07470, avg_loss=0.07357]\n",
      "Step 533335  [5.387 sec/step, loss=0.07390, avg_loss=0.07358]\n",
      "Step 533336  [5.411 sec/step, loss=0.07565, avg_loss=0.07363]\n",
      "Step 533337  [5.436 sec/step, loss=0.07460, avg_loss=0.07363]\n",
      "Step 533338  [5.417 sec/step, loss=0.07405, avg_loss=0.07362]\n",
      "Step 533339  [5.416 sec/step, loss=0.07543, avg_loss=0.07363]\n",
      "Step 533340  [5.419 sec/step, loss=0.07415, avg_loss=0.07363]\n",
      "Step 533341  [5.425 sec/step, loss=0.07534, avg_loss=0.07368]\n",
      "Step 533342  [5.411 sec/step, loss=0.06497, avg_loss=0.07358]\n",
      "Step 533343  [5.388 sec/step, loss=0.07185, avg_loss=0.07357]\n",
      "Step 533344  [5.399 sec/step, loss=0.07567, avg_loss=0.07357]\n",
      "Step 533345  [5.406 sec/step, loss=0.07534, avg_loss=0.07358]\n",
      "Step 533346  [5.407 sec/step, loss=0.07188, avg_loss=0.07356]\n",
      "Step 533347  [5.394 sec/step, loss=0.07469, avg_loss=0.07355]\n",
      "Step 533348  [5.449 sec/step, loss=0.06666, avg_loss=0.07347]\n",
      "Step 533349  [5.456 sec/step, loss=0.07571, avg_loss=0.07348]\n",
      "Step 533350  [5.463 sec/step, loss=0.07473, avg_loss=0.07352]\n",
      "Step 533351  [5.452 sec/step, loss=0.07069, avg_loss=0.07347]\n",
      "Step 533352  [5.406 sec/step, loss=0.07491, avg_loss=0.07356]\n",
      "Step 533353  [5.405 sec/step, loss=0.07500, avg_loss=0.07355]\n",
      "Generated 32 batches of size 32 in 2.409 sec\n",
      "Step 533354  [5.410 sec/step, loss=0.07560, avg_loss=0.07354]\n",
      "Step 533355  [5.419 sec/step, loss=0.07521, avg_loss=0.07354]\n",
      "Step 533356  [5.405 sec/step, loss=0.07061, avg_loss=0.07350]\n",
      "Step 533357  [5.391 sec/step, loss=0.07563, avg_loss=0.07352]\n",
      "Step 533358  [5.399 sec/step, loss=0.07309, avg_loss=0.07352]\n",
      "Step 533359  [5.388 sec/step, loss=0.07408, avg_loss=0.07351]\n",
      "Step 533360  [5.386 sec/step, loss=0.07375, avg_loss=0.07349]\n",
      "Step 533361  [5.404 sec/step, loss=0.07169, avg_loss=0.07355]\n",
      "Step 533362  [5.392 sec/step, loss=0.07280, avg_loss=0.07353]\n",
      "Step 533363  [5.363 sec/step, loss=0.06636, avg_loss=0.07345]\n",
      "Step 533364  [5.359 sec/step, loss=0.07459, avg_loss=0.07345]\n",
      "Step 533365  [5.361 sec/step, loss=0.07395, avg_loss=0.07347]\n",
      "Step 533366  [5.358 sec/step, loss=0.07532, avg_loss=0.07350]\n",
      "Step 533367  [5.354 sec/step, loss=0.07277, avg_loss=0.07349]\n",
      "Step 533368  [5.350 sec/step, loss=0.06974, avg_loss=0.07345]\n",
      "Step 533369  [5.358 sec/step, loss=0.07556, avg_loss=0.07346]\n",
      "Step 533370  [5.354 sec/step, loss=0.07043, avg_loss=0.07344]\n",
      "Step 533371  [5.339 sec/step, loss=0.07205, avg_loss=0.07341]\n",
      "Step 533372  [5.339 sec/step, loss=0.07344, avg_loss=0.07340]\n",
      "Step 533373  [5.323 sec/step, loss=0.07387, avg_loss=0.07342]\n",
      "Step 533374  [5.305 sec/step, loss=0.07352, avg_loss=0.07340]\n",
      "Step 533375  [5.300 sec/step, loss=0.07299, avg_loss=0.07338]\n",
      "Step 533376  [5.324 sec/step, loss=0.07286, avg_loss=0.07336]\n",
      "Step 533377  [5.337 sec/step, loss=0.07575, avg_loss=0.07337]\n",
      "Step 533378  [5.340 sec/step, loss=0.07578, avg_loss=0.07340]\n",
      "Step 533379  [5.347 sec/step, loss=0.07326, avg_loss=0.07341]\n",
      "Step 533380  [5.361 sec/step, loss=0.07484, avg_loss=0.07351]\n",
      "Step 533381  [5.385 sec/step, loss=0.07535, avg_loss=0.07352]\n",
      "Step 533382  [5.404 sec/step, loss=0.07388, avg_loss=0.07352]\n",
      "Step 533383  [5.445 sec/step, loss=0.06471, avg_loss=0.07341]\n",
      "Step 533384  [5.446 sec/step, loss=0.07553, avg_loss=0.07341]\n",
      "Step 533385  [5.457 sec/step, loss=0.07518, avg_loss=0.07343]\n",
      "Generated 32 batches of size 32 in 2.572 sec\n",
      "Step 533386  [5.447 sec/step, loss=0.07488, avg_loss=0.07342]\n",
      "Step 533387  [5.409 sec/step, loss=0.07447, avg_loss=0.07348]\n",
      "Step 533388  [5.394 sec/step, loss=0.07484, avg_loss=0.07347]\n",
      "Step 533389  [5.406 sec/step, loss=0.07422, avg_loss=0.07350]\n",
      "Step 533390  [5.403 sec/step, loss=0.07388, avg_loss=0.07349]\n",
      "Step 533391  [5.395 sec/step, loss=0.07593, avg_loss=0.07350]\n",
      "Step 533392  [5.419 sec/step, loss=0.07298, avg_loss=0.07352]\n",
      "Step 533393  [5.420 sec/step, loss=0.07452, avg_loss=0.07351]\n",
      "Step 533394  [5.416 sec/step, loss=0.07287, avg_loss=0.07348]\n",
      "Step 533395  [5.403 sec/step, loss=0.07230, avg_loss=0.07346]\n",
      "Step 533396  [5.395 sec/step, loss=0.07452, avg_loss=0.07348]\n",
      "Step 533397  [5.393 sec/step, loss=0.07633, avg_loss=0.07349]\n",
      "Step 533398  [5.398 sec/step, loss=0.07259, avg_loss=0.07346]\n",
      "Step 533399  [5.386 sec/step, loss=0.07308, avg_loss=0.07345]\n",
      "Step 533400  [5.378 sec/step, loss=0.07127, avg_loss=0.07341]\n",
      "Writing summary at step: 533400\n",
      "Step 533401  [5.385 sec/step, loss=0.07611, avg_loss=0.07343]\n",
      "Step 533402  [5.381 sec/step, loss=0.07504, avg_loss=0.07344]\n",
      "Step 533403  [5.353 sec/step, loss=0.07358, avg_loss=0.07351]\n",
      "Step 533404  [5.351 sec/step, loss=0.07459, avg_loss=0.07351]\n",
      "Step 533405  [5.350 sec/step, loss=0.07637, avg_loss=0.07353]\n",
      "Step 533406  [5.356 sec/step, loss=0.07167, avg_loss=0.07359]\n",
      "Step 533407  [5.375 sec/step, loss=0.07671, avg_loss=0.07363]\n",
      "Step 533408  [5.386 sec/step, loss=0.07656, avg_loss=0.07366]\n",
      "Step 533409  [5.389 sec/step, loss=0.07386, avg_loss=0.07366]\n",
      "Step 533410  [5.381 sec/step, loss=0.07318, avg_loss=0.07363]\n",
      "Step 533411  [5.381 sec/step, loss=0.07195, avg_loss=0.07362]\n",
      "Step 533412  [5.365 sec/step, loss=0.07447, avg_loss=0.07364]\n",
      "Step 533413  [5.359 sec/step, loss=0.07406, avg_loss=0.07364]\n",
      "Step 533414  [5.369 sec/step, loss=0.07602, avg_loss=0.07365]\n",
      "Step 533415  [5.431 sec/step, loss=0.06761, avg_loss=0.07361]\n",
      "Step 533416  [5.411 sec/step, loss=0.07446, avg_loss=0.07362]\n",
      "Generated 32 batches of size 32 in 2.396 sec\n",
      "Step 533417  [5.409 sec/step, loss=0.07538, avg_loss=0.07363]\n",
      "Step 533418  [5.408 sec/step, loss=0.07556, avg_loss=0.07365]\n",
      "Step 533419  [5.426 sec/step, loss=0.07676, avg_loss=0.07370]\n",
      "Step 533420  [5.409 sec/step, loss=0.06761, avg_loss=0.07366]\n",
      "Step 533421  [5.428 sec/step, loss=0.07748, avg_loss=0.07371]\n",
      "Step 533422  [5.396 sec/step, loss=0.07193, avg_loss=0.07368]\n",
      "Step 533423  [5.412 sec/step, loss=0.07414, avg_loss=0.07367]\n",
      "Step 533424  [5.408 sec/step, loss=0.07661, avg_loss=0.07367]\n",
      "Step 533425  [5.403 sec/step, loss=0.07530, avg_loss=0.07367]\n",
      "Step 533426  [5.401 sec/step, loss=0.07463, avg_loss=0.07367]\n",
      "Step 533427  [5.455 sec/step, loss=0.06587, avg_loss=0.07358]\n",
      "Step 533428  [5.451 sec/step, loss=0.07199, avg_loss=0.07355]\n",
      "Step 533429  [5.451 sec/step, loss=0.07423, avg_loss=0.07356]\n",
      "Step 533430  [5.434 sec/step, loss=0.07401, avg_loss=0.07357]\n",
      "Step 533431  [5.445 sec/step, loss=0.07427, avg_loss=0.07361]\n",
      "Step 533432  [5.448 sec/step, loss=0.07530, avg_loss=0.07363]\n",
      "Step 533433  [5.404 sec/step, loss=0.06650, avg_loss=0.07357]\n",
      "Step 533434  [5.433 sec/step, loss=0.07297, avg_loss=0.07355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 533435  [5.454 sec/step, loss=0.07550, avg_loss=0.07357]\n",
      "Step 533436  [5.430 sec/step, loss=0.07243, avg_loss=0.07353]\n",
      "Step 533437  [5.423 sec/step, loss=0.07594, avg_loss=0.07355]\n",
      "Step 533438  [5.436 sec/step, loss=0.07520, avg_loss=0.07356]\n",
      "Step 533439  [5.428 sec/step, loss=0.07333, avg_loss=0.07354]\n",
      "Step 533440  [5.440 sec/step, loss=0.07636, avg_loss=0.07356]\n",
      "Step 533441  [5.449 sec/step, loss=0.07610, avg_loss=0.07357]\n",
      "Step 533442  [5.458 sec/step, loss=0.07427, avg_loss=0.07366]\n",
      "Step 533443  [5.458 sec/step, loss=0.07548, avg_loss=0.07370]\n",
      "Step 533444  [5.450 sec/step, loss=0.07397, avg_loss=0.07368]\n",
      "Step 533445  [5.454 sec/step, loss=0.07637, avg_loss=0.07369]\n",
      "Step 533446  [5.449 sec/step, loss=0.07069, avg_loss=0.07368]\n",
      "Step 533447  [5.455 sec/step, loss=0.07624, avg_loss=0.07369]\n",
      "Step 533448  [5.407 sec/step, loss=0.07546, avg_loss=0.07378]\n",
      "Generated 32 batches of size 32 in 2.332 sec\n",
      "Step 533449  [5.403 sec/step, loss=0.07499, avg_loss=0.07377]\n",
      "Step 533450  [5.423 sec/step, loss=0.07588, avg_loss=0.07378]\n",
      "Step 533451  [5.422 sec/step, loss=0.07499, avg_loss=0.07383]\n",
      "Step 533452  [5.415 sec/step, loss=0.07252, avg_loss=0.07380]\n",
      "Step 533453  [5.396 sec/step, loss=0.07452, avg_loss=0.07380]\n",
      "Step 533454  [5.391 sec/step, loss=0.07339, avg_loss=0.07378]\n",
      "Step 533455  [5.376 sec/step, loss=0.07500, avg_loss=0.07377]\n",
      "Step 533456  [5.398 sec/step, loss=0.07459, avg_loss=0.07381]\n",
      "Step 533457  [5.388 sec/step, loss=0.07348, avg_loss=0.07379]\n",
      "Step 533458  [5.375 sec/step, loss=0.07283, avg_loss=0.07379]\n",
      "Step 533459  [5.419 sec/step, loss=0.06551, avg_loss=0.07370]\n",
      "Step 533460  [5.430 sec/step, loss=0.07603, avg_loss=0.07373]\n",
      "Step 533461  [5.424 sec/step, loss=0.07393, avg_loss=0.07375]\n",
      "Step 533462  [5.424 sec/step, loss=0.07379, avg_loss=0.07376]\n",
      "Step 533463  [5.453 sec/step, loss=0.07323, avg_loss=0.07383]\n",
      "Step 533464  [5.443 sec/step, loss=0.07425, avg_loss=0.07383]\n",
      "Step 533465  [5.445 sec/step, loss=0.07492, avg_loss=0.07384]\n",
      "Step 533466  [5.460 sec/step, loss=0.07568, avg_loss=0.07384]\n",
      "Step 533467  [5.462 sec/step, loss=0.07001, avg_loss=0.07381]\n",
      "Step 533468  [5.464 sec/step, loss=0.07490, avg_loss=0.07386]\n",
      "Step 533469  [5.453 sec/step, loss=0.07461, avg_loss=0.07385]\n",
      "Step 533470  [5.459 sec/step, loss=0.07485, avg_loss=0.07390]\n",
      "Step 533471  [5.466 sec/step, loss=0.07216, avg_loss=0.07390]\n",
      "Step 533472  [5.477 sec/step, loss=0.07274, avg_loss=0.07389]\n",
      "Step 533473  [5.489 sec/step, loss=0.07496, avg_loss=0.07390]\n",
      "Step 533474  [5.500 sec/step, loss=0.07464, avg_loss=0.07391]\n",
      "Step 533475  [5.505 sec/step, loss=0.07577, avg_loss=0.07394]\n",
      "Step 533476  [5.476 sec/step, loss=0.07329, avg_loss=0.07395]\n",
      "Step 533477  [5.470 sec/step, loss=0.07398, avg_loss=0.07393]\n",
      "Step 533478  [5.454 sec/step, loss=0.06396, avg_loss=0.07381]\n",
      "Step 533479  [5.454 sec/step, loss=0.07284, avg_loss=0.07381]\n",
      "Step 533480  [5.473 sec/step, loss=0.07546, avg_loss=0.07381]\n",
      "Generated 32 batches of size 32 in 2.411 sec\n",
      "Step 533481  [5.477 sec/step, loss=0.07513, avg_loss=0.07381]\n",
      "Step 533482  [5.474 sec/step, loss=0.07382, avg_loss=0.07381]\n",
      "Step 533483  [5.410 sec/step, loss=0.07024, avg_loss=0.07386]\n",
      "Step 533484  [5.405 sec/step, loss=0.07584, avg_loss=0.07387]\n",
      "Step 533485  [5.417 sec/step, loss=0.07572, avg_loss=0.07387]\n",
      "Step 533486  [5.418 sec/step, loss=0.07464, avg_loss=0.07387]\n",
      "Step 533487  [5.403 sec/step, loss=0.07388, avg_loss=0.07386]\n",
      "Step 533488  [5.406 sec/step, loss=0.07462, avg_loss=0.07386]\n",
      "Step 533489  [5.414 sec/step, loss=0.07591, avg_loss=0.07388]\n",
      "Step 533490  [5.420 sec/step, loss=0.07464, avg_loss=0.07389]\n",
      "Step 533491  [5.413 sec/step, loss=0.07510, avg_loss=0.07388]\n",
      "Step 533492  [5.389 sec/step, loss=0.07277, avg_loss=0.07388]\n",
      "Step 533493  [5.393 sec/step, loss=0.07445, avg_loss=0.07388]\n",
      "Step 533494  [5.397 sec/step, loss=0.07482, avg_loss=0.07390]\n",
      "Step 533495  [5.385 sec/step, loss=0.06623, avg_loss=0.07383]\n",
      "Step 533496  [5.398 sec/step, loss=0.07571, avg_loss=0.07385]\n",
      "Step 533497  [5.372 sec/step, loss=0.07189, avg_loss=0.07380]\n",
      "Step 533498  [5.365 sec/step, loss=0.07529, avg_loss=0.07383]\n",
      "Step 533499  [5.368 sec/step, loss=0.07330, avg_loss=0.07383]\n",
      "Step 533500  [5.355 sec/step, loss=0.07196, avg_loss=0.07384]\n",
      "Writing summary at step: 533500\n",
      "Step 533501  [5.343 sec/step, loss=0.07226, avg_loss=0.07380]\n",
      "Step 533502  [5.346 sec/step, loss=0.07413, avg_loss=0.07379]\n",
      "Step 533503  [5.322 sec/step, loss=0.07443, avg_loss=0.07380]\n",
      "Step 533504  [5.343 sec/step, loss=0.07347, avg_loss=0.07379]\n",
      "Step 533505  [5.346 sec/step, loss=0.07548, avg_loss=0.07378]\n",
      "Step 533506  [5.360 sec/step, loss=0.07409, avg_loss=0.07380]\n",
      "Step 533507  [5.359 sec/step, loss=0.07612, avg_loss=0.07380]\n",
      "Step 533508  [5.365 sec/step, loss=0.07587, avg_loss=0.07379]\n",
      "Step 533509  [5.359 sec/step, loss=0.07363, avg_loss=0.07379]\n",
      "Step 533510  [5.407 sec/step, loss=0.06608, avg_loss=0.07372]\n",
      "Step 533511  [5.419 sec/step, loss=0.07638, avg_loss=0.07376]\n",
      "Generated 32 batches of size 32 in 2.388 sec\n",
      "Step 533512  [5.418 sec/step, loss=0.07439, avg_loss=0.07376]\n",
      "Step 533513  [5.430 sec/step, loss=0.07421, avg_loss=0.07376]\n",
      "Step 533514  [5.420 sec/step, loss=0.07292, avg_loss=0.07373]\n",
      "Step 533515  [5.390 sec/step, loss=0.07390, avg_loss=0.07379]\n",
      "Step 533516  [5.394 sec/step, loss=0.07373, avg_loss=0.07379]\n",
      "Step 533517  [5.392 sec/step, loss=0.07612, avg_loss=0.07379]\n",
      "Step 533518  [5.394 sec/step, loss=0.07497, avg_loss=0.07379]\n",
      "Step 533519  [5.400 sec/step, loss=0.07586, avg_loss=0.07378]\n",
      "Step 533520  [5.421 sec/step, loss=0.07512, avg_loss=0.07385]\n",
      "Step 533521  [5.457 sec/step, loss=0.06669, avg_loss=0.07375]\n",
      "Step 533522  [5.478 sec/step, loss=0.07393, avg_loss=0.07377]\n",
      "Step 533523  [5.452 sec/step, loss=0.07559, avg_loss=0.07378]\n",
      "Step 533524  [5.452 sec/step, loss=0.07610, avg_loss=0.07378]\n",
      "Step 533525  [5.470 sec/step, loss=0.07288, avg_loss=0.07375]\n",
      "Step 533526  [5.464 sec/step, loss=0.07308, avg_loss=0.07374]\n",
      "Step 533527  [5.432 sec/step, loss=0.07539, avg_loss=0.07383]\n",
      "Step 533528  [5.439 sec/step, loss=0.07464, avg_loss=0.07386]\n",
      "Step 533529  [5.441 sec/step, loss=0.07466, avg_loss=0.07386]\n",
      "Step 533530  [5.449 sec/step, loss=0.07599, avg_loss=0.07388]\n",
      "Step 533531  [5.452 sec/step, loss=0.07486, avg_loss=0.07389]\n",
      "Step 533532  [5.452 sec/step, loss=0.07523, avg_loss=0.07389]\n",
      "Step 533533  [5.477 sec/step, loss=0.07456, avg_loss=0.07397]\n",
      "Step 533534  [5.449 sec/step, loss=0.07093, avg_loss=0.07395]\n",
      "Step 533535  [5.431 sec/step, loss=0.07381, avg_loss=0.07393]\n",
      "Step 533536  [5.454 sec/step, loss=0.07608, avg_loss=0.07397]\n",
      "Step 533537  [5.443 sec/step, loss=0.07439, avg_loss=0.07395]\n",
      "Step 533538  [5.440 sec/step, loss=0.07463, avg_loss=0.07395]\n",
      "Step 533539  [5.443 sec/step, loss=0.07220, avg_loss=0.07393]\n",
      "Step 533540  [5.443 sec/step, loss=0.07639, avg_loss=0.07393]\n",
      "Step 533541  [5.458 sec/step, loss=0.07358, avg_loss=0.07391]\n",
      "Step 533542  [5.470 sec/step, loss=0.07488, avg_loss=0.07392]\n",
      "Step 533543  [5.470 sec/step, loss=0.07529, avg_loss=0.07391]\n",
      "Generated 32 batches of size 32 in 2.722 sec\n",
      "Step 533544  [5.457 sec/step, loss=0.07163, avg_loss=0.07389]\n",
      "Step 533545  [5.461 sec/step, loss=0.07643, avg_loss=0.07389]\n",
      "Step 533546  [5.478 sec/step, loss=0.07492, avg_loss=0.07393]\n",
      "Step 533547  [5.470 sec/step, loss=0.07322, avg_loss=0.07390]\n",
      "Step 533548  [5.452 sec/step, loss=0.06700, avg_loss=0.07382]\n",
      "Step 533549  [5.456 sec/step, loss=0.07573, avg_loss=0.07383]\n",
      "Step 533550  [5.429 sec/step, loss=0.07422, avg_loss=0.07381]\n",
      "Step 533551  [5.420 sec/step, loss=0.07056, avg_loss=0.07376]\n",
      "Step 533552  [5.417 sec/step, loss=0.07398, avg_loss=0.07378]\n",
      "Step 533553  [5.437 sec/step, loss=0.07568, avg_loss=0.07379]\n",
      "Step 533554  [5.426 sec/step, loss=0.07423, avg_loss=0.07380]\n",
      "Step 533555  [5.430 sec/step, loss=0.07525, avg_loss=0.07380]\n",
      "Step 533556  [5.448 sec/step, loss=0.07506, avg_loss=0.07381]\n",
      "Step 533557  [5.462 sec/step, loss=0.07596, avg_loss=0.07383]\n",
      "Step 533558  [5.457 sec/step, loss=0.07452, avg_loss=0.07385]\n",
      "Step 533559  [5.425 sec/step, loss=0.07557, avg_loss=0.07395]\n",
      "Step 533560  [5.404 sec/step, loss=0.07408, avg_loss=0.07393]\n",
      "Step 533561  [5.406 sec/step, loss=0.07063, avg_loss=0.07390]\n",
      "Step 533562  [5.412 sec/step, loss=0.07315, avg_loss=0.07389]\n",
      "Step 533563  [5.416 sec/step, loss=0.07500, avg_loss=0.07391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 533564  [5.408 sec/step, loss=0.07231, avg_loss=0.07389]\n",
      "Step 533565  [5.403 sec/step, loss=0.07374, avg_loss=0.07388]\n",
      "Step 533566  [5.394 sec/step, loss=0.07130, avg_loss=0.07383]\n",
      "Step 533567  [5.391 sec/step, loss=0.07039, avg_loss=0.07384]\n",
      "Step 533568  [5.384 sec/step, loss=0.06954, avg_loss=0.07378]\n",
      "Step 533569  [5.427 sec/step, loss=0.06812, avg_loss=0.07372]\n",
      "Step 533570  [5.427 sec/step, loss=0.07507, avg_loss=0.07372]\n",
      "Step 533571  [5.433 sec/step, loss=0.07483, avg_loss=0.07375]\n",
      "Step 533572  [5.429 sec/step, loss=0.07595, avg_loss=0.07378]\n",
      "Step 533573  [5.409 sec/step, loss=0.07452, avg_loss=0.07377]\n",
      "Step 533574  [5.413 sec/step, loss=0.07624, avg_loss=0.07379]\n",
      "Step 533575  [5.408 sec/step, loss=0.07232, avg_loss=0.07376]\n",
      "Generated 32 batches of size 32 in 2.421 sec\n",
      "Step 533576  [5.428 sec/step, loss=0.07591, avg_loss=0.07378]\n",
      "Step 533577  [5.419 sec/step, loss=0.07504, avg_loss=0.07379]\n",
      "Step 533578  [5.443 sec/step, loss=0.07486, avg_loss=0.07390]\n",
      "Step 533579  [5.460 sec/step, loss=0.07604, avg_loss=0.07393]\n",
      "Step 533580  [5.438 sec/step, loss=0.07128, avg_loss=0.07389]\n",
      "Step 533581  [5.423 sec/step, loss=0.07545, avg_loss=0.07389]\n",
      "Step 533582  [5.416 sec/step, loss=0.07478, avg_loss=0.07390]\n",
      "Step 533583  [5.441 sec/step, loss=0.07278, avg_loss=0.07393]\n",
      "Step 533584  [5.422 sec/step, loss=0.06407, avg_loss=0.07381]\n",
      "Step 533585  [5.400 sec/step, loss=0.06974, avg_loss=0.07375]\n",
      "Step 533586  [5.402 sec/step, loss=0.07480, avg_loss=0.07375]\n",
      "Step 533587  [5.420 sec/step, loss=0.07574, avg_loss=0.07377]\n",
      "Step 533588  [5.435 sec/step, loss=0.07303, avg_loss=0.07376]\n",
      "Step 533589  [5.421 sec/step, loss=0.07336, avg_loss=0.07373]\n",
      "Step 533590  [5.432 sec/step, loss=0.07355, avg_loss=0.07372]\n",
      "Step 533591  [5.480 sec/step, loss=0.06696, avg_loss=0.07364]\n",
      "Step 533592  [5.488 sec/step, loss=0.07484, avg_loss=0.07366]\n",
      "Step 533593  [5.487 sec/step, loss=0.07368, avg_loss=0.07365]\n",
      "Step 533594  [5.493 sec/step, loss=0.07582, avg_loss=0.07366]\n",
      "Step 533595  [5.536 sec/step, loss=0.07362, avg_loss=0.07374]\n",
      "Step 533596  [5.534 sec/step, loss=0.07603, avg_loss=0.07374]\n",
      "Step 533597  [5.552 sec/step, loss=0.07414, avg_loss=0.07376]\n",
      "Step 533598  [5.555 sec/step, loss=0.07513, avg_loss=0.07376]\n",
      "Step 533599  [5.574 sec/step, loss=0.07552, avg_loss=0.07378]\n",
      "Step 533600  [5.593 sec/step, loss=0.07452, avg_loss=0.07381]\n",
      "Writing summary at step: 533600\n",
      "Step 533601  [5.607 sec/step, loss=0.07615, avg_loss=0.07385]\n",
      "Step 533602  [5.601 sec/step, loss=0.07386, avg_loss=0.07384]\n",
      "Step 533603  [5.605 sec/step, loss=0.07475, avg_loss=0.07385]\n",
      "Step 533604  [5.578 sec/step, loss=0.07492, avg_loss=0.07386]\n",
      "Step 533605  [5.560 sec/step, loss=0.07294, avg_loss=0.07384]\n",
      "Step 533606  [5.565 sec/step, loss=0.07449, avg_loss=0.07384]\n",
      "Generated 32 batches of size 32 in 2.405 sec\n",
      "Step 533607  [5.563 sec/step, loss=0.07190, avg_loss=0.07380]\n",
      "Step 533608  [5.558 sec/step, loss=0.07447, avg_loss=0.07378]\n",
      "Step 533609  [5.565 sec/step, loss=0.07397, avg_loss=0.07379]\n",
      "Step 533610  [5.516 sec/step, loss=0.07126, avg_loss=0.07384]\n",
      "Step 533611  [5.494 sec/step, loss=0.07235, avg_loss=0.07380]\n",
      "Step 533612  [5.485 sec/step, loss=0.07444, avg_loss=0.07380]\n",
      "Step 533613  [5.457 sec/step, loss=0.06753, avg_loss=0.07373]\n",
      "Step 533614  [5.462 sec/step, loss=0.07511, avg_loss=0.07375]\n",
      "Step 533615  [5.435 sec/step, loss=0.07058, avg_loss=0.07372]\n",
      "Step 533616  [5.438 sec/step, loss=0.07366, avg_loss=0.07372]\n",
      "Step 533617  [5.432 sec/step, loss=0.07458, avg_loss=0.07370]\n",
      "Step 533618  [5.437 sec/step, loss=0.07495, avg_loss=0.07370]\n",
      "Step 533619  [5.406 sec/step, loss=0.06622, avg_loss=0.07361]\n",
      "Step 533620  [5.411 sec/step, loss=0.07474, avg_loss=0.07360]\n",
      "Step 533621  [5.360 sec/step, loss=0.07299, avg_loss=0.07367]\n",
      "Step 533622  [5.362 sec/step, loss=0.07582, avg_loss=0.07369]\n",
      "Step 533623  [5.412 sec/step, loss=0.06634, avg_loss=0.07359]\n",
      "Step 533624  [5.412 sec/step, loss=0.07602, avg_loss=0.07359]\n",
      "Step 533625  [5.394 sec/step, loss=0.07470, avg_loss=0.07361]\n",
      "Step 533626  [5.398 sec/step, loss=0.07477, avg_loss=0.07363]\n",
      "Step 533627  [5.381 sec/step, loss=0.07493, avg_loss=0.07362]\n",
      "Step 533628  [5.389 sec/step, loss=0.07566, avg_loss=0.07363]\n",
      "Step 533629  [5.402 sec/step, loss=0.07595, avg_loss=0.07365]\n",
      "Step 533630  [5.415 sec/step, loss=0.07317, avg_loss=0.07362]\n",
      "Step 533631  [5.406 sec/step, loss=0.07405, avg_loss=0.07361]\n",
      "Step 533632  [5.413 sec/step, loss=0.07425, avg_loss=0.07360]\n",
      "Step 533633  [5.393 sec/step, loss=0.07412, avg_loss=0.07360]\n",
      "Step 533634  [5.415 sec/step, loss=0.07343, avg_loss=0.07362]\n",
      "Step 533635  [5.435 sec/step, loss=0.07534, avg_loss=0.07364]\n",
      "Step 533636  [5.427 sec/step, loss=0.07517, avg_loss=0.07363]\n",
      "Step 533637  [5.431 sec/step, loss=0.07217, avg_loss=0.07361]\n",
      "Step 533638  [5.429 sec/step, loss=0.07065, avg_loss=0.07357]\n",
      "Generated 32 batches of size 32 in 2.558 sec\n",
      "Step 533639  [5.433 sec/step, loss=0.07235, avg_loss=0.07357]\n",
      "Step 533640  [5.424 sec/step, loss=0.07256, avg_loss=0.07353]\n",
      "Step 533641  [5.403 sec/step, loss=0.07513, avg_loss=0.07354]\n",
      "Step 533642  [5.403 sec/step, loss=0.07459, avg_loss=0.07354]\n",
      "Step 533643  [5.387 sec/step, loss=0.07105, avg_loss=0.07350]\n",
      "Step 533644  [5.389 sec/step, loss=0.07238, avg_loss=0.07351]\n",
      "Step 533645  [5.384 sec/step, loss=0.07622, avg_loss=0.07350]\n",
      "Step 533646  [5.369 sec/step, loss=0.07016, avg_loss=0.07346]\n",
      "Step 533647  [5.380 sec/step, loss=0.07291, avg_loss=0.07345]\n",
      "Step 533648  [5.391 sec/step, loss=0.07207, avg_loss=0.07350]\n",
      "Step 533649  [5.401 sec/step, loss=0.07436, avg_loss=0.07349]\n",
      "Step 533650  [5.410 sec/step, loss=0.07042, avg_loss=0.07345]\n",
      "Step 533651  [5.433 sec/step, loss=0.07578, avg_loss=0.07350]\n",
      "Step 533652  [5.485 sec/step, loss=0.06796, avg_loss=0.07344]\n",
      "Step 533653  [5.477 sec/step, loss=0.07406, avg_loss=0.07343]\n",
      "Step 533654  [5.476 sec/step, loss=0.07492, avg_loss=0.07344]\n",
      "Step 533655  [5.482 sec/step, loss=0.07563, avg_loss=0.07344]\n",
      "Step 533656  [5.441 sec/step, loss=0.06648, avg_loss=0.07335]\n",
      "Step 533657  [5.421 sec/step, loss=0.07375, avg_loss=0.07333]\n",
      "Step 533658  [5.440 sec/step, loss=0.07571, avg_loss=0.07334]\n",
      "Step 533659  [5.408 sec/step, loss=0.07138, avg_loss=0.07330]\n",
      "Step 533660  [5.416 sec/step, loss=0.07489, avg_loss=0.07331]\n",
      "Step 533661  [5.414 sec/step, loss=0.07385, avg_loss=0.07334]\n",
      "Step 533662  [5.416 sec/step, loss=0.07335, avg_loss=0.07334]\n",
      "Step 533663  [5.406 sec/step, loss=0.07344, avg_loss=0.07333]\n",
      "Step 533664  [5.416 sec/step, loss=0.07518, avg_loss=0.07336]\n",
      "Step 533665  [5.434 sec/step, loss=0.07530, avg_loss=0.07337]\n",
      "Step 533666  [5.450 sec/step, loss=0.07475, avg_loss=0.07341]\n",
      "Step 533667  [5.458 sec/step, loss=0.07368, avg_loss=0.07344]\n",
      "Step 533668  [5.460 sec/step, loss=0.06994, avg_loss=0.07344]\n",
      "Step 533669  [5.423 sec/step, loss=0.07352, avg_loss=0.07350]\n",
      "Step 533670  [5.425 sec/step, loss=0.07225, avg_loss=0.07347]\n",
      "Generated 32 batches of size 32 in 2.415 sec\n",
      "Step 533671  [5.435 sec/step, loss=0.07601, avg_loss=0.07348]\n",
      "Step 533672  [5.439 sec/step, loss=0.07567, avg_loss=0.07348]\n",
      "Step 533673  [5.438 sec/step, loss=0.07523, avg_loss=0.07349]\n",
      "Step 533674  [5.435 sec/step, loss=0.07469, avg_loss=0.07347]\n",
      "Step 533675  [5.435 sec/step, loss=0.07467, avg_loss=0.07349]\n",
      "Step 533676  [5.429 sec/step, loss=0.07571, avg_loss=0.07349]\n",
      "Step 533677  [5.432 sec/step, loss=0.07383, avg_loss=0.07348]\n",
      "Step 533678  [5.414 sec/step, loss=0.07170, avg_loss=0.07345]\n",
      "Step 533679  [5.412 sec/step, loss=0.07401, avg_loss=0.07343]\n",
      "Step 533680  [5.412 sec/step, loss=0.07293, avg_loss=0.07344]\n",
      "Step 533681  [5.409 sec/step, loss=0.07369, avg_loss=0.07343]\n",
      "Step 533682  [5.405 sec/step, loss=0.07358, avg_loss=0.07341]\n",
      "Step 533683  [5.405 sec/step, loss=0.07327, avg_loss=0.07342]\n",
      "Step 533684  [5.424 sec/step, loss=0.07550, avg_loss=0.07353]\n",
      "Step 533685  [5.412 sec/step, loss=0.06375, avg_loss=0.07347]\n",
      "Step 533686  [5.407 sec/step, loss=0.07455, avg_loss=0.07347]\n",
      "Step 533687  [5.403 sec/step, loss=0.07287, avg_loss=0.07344]\n",
      "Step 533688  [5.397 sec/step, loss=0.07331, avg_loss=0.07345]\n",
      "Step 533689  [5.402 sec/step, loss=0.07522, avg_loss=0.07346]\n",
      "Step 533690  [5.418 sec/step, loss=0.07274, avg_loss=0.07346]\n",
      "Step 533691  [5.362 sec/step, loss=0.07360, avg_loss=0.07352]\n",
      "Step 533692  [5.349 sec/step, loss=0.07021, avg_loss=0.07348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 533693  [5.345 sec/step, loss=0.07621, avg_loss=0.07350]\n",
      "Step 533694  [5.326 sec/step, loss=0.07112, avg_loss=0.07345]\n",
      "Step 533695  [5.349 sec/step, loss=0.06670, avg_loss=0.07339]\n",
      "Step 533696  [5.349 sec/step, loss=0.07450, avg_loss=0.07337]\n",
      "Step 533697  [5.349 sec/step, loss=0.07428, avg_loss=0.07337]\n",
      "Step 533698  [5.359 sec/step, loss=0.07582, avg_loss=0.07338]\n",
      "Step 533699  [5.353 sec/step, loss=0.07553, avg_loss=0.07338]\n",
      "Step 533700  [5.341 sec/step, loss=0.07171, avg_loss=0.07335]\n",
      "Writing summary at step: 533700\n",
      "Step 533701  [5.329 sec/step, loss=0.07490, avg_loss=0.07334]\n",
      "Generated 32 batches of size 32 in 2.550 sec\n",
      "Step 533702  [5.339 sec/step, loss=0.07498, avg_loss=0.07335]\n",
      "Step 533703  [5.359 sec/step, loss=0.07249, avg_loss=0.07333]\n",
      "Step 533704  [5.366 sec/step, loss=0.07517, avg_loss=0.07333]\n",
      "Step 533705  [5.375 sec/step, loss=0.07455, avg_loss=0.07334]\n",
      "Step 533706  [5.365 sec/step, loss=0.07343, avg_loss=0.07333]\n",
      "Step 533707  [5.350 sec/step, loss=0.07394, avg_loss=0.07335]\n",
      "Step 533708  [5.335 sec/step, loss=0.07157, avg_loss=0.07333]\n",
      "Step 533709  [5.343 sec/step, loss=0.07468, avg_loss=0.07333]\n",
      "Step 533710  [5.356 sec/step, loss=0.07601, avg_loss=0.07338]\n",
      "Step 533711  [5.367 sec/step, loss=0.07336, avg_loss=0.07339]\n",
      "Step 533712  [5.366 sec/step, loss=0.07456, avg_loss=0.07339]\n",
      "Step 533713  [5.375 sec/step, loss=0.07364, avg_loss=0.07345]\n",
      "Step 533714  [5.377 sec/step, loss=0.07188, avg_loss=0.07342]\n",
      "Step 533715  [5.435 sec/step, loss=0.06602, avg_loss=0.07337]\n",
      "Step 533716  [5.454 sec/step, loss=0.07536, avg_loss=0.07339]\n",
      "Step 533717  [5.439 sec/step, loss=0.06647, avg_loss=0.07331]\n",
      "Step 533718  [5.424 sec/step, loss=0.07381, avg_loss=0.07330]\n",
      "Step 533719  [5.431 sec/step, loss=0.07062, avg_loss=0.07334]\n",
      "Step 533720  [5.430 sec/step, loss=0.07592, avg_loss=0.07336]\n",
      "Step 533721  [5.443 sec/step, loss=0.07538, avg_loss=0.07338]\n",
      "Step 533722  [5.432 sec/step, loss=0.07196, avg_loss=0.07334]\n",
      "Step 533723  [5.367 sec/step, loss=0.07142, avg_loss=0.07339]\n",
      "Step 533724  [5.358 sec/step, loss=0.07454, avg_loss=0.07338]\n",
      "Step 533725  [5.357 sec/step, loss=0.07288, avg_loss=0.07336]\n",
      "Step 533726  [5.361 sec/step, loss=0.07514, avg_loss=0.07336]\n",
      "Step 533727  [5.371 sec/step, loss=0.07386, avg_loss=0.07335]\n",
      "Step 533728  [5.384 sec/step, loss=0.07302, avg_loss=0.07332]\n",
      "Step 533729  [5.375 sec/step, loss=0.07225, avg_loss=0.07329]\n",
      "Step 533730  [5.360 sec/step, loss=0.07306, avg_loss=0.07329]\n",
      "Step 533731  [5.386 sec/step, loss=0.07345, avg_loss=0.07328]\n",
      "Step 533732  [5.379 sec/step, loss=0.07221, avg_loss=0.07326]\n",
      "Step 533733  [5.386 sec/step, loss=0.07492, avg_loss=0.07327]\n",
      "Generated 32 batches of size 32 in 2.388 sec\n",
      "Step 533734  [5.376 sec/step, loss=0.07426, avg_loss=0.07328]\n",
      "Step 533735  [5.363 sec/step, loss=0.07504, avg_loss=0.07327]\n",
      "Step 533736  [5.371 sec/step, loss=0.07553, avg_loss=0.07328]\n",
      "Step 533737  [5.375 sec/step, loss=0.07504, avg_loss=0.07331]\n",
      "Step 533738  [5.382 sec/step, loss=0.07409, avg_loss=0.07334]\n",
      "Step 533739  [5.374 sec/step, loss=0.07200, avg_loss=0.07334]\n",
      "Step 533740  [5.377 sec/step, loss=0.07465, avg_loss=0.07336]\n",
      "Step 533741  [5.377 sec/step, loss=0.07524, avg_loss=0.07336]\n",
      "Step 533742  [5.386 sec/step, loss=0.07582, avg_loss=0.07337]\n",
      "Step 533743  [5.396 sec/step, loss=0.07555, avg_loss=0.07342]\n",
      "Step 533744  [5.388 sec/step, loss=0.06641, avg_loss=0.07336]\n",
      "Step 533745  [5.375 sec/step, loss=0.07156, avg_loss=0.07331]\n",
      "Step 533746  [5.385 sec/step, loss=0.07526, avg_loss=0.07336]\n",
      "Step 533747  [5.384 sec/step, loss=0.07329, avg_loss=0.07336]\n",
      "Step 533748  [5.404 sec/step, loss=0.07561, avg_loss=0.07340]\n",
      "Step 533749  [5.381 sec/step, loss=0.07394, avg_loss=0.07340]\n",
      "Step 533750  [5.390 sec/step, loss=0.07529, avg_loss=0.07344]\n",
      "Step 533751  [5.429 sec/step, loss=0.06586, avg_loss=0.07335]\n",
      "Step 533752  [5.388 sec/step, loss=0.07476, avg_loss=0.07341]\n",
      "Step 533753  [5.413 sec/step, loss=0.07289, avg_loss=0.07340]\n",
      "Step 533754  [5.407 sec/step, loss=0.07386, avg_loss=0.07339]\n",
      "Step 533755  [5.394 sec/step, loss=0.07427, avg_loss=0.07338]\n",
      "Step 533756  [5.399 sec/step, loss=0.07112, avg_loss=0.07342]\n",
      "Step 533757  [5.418 sec/step, loss=0.07596, avg_loss=0.07345]\n",
      "Step 533758  [5.395 sec/step, loss=0.07404, avg_loss=0.07343]\n",
      "Step 533759  [5.415 sec/step, loss=0.07544, avg_loss=0.07347]\n",
      "Step 533760  [5.419 sec/step, loss=0.07490, avg_loss=0.07347]\n",
      "Step 533761  [5.423 sec/step, loss=0.07287, avg_loss=0.07346]\n",
      "Step 533762  [5.424 sec/step, loss=0.07306, avg_loss=0.07346]\n",
      "Step 533763  [5.406 sec/step, loss=0.07079, avg_loss=0.07343]\n",
      "Step 533764  [5.397 sec/step, loss=0.07281, avg_loss=0.07341]\n",
      "Step 533765  [5.390 sec/step, loss=0.07583, avg_loss=0.07341]\n",
      "Generated 32 batches of size 32 in 2.391 sec\n",
      "Step 533766  [5.390 sec/step, loss=0.07612, avg_loss=0.07343]\n",
      "Step 533767  [5.413 sec/step, loss=0.07558, avg_loss=0.07345]\n",
      "Step 533768  [5.418 sec/step, loss=0.07337, avg_loss=0.07348]\n",
      "Step 533769  [5.408 sec/step, loss=0.07273, avg_loss=0.07347]\n",
      "Step 533770  [5.413 sec/step, loss=0.07472, avg_loss=0.07350]\n",
      "Step 533771  [5.413 sec/step, loss=0.07415, avg_loss=0.07348]\n",
      "Step 533772  [5.412 sec/step, loss=0.07562, avg_loss=0.07348]\n",
      "Step 533773  [5.412 sec/step, loss=0.07457, avg_loss=0.07347]\n",
      "Step 533774  [5.408 sec/step, loss=0.07544, avg_loss=0.07348]\n",
      "Step 533775  [5.431 sec/step, loss=0.07352, avg_loss=0.07347]\n",
      "Step 533776  [5.423 sec/step, loss=0.07544, avg_loss=0.07346]\n",
      "Step 533777  [5.422 sec/step, loss=0.07304, avg_loss=0.07346]\n",
      "Step 533778  [5.437 sec/step, loss=0.07572, avg_loss=0.07350]\n",
      "Step 533779  [5.442 sec/step, loss=0.07589, avg_loss=0.07351]\n",
      "Step 533780  [5.448 sec/step, loss=0.07553, avg_loss=0.07354]\n",
      "Step 533781  [5.498 sec/step, loss=0.06688, avg_loss=0.07347]\n",
      "Step 533782  [5.495 sec/step, loss=0.07339, avg_loss=0.07347]\n",
      "Step 533783  [5.482 sec/step, loss=0.07477, avg_loss=0.07349]\n",
      "Step 533784  [5.485 sec/step, loss=0.07447, avg_loss=0.07348]\n",
      "Step 533785  [5.512 sec/step, loss=0.07658, avg_loss=0.07360]\n",
      "Step 533786  [5.521 sec/step, loss=0.07495, avg_loss=0.07361]\n",
      "Step 533787  [5.521 sec/step, loss=0.07465, avg_loss=0.07363]\n",
      "Step 533788  [5.519 sec/step, loss=0.07445, avg_loss=0.07364]\n",
      "Step 533789  [5.515 sec/step, loss=0.07488, avg_loss=0.07363]\n",
      "Step 533790  [5.501 sec/step, loss=0.07586, avg_loss=0.07366]\n",
      "Step 533791  [5.504 sec/step, loss=0.07298, avg_loss=0.07366]\n",
      "Step 533792  [5.525 sec/step, loss=0.07607, avg_loss=0.07372]\n",
      "Step 533793  [5.523 sec/step, loss=0.07512, avg_loss=0.07371]\n",
      "Step 533794  [5.535 sec/step, loss=0.07477, avg_loss=0.07374]\n",
      "Step 533795  [5.474 sec/step, loss=0.07408, avg_loss=0.07382]\n",
      "Step 533796  [5.457 sec/step, loss=0.07414, avg_loss=0.07381]\n",
      "Step 533797  [5.451 sec/step, loss=0.07109, avg_loss=0.07378]\n",
      "Generated 32 batches of size 32 in 2.742 sec\n",
      "Step 533798  [5.430 sec/step, loss=0.07158, avg_loss=0.07374]\n",
      "Step 533799  [5.433 sec/step, loss=0.07541, avg_loss=0.07374]\n",
      "Step 533800  [5.433 sec/step, loss=0.07341, avg_loss=0.07375]\n",
      "Writing summary at step: 533800\n",
      "Step 533801  [5.455 sec/step, loss=0.07348, avg_loss=0.07374]\n",
      "Step 533802  [5.454 sec/step, loss=0.07107, avg_loss=0.07370]\n",
      "Step 533803  [5.414 sec/step, loss=0.06602, avg_loss=0.07364]\n",
      "Step 533804  [5.408 sec/step, loss=0.07456, avg_loss=0.07363]\n",
      "Step 533805  [5.423 sec/step, loss=0.07334, avg_loss=0.07362]\n",
      "Step 533806  [5.429 sec/step, loss=0.07392, avg_loss=0.07362]\n",
      "Step 533807  [5.434 sec/step, loss=0.07225, avg_loss=0.07361]\n",
      "Step 533808  [5.449 sec/step, loss=0.07482, avg_loss=0.07364]\n",
      "Step 533809  [5.446 sec/step, loss=0.07535, avg_loss=0.07365]\n",
      "Step 533810  [5.433 sec/step, loss=0.07471, avg_loss=0.07363]\n",
      "Step 533811  [5.447 sec/step, loss=0.07451, avg_loss=0.07364]\n",
      "Step 533812  [5.452 sec/step, loss=0.07421, avg_loss=0.07364]\n",
      "Step 533813  [5.441 sec/step, loss=0.06572, avg_loss=0.07356]\n",
      "Step 533814  [5.430 sec/step, loss=0.07345, avg_loss=0.07358]\n",
      "Step 533815  [5.365 sec/step, loss=0.07163, avg_loss=0.07363]\n",
      "Step 533816  [5.351 sec/step, loss=0.07468, avg_loss=0.07363]\n",
      "Step 533817  [5.416 sec/step, loss=0.06580, avg_loss=0.07362]\n",
      "Step 533818  [5.425 sec/step, loss=0.07336, avg_loss=0.07362]\n",
      "Step 533819  [5.425 sec/step, loss=0.07382, avg_loss=0.07365]\n",
      "Step 533820  [5.421 sec/step, loss=0.07491, avg_loss=0.07364]\n",
      "Step 533821  [5.417 sec/step, loss=0.07603, avg_loss=0.07364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 533822  [5.416 sec/step, loss=0.07465, avg_loss=0.07367]\n",
      "Step 533823  [5.427 sec/step, loss=0.07312, avg_loss=0.07369]\n",
      "Step 533824  [5.455 sec/step, loss=0.07242, avg_loss=0.07367]\n",
      "Step 533825  [5.472 sec/step, loss=0.07549, avg_loss=0.07369]\n",
      "Step 533826  [5.464 sec/step, loss=0.07336, avg_loss=0.07367]\n",
      "Step 533827  [5.463 sec/step, loss=0.07588, avg_loss=0.07369]\n",
      "Step 533828  [5.444 sec/step, loss=0.07495, avg_loss=0.07371]\n",
      "Generated 32 batches of size 32 in 2.372 sec\n",
      "Step 533829  [5.455 sec/step, loss=0.07583, avg_loss=0.07375]\n",
      "Step 533830  [5.438 sec/step, loss=0.07310, avg_loss=0.07375]\n",
      "Step 533831  [5.419 sec/step, loss=0.07343, avg_loss=0.07375]\n",
      "Step 533832  [5.411 sec/step, loss=0.07095, avg_loss=0.07374]\n",
      "Step 533833  [5.429 sec/step, loss=0.07635, avg_loss=0.07375]\n",
      "Step 533834  [5.434 sec/step, loss=0.07422, avg_loss=0.07375]\n",
      "Step 533835  [5.430 sec/step, loss=0.07501, avg_loss=0.07375]\n",
      "Step 533836  [5.438 sec/step, loss=0.07506, avg_loss=0.07375]\n",
      "Step 533837  [5.442 sec/step, loss=0.07572, avg_loss=0.07375]\n",
      "Step 533838  [5.448 sec/step, loss=0.07502, avg_loss=0.07376]\n",
      "Step 533839  [5.458 sec/step, loss=0.07502, avg_loss=0.07379]\n",
      "Step 533840  [5.436 sec/step, loss=0.06471, avg_loss=0.07369]\n",
      "Step 533841  [5.439 sec/step, loss=0.07631, avg_loss=0.07370]\n",
      "Step 533842  [5.443 sec/step, loss=0.07540, avg_loss=0.07370]\n",
      "Step 533843  [5.448 sec/step, loss=0.07461, avg_loss=0.07369]\n",
      "Step 533844  [5.462 sec/step, loss=0.07438, avg_loss=0.07377]\n",
      "Step 533845  [5.516 sec/step, loss=0.06588, avg_loss=0.07371]\n",
      "Step 533846  [5.506 sec/step, loss=0.07167, avg_loss=0.07368]\n",
      "Step 533847  [5.493 sec/step, loss=0.07239, avg_loss=0.07367]\n",
      "Step 533848  [5.485 sec/step, loss=0.07571, avg_loss=0.07367]\n",
      "Step 533849  [5.502 sec/step, loss=0.07532, avg_loss=0.07368]\n",
      "Step 533850  [5.501 sec/step, loss=0.07371, avg_loss=0.07367]\n",
      "Step 533851  [5.444 sec/step, loss=0.07386, avg_loss=0.07375]\n",
      "Step 533852  [5.445 sec/step, loss=0.07463, avg_loss=0.07375]\n",
      "Step 533853  [5.427 sec/step, loss=0.07540, avg_loss=0.07377]\n",
      "Step 533854  [5.430 sec/step, loss=0.07168, avg_loss=0.07375]\n",
      "Step 533855  [5.434 sec/step, loss=0.07543, avg_loss=0.07376]\n",
      "Step 533856  [5.470 sec/step, loss=0.07513, avg_loss=0.07380]\n",
      "Step 533857  [5.455 sec/step, loss=0.07522, avg_loss=0.07379]\n",
      "Step 533858  [5.455 sec/step, loss=0.07368, avg_loss=0.07379]\n",
      "Step 533859  [5.458 sec/step, loss=0.07619, avg_loss=0.07380]\n",
      "Step 533860  [5.453 sec/step, loss=0.07390, avg_loss=0.07379]\n",
      "Generated 32 batches of size 32 in 2.347 sec\n",
      "Step 533861  [5.463 sec/step, loss=0.07444, avg_loss=0.07380]\n",
      "Step 533862  [5.466 sec/step, loss=0.07439, avg_loss=0.07382]\n",
      "Step 533863  [5.481 sec/step, loss=0.07340, avg_loss=0.07384]\n",
      "Step 533864  [5.489 sec/step, loss=0.07451, avg_loss=0.07386]\n",
      "Step 533865  [5.492 sec/step, loss=0.07405, avg_loss=0.07384]\n",
      "Step 533866  [5.462 sec/step, loss=0.07180, avg_loss=0.07380]\n",
      "Step 533867  [5.458 sec/step, loss=0.07532, avg_loss=0.07380]\n",
      "Step 533868  [5.460 sec/step, loss=0.07478, avg_loss=0.07381]\n",
      "Step 533869  [5.458 sec/step, loss=0.07322, avg_loss=0.07382]\n",
      "Step 533870  [5.453 sec/step, loss=0.07377, avg_loss=0.07381]\n",
      "Step 533871  [5.453 sec/step, loss=0.07335, avg_loss=0.07380]\n",
      "Step 533872  [5.435 sec/step, loss=0.07050, avg_loss=0.07375]\n",
      "Step 533873  [5.420 sec/step, loss=0.07363, avg_loss=0.07374]\n",
      "Step 533874  [5.411 sec/step, loss=0.07308, avg_loss=0.07371]\n",
      "Step 533875  [5.394 sec/step, loss=0.07592, avg_loss=0.07374]\n",
      "Step 533876  [5.401 sec/step, loss=0.07579, avg_loss=0.07374]\n",
      "Step 533877  [5.403 sec/step, loss=0.07451, avg_loss=0.07376]\n",
      "Step 533878  [5.402 sec/step, loss=0.07467, avg_loss=0.07375]\n",
      "Step 533879  [5.383 sec/step, loss=0.07158, avg_loss=0.07370]\n",
      "Step 533880  [5.394 sec/step, loss=0.07333, avg_loss=0.07368]\n",
      "Step 533881  [5.351 sec/step, loss=0.07452, avg_loss=0.07376]\n",
      "Step 533882  [5.354 sec/step, loss=0.07340, avg_loss=0.07376]\n",
      "Step 533883  [5.344 sec/step, loss=0.07114, avg_loss=0.07372]\n",
      "Step 533884  [5.359 sec/step, loss=0.07548, avg_loss=0.07373]\n",
      "Step 533885  [5.366 sec/step, loss=0.07511, avg_loss=0.07372]\n",
      "Step 533886  [5.362 sec/step, loss=0.07489, avg_loss=0.07372]\n",
      "Step 533887  [5.361 sec/step, loss=0.07397, avg_loss=0.07371]\n",
      "Step 533888  [5.351 sec/step, loss=0.07468, avg_loss=0.07371]\n",
      "Step 533889  [5.350 sec/step, loss=0.07501, avg_loss=0.07371]\n",
      "Step 533890  [5.331 sec/step, loss=0.07331, avg_loss=0.07369]\n",
      "Step 533891  [5.337 sec/step, loss=0.07529, avg_loss=0.07371]\n",
      "Step 533892  [5.332 sec/step, loss=0.07386, avg_loss=0.07369]\n",
      "Generated 32 batches of size 32 in 2.371 sec\n",
      "Step 533893  [5.332 sec/step, loss=0.07464, avg_loss=0.07368]\n",
      "Step 533894  [5.342 sec/step, loss=0.07560, avg_loss=0.07369]\n",
      "Step 533895  [5.364 sec/step, loss=0.07540, avg_loss=0.07370]\n",
      "Step 533896  [5.369 sec/step, loss=0.07307, avg_loss=0.07369]\n",
      "Step 533897  [5.352 sec/step, loss=0.06641, avg_loss=0.07365]\n",
      "Step 533898  [5.411 sec/step, loss=0.06554, avg_loss=0.07359]\n",
      "Step 533899  [5.425 sec/step, loss=0.07253, avg_loss=0.07356]\n",
      "Step 533900  [5.426 sec/step, loss=0.07490, avg_loss=0.07357]\n",
      "Writing summary at step: 533900\n",
      "Step 533901  [5.415 sec/step, loss=0.07328, avg_loss=0.07357]\n",
      "Step 533902  [5.464 sec/step, loss=0.06576, avg_loss=0.07352]\n",
      "Step 533903  [5.486 sec/step, loss=0.07507, avg_loss=0.07361]\n",
      "Step 533904  [5.482 sec/step, loss=0.07369, avg_loss=0.07360]\n",
      "Step 533905  [5.474 sec/step, loss=0.07337, avg_loss=0.07360]\n",
      "Step 533906  [5.479 sec/step, loss=0.07519, avg_loss=0.07361]\n",
      "Step 533907  [5.480 sec/step, loss=0.07430, avg_loss=0.07363]\n",
      "Step 533908  [5.471 sec/step, loss=0.07337, avg_loss=0.07362]\n",
      "Step 533909  [5.453 sec/step, loss=0.06478, avg_loss=0.07351]\n",
      "Step 533910  [5.452 sec/step, loss=0.07305, avg_loss=0.07350]\n",
      "Step 533911  [5.450 sec/step, loss=0.07585, avg_loss=0.07351]\n",
      "Step 533912  [5.458 sec/step, loss=0.07590, avg_loss=0.07353]\n",
      "Step 533913  [5.500 sec/step, loss=0.07465, avg_loss=0.07362]\n",
      "Step 533914  [5.510 sec/step, loss=0.07457, avg_loss=0.07363]\n",
      "Step 533915  [5.542 sec/step, loss=0.07511, avg_loss=0.07366]\n",
      "Step 533916  [5.551 sec/step, loss=0.07485, avg_loss=0.07366]\n",
      "Step 533917  [5.491 sec/step, loss=0.07369, avg_loss=0.07374]\n",
      "Step 533918  [5.498 sec/step, loss=0.07598, avg_loss=0.07377]\n",
      "Step 533919  [5.501 sec/step, loss=0.07413, avg_loss=0.07377]\n",
      "Step 533920  [5.511 sec/step, loss=0.07597, avg_loss=0.07378]\n",
      "Step 533921  [5.502 sec/step, loss=0.07168, avg_loss=0.07374]\n",
      "Step 533922  [5.516 sec/step, loss=0.07595, avg_loss=0.07375]\n",
      "Step 533923  [5.522 sec/step, loss=0.07472, avg_loss=0.07377]\n",
      "Generated 32 batches of size 32 in 2.413 sec\n",
      "Step 533924  [5.505 sec/step, loss=0.07464, avg_loss=0.07379]\n",
      "Step 533925  [5.488 sec/step, loss=0.07191, avg_loss=0.07375]\n",
      "Step 533926  [5.482 sec/step, loss=0.07127, avg_loss=0.07373]\n",
      "Step 533927  [5.475 sec/step, loss=0.07464, avg_loss=0.07372]\n",
      "Step 533928  [5.477 sec/step, loss=0.07302, avg_loss=0.07370]\n",
      "Step 533929  [5.462 sec/step, loss=0.07521, avg_loss=0.07370]\n",
      "Step 533930  [5.466 sec/step, loss=0.07429, avg_loss=0.07371]\n",
      "Step 533931  [5.456 sec/step, loss=0.07005, avg_loss=0.07367]\n",
      "Step 533932  [5.464 sec/step, loss=0.07499, avg_loss=0.07371]\n",
      "Step 533933  [5.475 sec/step, loss=0.07310, avg_loss=0.07368]\n",
      "Step 533934  [5.463 sec/step, loss=0.07487, avg_loss=0.07369]\n",
      "Step 533935  [5.481 sec/step, loss=0.07573, avg_loss=0.07369]\n",
      "Step 533936  [5.469 sec/step, loss=0.07431, avg_loss=0.07369]\n",
      "Step 533937  [5.464 sec/step, loss=0.07499, avg_loss=0.07368]\n",
      "Step 533938  [5.456 sec/step, loss=0.07393, avg_loss=0.07367]\n",
      "Step 533939  [5.464 sec/step, loss=0.07368, avg_loss=0.07366]\n",
      "Step 533940  [5.483 sec/step, loss=0.07550, avg_loss=0.07376]\n",
      "Step 533941  [5.481 sec/step, loss=0.07581, avg_loss=0.07376]\n",
      "Step 533942  [5.467 sec/step, loss=0.07209, avg_loss=0.07373]\n",
      "Step 533943  [5.462 sec/step, loss=0.07502, avg_loss=0.07373]\n",
      "Step 533944  [5.453 sec/step, loss=0.07171, avg_loss=0.07370]\n",
      "Step 533945  [5.411 sec/step, loss=0.07457, avg_loss=0.07379]\n",
      "Step 533946  [5.430 sec/step, loss=0.07562, avg_loss=0.07383]\n",
      "Step 533947  [5.484 sec/step, loss=0.06666, avg_loss=0.07377]\n",
      "Step 533948  [5.471 sec/step, loss=0.07182, avg_loss=0.07373]\n",
      "Step 533949  [5.453 sec/step, loss=0.07307, avg_loss=0.07371]\n",
      "Step 533950  [5.443 sec/step, loss=0.07113, avg_loss=0.07368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 533951  [5.440 sec/step, loss=0.07367, avg_loss=0.07368]\n",
      "Step 533952  [5.420 sec/step, loss=0.07141, avg_loss=0.07365]\n",
      "Step 533953  [5.412 sec/step, loss=0.07365, avg_loss=0.07363]\n",
      "Step 533954  [5.429 sec/step, loss=0.07580, avg_loss=0.07367]\n",
      "Step 533955  [5.447 sec/step, loss=0.07247, avg_loss=0.07364]\n",
      "Generated 32 batches of size 32 in 2.373 sec\n",
      "Step 533956  [5.432 sec/step, loss=0.07498, avg_loss=0.07364]\n",
      "Step 533957  [5.447 sec/step, loss=0.07612, avg_loss=0.07365]\n",
      "Step 533958  [5.460 sec/step, loss=0.07141, avg_loss=0.07363]\n",
      "Step 533959  [5.452 sec/step, loss=0.07388, avg_loss=0.07361]\n",
      "Step 533960  [5.447 sec/step, loss=0.07347, avg_loss=0.07360]\n",
      "Step 533961  [5.421 sec/step, loss=0.06670, avg_loss=0.07352]\n",
      "Step 533962  [5.413 sec/step, loss=0.07481, avg_loss=0.07353]\n",
      "Step 533963  [5.422 sec/step, loss=0.07562, avg_loss=0.07355]\n",
      "Step 533964  [5.414 sec/step, loss=0.07343, avg_loss=0.07354]\n",
      "Step 533965  [5.408 sec/step, loss=0.07432, avg_loss=0.07354]\n",
      "Step 533966  [5.423 sec/step, loss=0.07519, avg_loss=0.07358]\n",
      "Step 533967  [5.400 sec/step, loss=0.07406, avg_loss=0.07356]\n",
      "Step 533968  [5.406 sec/step, loss=0.07423, avg_loss=0.07356]\n",
      "Step 533969  [5.406 sec/step, loss=0.07345, avg_loss=0.07356]\n",
      "Step 533970  [5.411 sec/step, loss=0.07519, avg_loss=0.07357]\n",
      "Step 533971  [5.417 sec/step, loss=0.07487, avg_loss=0.07359]\n",
      "Step 533972  [5.423 sec/step, loss=0.07511, avg_loss=0.07364]\n",
      "Step 533973  [5.445 sec/step, loss=0.07567, avg_loss=0.07366]\n",
      "Step 533974  [5.449 sec/step, loss=0.07291, avg_loss=0.07365]\n",
      "Step 533975  [5.446 sec/step, loss=0.07475, avg_loss=0.07364]\n",
      "Step 533976  [5.440 sec/step, loss=0.07521, avg_loss=0.07364]\n",
      "Step 533977  [5.440 sec/step, loss=0.07542, avg_loss=0.07365]\n",
      "Step 533978  [5.450 sec/step, loss=0.07596, avg_loss=0.07366]\n",
      "Step 533979  [5.439 sec/step, loss=0.06624, avg_loss=0.07361]\n",
      "Step 533980  [5.426 sec/step, loss=0.07163, avg_loss=0.07359]\n",
      "Step 533981  [5.416 sec/step, loss=0.07227, avg_loss=0.07357]\n",
      "Step 533982  [5.430 sec/step, loss=0.07602, avg_loss=0.07359]\n",
      "Step 533983  [5.432 sec/step, loss=0.07144, avg_loss=0.07360]\n",
      "Step 533984  [5.413 sec/step, loss=0.07400, avg_loss=0.07358]\n",
      "Step 533985  [5.406 sec/step, loss=0.07617, avg_loss=0.07359]\n",
      "Step 533986  [5.387 sec/step, loss=0.07191, avg_loss=0.07356]\n",
      "Step 533987  [5.390 sec/step, loss=0.07611, avg_loss=0.07358]\n",
      "Generated 32 batches of size 32 in 2.437 sec\n",
      "Step 533988  [5.393 sec/step, loss=0.07465, avg_loss=0.07358]\n",
      "Step 533989  [5.446 sec/step, loss=0.06578, avg_loss=0.07349]\n",
      "Step 533990  [5.449 sec/step, loss=0.07402, avg_loss=0.07350]\n",
      "Step 533991  [5.472 sec/step, loss=0.07505, avg_loss=0.07350]\n",
      "Step 533992  [5.472 sec/step, loss=0.07481, avg_loss=0.07350]\n",
      "Step 533993  [5.467 sec/step, loss=0.07204, avg_loss=0.07348]\n",
      "Step 533994  [5.470 sec/step, loss=0.07526, avg_loss=0.07348]\n",
      "Step 533995  [5.468 sec/step, loss=0.07469, avg_loss=0.07347]\n",
      "Step 533996  [5.462 sec/step, loss=0.07366, avg_loss=0.07347]\n",
      "Step 533997  [5.466 sec/step, loss=0.07137, avg_loss=0.07352]\n",
      "Step 533998  [5.433 sec/step, loss=0.07400, avg_loss=0.07361]\n",
      "Step 533999  [5.414 sec/step, loss=0.07577, avg_loss=0.07364]\n",
      "Step 534000  [5.406 sec/step, loss=0.07122, avg_loss=0.07360]\n",
      "Writing summary at step: 534000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-534000\n",
      "Saving audio and alignment...\n",
      "Input: islaamii saal kaa vaddaa pinshnar koo larrkharraa kur shal kur dday gaa or ayk aaddh baddsuluukii bharrak kur vaairal zindziir ban dzaaay gii~\n",
      "Step 534001  [5.419 sec/step, loss=0.07373, avg_loss=0.07361]\n",
      "Step 534002  [5.372 sec/step, loss=0.07506, avg_loss=0.07370]\n",
      "Step 534003  [5.371 sec/step, loss=0.07399, avg_loss=0.07369]\n",
      "Step 534004  [5.382 sec/step, loss=0.07573, avg_loss=0.07371]\n",
      "Step 534005  [5.368 sec/step, loss=0.07366, avg_loss=0.07371]\n",
      "Step 534006  [5.365 sec/step, loss=0.07550, avg_loss=0.07372]\n",
      "Step 534007  [5.362 sec/step, loss=0.07104, avg_loss=0.07368]\n",
      "Step 534008  [5.365 sec/step, loss=0.07498, avg_loss=0.07370]\n",
      "Step 534009  [5.392 sec/step, loss=0.07384, avg_loss=0.07379]\n",
      "Step 534010  [5.386 sec/step, loss=0.07385, avg_loss=0.07380]\n",
      "Step 534011  [5.382 sec/step, loss=0.07456, avg_loss=0.07379]\n",
      "Step 534012  [5.361 sec/step, loss=0.07404, avg_loss=0.07377]\n",
      "Step 534013  [5.338 sec/step, loss=0.07482, avg_loss=0.07377]\n",
      "Step 534014  [5.331 sec/step, loss=0.07292, avg_loss=0.07375]\n",
      "Step 534015  [5.363 sec/step, loss=0.06681, avg_loss=0.07367]\n",
      "Step 534016  [5.364 sec/step, loss=0.07582, avg_loss=0.07368]\n",
      "Step 534017  [5.382 sec/step, loss=0.07402, avg_loss=0.07368]\n",
      "Generated 32 batches of size 32 in 2.349 sec\n",
      "Step 534018  [5.385 sec/step, loss=0.07549, avg_loss=0.07368]\n",
      "Step 534019  [5.392 sec/step, loss=0.07483, avg_loss=0.07368]\n",
      "Step 534020  [5.380 sec/step, loss=0.07126, avg_loss=0.07364]\n",
      "Step 534021  [5.383 sec/step, loss=0.07595, avg_loss=0.07368]\n",
      "Step 534022  [5.372 sec/step, loss=0.07519, avg_loss=0.07367]\n",
      "Step 534023  [5.354 sec/step, loss=0.06523, avg_loss=0.07358]\n",
      "Step 534024  [5.364 sec/step, loss=0.07434, avg_loss=0.07357]\n",
      "Step 534025  [5.367 sec/step, loss=0.07336, avg_loss=0.07359]\n",
      "Step 534026  [5.374 sec/step, loss=0.07402, avg_loss=0.07362]\n",
      "Step 534027  [5.363 sec/step, loss=0.07404, avg_loss=0.07361]\n",
      "Step 534028  [5.366 sec/step, loss=0.07634, avg_loss=0.07364]\n",
      "Step 534029  [5.366 sec/step, loss=0.07179, avg_loss=0.07361]\n",
      "Step 534030  [5.391 sec/step, loss=0.07442, avg_loss=0.07361]\n",
      "Step 534031  [5.405 sec/step, loss=0.07546, avg_loss=0.07367]\n",
      "Step 534032  [5.401 sec/step, loss=0.07352, avg_loss=0.07365]\n",
      "Step 534033  [5.372 sec/step, loss=0.07542, avg_loss=0.07367]\n",
      "Step 534034  [5.389 sec/step, loss=0.07496, avg_loss=0.07367]\n",
      "Step 534035  [5.360 sec/step, loss=0.07122, avg_loss=0.07363]\n",
      "Step 534036  [5.339 sec/step, loss=0.06709, avg_loss=0.07356]\n",
      "Step 534037  [5.343 sec/step, loss=0.07586, avg_loss=0.07357]\n",
      "Step 534038  [5.328 sec/step, loss=0.07145, avg_loss=0.07354]\n",
      "Step 534039  [5.326 sec/step, loss=0.07485, avg_loss=0.07355]\n",
      "Step 534040  [5.324 sec/step, loss=0.07456, avg_loss=0.07354]\n",
      "Step 534041  [5.326 sec/step, loss=0.07315, avg_loss=0.07352]\n",
      "Step 534042  [5.325 sec/step, loss=0.07595, avg_loss=0.07356]\n",
      "Step 534043  [5.328 sec/step, loss=0.07394, avg_loss=0.07355]\n",
      "Step 534044  [5.328 sec/step, loss=0.07393, avg_loss=0.07357]\n",
      "Step 534045  [5.317 sec/step, loss=0.07362, avg_loss=0.07356]\n",
      "Step 534046  [5.312 sec/step, loss=0.07358, avg_loss=0.07354]\n",
      "Step 534047  [5.272 sec/step, loss=0.07638, avg_loss=0.07363]\n",
      "Step 534048  [5.282 sec/step, loss=0.07432, avg_loss=0.07366]\n",
      "Step 534049  [5.283 sec/step, loss=0.07446, avg_loss=0.07367]\n",
      "Generated 32 batches of size 32 in 2.489 sec\n",
      "Step 534050  [5.286 sec/step, loss=0.07485, avg_loss=0.07371]\n",
      "Step 534051  [5.303 sec/step, loss=0.07593, avg_loss=0.07373]\n",
      "Step 534052  [5.319 sec/step, loss=0.07512, avg_loss=0.07377]\n",
      "Step 534053  [5.319 sec/step, loss=0.07371, avg_loss=0.07377]\n",
      "Step 534054  [5.321 sec/step, loss=0.07540, avg_loss=0.07377]\n",
      "Step 534055  [5.309 sec/step, loss=0.07564, avg_loss=0.07380]\n",
      "Step 534056  [5.295 sec/step, loss=0.07129, avg_loss=0.07376]\n",
      "Step 534057  [5.291 sec/step, loss=0.07432, avg_loss=0.07374]\n",
      "Step 534058  [5.337 sec/step, loss=0.06493, avg_loss=0.07368]\n",
      "Step 534059  [5.345 sec/step, loss=0.07590, avg_loss=0.07370]\n",
      "Step 534060  [5.355 sec/step, loss=0.07514, avg_loss=0.07372]\n",
      "Step 534061  [5.396 sec/step, loss=0.07524, avg_loss=0.07380]\n",
      "Step 534062  [5.399 sec/step, loss=0.07409, avg_loss=0.07379]\n",
      "Step 534063  [5.386 sec/step, loss=0.07457, avg_loss=0.07378]\n",
      "Step 534064  [5.398 sec/step, loss=0.07173, avg_loss=0.07377]\n",
      "Step 534065  [5.385 sec/step, loss=0.07375, avg_loss=0.07376]\n",
      "Step 534066  [5.394 sec/step, loss=0.07323, avg_loss=0.07374]\n",
      "Step 534067  [5.390 sec/step, loss=0.07124, avg_loss=0.07371]\n",
      "Step 534068  [5.394 sec/step, loss=0.07635, avg_loss=0.07373]\n",
      "Step 534069  [5.406 sec/step, loss=0.07508, avg_loss=0.07375]\n",
      "Step 534070  [5.396 sec/step, loss=0.07639, avg_loss=0.07376]\n",
      "Step 534071  [5.392 sec/step, loss=0.07708, avg_loss=0.07378]\n",
      "Step 534072  [5.408 sec/step, loss=0.07544, avg_loss=0.07379]\n",
      "Step 534073  [5.395 sec/step, loss=0.07508, avg_loss=0.07378]\n",
      "Step 534074  [5.399 sec/step, loss=0.07604, avg_loss=0.07381]\n",
      "Step 534075  [5.398 sec/step, loss=0.07930, avg_loss=0.07386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 534076  [5.404 sec/step, loss=0.07859, avg_loss=0.07389]\n",
      "Step 534077  [5.412 sec/step, loss=0.07826, avg_loss=0.07392]\n",
      "Step 534078  [5.388 sec/step, loss=0.07777, avg_loss=0.07394]\n",
      "Step 534079  [5.409 sec/step, loss=0.07798, avg_loss=0.07406]\n",
      "Step 534080  [5.410 sec/step, loss=0.07747, avg_loss=0.07411]\n",
      "Step 534081  [5.409 sec/step, loss=0.07670, avg_loss=0.07416]\n",
      "Generated 32 batches of size 32 in 2.530 sec\n",
      "Step 534082  [5.395 sec/step, loss=0.07510, avg_loss=0.07415]\n",
      "Step 534083  [5.423 sec/step, loss=0.07783, avg_loss=0.07421]\n",
      "Step 534084  [5.423 sec/step, loss=0.07629, avg_loss=0.07424]\n",
      "Step 534085  [5.405 sec/step, loss=0.07277, avg_loss=0.07420]\n",
      "Step 534086  [5.433 sec/step, loss=0.07772, avg_loss=0.07426]\n",
      "Step 534087  [5.434 sec/step, loss=0.07672, avg_loss=0.07427]\n",
      "Step 534088  [5.481 sec/step, loss=0.06701, avg_loss=0.07419]\n",
      "Step 534089  [5.416 sec/step, loss=0.06701, avg_loss=0.07420]\n",
      "Step 534090  [5.416 sec/step, loss=0.07549, avg_loss=0.07422]\n",
      "Step 534091  [5.393 sec/step, loss=0.07665, avg_loss=0.07423]\n",
      "Step 534092  [5.378 sec/step, loss=0.07559, avg_loss=0.07424]\n",
      "Step 534093  [5.381 sec/step, loss=0.07549, avg_loss=0.07428]\n",
      "Step 534094  [5.390 sec/step, loss=0.07532, avg_loss=0.07428]\n",
      "Step 534095  [5.390 sec/step, loss=0.07743, avg_loss=0.07430]\n",
      "Step 534096  [5.404 sec/step, loss=0.07518, avg_loss=0.07432]\n",
      "Step 534097  [5.403 sec/step, loss=0.07272, avg_loss=0.07433]\n",
      "Step 534098  [5.402 sec/step, loss=0.07712, avg_loss=0.07436]\n",
      "Step 534099  [5.390 sec/step, loss=0.07339, avg_loss=0.07434]\n",
      "Step 534100  [5.399 sec/step, loss=0.07666, avg_loss=0.07439]\n",
      "Writing summary at step: 534100\n",
      "Step 534101  [5.372 sec/step, loss=0.07507, avg_loss=0.07441]\n",
      "Step 534102  [5.372 sec/step, loss=0.07645, avg_loss=0.07442]\n",
      "Step 534103  [5.357 sec/step, loss=0.07104, avg_loss=0.07439]\n",
      "Step 534104  [5.345 sec/step, loss=0.07703, avg_loss=0.07441]\n",
      "Step 534105  [5.362 sec/step, loss=0.07757, avg_loss=0.07444]\n",
      "Step 534106  [5.405 sec/step, loss=0.06876, avg_loss=0.07438]\n",
      "Step 534107  [5.416 sec/step, loss=0.07552, avg_loss=0.07442]\n",
      "Step 534108  [5.429 sec/step, loss=0.07457, avg_loss=0.07442]\n",
      "Step 534109  [5.441 sec/step, loss=0.07467, avg_loss=0.07443]\n",
      "Step 534110  [5.446 sec/step, loss=0.07229, avg_loss=0.07441]\n",
      "Step 534111  [5.431 sec/step, loss=0.07502, avg_loss=0.07442]\n",
      "Step 534112  [5.434 sec/step, loss=0.07471, avg_loss=0.07442]\n",
      "Generated 32 batches of size 32 in 2.366 sec\n",
      "Step 534113  [5.448 sec/step, loss=0.07648, avg_loss=0.07444]\n",
      "Step 534114  [5.464 sec/step, loss=0.07719, avg_loss=0.07448]\n",
      "Step 534115  [5.406 sec/step, loss=0.07419, avg_loss=0.07455]\n",
      "Step 534116  [5.398 sec/step, loss=0.07543, avg_loss=0.07455]\n",
      "Step 534117  [5.391 sec/step, loss=0.07432, avg_loss=0.07455]\n",
      "Step 534118  [5.395 sec/step, loss=0.07658, avg_loss=0.07456]\n",
      "Step 534119  [5.393 sec/step, loss=0.07503, avg_loss=0.07457]\n",
      "Step 534120  [5.397 sec/step, loss=0.07520, avg_loss=0.07461]\n",
      "Step 534121  [5.378 sec/step, loss=0.06754, avg_loss=0.07452]\n",
      "Step 534122  [5.394 sec/step, loss=0.07592, avg_loss=0.07453]\n",
      "Step 534123  [5.410 sec/step, loss=0.07449, avg_loss=0.07462]\n",
      "Step 534124  [5.376 sec/step, loss=0.07152, avg_loss=0.07459]\n",
      "Step 534125  [5.387 sec/step, loss=0.07656, avg_loss=0.07463]\n",
      "Step 534126  [5.389 sec/step, loss=0.07316, avg_loss=0.07462]\n",
      "Step 534127  [5.401 sec/step, loss=0.07605, avg_loss=0.07464]\n",
      "Step 534128  [5.402 sec/step, loss=0.07627, avg_loss=0.07464]\n",
      "Step 534129  [5.403 sec/step, loss=0.07154, avg_loss=0.07463]\n",
      "Step 534130  [5.380 sec/step, loss=0.07538, avg_loss=0.07464]\n",
      "Step 534131  [5.368 sec/step, loss=0.07368, avg_loss=0.07463]\n",
      "Step 534132  [5.367 sec/step, loss=0.07465, avg_loss=0.07464]\n",
      "Step 534133  [5.365 sec/step, loss=0.07432, avg_loss=0.07463]\n",
      "Step 534134  [5.336 sec/step, loss=0.07137, avg_loss=0.07459]\n",
      "Step 534135  [5.362 sec/step, loss=0.07586, avg_loss=0.07464]\n",
      "Step 534136  [5.390 sec/step, loss=0.07478, avg_loss=0.07471]\n",
      "Step 534137  [5.379 sec/step, loss=0.07468, avg_loss=0.07470]\n",
      "Step 534138  [5.380 sec/step, loss=0.07368, avg_loss=0.07472]\n",
      "Step 534139  [5.372 sec/step, loss=0.07230, avg_loss=0.07470]\n",
      "Step 534140  [5.399 sec/step, loss=0.07318, avg_loss=0.07468]\n",
      "Step 534141  [5.391 sec/step, loss=0.07495, avg_loss=0.07470]\n",
      "Step 534142  [5.400 sec/step, loss=0.07619, avg_loss=0.07471]\n",
      "Step 534143  [5.407 sec/step, loss=0.07507, avg_loss=0.07472]\n",
      "Step 534144  [5.418 sec/step, loss=0.07569, avg_loss=0.07473]\n",
      "Generated 32 batches of size 32 in 2.545 sec\n",
      "Step 534145  [5.424 sec/step, loss=0.07529, avg_loss=0.07475]\n",
      "Step 534146  [5.428 sec/step, loss=0.07734, avg_loss=0.07479]\n",
      "Step 534147  [5.467 sec/step, loss=0.06654, avg_loss=0.07469]\n",
      "Step 534148  [5.469 sec/step, loss=0.07520, avg_loss=0.07470]\n",
      "Step 534149  [5.469 sec/step, loss=0.07321, avg_loss=0.07469]\n",
      "Step 534150  [5.465 sec/step, loss=0.07526, avg_loss=0.07469]\n",
      "Step 534151  [5.471 sec/step, loss=0.07618, avg_loss=0.07469]\n",
      "Step 534152  [5.475 sec/step, loss=0.07337, avg_loss=0.07468]\n",
      "Step 534153  [5.458 sec/step, loss=0.06500, avg_loss=0.07459]\n",
      "Step 534154  [5.495 sec/step, loss=0.06595, avg_loss=0.07449]\n",
      "Step 534155  [5.471 sec/step, loss=0.06622, avg_loss=0.07440]\n",
      "Step 534156  [5.481 sec/step, loss=0.07181, avg_loss=0.07440]\n",
      "Step 534157  [5.483 sec/step, loss=0.07613, avg_loss=0.07442]\n",
      "Step 534158  [5.451 sec/step, loss=0.07588, avg_loss=0.07453]\n",
      "Step 534159  [5.457 sec/step, loss=0.07624, avg_loss=0.07454]\n",
      "Step 534160  [5.441 sec/step, loss=0.07204, avg_loss=0.07450]\n",
      "Step 534161  [5.420 sec/step, loss=0.07448, avg_loss=0.07450]\n",
      "Step 534162  [5.425 sec/step, loss=0.07279, avg_loss=0.07448]\n",
      "Step 534163  [5.443 sec/step, loss=0.07613, avg_loss=0.07450]\n",
      "Step 534164  [5.428 sec/step, loss=0.07195, avg_loss=0.07450]\n",
      "Step 534165  [5.428 sec/step, loss=0.07410, avg_loss=0.07451]\n",
      "Step 534166  [5.411 sec/step, loss=0.07372, avg_loss=0.07451]\n",
      "Step 534167  [5.422 sec/step, loss=0.07156, avg_loss=0.07451]\n",
      "Step 534168  [5.415 sec/step, loss=0.07610, avg_loss=0.07451]\n",
      "Step 534169  [5.406 sec/step, loss=0.07320, avg_loss=0.07449]\n",
      "Step 534170  [5.415 sec/step, loss=0.07524, avg_loss=0.07448]\n",
      "Step 534171  [5.397 sec/step, loss=0.07504, avg_loss=0.07446]\n",
      "Step 534172  [5.393 sec/step, loss=0.07548, avg_loss=0.07446]\n",
      "Step 534173  [5.384 sec/step, loss=0.07390, avg_loss=0.07445]\n",
      "Step 534174  [5.383 sec/step, loss=0.07182, avg_loss=0.07441]\n",
      "Step 534175  [5.387 sec/step, loss=0.07661, avg_loss=0.07438]\n",
      "Step 534176  [5.377 sec/step, loss=0.07384, avg_loss=0.07433]\n",
      "Generated 32 batches of size 32 in 2.572 sec\n",
      "Step 534177  [5.368 sec/step, loss=0.07499, avg_loss=0.07430]\n",
      "Step 534178  [5.390 sec/step, loss=0.07593, avg_loss=0.07428]\n",
      "Step 534179  [5.387 sec/step, loss=0.07486, avg_loss=0.07425]\n",
      "Step 534180  [5.400 sec/step, loss=0.07405, avg_loss=0.07422]\n",
      "Step 534181  [5.414 sec/step, loss=0.07317, avg_loss=0.07418]\n",
      "Step 534182  [5.447 sec/step, loss=0.07336, avg_loss=0.07416]\n",
      "Step 534183  [5.440 sec/step, loss=0.07628, avg_loss=0.07415]\n",
      "Step 534184  [5.440 sec/step, loss=0.07489, avg_loss=0.07413]\n",
      "Step 534185  [5.447 sec/step, loss=0.07377, avg_loss=0.07414]\n",
      "Step 534186  [5.434 sec/step, loss=0.07273, avg_loss=0.07409]\n",
      "Step 534187  [5.436 sec/step, loss=0.07548, avg_loss=0.07408]\n",
      "Step 534188  [5.399 sec/step, loss=0.07359, avg_loss=0.07415]\n",
      "Step 534189  [5.407 sec/step, loss=0.07327, avg_loss=0.07421]\n",
      "Step 534190  [5.416 sec/step, loss=0.07480, avg_loss=0.07420]\n",
      "Step 534191  [5.410 sec/step, loss=0.07414, avg_loss=0.07418]\n",
      "Step 534192  [5.425 sec/step, loss=0.07564, avg_loss=0.07418]\n",
      "Step 534193  [5.417 sec/step, loss=0.07480, avg_loss=0.07417]\n",
      "Step 534194  [5.392 sec/step, loss=0.07442, avg_loss=0.07416]\n",
      "Step 534195  [5.387 sec/step, loss=0.07456, avg_loss=0.07413]\n",
      "Step 534196  [5.397 sec/step, loss=0.07518, avg_loss=0.07413]\n",
      "Step 534197  [5.417 sec/step, loss=0.07632, avg_loss=0.07417]\n",
      "Step 534198  [5.401 sec/step, loss=0.07438, avg_loss=0.07414]\n",
      "Step 534199  [5.409 sec/step, loss=0.07201, avg_loss=0.07413]\n",
      "Step 534200  [5.434 sec/step, loss=0.07525, avg_loss=0.07411]\n",
      "Writing summary at step: 534200\n",
      "Step 534201  [5.449 sec/step, loss=0.07626, avg_loss=0.07413]\n",
      "Step 534202  [5.459 sec/step, loss=0.07581, avg_loss=0.07412]\n",
      "Step 534203  [5.472 sec/step, loss=0.07526, avg_loss=0.07416]\n",
      "Step 534204  [5.485 sec/step, loss=0.07574, avg_loss=0.07415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 534205  [5.482 sec/step, loss=0.07484, avg_loss=0.07412]\n",
      "Step 534206  [5.415 sec/step, loss=0.06414, avg_loss=0.07408]\n",
      "Step 534207  [5.417 sec/step, loss=0.07459, avg_loss=0.07407]\n",
      "Generated 32 batches of size 32 in 2.368 sec\n",
      "Step 534208  [5.421 sec/step, loss=0.07555, avg_loss=0.07408]\n",
      "Step 534209  [5.388 sec/step, loss=0.07402, avg_loss=0.07407]\n",
      "Step 534210  [5.376 sec/step, loss=0.07090, avg_loss=0.07406]\n",
      "Step 534211  [5.385 sec/step, loss=0.07304, avg_loss=0.07404]\n",
      "Step 534212  [5.385 sec/step, loss=0.07319, avg_loss=0.07402]\n",
      "Step 534213  [5.369 sec/step, loss=0.07075, avg_loss=0.07396]\n",
      "Step 534214  [5.348 sec/step, loss=0.07096, avg_loss=0.07390]\n",
      "Step 534215  [5.348 sec/step, loss=0.07392, avg_loss=0.07390]\n",
      "Step 534216  [5.397 sec/step, loss=0.06596, avg_loss=0.07380]\n",
      "Step 534217  [5.401 sec/step, loss=0.07403, avg_loss=0.07380]\n",
      "Step 534218  [5.385 sec/step, loss=0.07480, avg_loss=0.07378]\n",
      "Step 534219  [5.401 sec/step, loss=0.07551, avg_loss=0.07379]\n",
      "Step 534220  [5.391 sec/step, loss=0.07210, avg_loss=0.07376]\n",
      "Step 534221  [5.402 sec/step, loss=0.07343, avg_loss=0.07382]\n",
      "Step 534222  [5.387 sec/step, loss=0.07529, avg_loss=0.07381]\n",
      "Step 534223  [5.405 sec/step, loss=0.07489, avg_loss=0.07381]\n",
      "Step 534224  [5.405 sec/step, loss=0.07279, avg_loss=0.07383]\n",
      "Step 534225  [5.391 sec/step, loss=0.07345, avg_loss=0.07380]\n",
      "Step 534226  [5.394 sec/step, loss=0.07455, avg_loss=0.07381]\n",
      "Step 534227  [5.390 sec/step, loss=0.07329, avg_loss=0.07378]\n",
      "Step 534228  [5.384 sec/step, loss=0.07572, avg_loss=0.07378]\n",
      "Step 534229  [5.394 sec/step, loss=0.07396, avg_loss=0.07380]\n",
      "Step 534230  [5.376 sec/step, loss=0.06791, avg_loss=0.07373]\n",
      "Step 534231  [5.381 sec/step, loss=0.07462, avg_loss=0.07373]\n",
      "Step 534232  [5.397 sec/step, loss=0.07376, avg_loss=0.07373]\n",
      "Step 534233  [5.413 sec/step, loss=0.07557, avg_loss=0.07374]\n",
      "Step 534234  [5.427 sec/step, loss=0.07469, avg_loss=0.07377]\n",
      "Step 534235  [5.404 sec/step, loss=0.07383, avg_loss=0.07375]\n",
      "Step 534236  [5.443 sec/step, loss=0.06544, avg_loss=0.07366]\n",
      "Step 534237  [5.435 sec/step, loss=0.07334, avg_loss=0.07364]\n",
      "Step 534238  [5.454 sec/step, loss=0.07650, avg_loss=0.07367]\n",
      "Step 534239  [5.464 sec/step, loss=0.07612, avg_loss=0.07371]\n",
      "Generated 32 batches of size 32 in 2.442 sec\n",
      "Step 534240  [5.456 sec/step, loss=0.07544, avg_loss=0.07373]\n",
      "Step 534241  [5.454 sec/step, loss=0.07222, avg_loss=0.07371]\n",
      "Step 534242  [5.441 sec/step, loss=0.07363, avg_loss=0.07368]\n",
      "Step 534243  [5.435 sec/step, loss=0.07468, avg_loss=0.07368]\n",
      "Step 534244  [5.437 sec/step, loss=0.07466, avg_loss=0.07367]\n",
      "Step 534245  [5.465 sec/step, loss=0.07361, avg_loss=0.07365]\n",
      "Step 534246  [5.463 sec/step, loss=0.07403, avg_loss=0.07362]\n",
      "Step 534247  [5.402 sec/step, loss=0.07051, avg_loss=0.07366]\n",
      "Step 534248  [5.403 sec/step, loss=0.07597, avg_loss=0.07366]\n",
      "Step 534249  [5.414 sec/step, loss=0.07541, avg_loss=0.07369]\n",
      "Step 534250  [5.420 sec/step, loss=0.07546, avg_loss=0.07369]\n",
      "Step 534251  [5.409 sec/step, loss=0.07403, avg_loss=0.07367]\n",
      "Step 534252  [5.390 sec/step, loss=0.07041, avg_loss=0.07364]\n",
      "Step 534253  [5.413 sec/step, loss=0.07374, avg_loss=0.07372]\n",
      "Step 534254  [5.347 sec/step, loss=0.06427, avg_loss=0.07371]\n",
      "Step 534255  [5.381 sec/step, loss=0.07543, avg_loss=0.07380]\n",
      "Step 534256  [5.388 sec/step, loss=0.07591, avg_loss=0.07384]\n",
      "Step 534257  [5.376 sec/step, loss=0.07513, avg_loss=0.07383]\n",
      "Step 534258  [5.367 sec/step, loss=0.07597, avg_loss=0.07383]\n",
      "Step 534259  [5.348 sec/step, loss=0.07242, avg_loss=0.07379]\n",
      "Step 534260  [5.363 sec/step, loss=0.07248, avg_loss=0.07380]\n",
      "Step 534261  [5.352 sec/step, loss=0.07395, avg_loss=0.07379]\n",
      "Step 534262  [5.349 sec/step, loss=0.07484, avg_loss=0.07381]\n",
      "Step 534263  [5.348 sec/step, loss=0.07594, avg_loss=0.07381]\n",
      "Step 534264  [5.348 sec/step, loss=0.07377, avg_loss=0.07383]\n",
      "Step 534265  [5.364 sec/step, loss=0.07584, avg_loss=0.07385]\n",
      "Step 534266  [5.376 sec/step, loss=0.07483, avg_loss=0.07386]\n",
      "Step 534267  [5.377 sec/step, loss=0.07480, avg_loss=0.07389]\n",
      "Step 534268  [5.380 sec/step, loss=0.07499, avg_loss=0.07388]\n",
      "Step 534269  [5.392 sec/step, loss=0.07591, avg_loss=0.07391]\n",
      "Step 534270  [5.439 sec/step, loss=0.06613, avg_loss=0.07381]\n",
      "Step 534271  [5.458 sec/step, loss=0.07562, avg_loss=0.07382]\n",
      "Generated 32 batches of size 32 in 2.536 sec\n",
      "Step 534272  [5.447 sec/step, loss=0.07362, avg_loss=0.07380]\n",
      "Step 534273  [5.459 sec/step, loss=0.07423, avg_loss=0.07381]\n",
      "Step 534274  [5.462 sec/step, loss=0.07362, avg_loss=0.07382]\n",
      "Step 534275  [5.442 sec/step, loss=0.07147, avg_loss=0.07377]\n",
      "Step 534276  [5.441 sec/step, loss=0.07313, avg_loss=0.07376]\n",
      "Step 534277  [5.465 sec/step, loss=0.07247, avg_loss=0.07374]\n",
      "Step 534278  [5.451 sec/step, loss=0.07068, avg_loss=0.07369]\n",
      "Step 534279  [5.459 sec/step, loss=0.07479, avg_loss=0.07369]\n",
      "Step 534280  [5.445 sec/step, loss=0.07452, avg_loss=0.07369]\n",
      "Step 534281  [5.430 sec/step, loss=0.07369, avg_loss=0.07370]\n",
      "Step 534282  [5.422 sec/step, loss=0.07359, avg_loss=0.07370]\n",
      "Step 534283  [5.415 sec/step, loss=0.07231, avg_loss=0.07366]\n",
      "Step 534284  [5.420 sec/step, loss=0.07480, avg_loss=0.07366]\n",
      "Step 534285  [5.435 sec/step, loss=0.07563, avg_loss=0.07368]\n",
      "Step 534286  [5.436 sec/step, loss=0.07512, avg_loss=0.07370]\n",
      "Step 534287  [5.438 sec/step, loss=0.07439, avg_loss=0.07369]\n",
      "Step 534288  [5.423 sec/step, loss=0.07108, avg_loss=0.07366]\n",
      "Step 534289  [5.439 sec/step, loss=0.07551, avg_loss=0.07369]\n",
      "Step 534290  [5.462 sec/step, loss=0.07323, avg_loss=0.07367]\n",
      "Step 534291  [5.459 sec/step, loss=0.07345, avg_loss=0.07366]\n",
      "Step 534292  [5.468 sec/step, loss=0.07642, avg_loss=0.07367]\n",
      "Step 534293  [5.475 sec/step, loss=0.07309, avg_loss=0.07365]\n",
      "Step 534294  [5.484 sec/step, loss=0.07566, avg_loss=0.07367]\n",
      "Step 534295  [5.480 sec/step, loss=0.07052, avg_loss=0.07363]\n",
      "Step 534296  [5.468 sec/step, loss=0.07479, avg_loss=0.07362]\n",
      "Step 534297  [5.456 sec/step, loss=0.07277, avg_loss=0.07359]\n",
      "Step 534298  [5.456 sec/step, loss=0.07460, avg_loss=0.07359]\n",
      "Step 534299  [5.461 sec/step, loss=0.07525, avg_loss=0.07362]\n",
      "Step 534300  [5.420 sec/step, loss=0.06841, avg_loss=0.07355]\n",
      "Writing summary at step: 534300\n",
      "Step 534301  [5.416 sec/step, loss=0.07612, avg_loss=0.07355]\n",
      "Step 534302  [5.398 sec/step, loss=0.07452, avg_loss=0.07354]\n",
      "Generated 32 batches of size 32 in 2.558 sec\n",
      "Step 534303  [5.398 sec/step, loss=0.07325, avg_loss=0.07352]\n",
      "Step 534304  [5.394 sec/step, loss=0.07454, avg_loss=0.07351]\n",
      "Step 534305  [5.404 sec/step, loss=0.07604, avg_loss=0.07352]\n",
      "Step 534306  [5.473 sec/step, loss=0.06673, avg_loss=0.07355]\n",
      "Step 534307  [5.477 sec/step, loss=0.07403, avg_loss=0.07354]\n",
      "Step 534308  [5.450 sec/step, loss=0.07222, avg_loss=0.07351]\n",
      "Step 534309  [5.463 sec/step, loss=0.07486, avg_loss=0.07351]\n",
      "Step 534310  [5.467 sec/step, loss=0.07104, avg_loss=0.07352]\n",
      "Step 534311  [5.459 sec/step, loss=0.07390, avg_loss=0.07352]\n",
      "Step 534312  [5.469 sec/step, loss=0.07445, avg_loss=0.07354]\n",
      "Step 534313  [5.482 sec/step, loss=0.07530, avg_loss=0.07358]\n",
      "Step 534314  [5.491 sec/step, loss=0.07491, avg_loss=0.07362]\n",
      "Step 534315  [5.517 sec/step, loss=0.07475, avg_loss=0.07363]\n",
      "Step 534316  [5.456 sec/step, loss=0.07344, avg_loss=0.07371]\n",
      "Step 534317  [5.458 sec/step, loss=0.07556, avg_loss=0.07372]\n",
      "Step 534318  [5.457 sec/step, loss=0.07341, avg_loss=0.07371]\n",
      "Step 534319  [5.427 sec/step, loss=0.06501, avg_loss=0.07360]\n",
      "Step 534320  [5.421 sec/step, loss=0.07070, avg_loss=0.07359]\n",
      "Step 534321  [5.429 sec/step, loss=0.07460, avg_loss=0.07360]\n",
      "Step 534322  [5.428 sec/step, loss=0.07355, avg_loss=0.07358]\n",
      "Step 534323  [5.422 sec/step, loss=0.07589, avg_loss=0.07359]\n",
      "Step 534324  [5.429 sec/step, loss=0.07439, avg_loss=0.07361]\n",
      "Step 534325  [5.419 sec/step, loss=0.07125, avg_loss=0.07359]\n",
      "Step 534326  [5.445 sec/step, loss=0.07485, avg_loss=0.07359]\n",
      "Step 534327  [5.459 sec/step, loss=0.07460, avg_loss=0.07360]\n",
      "Step 534328  [5.450 sec/step, loss=0.07425, avg_loss=0.07359]\n",
      "Step 534329  [5.435 sec/step, loss=0.07321, avg_loss=0.07358]\n",
      "Step 534330  [5.449 sec/step, loss=0.07476, avg_loss=0.07365]\n",
      "Step 534331  [5.451 sec/step, loss=0.07219, avg_loss=0.07362]\n",
      "Step 534332  [5.440 sec/step, loss=0.07143, avg_loss=0.07360]\n",
      "Step 534333  [5.433 sec/step, loss=0.07465, avg_loss=0.07359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 534334  [5.435 sec/step, loss=0.07305, avg_loss=0.07358]\n",
      "Generated 32 batches of size 32 in 2.665 sec\n",
      "Step 534335  [5.501 sec/step, loss=0.06594, avg_loss=0.07350]\n",
      "Step 534336  [5.460 sec/step, loss=0.07641, avg_loss=0.07361]\n",
      "Step 534337  [5.469 sec/step, loss=0.07399, avg_loss=0.07361]\n",
      "Step 534338  [5.465 sec/step, loss=0.07522, avg_loss=0.07360]\n",
      "Step 534339  [5.463 sec/step, loss=0.07586, avg_loss=0.07360]\n",
      "Step 534340  [5.456 sec/step, loss=0.07437, avg_loss=0.07359]\n",
      "Step 534341  [5.459 sec/step, loss=0.07298, avg_loss=0.07359]\n",
      "Step 534342  [5.476 sec/step, loss=0.07530, avg_loss=0.07361]\n",
      "Step 534343  [5.470 sec/step, loss=0.07327, avg_loss=0.07360]\n",
      "Step 534344  [5.479 sec/step, loss=0.07564, avg_loss=0.07361]\n",
      "Step 534345  [5.445 sec/step, loss=0.07302, avg_loss=0.07360]\n",
      "Step 534346  [5.440 sec/step, loss=0.07514, avg_loss=0.07361]\n",
      "Step 534347  [5.451 sec/step, loss=0.07130, avg_loss=0.07362]\n",
      "Step 534348  [5.448 sec/step, loss=0.07446, avg_loss=0.07360]\n",
      "Step 534349  [5.440 sec/step, loss=0.07490, avg_loss=0.07360]\n",
      "Step 534350  [5.487 sec/step, loss=0.06628, avg_loss=0.07351]\n",
      "Step 534351  [5.497 sec/step, loss=0.07321, avg_loss=0.07350]\n",
      "Step 534352  [5.501 sec/step, loss=0.07133, avg_loss=0.07351]\n",
      "Step 534353  [5.499 sec/step, loss=0.07511, avg_loss=0.07352]\n",
      "Step 534354  [5.514 sec/step, loss=0.07246, avg_loss=0.07360]\n",
      "Step 534355  [5.499 sec/step, loss=0.07188, avg_loss=0.07357]\n",
      "Step 534356  [5.483 sec/step, loss=0.07489, avg_loss=0.07356]\n",
      "Step 534357  [5.514 sec/step, loss=0.07358, avg_loss=0.07354]\n",
      "Step 534358  [5.504 sec/step, loss=0.07400, avg_loss=0.07352]\n",
      "Step 534359  [5.523 sec/step, loss=0.07519, avg_loss=0.07355]\n",
      "Step 534360  [5.528 sec/step, loss=0.07581, avg_loss=0.07358]\n",
      "Step 534361  [5.537 sec/step, loss=0.07265, avg_loss=0.07357]\n",
      "Step 534362  [5.543 sec/step, loss=0.07462, avg_loss=0.07357]\n",
      "Step 534363  [5.525 sec/step, loss=0.07168, avg_loss=0.07353]\n",
      "Step 534364  [5.550 sec/step, loss=0.07561, avg_loss=0.07354]\n",
      "Step 534365  [5.555 sec/step, loss=0.07358, avg_loss=0.07352]\n",
      "Step 534366  [5.552 sec/step, loss=0.07453, avg_loss=0.07352]\n",
      "Generated 32 batches of size 32 in 2.445 sec\n",
      "Step 534367  [5.583 sec/step, loss=0.07349, avg_loss=0.07351]\n",
      "Step 534368  [5.590 sec/step, loss=0.07373, avg_loss=0.07349]\n",
      "Step 534369  [5.572 sec/step, loss=0.07385, avg_loss=0.07347]\n",
      "Step 534370  [5.507 sec/step, loss=0.07087, avg_loss=0.07352]\n",
      "Step 534371  [5.476 sec/step, loss=0.06615, avg_loss=0.07343]\n",
      "Step 534372  [5.470 sec/step, loss=0.07398, avg_loss=0.07343]\n",
      "Step 534373  [5.474 sec/step, loss=0.07466, avg_loss=0.07343]\n",
      "Step 534374  [5.463 sec/step, loss=0.07266, avg_loss=0.07342]\n",
      "Step 534375  [5.484 sec/step, loss=0.07605, avg_loss=0.07347]\n",
      "Step 534376  [5.495 sec/step, loss=0.07556, avg_loss=0.07349]\n",
      "Step 534377  [5.487 sec/step, loss=0.07500, avg_loss=0.07352]\n",
      "Step 534378  [5.500 sec/step, loss=0.07593, avg_loss=0.07357]\n",
      "Step 534379  [5.501 sec/step, loss=0.07374, avg_loss=0.07356]\n",
      "Step 534380  [5.500 sec/step, loss=0.07135, avg_loss=0.07353]\n",
      "Step 534381  [5.506 sec/step, loss=0.07335, avg_loss=0.07353]\n",
      "Step 534382  [5.493 sec/step, loss=0.07459, avg_loss=0.07354]\n",
      "Step 534383  [5.487 sec/step, loss=0.07486, avg_loss=0.07356]\n",
      "Step 534384  [5.485 sec/step, loss=0.07453, avg_loss=0.07356]\n",
      "Step 534385  [5.487 sec/step, loss=0.07583, avg_loss=0.07356]\n",
      "Step 534386  [5.490 sec/step, loss=0.07392, avg_loss=0.07355]\n",
      "Step 534387  [5.468 sec/step, loss=0.07384, avg_loss=0.07354]\n",
      "Step 534388  [5.484 sec/step, loss=0.07295, avg_loss=0.07356]\n",
      "Step 534389  [5.462 sec/step, loss=0.07061, avg_loss=0.07351]\n",
      "Step 534390  [5.418 sec/step, loss=0.06444, avg_loss=0.07343]\n",
      "Step 534391  [5.415 sec/step, loss=0.07031, avg_loss=0.07339]\n",
      "Step 534392  [5.423 sec/step, loss=0.07460, avg_loss=0.07338]\n",
      "Step 534393  [5.428 sec/step, loss=0.07561, avg_loss=0.07340]\n",
      "Step 534394  [5.434 sec/step, loss=0.07572, avg_loss=0.07340]\n",
      "Step 534395  [5.441 sec/step, loss=0.07430, avg_loss=0.07344]\n",
      "Step 534396  [5.437 sec/step, loss=0.07444, avg_loss=0.07344]\n",
      "Step 534397  [5.437 sec/step, loss=0.07345, avg_loss=0.07344]\n",
      "Step 534398  [5.490 sec/step, loss=0.06506, avg_loss=0.07335]\n",
      "Generated 32 batches of size 32 in 2.457 sec\n",
      "Step 534399  [5.491 sec/step, loss=0.07512, avg_loss=0.07335]\n",
      "Step 534400  [5.514 sec/step, loss=0.07460, avg_loss=0.07341]\n",
      "Writing summary at step: 534400\n",
      "Step 534401  [5.502 sec/step, loss=0.07332, avg_loss=0.07338]\n",
      "Step 534402  [5.509 sec/step, loss=0.07407, avg_loss=0.07338]\n",
      "Step 534403  [5.520 sec/step, loss=0.07520, avg_loss=0.07340]\n",
      "Step 534404  [5.515 sec/step, loss=0.07477, avg_loss=0.07340]\n",
      "Step 534405  [5.493 sec/step, loss=0.07346, avg_loss=0.07337]\n",
      "Step 534406  [5.439 sec/step, loss=0.07196, avg_loss=0.07342]\n",
      "Step 534407  [5.424 sec/step, loss=0.07451, avg_loss=0.07343]\n",
      "Step 534408  [5.434 sec/step, loss=0.07280, avg_loss=0.07343]\n",
      "Step 534409  [5.421 sec/step, loss=0.07365, avg_loss=0.07342]\n",
      "Step 534410  [5.437 sec/step, loss=0.07451, avg_loss=0.07346]\n",
      "Step 534411  [5.447 sec/step, loss=0.07270, avg_loss=0.07345]\n",
      "Step 534412  [5.438 sec/step, loss=0.07268, avg_loss=0.07343]\n",
      "Step 534413  [5.419 sec/step, loss=0.07301, avg_loss=0.07340]\n",
      "Step 534414  [5.425 sec/step, loss=0.07161, avg_loss=0.07337]\n",
      "Step 534415  [5.420 sec/step, loss=0.07496, avg_loss=0.07337]\n",
      "Step 534416  [5.443 sec/step, loss=0.07335, avg_loss=0.07337]\n",
      "Step 534417  [5.438 sec/step, loss=0.07497, avg_loss=0.07337]\n",
      "Step 534418  [5.468 sec/step, loss=0.07288, avg_loss=0.07336]\n",
      "Step 534419  [5.495 sec/step, loss=0.07438, avg_loss=0.07346]\n",
      "Step 534420  [5.517 sec/step, loss=0.07576, avg_loss=0.07351]\n",
      "Step 534421  [5.530 sec/step, loss=0.07589, avg_loss=0.07352]\n",
      "Step 534422  [5.531 sec/step, loss=0.07173, avg_loss=0.07350]\n",
      "Step 534423  [5.512 sec/step, loss=0.07426, avg_loss=0.07348]\n",
      "Step 534424  [5.528 sec/step, loss=0.07604, avg_loss=0.07350]\n",
      "Step 534425  [5.539 sec/step, loss=0.07489, avg_loss=0.07354]\n",
      "Step 534426  [5.505 sec/step, loss=0.07160, avg_loss=0.07350]\n",
      "Step 534427  [5.499 sec/step, loss=0.07539, avg_loss=0.07351]\n",
      "Step 534428  [5.505 sec/step, loss=0.07425, avg_loss=0.07351]\n",
      "Step 534429  [5.494 sec/step, loss=0.06790, avg_loss=0.07346]\n",
      "Generated 32 batches of size 32 in 2.411 sec\n",
      "Step 534430  [5.509 sec/step, loss=0.07556, avg_loss=0.07347]\n",
      "Step 534431  [5.508 sec/step, loss=0.07150, avg_loss=0.07346]\n",
      "Step 534432  [5.514 sec/step, loss=0.07572, avg_loss=0.07350]\n",
      "Step 534433  [5.519 sec/step, loss=0.07402, avg_loss=0.07350]\n",
      "Step 534434  [5.565 sec/step, loss=0.06557, avg_loss=0.07342]\n",
      "Step 534435  [5.517 sec/step, loss=0.07541, avg_loss=0.07352]\n",
      "Step 534436  [5.509 sec/step, loss=0.07458, avg_loss=0.07350]\n",
      "Step 534437  [5.530 sec/step, loss=0.07253, avg_loss=0.07348]\n",
      "Step 534438  [5.513 sec/step, loss=0.07243, avg_loss=0.07346]\n",
      "Step 534439  [5.522 sec/step, loss=0.07529, avg_loss=0.07345]\n",
      "Step 534440  [5.514 sec/step, loss=0.07446, avg_loss=0.07345]\n",
      "Step 534441  [5.505 sec/step, loss=0.07359, avg_loss=0.07346]\n",
      "Step 534442  [5.497 sec/step, loss=0.07475, avg_loss=0.07345]\n",
      "Step 534443  [5.508 sec/step, loss=0.07603, avg_loss=0.07348]\n",
      "Step 534444  [5.479 sec/step, loss=0.06426, avg_loss=0.07337]\n",
      "Step 534445  [5.491 sec/step, loss=0.07619, avg_loss=0.07340]\n",
      "Step 534446  [5.492 sec/step, loss=0.07542, avg_loss=0.07340]\n",
      "Step 534447  [5.492 sec/step, loss=0.07102, avg_loss=0.07340]\n",
      "Step 534448  [5.483 sec/step, loss=0.07278, avg_loss=0.07338]\n",
      "Step 534449  [5.474 sec/step, loss=0.07310, avg_loss=0.07336]\n",
      "Step 534450  [5.438 sec/step, loss=0.07342, avg_loss=0.07343]\n",
      "Step 534451  [5.413 sec/step, loss=0.07137, avg_loss=0.07342]\n",
      "Step 534452  [5.413 sec/step, loss=0.07362, avg_loss=0.07344]\n",
      "Step 534453  [5.413 sec/step, loss=0.07460, avg_loss=0.07343]\n",
      "Step 534454  [5.407 sec/step, loss=0.07298, avg_loss=0.07344]\n",
      "Step 534455  [5.454 sec/step, loss=0.06664, avg_loss=0.07339]\n",
      "Step 534456  [5.457 sec/step, loss=0.07450, avg_loss=0.07338]\n",
      "Step 534457  [5.427 sec/step, loss=0.07497, avg_loss=0.07340]\n",
      "Step 534458  [5.428 sec/step, loss=0.07373, avg_loss=0.07339]\n",
      "Step 534459  [5.419 sec/step, loss=0.07394, avg_loss=0.07338]\n",
      "Step 534460  [5.427 sec/step, loss=0.07623, avg_loss=0.07339]\n",
      "Step 534461  [5.436 sec/step, loss=0.07567, avg_loss=0.07342]\n",
      "Generated 32 batches of size 32 in 2.444 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 534462  [5.433 sec/step, loss=0.07359, avg_loss=0.07341]\n",
      "Step 534463  [5.426 sec/step, loss=0.07059, avg_loss=0.07339]\n",
      "Step 534464  [5.422 sec/step, loss=0.07297, avg_loss=0.07337]\n",
      "Step 534465  [5.436 sec/step, loss=0.07306, avg_loss=0.07336]\n",
      "Step 534466  [5.448 sec/step, loss=0.07645, avg_loss=0.07338]\n",
      "Step 534467  [5.417 sec/step, loss=0.07515, avg_loss=0.07340]\n",
      "Step 534468  [5.415 sec/step, loss=0.07452, avg_loss=0.07341]\n",
      "Step 534469  [5.428 sec/step, loss=0.07513, avg_loss=0.07342]\n",
      "Step 534470  [5.451 sec/step, loss=0.07366, avg_loss=0.07345]\n",
      "Step 534471  [5.468 sec/step, loss=0.07376, avg_loss=0.07352]\n",
      "Step 534472  [5.472 sec/step, loss=0.07267, avg_loss=0.07351]\n",
      "Step 534473  [5.479 sec/step, loss=0.07277, avg_loss=0.07349]\n",
      "Step 534474  [5.479 sec/step, loss=0.07360, avg_loss=0.07350]\n",
      "Step 534475  [5.457 sec/step, loss=0.07182, avg_loss=0.07346]\n",
      "Step 534476  [5.448 sec/step, loss=0.07399, avg_loss=0.07344]\n",
      "Step 534477  [5.456 sec/step, loss=0.07233, avg_loss=0.07342]\n",
      "Step 534478  [5.461 sec/step, loss=0.07563, avg_loss=0.07341]\n",
      "Step 534479  [5.458 sec/step, loss=0.07581, avg_loss=0.07343]\n",
      "Step 534480  [5.470 sec/step, loss=0.07565, avg_loss=0.07348]\n",
      "Step 534481  [5.478 sec/step, loss=0.07453, avg_loss=0.07349]\n",
      "Step 534482  [5.484 sec/step, loss=0.07321, avg_loss=0.07348]\n",
      "Step 534483  [5.493 sec/step, loss=0.07456, avg_loss=0.07347]\n",
      "Step 534484  [5.503 sec/step, loss=0.07566, avg_loss=0.07348]\n",
      "Step 534485  [5.492 sec/step, loss=0.07530, avg_loss=0.07348]\n",
      "Step 534486  [5.489 sec/step, loss=0.07180, avg_loss=0.07346]\n",
      "Step 534487  [5.508 sec/step, loss=0.07542, avg_loss=0.07347]\n",
      "Step 534488  [5.498 sec/step, loss=0.07454, avg_loss=0.07349]\n",
      "Step 534489  [5.509 sec/step, loss=0.07483, avg_loss=0.07353]\n",
      "Step 534490  [5.528 sec/step, loss=0.07493, avg_loss=0.07364]\n",
      "Step 534491  [5.538 sec/step, loss=0.07464, avg_loss=0.07368]\n",
      "Step 534492  [5.523 sec/step, loss=0.07560, avg_loss=0.07369]\n",
      "Step 534493  [5.506 sec/step, loss=0.07387, avg_loss=0.07367]\n",
      "Generated 32 batches of size 32 in 2.404 sec\n",
      "Step 534494  [5.514 sec/step, loss=0.07494, avg_loss=0.07366]\n",
      "Step 534495  [5.514 sec/step, loss=0.07406, avg_loss=0.07366]\n",
      "Step 534496  [5.506 sec/step, loss=0.07269, avg_loss=0.07364]\n",
      "Step 534497  [5.502 sec/step, loss=0.07096, avg_loss=0.07362]\n",
      "Step 534498  [5.447 sec/step, loss=0.07338, avg_loss=0.07370]\n",
      "Step 534499  [5.437 sec/step, loss=0.07175, avg_loss=0.07367]\n",
      "Step 534500  [5.415 sec/step, loss=0.06536, avg_loss=0.07358]\n",
      "Writing summary at step: 534500\n",
      "Step 534501  [5.423 sec/step, loss=0.07409, avg_loss=0.07358]\n",
      "Step 534502  [5.428 sec/step, loss=0.07453, avg_loss=0.07359]\n",
      "Step 534503  [5.414 sec/step, loss=0.07317, avg_loss=0.07357]\n",
      "Step 534504  [5.408 sec/step, loss=0.07325, avg_loss=0.07355]\n",
      "Step 534505  [5.414 sec/step, loss=0.07408, avg_loss=0.07356]\n",
      "Step 534506  [5.419 sec/step, loss=0.07463, avg_loss=0.07359]\n",
      "Step 534507  [5.434 sec/step, loss=0.07527, avg_loss=0.07359]\n",
      "Step 534508  [5.465 sec/step, loss=0.07285, avg_loss=0.07359]\n",
      "Step 534509  [5.480 sec/step, loss=0.07458, avg_loss=0.07360]\n",
      "Step 534510  [5.465 sec/step, loss=0.07392, avg_loss=0.07360]\n",
      "Step 534511  [5.478 sec/step, loss=0.07370, avg_loss=0.07361]\n",
      "Step 534512  [5.531 sec/step, loss=0.06695, avg_loss=0.07355]\n",
      "Step 534513  [5.550 sec/step, loss=0.07246, avg_loss=0.07354]\n",
      "Step 534514  [5.554 sec/step, loss=0.07611, avg_loss=0.07359]\n",
      "Step 534515  [5.538 sec/step, loss=0.07200, avg_loss=0.07356]\n",
      "Step 534516  [5.532 sec/step, loss=0.07425, avg_loss=0.07357]\n",
      "Step 534517  [5.514 sec/step, loss=0.06615, avg_loss=0.07348]\n",
      "Step 534518  [5.499 sec/step, loss=0.07555, avg_loss=0.07351]\n",
      "Step 534519  [5.494 sec/step, loss=0.07542, avg_loss=0.07352]\n",
      "Step 534520  [5.481 sec/step, loss=0.07477, avg_loss=0.07351]\n",
      "Step 534521  [5.476 sec/step, loss=0.07667, avg_loss=0.07352]\n",
      "Step 534522  [5.472 sec/step, loss=0.07278, avg_loss=0.07353]\n",
      "Step 534523  [5.483 sec/step, loss=0.07444, avg_loss=0.07353]\n",
      "Step 534524  [5.465 sec/step, loss=0.07125, avg_loss=0.07348]\n",
      "Generated 32 batches of size 32 in 2.545 sec\n",
      "Step 534525  [5.465 sec/step, loss=0.07379, avg_loss=0.07347]\n",
      "Step 534526  [5.495 sec/step, loss=0.07315, avg_loss=0.07348]\n",
      "Step 534527  [5.496 sec/step, loss=0.07449, avg_loss=0.07348]\n",
      "Step 534528  [5.489 sec/step, loss=0.07469, avg_loss=0.07348]\n",
      "Step 534529  [5.510 sec/step, loss=0.07565, avg_loss=0.07356]\n",
      "Step 534530  [5.512 sec/step, loss=0.07581, avg_loss=0.07356]\n",
      "Step 534531  [5.500 sec/step, loss=0.07126, avg_loss=0.07356]\n",
      "Step 534532  [5.498 sec/step, loss=0.07420, avg_loss=0.07354]\n",
      "Step 534533  [5.483 sec/step, loss=0.07275, avg_loss=0.07353]\n",
      "Step 534534  [5.436 sec/step, loss=0.07366, avg_loss=0.07361]\n",
      "Step 534535  [5.441 sec/step, loss=0.07528, avg_loss=0.07361]\n",
      "Step 534536  [5.440 sec/step, loss=0.07321, avg_loss=0.07360]\n",
      "Step 534537  [5.430 sec/step, loss=0.07604, avg_loss=0.07363]\n",
      "Step 534538  [5.433 sec/step, loss=0.07358, avg_loss=0.07364]\n",
      "Step 534539  [5.407 sec/step, loss=0.07306, avg_loss=0.07362]\n",
      "Step 534540  [5.388 sec/step, loss=0.06661, avg_loss=0.07354]\n",
      "Step 534541  [5.395 sec/step, loss=0.07433, avg_loss=0.07355]\n",
      "Step 534542  [5.380 sec/step, loss=0.07104, avg_loss=0.07351]\n",
      "Step 534543  [5.381 sec/step, loss=0.07575, avg_loss=0.07351]\n",
      "Step 534544  [5.398 sec/step, loss=0.07443, avg_loss=0.07361]\n",
      "Step 534545  [5.391 sec/step, loss=0.07523, avg_loss=0.07360]\n",
      "Step 534546  [5.404 sec/step, loss=0.07533, avg_loss=0.07360]\n",
      "Step 534547  [5.402 sec/step, loss=0.07477, avg_loss=0.07364]\n",
      "Step 534548  [5.424 sec/step, loss=0.07579, avg_loss=0.07367]\n",
      "Step 534549  [5.434 sec/step, loss=0.07439, avg_loss=0.07368]\n",
      "Step 534550  [5.430 sec/step, loss=0.07583, avg_loss=0.07370]\n",
      "Step 534551  [5.494 sec/step, loss=0.06572, avg_loss=0.07365]\n",
      "Step 534552  [5.513 sec/step, loss=0.07576, avg_loss=0.07367]\n",
      "Step 534553  [5.505 sec/step, loss=0.07216, avg_loss=0.07364]\n",
      "Step 534554  [5.518 sec/step, loss=0.07386, avg_loss=0.07365]\n",
      "Step 534555  [5.463 sec/step, loss=0.07311, avg_loss=0.07372]\n",
      "Step 534556  [5.473 sec/step, loss=0.07418, avg_loss=0.07371]\n",
      "Generated 32 batches of size 32 in 2.448 sec\n",
      "Step 534557  [5.509 sec/step, loss=0.07291, avg_loss=0.07369]\n",
      "Step 534558  [5.502 sec/step, loss=0.07398, avg_loss=0.07370]\n",
      "Step 534559  [5.502 sec/step, loss=0.07513, avg_loss=0.07371]\n",
      "Step 534560  [5.503 sec/step, loss=0.07517, avg_loss=0.07370]\n",
      "Step 534561  [5.478 sec/step, loss=0.07110, avg_loss=0.07365]\n",
      "Step 534562  [5.480 sec/step, loss=0.07418, avg_loss=0.07366]\n",
      "Step 534563  [5.487 sec/step, loss=0.07320, avg_loss=0.07368]\n",
      "Step 534564  [5.475 sec/step, loss=0.07503, avg_loss=0.07370]\n",
      "Step 534565  [5.448 sec/step, loss=0.07398, avg_loss=0.07371]\n",
      "Step 534566  [5.455 sec/step, loss=0.07323, avg_loss=0.07368]\n",
      "Step 534567  [5.471 sec/step, loss=0.07601, avg_loss=0.07369]\n",
      "Step 534568  [5.473 sec/step, loss=0.07610, avg_loss=0.07371]\n",
      "Step 534569  [5.471 sec/step, loss=0.07434, avg_loss=0.07370]\n",
      "Step 534570  [5.513 sec/step, loss=0.06631, avg_loss=0.07362]\n",
      "Step 534571  [5.514 sec/step, loss=0.07227, avg_loss=0.07361]\n",
      "Step 534572  [5.517 sec/step, loss=0.07064, avg_loss=0.07359]\n",
      "Step 534573  [5.498 sec/step, loss=0.07347, avg_loss=0.07360]\n",
      "Step 534574  [5.511 sec/step, loss=0.07582, avg_loss=0.07362]\n",
      "Step 534575  [5.506 sec/step, loss=0.06509, avg_loss=0.07355]\n",
      "Step 534576  [5.520 sec/step, loss=0.07545, avg_loss=0.07357]\n",
      "Step 534577  [5.502 sec/step, loss=0.07244, avg_loss=0.07357]\n",
      "Step 534578  [5.500 sec/step, loss=0.07392, avg_loss=0.07355]\n",
      "Step 534579  [5.492 sec/step, loss=0.07506, avg_loss=0.07354]\n",
      "Step 534580  [5.490 sec/step, loss=0.07519, avg_loss=0.07354]\n",
      "Step 534581  [5.478 sec/step, loss=0.07316, avg_loss=0.07352]\n",
      "Step 534582  [5.472 sec/step, loss=0.07549, avg_loss=0.07355]\n",
      "Step 534583  [5.464 sec/step, loss=0.07441, avg_loss=0.07355]\n",
      "Step 534584  [5.436 sec/step, loss=0.07120, avg_loss=0.07350]\n",
      "Step 534585  [5.431 sec/step, loss=0.07303, avg_loss=0.07348]\n",
      "Step 534586  [5.430 sec/step, loss=0.07391, avg_loss=0.07350]\n",
      "Step 534587  [5.423 sec/step, loss=0.07583, avg_loss=0.07350]\n",
      "Step 534588  [5.424 sec/step, loss=0.07213, avg_loss=0.07348]\n",
      "Generated 32 batches of size 32 in 2.364 sec\n",
      "Step 534589  [5.439 sec/step, loss=0.07588, avg_loss=0.07349]\n",
      "Step 534590  [5.465 sec/step, loss=0.07346, avg_loss=0.07348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 534591  [5.474 sec/step, loss=0.07433, avg_loss=0.07347]\n",
      "Step 534592  [5.454 sec/step, loss=0.07047, avg_loss=0.07342]\n",
      "Step 534593  [5.454 sec/step, loss=0.07342, avg_loss=0.07342]\n",
      "Step 534594  [5.436 sec/step, loss=0.07459, avg_loss=0.07341]\n",
      "Step 534595  [5.424 sec/step, loss=0.07313, avg_loss=0.07340]\n",
      "Step 534596  [5.442 sec/step, loss=0.07446, avg_loss=0.07342]\n",
      "Step 534597  [5.456 sec/step, loss=0.07537, avg_loss=0.07347]\n",
      "Step 534598  [5.452 sec/step, loss=0.07311, avg_loss=0.07346]\n",
      "Step 534599  [5.439 sec/step, loss=0.07145, avg_loss=0.07346]\n",
      "Step 534600  [5.448 sec/step, loss=0.07378, avg_loss=0.07354]\n",
      "Writing summary at step: 534600\n",
      "Step 534601  [5.452 sec/step, loss=0.07586, avg_loss=0.07356]\n",
      "Step 534602  [5.448 sec/step, loss=0.07500, avg_loss=0.07357]\n",
      "Step 534603  [5.460 sec/step, loss=0.07548, avg_loss=0.07359]\n",
      "Step 534604  [5.468 sec/step, loss=0.07461, avg_loss=0.07360]\n",
      "Step 534605  [5.497 sec/step, loss=0.07126, avg_loss=0.07357]\n",
      "Step 534606  [5.492 sec/step, loss=0.07336, avg_loss=0.07356]\n",
      "Step 534607  [5.477 sec/step, loss=0.07142, avg_loss=0.07352]\n",
      "Step 534608  [5.463 sec/step, loss=0.07580, avg_loss=0.07355]\n",
      "Step 534609  [5.467 sec/step, loss=0.07452, avg_loss=0.07355]\n",
      "Step 534610  [5.479 sec/step, loss=0.07492, avg_loss=0.07356]\n",
      "Step 534611  [5.456 sec/step, loss=0.07353, avg_loss=0.07356]\n",
      "Step 534612  [5.395 sec/step, loss=0.07120, avg_loss=0.07360]\n",
      "Step 534613  [5.379 sec/step, loss=0.07208, avg_loss=0.07360]\n",
      "Step 534614  [5.368 sec/step, loss=0.07475, avg_loss=0.07359]\n",
      "Step 534615  [5.375 sec/step, loss=0.07424, avg_loss=0.07361]\n",
      "Step 534616  [5.386 sec/step, loss=0.07514, avg_loss=0.07362]\n",
      "Step 534617  [5.416 sec/step, loss=0.07576, avg_loss=0.07371]\n",
      "Step 534618  [5.405 sec/step, loss=0.07492, avg_loss=0.07371]\n",
      "Step 534619  [5.404 sec/step, loss=0.07401, avg_loss=0.07369]\n",
      "Generated 32 batches of size 32 in 2.373 sec\n",
      "Step 534620  [5.421 sec/step, loss=0.07527, avg_loss=0.07370]\n",
      "Step 534621  [5.413 sec/step, loss=0.07403, avg_loss=0.07367]\n",
      "Step 534622  [5.465 sec/step, loss=0.06558, avg_loss=0.07360]\n",
      "Step 534623  [5.446 sec/step, loss=0.06653, avg_loss=0.07352]\n",
      "Step 534624  [5.453 sec/step, loss=0.07481, avg_loss=0.07356]\n",
      "Step 534625  [5.457 sec/step, loss=0.07321, avg_loss=0.07355]\n",
      "Step 534626  [5.444 sec/step, loss=0.07380, avg_loss=0.07356]\n",
      "Step 534627  [5.435 sec/step, loss=0.07386, avg_loss=0.07355]\n",
      "Step 534628  [5.452 sec/step, loss=0.07482, avg_loss=0.07355]\n",
      "Step 534629  [5.454 sec/step, loss=0.07498, avg_loss=0.07355]\n",
      "Step 534630  [5.443 sec/step, loss=0.07345, avg_loss=0.07352]\n",
      "Step 534631  [5.470 sec/step, loss=0.07391, avg_loss=0.07355]\n",
      "Step 534632  [5.471 sec/step, loss=0.07461, avg_loss=0.07355]\n",
      "Step 534633  [5.491 sec/step, loss=0.07567, avg_loss=0.07358]\n",
      "Step 534634  [5.485 sec/step, loss=0.07199, avg_loss=0.07356]\n",
      "Step 534635  [5.482 sec/step, loss=0.07274, avg_loss=0.07354]\n",
      "Step 534636  [5.490 sec/step, loss=0.07604, avg_loss=0.07357]\n",
      "Step 534637  [5.479 sec/step, loss=0.07367, avg_loss=0.07354]\n",
      "Step 534638  [5.497 sec/step, loss=0.07560, avg_loss=0.07356]\n",
      "Step 534639  [5.516 sec/step, loss=0.07402, avg_loss=0.07357]\n",
      "Step 534640  [5.547 sec/step, loss=0.07493, avg_loss=0.07366]\n",
      "Step 534641  [5.543 sec/step, loss=0.07465, avg_loss=0.07366]\n",
      "Step 534642  [5.546 sec/step, loss=0.07343, avg_loss=0.07368]\n",
      "Step 534643  [5.539 sec/step, loss=0.07397, avg_loss=0.07367]\n",
      "Step 534644  [5.534 sec/step, loss=0.07161, avg_loss=0.07364]\n",
      "Step 534645  [5.522 sec/step, loss=0.07380, avg_loss=0.07362]\n",
      "Step 534646  [5.508 sec/step, loss=0.07563, avg_loss=0.07363]\n",
      "Step 534647  [5.520 sec/step, loss=0.07533, avg_loss=0.07363]\n",
      "Step 534648  [5.528 sec/step, loss=0.07511, avg_loss=0.07363]\n",
      "Step 534649  [5.522 sec/step, loss=0.07285, avg_loss=0.07361]\n",
      "Step 534650  [5.513 sec/step, loss=0.07478, avg_loss=0.07360]\n",
      "Step 534651  [5.515 sec/step, loss=0.06645, avg_loss=0.07361]\n",
      "Generated 32 batches of size 32 in 2.410 sec\n",
      "Step 534652  [5.529 sec/step, loss=0.07598, avg_loss=0.07361]\n",
      "Step 534653  [5.543 sec/step, loss=0.07569, avg_loss=0.07364]\n",
      "Step 534654  [5.532 sec/step, loss=0.07320, avg_loss=0.07364]\n",
      "Step 534655  [5.534 sec/step, loss=0.07019, avg_loss=0.07361]\n",
      "Step 534656  [5.530 sec/step, loss=0.07444, avg_loss=0.07361]\n",
      "Step 534657  [5.480 sec/step, loss=0.06439, avg_loss=0.07353]\n",
      "Step 534658  [5.484 sec/step, loss=0.07476, avg_loss=0.07353]\n",
      "Step 534659  [5.468 sec/step, loss=0.07089, avg_loss=0.07349]\n",
      "Step 534660  [5.440 sec/step, loss=0.07110, avg_loss=0.07345]\n",
      "Step 534661  [5.455 sec/step, loss=0.07199, avg_loss=0.07346]\n",
      "Step 534662  [5.452 sec/step, loss=0.07531, avg_loss=0.07347]\n",
      "Step 534663  [5.468 sec/step, loss=0.07560, avg_loss=0.07350]\n",
      "Step 534664  [5.468 sec/step, loss=0.07074, avg_loss=0.07345]\n",
      "Step 534665  [5.457 sec/step, loss=0.07353, avg_loss=0.07345]\n",
      "Step 534666  [5.487 sec/step, loss=0.06548, avg_loss=0.07337]\n",
      "Step 534667  [5.476 sec/step, loss=0.07186, avg_loss=0.07333]\n",
      "Step 534668  [5.474 sec/step, loss=0.07524, avg_loss=0.07332]\n",
      "Step 534669  [5.480 sec/step, loss=0.07584, avg_loss=0.07334]\n",
      "Step 534670  [5.444 sec/step, loss=0.07596, avg_loss=0.07343]\n",
      "Step 534671  [5.443 sec/step, loss=0.07433, avg_loss=0.07345]\n",
      "Step 534672  [5.453 sec/step, loss=0.07464, avg_loss=0.07349]\n",
      "Step 534673  [5.461 sec/step, loss=0.07409, avg_loss=0.07350]\n",
      "Step 534674  [5.448 sec/step, loss=0.07395, avg_loss=0.07348]\n",
      "Step 534675  [5.479 sec/step, loss=0.07339, avg_loss=0.07356]\n",
      "Step 534676  [5.462 sec/step, loss=0.07278, avg_loss=0.07354]\n",
      "Step 534677  [5.467 sec/step, loss=0.07344, avg_loss=0.07355]\n",
      "Step 534678  [5.467 sec/step, loss=0.07607, avg_loss=0.07357]\n",
      "Step 534679  [5.467 sec/step, loss=0.07496, avg_loss=0.07357]\n",
      "Step 534680  [5.455 sec/step, loss=0.07292, avg_loss=0.07354]\n",
      "Step 534681  [5.464 sec/step, loss=0.07410, avg_loss=0.07355]\n",
      "Step 534682  [5.438 sec/step, loss=0.06567, avg_loss=0.07346]\n",
      "Step 534683  [5.442 sec/step, loss=0.07495, avg_loss=0.07346]\n",
      "Generated 32 batches of size 32 in 2.462 sec\n",
      "Step 534684  [5.488 sec/step, loss=0.07255, avg_loss=0.07347]\n",
      "Step 534685  [5.512 sec/step, loss=0.07314, avg_loss=0.07347]\n",
      "Step 534686  [5.498 sec/step, loss=0.07170, avg_loss=0.07345]\n",
      "Step 534687  [5.492 sec/step, loss=0.06968, avg_loss=0.07339]\n",
      "Step 534688  [5.477 sec/step, loss=0.07107, avg_loss=0.07338]\n",
      "Step 534689  [5.473 sec/step, loss=0.07596, avg_loss=0.07338]\n",
      "Step 534690  [5.441 sec/step, loss=0.07354, avg_loss=0.07338]\n",
      "Step 534691  [5.437 sec/step, loss=0.07469, avg_loss=0.07339]\n",
      "Step 534692  [5.454 sec/step, loss=0.07508, avg_loss=0.07343]\n",
      "Step 534693  [5.464 sec/step, loss=0.07293, avg_loss=0.07343]\n",
      "Step 534694  [5.455 sec/step, loss=0.07448, avg_loss=0.07343]\n",
      "Step 534695  [5.477 sec/step, loss=0.07509, avg_loss=0.07345]\n",
      "Step 534696  [5.454 sec/step, loss=0.07390, avg_loss=0.07344]\n",
      "Step 534697  [5.461 sec/step, loss=0.07603, avg_loss=0.07345]\n",
      "Step 534698  [5.493 sec/step, loss=0.07218, avg_loss=0.07344]\n",
      "Step 534699  [5.500 sec/step, loss=0.07280, avg_loss=0.07345]\n",
      "Step 534700  [5.513 sec/step, loss=0.07531, avg_loss=0.07347]\n",
      "Writing summary at step: 534700\n",
      "Step 534701  [5.494 sec/step, loss=0.07319, avg_loss=0.07344]\n",
      "Step 534702  [5.502 sec/step, loss=0.07590, avg_loss=0.07345]\n",
      "Step 534703  [5.480 sec/step, loss=0.07086, avg_loss=0.07340]\n",
      "Step 534704  [5.478 sec/step, loss=0.07388, avg_loss=0.07339]\n",
      "Step 534705  [5.469 sec/step, loss=0.07501, avg_loss=0.07343]\n",
      "Step 534706  [5.477 sec/step, loss=0.07471, avg_loss=0.07345]\n",
      "Step 534707  [5.487 sec/step, loss=0.07563, avg_loss=0.07349]\n",
      "Step 534708  [5.476 sec/step, loss=0.07370, avg_loss=0.07347]\n",
      "Step 534709  [5.476 sec/step, loss=0.07636, avg_loss=0.07349]\n",
      "Step 534710  [5.475 sec/step, loss=0.07374, avg_loss=0.07347]\n",
      "Step 534711  [5.489 sec/step, loss=0.07308, avg_loss=0.07347]\n",
      "Step 534712  [5.497 sec/step, loss=0.07309, avg_loss=0.07349]\n",
      "Step 534713  [5.508 sec/step, loss=0.07334, avg_loss=0.07350]\n",
      "Step 534714  [5.512 sec/step, loss=0.07523, avg_loss=0.07351]\n",
      "Generated 32 batches of size 32 in 2.533 sec\n",
      "Step 534715  [5.501 sec/step, loss=0.07033, avg_loss=0.07347]\n",
      "Step 534716  [5.466 sec/step, loss=0.06599, avg_loss=0.07337]\n",
      "Step 534717  [5.503 sec/step, loss=0.06583, avg_loss=0.07328]\n",
      "Step 534718  [5.498 sec/step, loss=0.07334, avg_loss=0.07326]\n",
      "Step 534719  [5.495 sec/step, loss=0.07489, avg_loss=0.07327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 534720  [5.488 sec/step, loss=0.07271, avg_loss=0.07324]\n",
      "Step 534721  [5.503 sec/step, loss=0.07518, avg_loss=0.07325]\n",
      "Step 534722  [5.464 sec/step, loss=0.07593, avg_loss=0.07336]\n",
      "Step 534723  [5.478 sec/step, loss=0.07468, avg_loss=0.07344]\n",
      "Step 534724  [5.481 sec/step, loss=0.07451, avg_loss=0.07344]\n",
      "Step 534725  [5.494 sec/step, loss=0.07575, avg_loss=0.07346]\n",
      "Step 534726  [5.492 sec/step, loss=0.07179, avg_loss=0.07344]\n",
      "Step 534727  [5.492 sec/step, loss=0.07392, avg_loss=0.07344]\n",
      "Step 534728  [5.470 sec/step, loss=0.07332, avg_loss=0.07343]\n",
      "Step 534729  [5.485 sec/step, loss=0.07268, avg_loss=0.07340]\n",
      "Step 534730  [5.476 sec/step, loss=0.07322, avg_loss=0.07340]\n",
      "Step 534731  [5.445 sec/step, loss=0.06530, avg_loss=0.07332]\n",
      "Step 534732  [5.440 sec/step, loss=0.07481, avg_loss=0.07332]\n",
      "Step 534733  [5.453 sec/step, loss=0.07273, avg_loss=0.07329]\n",
      "Step 534734  [5.452 sec/step, loss=0.07238, avg_loss=0.07329]\n",
      "Step 534735  [5.459 sec/step, loss=0.07558, avg_loss=0.07332]\n",
      "Step 534736  [5.446 sec/step, loss=0.07317, avg_loss=0.07329]\n",
      "Step 534737  [5.447 sec/step, loss=0.07483, avg_loss=0.07330]\n",
      "Step 534738  [5.449 sec/step, loss=0.07578, avg_loss=0.07331]\n",
      "Step 534739  [5.489 sec/step, loss=0.06599, avg_loss=0.07322]\n",
      "Step 534740  [5.467 sec/step, loss=0.07356, avg_loss=0.07321]\n",
      "Step 534741  [5.474 sec/step, loss=0.07474, avg_loss=0.07321]\n",
      "Step 534742  [5.470 sec/step, loss=0.07148, avg_loss=0.07319]\n",
      "Step 534743  [5.474 sec/step, loss=0.07425, avg_loss=0.07320]\n",
      "Step 534744  [5.477 sec/step, loss=0.07468, avg_loss=0.07323]\n",
      "Step 534745  [5.474 sec/step, loss=0.07176, avg_loss=0.07321]\n",
      "Step 534746  [5.462 sec/step, loss=0.07394, avg_loss=0.07319]\n",
      "Generated 32 batches of size 32 in 2.399 sec\n",
      "Step 534747  [5.466 sec/step, loss=0.07509, avg_loss=0.07319]\n",
      "Step 534748  [5.446 sec/step, loss=0.07180, avg_loss=0.07315]\n",
      "Step 534749  [5.460 sec/step, loss=0.07369, avg_loss=0.07316]\n",
      "Step 534750  [5.473 sec/step, loss=0.07405, avg_loss=0.07315]\n",
      "Step 534751  [5.425 sec/step, loss=0.07518, avg_loss=0.07324]\n",
      "Step 534752  [5.413 sec/step, loss=0.07583, avg_loss=0.07324]\n",
      "Step 534753  [5.411 sec/step, loss=0.07604, avg_loss=0.07324]\n",
      "Step 534754  [5.426 sec/step, loss=0.07582, avg_loss=0.07327]\n",
      "Step 534755  [5.431 sec/step, loss=0.07141, avg_loss=0.07328]\n",
      "Step 534756  [5.436 sec/step, loss=0.07320, avg_loss=0.07327]\n",
      "Step 534757  [5.452 sec/step, loss=0.07447, avg_loss=0.07337]\n",
      "Step 534758  [5.467 sec/step, loss=0.07557, avg_loss=0.07338]\n",
      "Step 534759  [5.466 sec/step, loss=0.07084, avg_loss=0.07338]\n",
      "Step 534760  [5.466 sec/step, loss=0.07179, avg_loss=0.07338]\n",
      "Step 534761  [5.483 sec/step, loss=0.07517, avg_loss=0.07342]\n",
      "Step 534762  [5.487 sec/step, loss=0.07405, avg_loss=0.07340]\n",
      "Step 534763  [5.484 sec/step, loss=0.07547, avg_loss=0.07340]\n",
      "Step 534764  [5.491 sec/step, loss=0.07261, avg_loss=0.07342]\n",
      "Step 534765  [5.499 sec/step, loss=0.07252, avg_loss=0.07341]\n",
      "Step 534766  [5.458 sec/step, loss=0.07403, avg_loss=0.07350]\n",
      "Step 534767  [5.466 sec/step, loss=0.07540, avg_loss=0.07353]\n",
      "Step 534768  [5.447 sec/step, loss=0.07407, avg_loss=0.07352]\n",
      "Step 534769  [5.436 sec/step, loss=0.07310, avg_loss=0.07349]\n",
      "Step 534770  [5.424 sec/step, loss=0.07507, avg_loss=0.07348]\n",
      "Step 534771  [5.420 sec/step, loss=0.07481, avg_loss=0.07349]\n",
      "Step 534772  [5.413 sec/step, loss=0.07419, avg_loss=0.07348]\n",
      "Step 534773  [5.413 sec/step, loss=0.07341, avg_loss=0.07348]\n",
      "Step 534774  [5.412 sec/step, loss=0.07394, avg_loss=0.07348]\n",
      "Step 534775  [5.392 sec/step, loss=0.07335, avg_loss=0.07348]\n",
      "Step 534776  [5.394 sec/step, loss=0.07355, avg_loss=0.07349]\n",
      "Step 534777  [5.410 sec/step, loss=0.07201, avg_loss=0.07347]\n",
      "Step 534778  [5.378 sec/step, loss=0.06602, avg_loss=0.07337]\n",
      "Generated 32 batches of size 32 in 2.428 sec\n",
      "Step 534779  [5.394 sec/step, loss=0.07566, avg_loss=0.07338]\n",
      "Step 534780  [5.402 sec/step, loss=0.07438, avg_loss=0.07339]\n",
      "Step 534781  [5.395 sec/step, loss=0.07067, avg_loss=0.07336]\n",
      "Step 534782  [5.428 sec/step, loss=0.07485, avg_loss=0.07345]\n",
      "Step 534783  [5.431 sec/step, loss=0.07541, avg_loss=0.07345]\n",
      "Step 534784  [5.399 sec/step, loss=0.07405, avg_loss=0.07347]\n",
      "Step 534785  [5.430 sec/step, loss=0.06504, avg_loss=0.07339]\n",
      "Step 534786  [5.452 sec/step, loss=0.07588, avg_loss=0.07343]\n",
      "Step 534787  [5.459 sec/step, loss=0.07555, avg_loss=0.07349]\n",
      "Step 534788  [5.470 sec/step, loss=0.07425, avg_loss=0.07352]\n",
      "Step 534789  [5.489 sec/step, loss=0.07290, avg_loss=0.07349]\n",
      "Step 534790  [5.489 sec/step, loss=0.07449, avg_loss=0.07350]\n",
      "Step 534791  [5.475 sec/step, loss=0.06997, avg_loss=0.07345]\n",
      "Step 534792  [5.472 sec/step, loss=0.07547, avg_loss=0.07346]\n",
      "Step 534793  [5.476 sec/step, loss=0.07141, avg_loss=0.07344]\n",
      "Step 534794  [5.491 sec/step, loss=0.07287, avg_loss=0.07342]\n",
      "Step 534795  [5.474 sec/step, loss=0.07352, avg_loss=0.07341]\n",
      "Step 534796  [5.498 sec/step, loss=0.07336, avg_loss=0.07340]\n",
      "Step 534797  [5.475 sec/step, loss=0.07020, avg_loss=0.07335]\n",
      "Step 534798  [5.440 sec/step, loss=0.07370, avg_loss=0.07336]\n",
      "Step 534799  [5.461 sec/step, loss=0.07582, avg_loss=0.07339]\n",
      "Step 534800  [5.461 sec/step, loss=0.07510, avg_loss=0.07339]\n",
      "Writing summary at step: 534800\n",
      "Step 534801  [5.471 sec/step, loss=0.07384, avg_loss=0.07340]\n",
      "Step 534802  [5.463 sec/step, loss=0.07429, avg_loss=0.07338]\n",
      "Step 534803  [5.475 sec/step, loss=0.07519, avg_loss=0.07342]\n",
      "Step 534804  [5.488 sec/step, loss=0.07542, avg_loss=0.07344]\n",
      "Step 534805  [5.482 sec/step, loss=0.07555, avg_loss=0.07344]\n",
      "Step 534806  [5.485 sec/step, loss=0.07594, avg_loss=0.07346]\n",
      "Step 534807  [5.470 sec/step, loss=0.07315, avg_loss=0.07343]\n",
      "Step 534808  [5.480 sec/step, loss=0.07446, avg_loss=0.07344]\n",
      "Step 534809  [5.465 sec/step, loss=0.07315, avg_loss=0.07341]\n",
      "Generated 32 batches of size 32 in 2.388 sec\n",
      "Step 534810  [5.471 sec/step, loss=0.07472, avg_loss=0.07342]\n",
      "Step 534811  [5.476 sec/step, loss=0.07536, avg_loss=0.07344]\n",
      "Step 534812  [5.477 sec/step, loss=0.07225, avg_loss=0.07343]\n",
      "Step 534813  [5.454 sec/step, loss=0.06513, avg_loss=0.07335]\n",
      "Step 534814  [5.453 sec/step, loss=0.07259, avg_loss=0.07332]\n",
      "Step 534815  [5.478 sec/step, loss=0.07296, avg_loss=0.07335]\n",
      "Step 534816  [5.499 sec/step, loss=0.07537, avg_loss=0.07344]\n",
      "Step 534817  [5.499 sec/step, loss=0.06676, avg_loss=0.07345]\n",
      "Step 534818  [5.509 sec/step, loss=0.07585, avg_loss=0.07348]\n",
      "Step 534819  [5.505 sec/step, loss=0.07482, avg_loss=0.07348]\n",
      "Step 534820  [5.548 sec/step, loss=0.06487, avg_loss=0.07340]\n",
      "Step 534821  [5.541 sec/step, loss=0.07657, avg_loss=0.07341]\n",
      "Step 534822  [5.542 sec/step, loss=0.07381, avg_loss=0.07339]\n",
      "Step 534823  [5.537 sec/step, loss=0.07446, avg_loss=0.07339]\n",
      "Step 534824  [5.546 sec/step, loss=0.07553, avg_loss=0.07340]\n",
      "Step 534825  [5.523 sec/step, loss=0.07195, avg_loss=0.07336]\n",
      "Step 534826  [5.535 sec/step, loss=0.07630, avg_loss=0.07340]\n",
      "Step 534827  [5.531 sec/step, loss=0.07340, avg_loss=0.07340]\n",
      "Step 534828  [5.536 sec/step, loss=0.07219, avg_loss=0.07339]\n",
      "Step 534829  [5.517 sec/step, loss=0.07569, avg_loss=0.07342]\n",
      "Step 534830  [5.507 sec/step, loss=0.07171, avg_loss=0.07340]\n",
      "Step 534831  [5.536 sec/step, loss=0.07473, avg_loss=0.07350]\n",
      "Step 534832  [5.535 sec/step, loss=0.07553, avg_loss=0.07350]\n",
      "Step 534833  [5.517 sec/step, loss=0.07515, avg_loss=0.07353]\n",
      "Step 534834  [5.530 sec/step, loss=0.07576, avg_loss=0.07356]\n",
      "Step 534835  [5.519 sec/step, loss=0.07145, avg_loss=0.07352]\n",
      "Step 534836  [5.525 sec/step, loss=0.07511, avg_loss=0.07354]\n",
      "Step 534837  [5.537 sec/step, loss=0.07573, avg_loss=0.07355]\n",
      "Step 534838  [5.542 sec/step, loss=0.07571, avg_loss=0.07355]\n",
      "Step 534839  [5.474 sec/step, loss=0.06603, avg_loss=0.07355]\n",
      "Step 534840  [5.510 sec/step, loss=0.07315, avg_loss=0.07355]\n",
      "Step 534841  [5.508 sec/step, loss=0.07535, avg_loss=0.07355]\n",
      "Generated 32 batches of size 32 in 2.498 sec\n",
      "Step 534842  [5.523 sec/step, loss=0.07527, avg_loss=0.07359]\n",
      "Step 534843  [5.526 sec/step, loss=0.07472, avg_loss=0.07359]\n",
      "Step 534844  [5.529 sec/step, loss=0.07297, avg_loss=0.07358]\n",
      "Step 534845  [5.537 sec/step, loss=0.07339, avg_loss=0.07359]\n",
      "Step 534846  [5.552 sec/step, loss=0.07512, avg_loss=0.07361]\n",
      "Step 534847  [5.529 sec/step, loss=0.07376, avg_loss=0.07359]\n",
      "Step 534848  [5.535 sec/step, loss=0.07333, avg_loss=0.07361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 534849  [5.521 sec/step, loss=0.07308, avg_loss=0.07360]\n",
      "Step 534850  [5.514 sec/step, loss=0.07615, avg_loss=0.07362]\n",
      "Step 534851  [5.513 sec/step, loss=0.07564, avg_loss=0.07363]\n",
      "Step 534852  [5.491 sec/step, loss=0.07083, avg_loss=0.07358]\n",
      "Step 534853  [5.500 sec/step, loss=0.07208, avg_loss=0.07354]\n",
      "Step 534854  [5.491 sec/step, loss=0.07495, avg_loss=0.07353]\n",
      "Step 534855  [5.488 sec/step, loss=0.07266, avg_loss=0.07354]\n",
      "Step 534856  [5.470 sec/step, loss=0.07383, avg_loss=0.07355]\n",
      "Step 534857  [5.484 sec/step, loss=0.07398, avg_loss=0.07354]\n",
      "Step 534858  [5.473 sec/step, loss=0.07429, avg_loss=0.07353]\n",
      "Step 534859  [5.497 sec/step, loss=0.07597, avg_loss=0.07358]\n",
      "Step 534860  [5.513 sec/step, loss=0.07478, avg_loss=0.07361]\n",
      "Step 534861  [5.504 sec/step, loss=0.07342, avg_loss=0.07359]\n",
      "Step 534862  [5.546 sec/step, loss=0.06690, avg_loss=0.07352]\n",
      "Step 534863  [5.521 sec/step, loss=0.06565, avg_loss=0.07342]\n",
      "Step 534864  [5.513 sec/step, loss=0.07528, avg_loss=0.07345]\n",
      "Step 534865  [5.521 sec/step, loss=0.07571, avg_loss=0.07348]\n",
      "Step 534866  [5.517 sec/step, loss=0.07474, avg_loss=0.07349]\n",
      "Step 534867  [5.501 sec/step, loss=0.07397, avg_loss=0.07347]\n",
      "Step 534868  [5.516 sec/step, loss=0.07263, avg_loss=0.07346]\n",
      "Step 534869  [5.547 sec/step, loss=0.07331, avg_loss=0.07346]\n",
      "Step 534870  [5.552 sec/step, loss=0.07360, avg_loss=0.07345]\n",
      "Step 534871  [5.546 sec/step, loss=0.07409, avg_loss=0.07344]\n",
      "Step 534872  [5.556 sec/step, loss=0.07469, avg_loss=0.07345]\n",
      "Step 534873  [5.565 sec/step, loss=0.07592, avg_loss=0.07347]\n",
      "Generated 32 batches of size 32 in 2.379 sec\n",
      "Step 534874  [5.580 sec/step, loss=0.07166, avg_loss=0.07345]\n",
      "Step 534875  [5.573 sec/step, loss=0.07090, avg_loss=0.07342]\n",
      "Step 534876  [5.586 sec/step, loss=0.07586, avg_loss=0.07345]\n",
      "Step 534877  [5.551 sec/step, loss=0.07235, avg_loss=0.07345]\n",
      "Step 534878  [5.565 sec/step, loss=0.07207, avg_loss=0.07351]\n",
      "Step 534879  [5.549 sec/step, loss=0.07032, avg_loss=0.07346]\n",
      "Step 534880  [5.550 sec/step, loss=0.07534, avg_loss=0.07347]\n",
      "Step 534881  [5.550 sec/step, loss=0.07445, avg_loss=0.07350]\n",
      "Step 534882  [5.551 sec/step, loss=0.07574, avg_loss=0.07351]\n",
      "Step 534883  [5.554 sec/step, loss=0.07564, avg_loss=0.07352]\n",
      "Step 534884  [5.543 sec/step, loss=0.07028, avg_loss=0.07348]\n",
      "Step 534885  [5.510 sec/step, loss=0.07470, avg_loss=0.07357]\n",
      "Step 534886  [5.503 sec/step, loss=0.07436, avg_loss=0.07356]\n",
      "Step 534887  [5.498 sec/step, loss=0.07437, avg_loss=0.07355]\n",
      "Step 534888  [5.490 sec/step, loss=0.07366, avg_loss=0.07354]\n",
      "Step 534889  [5.464 sec/step, loss=0.07469, avg_loss=0.07356]\n",
      "Step 534890  [5.477 sec/step, loss=0.07530, avg_loss=0.07357]\n",
      "Step 534891  [5.471 sec/step, loss=0.06651, avg_loss=0.07353]\n",
      "Step 534892  [5.469 sec/step, loss=0.07364, avg_loss=0.07351]\n",
      "Step 534893  [5.469 sec/step, loss=0.07436, avg_loss=0.07354]\n",
      "Step 534894  [5.509 sec/step, loss=0.06638, avg_loss=0.07348]\n",
      "Step 534895  [5.519 sec/step, loss=0.07574, avg_loss=0.07350]\n",
      "Step 534896  [5.499 sec/step, loss=0.07322, avg_loss=0.07350]\n",
      "Step 534897  [5.517 sec/step, loss=0.07406, avg_loss=0.07354]\n",
      "Step 534898  [5.542 sec/step, loss=0.07526, avg_loss=0.07355]\n",
      "Step 534899  [5.539 sec/step, loss=0.07562, avg_loss=0.07355]\n",
      "Step 534900  [5.531 sec/step, loss=0.07380, avg_loss=0.07354]\n",
      "Writing summary at step: 534900\n",
      "Step 534901  [5.527 sec/step, loss=0.07503, avg_loss=0.07355]\n",
      "Step 534902  [5.536 sec/step, loss=0.07517, avg_loss=0.07356]\n",
      "Step 534903  [5.540 sec/step, loss=0.07508, avg_loss=0.07356]\n",
      "Step 534904  [5.516 sec/step, loss=0.07376, avg_loss=0.07354]\n",
      "Generated 32 batches of size 32 in 2.394 sec\n",
      "Step 534905  [5.534 sec/step, loss=0.07512, avg_loss=0.07354]\n",
      "Step 534906  [5.527 sec/step, loss=0.07458, avg_loss=0.07352]\n",
      "Step 534907  [5.520 sec/step, loss=0.07052, avg_loss=0.07350]\n",
      "Step 534908  [5.514 sec/step, loss=0.07413, avg_loss=0.07349]\n",
      "Step 534909  [5.519 sec/step, loss=0.07026, avg_loss=0.07347]\n",
      "Step 534910  [5.521 sec/step, loss=0.07563, avg_loss=0.07347]\n",
      "Step 534911  [5.513 sec/step, loss=0.07296, avg_loss=0.07345]\n",
      "Step 534912  [5.526 sec/step, loss=0.07432, avg_loss=0.07347]\n",
      "Step 534913  [5.553 sec/step, loss=0.07362, avg_loss=0.07356]\n",
      "Step 534914  [5.555 sec/step, loss=0.07544, avg_loss=0.07359]\n",
      "Step 534915  [5.534 sec/step, loss=0.07165, avg_loss=0.07357]\n",
      "Step 534916  [5.545 sec/step, loss=0.07321, avg_loss=0.07355]\n",
      "Step 534917  [5.487 sec/step, loss=0.07276, avg_loss=0.07361]\n",
      "Step 534918  [5.478 sec/step, loss=0.07459, avg_loss=0.07360]\n",
      "Step 534919  [5.484 sec/step, loss=0.07387, avg_loss=0.07359]\n",
      "Step 534920  [5.432 sec/step, loss=0.07459, avg_loss=0.07369]\n",
      "Step 534921  [5.428 sec/step, loss=0.07409, avg_loss=0.07366]\n",
      "Step 534922  [5.416 sec/step, loss=0.07453, avg_loss=0.07367]\n",
      "Step 534923  [5.451 sec/step, loss=0.07264, avg_loss=0.07365]\n",
      "Step 534924  [5.451 sec/step, loss=0.07514, avg_loss=0.07365]\n",
      "Step 534925  [5.462 sec/step, loss=0.07549, avg_loss=0.07368]\n",
      "Step 534926  [5.430 sec/step, loss=0.07157, avg_loss=0.07363]\n",
      "Step 534927  [5.453 sec/step, loss=0.07305, avg_loss=0.07363]\n",
      "Step 534928  [5.451 sec/step, loss=0.07238, avg_loss=0.07363]\n",
      "Step 534929  [5.451 sec/step, loss=0.07086, avg_loss=0.07358]\n",
      "Step 534930  [5.462 sec/step, loss=0.07332, avg_loss=0.07360]\n",
      "Step 534931  [5.466 sec/step, loss=0.07569, avg_loss=0.07361]\n",
      "Step 534932  [5.477 sec/step, loss=0.07449, avg_loss=0.07360]\n",
      "Step 534933  [5.512 sec/step, loss=0.06948, avg_loss=0.07354]\n",
      "Step 534934  [5.506 sec/step, loss=0.07559, avg_loss=0.07354]\n",
      "Step 534935  [5.491 sec/step, loss=0.07398, avg_loss=0.07357]\n",
      "Step 534936  [5.504 sec/step, loss=0.07605, avg_loss=0.07358]\n",
      "Generated 32 batches of size 32 in 2.494 sec\n",
      "Step 534937  [5.483 sec/step, loss=0.07092, avg_loss=0.07353]\n",
      "Step 534938  [5.480 sec/step, loss=0.07345, avg_loss=0.07351]\n",
      "Step 534939  [5.506 sec/step, loss=0.07567, avg_loss=0.07360]\n",
      "Step 534940  [5.462 sec/step, loss=0.06470, avg_loss=0.07352]\n",
      "Step 534941  [5.452 sec/step, loss=0.07356, avg_loss=0.07350]\n",
      "Step 534942  [5.459 sec/step, loss=0.07588, avg_loss=0.07351]\n",
      "Step 534943  [5.449 sec/step, loss=0.07503, avg_loss=0.07351]\n",
      "Step 534944  [5.449 sec/step, loss=0.07384, avg_loss=0.07352]\n",
      "Step 534945  [5.460 sec/step, loss=0.07500, avg_loss=0.07353]\n",
      "Step 534946  [5.445 sec/step, loss=0.07075, avg_loss=0.07349]\n",
      "Step 534947  [5.461 sec/step, loss=0.07506, avg_loss=0.07350]\n",
      "Step 534948  [5.462 sec/step, loss=0.07327, avg_loss=0.07350]\n",
      "Step 534949  [5.483 sec/step, loss=0.07535, avg_loss=0.07352]\n",
      "Step 534950  [5.462 sec/step, loss=0.07165, avg_loss=0.07348]\n",
      "Step 534951  [5.458 sec/step, loss=0.07463, avg_loss=0.07347]\n",
      "Step 534952  [5.478 sec/step, loss=0.07490, avg_loss=0.07351]\n",
      "Step 534953  [5.444 sec/step, loss=0.06579, avg_loss=0.07345]\n",
      "Step 534954  [5.450 sec/step, loss=0.07443, avg_loss=0.07344]\n",
      "Step 534955  [5.451 sec/step, loss=0.07192, avg_loss=0.07343]\n",
      "Step 534956  [5.462 sec/step, loss=0.07167, avg_loss=0.07341]\n",
      "Step 534957  [5.451 sec/step, loss=0.07487, avg_loss=0.07342]\n",
      "Step 534958  [5.465 sec/step, loss=0.07547, avg_loss=0.07343]\n",
      "Step 534959  [5.452 sec/step, loss=0.07444, avg_loss=0.07342]\n",
      "Step 534960  [5.454 sec/step, loss=0.07397, avg_loss=0.07341]\n",
      "Step 534961  [5.470 sec/step, loss=0.07281, avg_loss=0.07340]\n",
      "Step 534962  [5.426 sec/step, loss=0.07552, avg_loss=0.07349]\n",
      "Step 534963  [5.439 sec/step, loss=0.07324, avg_loss=0.07357]\n",
      "Step 534964  [5.492 sec/step, loss=0.06619, avg_loss=0.07348]\n",
      "Step 534965  [5.484 sec/step, loss=0.07204, avg_loss=0.07344]\n",
      "Step 534966  [5.472 sec/step, loss=0.07374, avg_loss=0.07343]\n",
      "Step 534967  [5.479 sec/step, loss=0.07500, avg_loss=0.07344]\n",
      "Step 534968  [5.484 sec/step, loss=0.07590, avg_loss=0.07347]\n",
      "Generated 32 batches of size 32 in 2.520 sec\n",
      "Step 534969  [5.453 sec/step, loss=0.07052, avg_loss=0.07344]\n",
      "Step 534970  [5.458 sec/step, loss=0.07588, avg_loss=0.07347]\n",
      "Step 534971  [5.467 sec/step, loss=0.07395, avg_loss=0.07347]\n",
      "Step 534972  [5.451 sec/step, loss=0.06949, avg_loss=0.07341]\n",
      "Step 534973  [5.453 sec/step, loss=0.07387, avg_loss=0.07339]\n",
      "Step 534974  [5.457 sec/step, loss=0.07552, avg_loss=0.07343]\n",
      "Step 534975  [5.471 sec/step, loss=0.07126, avg_loss=0.07343]\n",
      "Step 534976  [5.462 sec/step, loss=0.07532, avg_loss=0.07343]\n",
      "Step 534977  [5.465 sec/step, loss=0.07333, avg_loss=0.07344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 534978  [5.466 sec/step, loss=0.07478, avg_loss=0.07347]\n",
      "Step 534979  [5.517 sec/step, loss=0.06547, avg_loss=0.07342]\n",
      "Step 534980  [5.511 sec/step, loss=0.07361, avg_loss=0.07340]\n",
      "Step 534981  [5.524 sec/step, loss=0.07499, avg_loss=0.07341]\n",
      "Step 534982  [5.522 sec/step, loss=0.07357, avg_loss=0.07338]\n",
      "Step 534983  [5.528 sec/step, loss=0.07594, avg_loss=0.07339]\n",
      "Step 534984  [5.544 sec/step, loss=0.07461, avg_loss=0.07343]\n",
      "Step 534985  [5.519 sec/step, loss=0.07328, avg_loss=0.07342]\n",
      "Step 534986  [5.510 sec/step, loss=0.07362, avg_loss=0.07341]\n",
      "Step 534987  [5.521 sec/step, loss=0.07592, avg_loss=0.07342]\n",
      "Step 534988  [5.553 sec/step, loss=0.07221, avg_loss=0.07341]\n",
      "Step 534989  [5.562 sec/step, loss=0.07462, avg_loss=0.07341]\n",
      "Step 534990  [5.553 sec/step, loss=0.07262, avg_loss=0.07338]\n",
      "Step 534991  [5.571 sec/step, loss=0.07540, avg_loss=0.07347]\n",
      "Step 534992  [5.574 sec/step, loss=0.07508, avg_loss=0.07349]\n",
      "Step 534993  [5.579 sec/step, loss=0.07591, avg_loss=0.07350]\n",
      "Step 534994  [5.542 sec/step, loss=0.07536, avg_loss=0.07359]\n",
      "Step 534995  [5.533 sec/step, loss=0.07492, avg_loss=0.07358]\n",
      "Step 534996  [5.549 sec/step, loss=0.07595, avg_loss=0.07361]\n",
      "Step 534997  [5.542 sec/step, loss=0.07152, avg_loss=0.07359]\n",
      "Step 534998  [5.556 sec/step, loss=0.07288, avg_loss=0.07356]\n",
      "Step 534999  [5.535 sec/step, loss=0.07111, avg_loss=0.07352]\n",
      "Step 535000  [5.543 sec/step, loss=0.07466, avg_loss=0.07352]\n",
      "Writing summary at step: 535000\n",
      "Generated 32 batches of size 32 in 2.380 sec\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-535000\n",
      "Saving audio and alignment...\n",
      "Input: kabiir barrhaii kaa shaehhraa or faem hii as kii maiishatd kaa bharuusaa hae~______________\n",
      "Step 535001  [5.530 sec/step, loss=0.06478, avg_loss=0.07342]\n",
      "Step 535002  [5.521 sec/step, loss=0.07244, avg_loss=0.07340]\n",
      "Step 535003  [5.525 sec/step, loss=0.07154, avg_loss=0.07336]\n",
      "Step 535004  [5.530 sec/step, loss=0.07305, avg_loss=0.07335]\n",
      "Step 535005  [5.501 sec/step, loss=0.07426, avg_loss=0.07334]\n",
      "Step 535006  [5.507 sec/step, loss=0.07534, avg_loss=0.07335]\n",
      "Step 535007  [5.514 sec/step, loss=0.07073, avg_loss=0.07335]\n",
      "Step 535008  [5.515 sec/step, loss=0.07556, avg_loss=0.07337]\n",
      "Step 535009  [5.535 sec/step, loss=0.07481, avg_loss=0.07341]\n",
      "Step 535010  [5.509 sec/step, loss=0.07131, avg_loss=0.07337]\n",
      "Step 535011  [5.508 sec/step, loss=0.07402, avg_loss=0.07338]\n",
      "Step 535012  [5.499 sec/step, loss=0.07361, avg_loss=0.07337]\n",
      "Step 535013  [5.482 sec/step, loss=0.07355, avg_loss=0.07337]\n",
      "Step 535014  [5.492 sec/step, loss=0.07559, avg_loss=0.07337]\n",
      "Step 535015  [5.490 sec/step, loss=0.07492, avg_loss=0.07341]\n",
      "Step 535016  [5.487 sec/step, loss=0.07529, avg_loss=0.07343]\n",
      "Step 535017  [5.544 sec/step, loss=0.06577, avg_loss=0.07336]\n",
      "Step 535018  [5.539 sec/step, loss=0.07205, avg_loss=0.07333]\n",
      "Step 535019  [5.538 sec/step, loss=0.07460, avg_loss=0.07334]\n",
      "Step 535020  [5.541 sec/step, loss=0.07281, avg_loss=0.07332]\n",
      "Step 535021  [5.538 sec/step, loss=0.07514, avg_loss=0.07333]\n",
      "Step 535022  [5.563 sec/step, loss=0.07260, avg_loss=0.07331]\n",
      "Step 535023  [5.540 sec/step, loss=0.07429, avg_loss=0.07333]\n",
      "Step 535024  [5.542 sec/step, loss=0.07549, avg_loss=0.07333]\n",
      "Step 535025  [5.548 sec/step, loss=0.07541, avg_loss=0.07333]\n",
      "Step 535026  [5.551 sec/step, loss=0.07092, avg_loss=0.07333]\n",
      "Step 535027  [5.529 sec/step, loss=0.07061, avg_loss=0.07330]\n",
      "Step 535028  [5.535 sec/step, loss=0.07500, avg_loss=0.07333]\n",
      "Step 535029  [5.524 sec/step, loss=0.07352, avg_loss=0.07335]\n",
      "Step 535030  [5.543 sec/step, loss=0.07478, avg_loss=0.07337]\n",
      "Generated 32 batches of size 32 in 2.442 sec\n",
      "Step 535031  [5.538 sec/step, loss=0.07232, avg_loss=0.07334]\n",
      "Step 535032  [5.531 sec/step, loss=0.07360, avg_loss=0.07333]\n",
      "Step 535033  [5.485 sec/step, loss=0.07174, avg_loss=0.07335]\n",
      "Step 535034  [5.488 sec/step, loss=0.07527, avg_loss=0.07335]\n",
      "Step 535035  [5.499 sec/step, loss=0.07241, avg_loss=0.07333]\n",
      "Step 535036  [5.468 sec/step, loss=0.06666, avg_loss=0.07324]\n",
      "Step 535037  [5.484 sec/step, loss=0.07564, avg_loss=0.07328]\n",
      "Step 535038  [5.471 sec/step, loss=0.07480, avg_loss=0.07330]\n",
      "Step 535039  [5.454 sec/step, loss=0.07348, avg_loss=0.07328]\n",
      "Step 535040  [5.478 sec/step, loss=0.07486, avg_loss=0.07338]\n",
      "Step 535041  [5.480 sec/step, loss=0.07244, avg_loss=0.07337]\n",
      "Step 535042  [5.458 sec/step, loss=0.07365, avg_loss=0.07334]\n",
      "Step 535043  [5.461 sec/step, loss=0.07283, avg_loss=0.07332]\n",
      "Step 535044  [5.483 sec/step, loss=0.07342, avg_loss=0.07332]\n",
      "Step 535045  [5.490 sec/step, loss=0.07570, avg_loss=0.07332]\n",
      "Step 535046  [5.492 sec/step, loss=0.07343, avg_loss=0.07335]\n",
      "Step 535047  [5.484 sec/step, loss=0.07284, avg_loss=0.07333]\n",
      "Step 535048  [5.462 sec/step, loss=0.07074, avg_loss=0.07330]\n",
      "Step 535049  [5.444 sec/step, loss=0.07434, avg_loss=0.07329]\n",
      "Step 535050  [5.442 sec/step, loss=0.06432, avg_loss=0.07322]\n",
      "Step 535051  [5.440 sec/step, loss=0.07459, avg_loss=0.07322]\n",
      "Step 535052  [5.446 sec/step, loss=0.07566, avg_loss=0.07323]\n",
      "Step 535053  [5.511 sec/step, loss=0.06629, avg_loss=0.07323]\n",
      "Step 535054  [5.490 sec/step, loss=0.07075, avg_loss=0.07320]\n",
      "Step 535055  [5.503 sec/step, loss=0.07544, avg_loss=0.07323]\n",
      "Step 535056  [5.497 sec/step, loss=0.07156, avg_loss=0.07323]\n",
      "Step 535057  [5.496 sec/step, loss=0.07552, avg_loss=0.07324]\n",
      "Step 535058  [5.486 sec/step, loss=0.07400, avg_loss=0.07322]\n",
      "Step 535059  [5.491 sec/step, loss=0.07387, avg_loss=0.07322]\n",
      "Step 535060  [5.480 sec/step, loss=0.07258, avg_loss=0.07320]\n",
      "Step 535061  [5.460 sec/step, loss=0.07606, avg_loss=0.07323]\n",
      "Step 535062  [5.476 sec/step, loss=0.07372, avg_loss=0.07322]\n",
      "Generated 32 batches of size 32 in 2.375 sec\n",
      "Step 535063  [5.487 sec/step, loss=0.07515, avg_loss=0.07324]\n",
      "Step 535064  [5.438 sec/step, loss=0.07425, avg_loss=0.07332]\n",
      "Step 535065  [5.448 sec/step, loss=0.07579, avg_loss=0.07335]\n",
      "Step 535066  [5.469 sec/step, loss=0.07340, avg_loss=0.07335]\n",
      "Step 535067  [5.463 sec/step, loss=0.07459, avg_loss=0.07335]\n",
      "Step 535068  [5.455 sec/step, loss=0.07462, avg_loss=0.07333]\n",
      "Step 535069  [5.471 sec/step, loss=0.07383, avg_loss=0.07337]\n",
      "Step 535070  [5.460 sec/step, loss=0.07447, avg_loss=0.07335]\n",
      "Step 535071  [5.468 sec/step, loss=0.07585, avg_loss=0.07337]\n",
      "Step 535072  [5.497 sec/step, loss=0.07477, avg_loss=0.07342]\n",
      "Step 535073  [5.494 sec/step, loss=0.07589, avg_loss=0.07344]\n",
      "Step 535074  [5.487 sec/step, loss=0.07454, avg_loss=0.07343]\n",
      "Step 535075  [5.489 sec/step, loss=0.07421, avg_loss=0.07346]\n",
      "Step 535076  [5.478 sec/step, loss=0.06995, avg_loss=0.07341]\n",
      "Step 535077  [5.491 sec/step, loss=0.07360, avg_loss=0.07341]\n",
      "Step 535078  [5.505 sec/step, loss=0.07598, avg_loss=0.07342]\n",
      "Step 535079  [5.439 sec/step, loss=0.06623, avg_loss=0.07343]\n",
      "Step 535080  [5.426 sec/step, loss=0.07227, avg_loss=0.07342]\n",
      "Step 535081  [5.425 sec/step, loss=0.07434, avg_loss=0.07341]\n",
      "Step 535082  [5.411 sec/step, loss=0.07416, avg_loss=0.07342]\n",
      "Step 535083  [5.394 sec/step, loss=0.07439, avg_loss=0.07340]\n",
      "Step 535084  [5.388 sec/step, loss=0.07347, avg_loss=0.07339]\n",
      "Step 535085  [5.388 sec/step, loss=0.07357, avg_loss=0.07339]\n",
      "Step 535086  [5.391 sec/step, loss=0.07095, avg_loss=0.07337]\n",
      "Step 535087  [5.383 sec/step, loss=0.07184, avg_loss=0.07333]\n",
      "Step 535088  [5.347 sec/step, loss=0.07121, avg_loss=0.07332]\n",
      "Step 535089  [5.387 sec/step, loss=0.06546, avg_loss=0.07323]\n",
      "Step 535090  [5.391 sec/step, loss=0.07478, avg_loss=0.07325]\n",
      "Step 535091  [5.401 sec/step, loss=0.07600, avg_loss=0.07325]\n",
      "Step 535092  [5.393 sec/step, loss=0.07343, avg_loss=0.07324]\n",
      "Step 535093  [5.384 sec/step, loss=0.07505, avg_loss=0.07323]\n",
      "Step 535094  [5.363 sec/step, loss=0.07330, avg_loss=0.07321]\n",
      "Generated 32 batches of size 32 in 2.405 sec\n",
      "Step 535095  [5.384 sec/step, loss=0.07553, avg_loss=0.07321]\n",
      "Step 535096  [5.383 sec/step, loss=0.07566, avg_loss=0.07321]\n",
      "Step 535097  [5.390 sec/step, loss=0.07472, avg_loss=0.07324]\n",
      "Step 535098  [5.358 sec/step, loss=0.07520, avg_loss=0.07327]\n",
      "Step 535099  [5.387 sec/step, loss=0.07478, avg_loss=0.07330]\n",
      "Step 535100  [5.378 sec/step, loss=0.07334, avg_loss=0.07329]\n",
      "Writing summary at step: 535100\n",
      "Step 535101  [5.397 sec/step, loss=0.07528, avg_loss=0.07339]\n",
      "Step 535102  [5.401 sec/step, loss=0.07599, avg_loss=0.07343]\n",
      "Step 535103  [5.403 sec/step, loss=0.07396, avg_loss=0.07345]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 535104  [5.408 sec/step, loss=0.07454, avg_loss=0.07347]\n",
      "Step 535105  [5.397 sec/step, loss=0.07387, avg_loss=0.07346]\n",
      "Step 535106  [5.393 sec/step, loss=0.07214, avg_loss=0.07343]\n",
      "Step 535107  [5.395 sec/step, loss=0.07490, avg_loss=0.07347]\n",
      "Step 535108  [5.389 sec/step, loss=0.07510, avg_loss=0.07347]\n",
      "Step 535109  [5.421 sec/step, loss=0.06523, avg_loss=0.07337]\n",
      "Step 535110  [5.427 sec/step, loss=0.07299, avg_loss=0.07339]\n",
      "Step 535111  [5.432 sec/step, loss=0.07324, avg_loss=0.07338]\n",
      "Step 535112  [5.436 sec/step, loss=0.07468, avg_loss=0.07339]\n",
      "Step 535113  [5.456 sec/step, loss=0.07519, avg_loss=0.07341]\n",
      "Step 535114  [5.461 sec/step, loss=0.07447, avg_loss=0.07340]\n",
      "Step 535115  [5.474 sec/step, loss=0.07569, avg_loss=0.07341]\n",
      "Step 535116  [5.491 sec/step, loss=0.07294, avg_loss=0.07338]\n",
      "Step 535117  [5.442 sec/step, loss=0.07377, avg_loss=0.07346]\n",
      "Step 535118  [5.455 sec/step, loss=0.07462, avg_loss=0.07349]\n",
      "Step 535119  [5.459 sec/step, loss=0.07599, avg_loss=0.07350]\n",
      "Step 535120  [5.443 sec/step, loss=0.07198, avg_loss=0.07349]\n",
      "Step 535121  [5.453 sec/step, loss=0.07508, avg_loss=0.07349]\n",
      "Step 535122  [5.441 sec/step, loss=0.07567, avg_loss=0.07352]\n",
      "Step 535123  [5.435 sec/step, loss=0.07069, avg_loss=0.07349]\n",
      "Step 535124  [5.418 sec/step, loss=0.07387, avg_loss=0.07347]\n",
      "Step 535125  [5.421 sec/step, loss=0.07562, avg_loss=0.07347]\n",
      "Generated 32 batches of size 32 in 2.500 sec\n",
      "Step 535126  [5.438 sec/step, loss=0.07380, avg_loss=0.07350]\n",
      "Step 535127  [5.438 sec/step, loss=0.07371, avg_loss=0.07353]\n",
      "Step 535128  [5.434 sec/step, loss=0.07467, avg_loss=0.07353]\n",
      "Step 535129  [5.445 sec/step, loss=0.07429, avg_loss=0.07354]\n",
      "Step 535130  [5.438 sec/step, loss=0.07529, avg_loss=0.07354]\n",
      "Step 535131  [5.427 sec/step, loss=0.07329, avg_loss=0.07355]\n",
      "Step 535132  [5.423 sec/step, loss=0.07168, avg_loss=0.07353]\n",
      "Step 535133  [5.415 sec/step, loss=0.07115, avg_loss=0.07353]\n",
      "Step 535134  [5.388 sec/step, loss=0.06593, avg_loss=0.07343]\n",
      "Step 535135  [5.395 sec/step, loss=0.07481, avg_loss=0.07346]\n",
      "Step 535136  [5.413 sec/step, loss=0.07565, avg_loss=0.07355]\n",
      "Step 535137  [5.405 sec/step, loss=0.07537, avg_loss=0.07355]\n",
      "Step 535138  [5.404 sec/step, loss=0.07170, avg_loss=0.07351]\n",
      "Step 535139  [5.422 sec/step, loss=0.07355, avg_loss=0.07352]\n",
      "Step 535140  [5.416 sec/step, loss=0.07155, avg_loss=0.07348]\n",
      "Step 535141  [5.432 sec/step, loss=0.07612, avg_loss=0.07352]\n",
      "Step 535142  [5.428 sec/step, loss=0.07134, avg_loss=0.07350]\n",
      "Step 535143  [5.429 sec/step, loss=0.07400, avg_loss=0.07351]\n",
      "Step 535144  [5.455 sec/step, loss=0.06564, avg_loss=0.07343]\n",
      "Step 535145  [5.454 sec/step, loss=0.07382, avg_loss=0.07341]\n",
      "Step 535146  [5.462 sec/step, loss=0.07434, avg_loss=0.07342]\n",
      "Step 535147  [5.475 sec/step, loss=0.07547, avg_loss=0.07345]\n",
      "Step 535148  [5.482 sec/step, loss=0.07367, avg_loss=0.07348]\n",
      "Step 535149  [5.475 sec/step, loss=0.07315, avg_loss=0.07346]\n",
      "Step 535150  [5.496 sec/step, loss=0.07519, avg_loss=0.07357]\n",
      "Step 535151  [5.488 sec/step, loss=0.07369, avg_loss=0.07356]\n",
      "Step 535152  [5.483 sec/step, loss=0.07550, avg_loss=0.07356]\n",
      "Step 535153  [5.438 sec/step, loss=0.07500, avg_loss=0.07365]\n",
      "Step 535154  [5.469 sec/step, loss=0.07506, avg_loss=0.07369]\n",
      "Step 535155  [5.451 sec/step, loss=0.07520, avg_loss=0.07369]\n",
      "Step 535156  [5.452 sec/step, loss=0.07453, avg_loss=0.07372]\n",
      "Step 535157  [5.447 sec/step, loss=0.07356, avg_loss=0.07370]\n",
      "Generated 32 batches of size 32 in 2.524 sec\n",
      "Step 535158  [5.440 sec/step, loss=0.07353, avg_loss=0.07370]\n",
      "Step 535159  [5.439 sec/step, loss=0.07435, avg_loss=0.07370]\n",
      "Step 535160  [5.459 sec/step, loss=0.07608, avg_loss=0.07374]\n",
      "Step 535161  [5.468 sec/step, loss=0.07589, avg_loss=0.07373]\n",
      "Step 535162  [5.454 sec/step, loss=0.07577, avg_loss=0.07375]\n",
      "Step 535163  [5.472 sec/step, loss=0.07278, avg_loss=0.07373]\n",
      "Step 535164  [5.455 sec/step, loss=0.06607, avg_loss=0.07365]\n",
      "Step 535165  [5.451 sec/step, loss=0.07174, avg_loss=0.07361]\n",
      "Step 535166  [5.426 sec/step, loss=0.07109, avg_loss=0.07358]\n",
      "Step 535167  [5.428 sec/step, loss=0.07412, avg_loss=0.07358]\n",
      "Step 535168  [5.420 sec/step, loss=0.07371, avg_loss=0.07357]\n",
      "Step 535169  [5.417 sec/step, loss=0.07593, avg_loss=0.07359]\n",
      "Step 535170  [5.429 sec/step, loss=0.07623, avg_loss=0.07361]\n",
      "Step 535171  [5.435 sec/step, loss=0.07431, avg_loss=0.07359]\n",
      "Step 535172  [5.437 sec/step, loss=0.07294, avg_loss=0.07358]\n",
      "Step 535173  [5.415 sec/step, loss=0.07135, avg_loss=0.07353]\n",
      "Step 535174  [5.400 sec/step, loss=0.07119, avg_loss=0.07350]\n",
      "Step 535175  [5.404 sec/step, loss=0.07570, avg_loss=0.07351]\n",
      "Step 535176  [5.409 sec/step, loss=0.07193, avg_loss=0.07353]\n",
      "Step 535177  [5.402 sec/step, loss=0.07493, avg_loss=0.07354]\n",
      "Step 535178  [5.403 sec/step, loss=0.07598, avg_loss=0.07354]\n",
      "Step 535179  [5.401 sec/step, loss=0.06554, avg_loss=0.07354]\n",
      "Step 535180  [5.415 sec/step, loss=0.07147, avg_loss=0.07353]\n",
      "Step 535181  [5.455 sec/step, loss=0.06692, avg_loss=0.07346]\n",
      "Step 535182  [5.452 sec/step, loss=0.07447, avg_loss=0.07346]\n",
      "Step 535183  [5.456 sec/step, loss=0.07497, avg_loss=0.07346]\n",
      "Step 535184  [5.461 sec/step, loss=0.07558, avg_loss=0.07349]\n",
      "Step 535185  [5.464 sec/step, loss=0.07496, avg_loss=0.07350]\n",
      "Step 535186  [5.476 sec/step, loss=0.07561, avg_loss=0.07355]\n",
      "Step 535187  [5.480 sec/step, loss=0.07408, avg_loss=0.07357]\n",
      "Step 535188  [5.502 sec/step, loss=0.07401, avg_loss=0.07360]\n",
      "Step 535189  [5.455 sec/step, loss=0.07206, avg_loss=0.07366]\n",
      "Generated 32 batches of size 32 in 2.413 sec\n",
      "Step 535190  [5.478 sec/step, loss=0.07384, avg_loss=0.07365]\n",
      "Step 535191  [5.477 sec/step, loss=0.07606, avg_loss=0.07365]\n",
      "Step 535192  [5.478 sec/step, loss=0.07337, avg_loss=0.07365]\n",
      "Step 535193  [5.486 sec/step, loss=0.07533, avg_loss=0.07366]\n",
      "Step 535194  [5.500 sec/step, loss=0.07270, avg_loss=0.07365]\n",
      "Step 535195  [5.478 sec/step, loss=0.07158, avg_loss=0.07361]\n",
      "Step 535196  [5.460 sec/step, loss=0.07374, avg_loss=0.07359]\n",
      "Step 535197  [5.456 sec/step, loss=0.07386, avg_loss=0.07358]\n",
      "Step 535198  [5.452 sec/step, loss=0.07305, avg_loss=0.07356]\n",
      "Step 535199  [5.445 sec/step, loss=0.07520, avg_loss=0.07357]\n",
      "Step 535200  [5.450 sec/step, loss=0.07463, avg_loss=0.07358]\n",
      "Writing summary at step: 535200\n",
      "Step 535201  [5.453 sec/step, loss=0.07442, avg_loss=0.07357]\n",
      "Step 535202  [5.452 sec/step, loss=0.07547, avg_loss=0.07356]\n",
      "Step 535203  [5.444 sec/step, loss=0.07351, avg_loss=0.07356]\n",
      "Step 535204  [5.441 sec/step, loss=0.07047, avg_loss=0.07352]\n",
      "Step 535205  [5.453 sec/step, loss=0.07472, avg_loss=0.07353]\n",
      "Step 535206  [5.453 sec/step, loss=0.07292, avg_loss=0.07354]\n",
      "Step 535207  [5.475 sec/step, loss=0.07465, avg_loss=0.07353]\n",
      "Step 535208  [5.470 sec/step, loss=0.07296, avg_loss=0.07351]\n",
      "Step 535209  [5.429 sec/step, loss=0.07530, avg_loss=0.07361]\n",
      "Step 535210  [5.438 sec/step, loss=0.07455, avg_loss=0.07363]\n",
      "Step 535211  [5.452 sec/step, loss=0.07484, avg_loss=0.07364]\n",
      "Step 535212  [5.458 sec/step, loss=0.07506, avg_loss=0.07365]\n",
      "Step 535213  [5.443 sec/step, loss=0.07244, avg_loss=0.07362]\n",
      "Step 535214  [5.419 sec/step, loss=0.07342, avg_loss=0.07361]\n",
      "Step 535215  [5.403 sec/step, loss=0.07374, avg_loss=0.07359]\n",
      "Step 535216  [5.380 sec/step, loss=0.07413, avg_loss=0.07360]\n",
      "Step 535217  [5.365 sec/step, loss=0.07131, avg_loss=0.07358]\n",
      "Step 535218  [5.350 sec/step, loss=0.07137, avg_loss=0.07355]\n",
      "Step 535219  [5.340 sec/step, loss=0.07461, avg_loss=0.07353]\n",
      "Step 535220  [5.367 sec/step, loss=0.07455, avg_loss=0.07356]\n",
      "Generated 32 batches of size 32 in 2.574 sec\n",
      "Step 535221  [5.357 sec/step, loss=0.07512, avg_loss=0.07356]\n",
      "Step 535222  [5.339 sec/step, loss=0.07305, avg_loss=0.07353]\n",
      "Step 535223  [5.330 sec/step, loss=0.07359, avg_loss=0.07356]\n",
      "Step 535224  [5.385 sec/step, loss=0.06423, avg_loss=0.07346]\n",
      "Step 535225  [5.400 sec/step, loss=0.07436, avg_loss=0.07345]\n",
      "Step 535226  [5.403 sec/step, loss=0.07531, avg_loss=0.07347]\n",
      "Step 535227  [5.390 sec/step, loss=0.06489, avg_loss=0.07338]\n",
      "Step 535228  [5.394 sec/step, loss=0.07492, avg_loss=0.07338]\n",
      "Step 535229  [5.403 sec/step, loss=0.07549, avg_loss=0.07339]\n",
      "Step 535230  [5.408 sec/step, loss=0.07644, avg_loss=0.07340]\n",
      "Step 535231  [5.410 sec/step, loss=0.07450, avg_loss=0.07342]\n",
      "Step 535232  [5.422 sec/step, loss=0.07571, avg_loss=0.07346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 535233  [5.433 sec/step, loss=0.07565, avg_loss=0.07350]\n",
      "Step 535234  [5.448 sec/step, loss=0.07511, avg_loss=0.07359]\n",
      "Step 535235  [5.442 sec/step, loss=0.07491, avg_loss=0.07359]\n",
      "Step 535236  [5.453 sec/step, loss=0.07417, avg_loss=0.07358]\n",
      "Step 535237  [5.442 sec/step, loss=0.07397, avg_loss=0.07357]\n",
      "Step 535238  [5.445 sec/step, loss=0.07408, avg_loss=0.07359]\n",
      "Step 535239  [5.442 sec/step, loss=0.07531, avg_loss=0.07361]\n",
      "Step 535240  [5.460 sec/step, loss=0.07302, avg_loss=0.07362]\n",
      "Step 535241  [5.449 sec/step, loss=0.07099, avg_loss=0.07357]\n",
      "Step 535242  [5.490 sec/step, loss=0.07354, avg_loss=0.07359]\n",
      "Step 535243  [5.475 sec/step, loss=0.06509, avg_loss=0.07350]\n",
      "Step 535244  [5.427 sec/step, loss=0.07429, avg_loss=0.07359]\n",
      "Step 535245  [5.412 sec/step, loss=0.07072, avg_loss=0.07356]\n",
      "Step 535246  [5.417 sec/step, loss=0.07443, avg_loss=0.07356]\n",
      "Step 535247  [5.414 sec/step, loss=0.07506, avg_loss=0.07356]\n",
      "Step 535248  [5.431 sec/step, loss=0.07406, avg_loss=0.07356]\n",
      "Step 535249  [5.438 sec/step, loss=0.07307, avg_loss=0.07356]\n",
      "Step 535250  [5.430 sec/step, loss=0.07448, avg_loss=0.07355]\n",
      "Step 535251  [5.437 sec/step, loss=0.07104, avg_loss=0.07353]\n",
      "Step 535252  [5.441 sec/step, loss=0.07574, avg_loss=0.07353]\n",
      "Generated 32 batches of size 32 in 2.490 sec\n",
      "Step 535253  [5.433 sec/step, loss=0.07250, avg_loss=0.07350]\n",
      "Step 535254  [5.426 sec/step, loss=0.07588, avg_loss=0.07351]\n",
      "Step 535255  [5.433 sec/step, loss=0.07448, avg_loss=0.07350]\n",
      "Step 535256  [5.431 sec/step, loss=0.07326, avg_loss=0.07349]\n",
      "Step 535257  [5.441 sec/step, loss=0.07569, avg_loss=0.07351]\n",
      "Step 535258  [5.432 sec/step, loss=0.06979, avg_loss=0.07348]\n",
      "Step 535259  [5.479 sec/step, loss=0.06655, avg_loss=0.07340]\n",
      "Step 535260  [5.458 sec/step, loss=0.07364, avg_loss=0.07337]\n",
      "Step 535261  [5.430 sec/step, loss=0.07058, avg_loss=0.07332]\n",
      "Step 535262  [5.423 sec/step, loss=0.07526, avg_loss=0.07331]\n",
      "Step 535263  [5.425 sec/step, loss=0.07248, avg_loss=0.07331]\n",
      "Step 535264  [5.452 sec/step, loss=0.07300, avg_loss=0.07338]\n",
      "Step 535265  [5.453 sec/step, loss=0.07625, avg_loss=0.07343]\n",
      "Step 535266  [5.451 sec/step, loss=0.07113, avg_loss=0.07343]\n",
      "Step 535267  [5.465 sec/step, loss=0.07495, avg_loss=0.07343]\n",
      "Step 535268  [5.481 sec/step, loss=0.07530, avg_loss=0.07345]\n",
      "Step 535269  [5.456 sec/step, loss=0.06554, avg_loss=0.07335]\n",
      "Step 535270  [5.443 sec/step, loss=0.07477, avg_loss=0.07333]\n",
      "Step 535271  [5.422 sec/step, loss=0.07219, avg_loss=0.07331]\n",
      "Step 535272  [5.386 sec/step, loss=0.07413, avg_loss=0.07332]\n",
      "Step 535273  [5.401 sec/step, loss=0.07452, avg_loss=0.07335]\n",
      "Step 535274  [5.410 sec/step, loss=0.07414, avg_loss=0.07338]\n",
      "Step 535275  [5.394 sec/step, loss=0.06980, avg_loss=0.07333]\n",
      "Step 535276  [5.407 sec/step, loss=0.07488, avg_loss=0.07335]\n",
      "Step 535277  [5.456 sec/step, loss=0.06590, avg_loss=0.07326]\n",
      "Step 535278  [5.447 sec/step, loss=0.07496, avg_loss=0.07325]\n",
      "Step 535279  [5.464 sec/step, loss=0.07352, avg_loss=0.07333]\n",
      "Step 535280  [5.460 sec/step, loss=0.07106, avg_loss=0.07333]\n",
      "Step 535281  [5.416 sec/step, loss=0.07496, avg_loss=0.07341]\n",
      "Step 535282  [5.426 sec/step, loss=0.07532, avg_loss=0.07342]\n",
      "Step 535283  [5.422 sec/step, loss=0.07539, avg_loss=0.07342]\n",
      "Step 535284  [5.429 sec/step, loss=0.07572, avg_loss=0.07342]\n",
      "Generated 32 batches of size 32 in 2.379 sec\n",
      "Step 535285  [5.443 sec/step, loss=0.07359, avg_loss=0.07341]\n",
      "Step 535286  [5.434 sec/step, loss=0.07473, avg_loss=0.07340]\n",
      "Step 535287  [5.426 sec/step, loss=0.07226, avg_loss=0.07338]\n",
      "Step 535288  [5.434 sec/step, loss=0.07539, avg_loss=0.07340]\n",
      "Step 535289  [5.444 sec/step, loss=0.07605, avg_loss=0.07344]\n",
      "Step 535290  [5.413 sec/step, loss=0.07352, avg_loss=0.07343]\n",
      "Step 535291  [5.414 sec/step, loss=0.07546, avg_loss=0.07343]\n",
      "Step 535292  [5.410 sec/step, loss=0.07357, avg_loss=0.07343]\n",
      "Step 535293  [5.403 sec/step, loss=0.07058, avg_loss=0.07338]\n",
      "Step 535294  [5.416 sec/step, loss=0.07281, avg_loss=0.07338]\n",
      "Step 535295  [5.407 sec/step, loss=0.07237, avg_loss=0.07339]\n",
      "Step 535296  [5.426 sec/step, loss=0.07597, avg_loss=0.07341]\n",
      "Step 535297  [5.430 sec/step, loss=0.07434, avg_loss=0.07342]\n",
      "Step 535298  [5.451 sec/step, loss=0.07568, avg_loss=0.07345]\n",
      "Step 535299  [5.440 sec/step, loss=0.07436, avg_loss=0.07344]\n",
      "Step 535300  [5.431 sec/step, loss=0.07354, avg_loss=0.07343]\n",
      "Writing summary at step: 535300\n",
      "Step 535301  [5.437 sec/step, loss=0.07577, avg_loss=0.07344]\n",
      "Step 535302  [5.422 sec/step, loss=0.07125, avg_loss=0.07340]\n",
      "Step 535303  [5.419 sec/step, loss=0.07356, avg_loss=0.07340]\n",
      "Step 535304  [5.426 sec/step, loss=0.07448, avg_loss=0.07344]\n",
      "Step 535305  [5.424 sec/step, loss=0.07393, avg_loss=0.07343]\n",
      "Step 535306  [5.433 sec/step, loss=0.07315, avg_loss=0.07343]\n",
      "Step 535307  [5.431 sec/step, loss=0.07562, avg_loss=0.07344]\n",
      "Step 535308  [5.436 sec/step, loss=0.07149, avg_loss=0.07343]\n",
      "Step 535309  [5.413 sec/step, loss=0.07206, avg_loss=0.07340]\n",
      "Step 535310  [5.461 sec/step, loss=0.06587, avg_loss=0.07331]\n",
      "Step 535311  [5.444 sec/step, loss=0.07452, avg_loss=0.07330]\n",
      "Step 535312  [5.434 sec/step, loss=0.07505, avg_loss=0.07330]\n",
      "Step 535313  [5.418 sec/step, loss=0.06724, avg_loss=0.07325]\n",
      "Step 535314  [5.423 sec/step, loss=0.07471, avg_loss=0.07327]\n",
      "Step 535315  [5.457 sec/step, loss=0.07285, avg_loss=0.07326]\n",
      "Generated 32 batches of size 32 in 2.654 sec\n",
      "Step 535316  [5.455 sec/step, loss=0.07265, avg_loss=0.07324]\n",
      "Step 535317  [5.474 sec/step, loss=0.07270, avg_loss=0.07326]\n",
      "Step 535318  [5.483 sec/step, loss=0.07458, avg_loss=0.07329]\n",
      "Step 535319  [5.496 sec/step, loss=0.07336, avg_loss=0.07328]\n",
      "Step 535320  [5.491 sec/step, loss=0.07590, avg_loss=0.07329]\n",
      "Step 535321  [5.495 sec/step, loss=0.07431, avg_loss=0.07328]\n",
      "Step 535322  [5.492 sec/step, loss=0.07329, avg_loss=0.07328]\n",
      "Step 535323  [5.513 sec/step, loss=0.07495, avg_loss=0.07330]\n",
      "Step 535324  [5.459 sec/step, loss=0.07362, avg_loss=0.07339]\n",
      "Step 535325  [5.439 sec/step, loss=0.07369, avg_loss=0.07338]\n",
      "Step 535326  [5.429 sec/step, loss=0.07422, avg_loss=0.07337]\n",
      "Step 535327  [5.442 sec/step, loss=0.07169, avg_loss=0.07344]\n",
      "Step 535328  [5.451 sec/step, loss=0.07582, avg_loss=0.07345]\n",
      "Step 535329  [5.432 sec/step, loss=0.07359, avg_loss=0.07343]\n",
      "Step 535330  [5.413 sec/step, loss=0.07153, avg_loss=0.07338]\n",
      "Step 535331  [5.411 sec/step, loss=0.07417, avg_loss=0.07338]\n",
      "Step 535332  [5.387 sec/step, loss=0.06567, avg_loss=0.07328]\n",
      "Step 535333  [5.401 sec/step, loss=0.07507, avg_loss=0.07327]\n",
      "Step 535334  [5.454 sec/step, loss=0.06677, avg_loss=0.07319]\n",
      "Step 535335  [5.464 sec/step, loss=0.07510, avg_loss=0.07319]\n",
      "Step 535336  [5.457 sec/step, loss=0.07479, avg_loss=0.07320]\n",
      "Step 535337  [5.472 sec/step, loss=0.07409, avg_loss=0.07320]\n",
      "Step 535338  [5.474 sec/step, loss=0.07448, avg_loss=0.07320]\n",
      "Step 535339  [5.455 sec/step, loss=0.07067, avg_loss=0.07316]\n",
      "Step 535340  [5.462 sec/step, loss=0.07255, avg_loss=0.07315]\n",
      "Step 535341  [5.463 sec/step, loss=0.07513, avg_loss=0.07319]\n",
      "Step 535342  [5.448 sec/step, loss=0.07520, avg_loss=0.07321]\n",
      "Step 535343  [5.483 sec/step, loss=0.07435, avg_loss=0.07330]\n",
      "Step 535344  [5.479 sec/step, loss=0.07462, avg_loss=0.07331]\n",
      "Step 535345  [5.484 sec/step, loss=0.07534, avg_loss=0.07335]\n",
      "Step 535346  [5.465 sec/step, loss=0.07157, avg_loss=0.07332]\n",
      "Step 535347  [5.468 sec/step, loss=0.07434, avg_loss=0.07332]\n",
      "Generated 32 batches of size 32 in 2.396 sec\n",
      "Step 535348  [5.468 sec/step, loss=0.07596, avg_loss=0.07333]\n",
      "Step 535349  [5.478 sec/step, loss=0.07583, avg_loss=0.07336]\n",
      "Step 535350  [5.474 sec/step, loss=0.07407, avg_loss=0.07336]\n",
      "Step 535351  [5.475 sec/step, loss=0.07490, avg_loss=0.07340]\n",
      "Step 535352  [5.461 sec/step, loss=0.07284, avg_loss=0.07337]\n",
      "Step 535353  [5.458 sec/step, loss=0.07255, avg_loss=0.07337]\n",
      "Step 535354  [5.454 sec/step, loss=0.07367, avg_loss=0.07335]\n",
      "Step 535355  [5.460 sec/step, loss=0.07367, avg_loss=0.07334]\n",
      "Step 535356  [5.460 sec/step, loss=0.07310, avg_loss=0.07334]\n",
      "Step 535357  [5.452 sec/step, loss=0.07454, avg_loss=0.07332]\n",
      "Step 535358  [5.476 sec/step, loss=0.07358, avg_loss=0.07336]\n",
      "Step 535359  [5.422 sec/step, loss=0.07523, avg_loss=0.07345]\n",
      "Step 535360  [5.444 sec/step, loss=0.07600, avg_loss=0.07347]\n",
      "Step 535361  [5.509 sec/step, loss=0.06497, avg_loss=0.07342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 535362  [5.496 sec/step, loss=0.07352, avg_loss=0.07340]\n",
      "Step 535363  [5.465 sec/step, loss=0.07316, avg_loss=0.07341]\n",
      "Step 535364  [5.457 sec/step, loss=0.07458, avg_loss=0.07342]\n",
      "Step 535365  [5.465 sec/step, loss=0.07541, avg_loss=0.07341]\n",
      "Step 535366  [5.472 sec/step, loss=0.07330, avg_loss=0.07344]\n",
      "Step 535367  [5.447 sec/step, loss=0.07137, avg_loss=0.07340]\n",
      "Step 535368  [5.439 sec/step, loss=0.07366, avg_loss=0.07338]\n",
      "Step 535369  [5.448 sec/step, loss=0.07358, avg_loss=0.07346]\n",
      "Step 535370  [5.458 sec/step, loss=0.07498, avg_loss=0.07347]\n",
      "Step 535371  [5.466 sec/step, loss=0.07588, avg_loss=0.07350]\n",
      "Step 535372  [5.474 sec/step, loss=0.07070, avg_loss=0.07347]\n",
      "Step 535373  [5.469 sec/step, loss=0.07304, avg_loss=0.07345]\n",
      "Step 535374  [5.490 sec/step, loss=0.07339, avg_loss=0.07345]\n",
      "Step 535375  [5.504 sec/step, loss=0.07465, avg_loss=0.07349]\n",
      "Step 535376  [5.494 sec/step, loss=0.07398, avg_loss=0.07349]\n",
      "Step 535377  [5.427 sec/step, loss=0.06537, avg_loss=0.07348]\n",
      "Step 535378  [5.430 sec/step, loss=0.07405, avg_loss=0.07347]\n",
      "Step 535379  [5.432 sec/step, loss=0.07429, avg_loss=0.07348]\n",
      "Generated 32 batches of size 32 in 2.418 sec\n",
      "Step 535380  [5.441 sec/step, loss=0.07421, avg_loss=0.07351]\n",
      "Step 535381  [5.441 sec/step, loss=0.07497, avg_loss=0.07351]\n",
      "Step 535382  [5.444 sec/step, loss=0.07285, avg_loss=0.07349]\n",
      "Step 535383  [5.450 sec/step, loss=0.07465, avg_loss=0.07348]\n",
      "Step 535384  [5.449 sec/step, loss=0.07599, avg_loss=0.07348]\n",
      "Step 535385  [5.466 sec/step, loss=0.07275, avg_loss=0.07347]\n",
      "Step 535386  [5.477 sec/step, loss=0.07537, avg_loss=0.07348]\n",
      "Step 535387  [5.486 sec/step, loss=0.07415, avg_loss=0.07350]\n",
      "Step 535388  [5.454 sec/step, loss=0.07069, avg_loss=0.07345]\n",
      "Step 535389  [5.436 sec/step, loss=0.07336, avg_loss=0.07342]\n",
      "Step 535390  [5.447 sec/step, loss=0.07560, avg_loss=0.07344]\n",
      "Step 535391  [5.435 sec/step, loss=0.07337, avg_loss=0.07342]\n",
      "Step 535392  [5.442 sec/step, loss=0.07380, avg_loss=0.07343]\n",
      "Step 535393  [5.441 sec/step, loss=0.07480, avg_loss=0.07347]\n",
      "Step 535394  [5.426 sec/step, loss=0.07487, avg_loss=0.07349]\n",
      "Step 535395  [5.443 sec/step, loss=0.07564, avg_loss=0.07352]\n",
      "Step 535396  [5.431 sec/step, loss=0.07464, avg_loss=0.07351]\n",
      "Step 535397  [5.419 sec/step, loss=0.07237, avg_loss=0.07349]\n",
      "Step 535398  [5.408 sec/step, loss=0.07419, avg_loss=0.07347]\n",
      "Step 535399  [5.419 sec/step, loss=0.07625, avg_loss=0.07349]\n",
      "Step 535400  [5.475 sec/step, loss=0.06727, avg_loss=0.07343]\n",
      "Writing summary at step: 535400\n",
      "Step 535401  [5.464 sec/step, loss=0.07356, avg_loss=0.07341]\n",
      "Step 535402  [5.483 sec/step, loss=0.07541, avg_loss=0.07345]\n",
      "Step 535403  [5.477 sec/step, loss=0.07010, avg_loss=0.07341]\n",
      "Step 535404  [5.493 sec/step, loss=0.07269, avg_loss=0.07340]\n",
      "Step 535405  [5.520 sec/step, loss=0.07289, avg_loss=0.07339]\n",
      "Step 535406  [5.509 sec/step, loss=0.07519, avg_loss=0.07341]\n",
      "Step 535407  [5.493 sec/step, loss=0.07091, avg_loss=0.07336]\n",
      "Step 535408  [5.485 sec/step, loss=0.07385, avg_loss=0.07338]\n",
      "Step 535409  [5.509 sec/step, loss=0.07512, avg_loss=0.07341]\n",
      "Step 535410  [5.461 sec/step, loss=0.07470, avg_loss=0.07350]\n",
      "Generated 32 batches of size 32 in 2.441 sec\n",
      "Step 535411  [5.469 sec/step, loss=0.07436, avg_loss=0.07350]\n",
      "Step 535412  [5.480 sec/step, loss=0.07513, avg_loss=0.07350]\n",
      "Step 535413  [5.479 sec/step, loss=0.06620, avg_loss=0.07349]\n",
      "Step 535414  [5.478 sec/step, loss=0.07192, avg_loss=0.07346]\n",
      "Step 535415  [5.449 sec/step, loss=0.07517, avg_loss=0.07349]\n",
      "Step 535416  [5.433 sec/step, loss=0.07143, avg_loss=0.07347]\n",
      "Step 535417  [5.417 sec/step, loss=0.07329, avg_loss=0.07348]\n",
      "Step 535418  [5.435 sec/step, loss=0.07517, avg_loss=0.07349]\n",
      "Step 535419  [5.431 sec/step, loss=0.07429, avg_loss=0.07350]\n",
      "Step 535420  [5.420 sec/step, loss=0.07435, avg_loss=0.07348]\n",
      "Step 535421  [5.416 sec/step, loss=0.07202, avg_loss=0.07346]\n",
      "Step 535422  [5.427 sec/step, loss=0.07453, avg_loss=0.07347]\n",
      "Step 535423  [5.409 sec/step, loss=0.07314, avg_loss=0.07345]\n",
      "Step 535424  [5.431 sec/step, loss=0.07560, avg_loss=0.07347]\n",
      "Step 535425  [5.423 sec/step, loss=0.07313, avg_loss=0.07347]\n",
      "Step 535426  [5.432 sec/step, loss=0.07502, avg_loss=0.07347]\n",
      "Step 535427  [5.436 sec/step, loss=0.07498, avg_loss=0.07351]\n",
      "Step 535428  [5.429 sec/step, loss=0.07398, avg_loss=0.07349]\n",
      "Step 535429  [5.454 sec/step, loss=0.07541, avg_loss=0.07351]\n",
      "Step 535430  [5.451 sec/step, loss=0.07024, avg_loss=0.07349]\n",
      "Step 535431  [5.454 sec/step, loss=0.07424, avg_loss=0.07349]\n",
      "Step 535432  [5.463 sec/step, loss=0.07327, avg_loss=0.07357]\n",
      "Step 535433  [5.444 sec/step, loss=0.07319, avg_loss=0.07355]\n",
      "Step 535434  [5.377 sec/step, loss=0.06522, avg_loss=0.07354]\n",
      "Step 535435  [5.363 sec/step, loss=0.07401, avg_loss=0.07352]\n",
      "Step 535436  [5.371 sec/step, loss=0.07412, avg_loss=0.07352]\n",
      "Step 535437  [5.355 sec/step, loss=0.07200, avg_loss=0.07350]\n",
      "Step 535438  [5.359 sec/step, loss=0.07489, avg_loss=0.07350]\n",
      "Step 535439  [5.377 sec/step, loss=0.07577, avg_loss=0.07355]\n",
      "Step 535440  [5.347 sec/step, loss=0.07466, avg_loss=0.07357]\n",
      "Step 535441  [5.351 sec/step, loss=0.07546, avg_loss=0.07358]\n",
      "Step 535442  [5.382 sec/step, loss=0.06829, avg_loss=0.07351]\n",
      "Generated 32 batches of size 32 in 2.465 sec\n",
      "Step 535443  [5.370 sec/step, loss=0.07488, avg_loss=0.07351]\n",
      "Step 535444  [5.381 sec/step, loss=0.07563, avg_loss=0.07352]\n",
      "Step 535445  [5.377 sec/step, loss=0.07216, avg_loss=0.07349]\n",
      "Step 535446  [5.377 sec/step, loss=0.07195, avg_loss=0.07350]\n",
      "Step 535447  [5.368 sec/step, loss=0.07537, avg_loss=0.07351]\n",
      "Step 535448  [5.381 sec/step, loss=0.07215, avg_loss=0.07347]\n",
      "Step 535449  [5.382 sec/step, loss=0.07410, avg_loss=0.07345]\n",
      "Step 535450  [5.401 sec/step, loss=0.07555, avg_loss=0.07346]\n",
      "Step 535451  [5.418 sec/step, loss=0.07568, avg_loss=0.07347]\n",
      "Step 535452  [5.407 sec/step, loss=0.07357, avg_loss=0.07348]\n",
      "Step 535453  [5.420 sec/step, loss=0.07582, avg_loss=0.07351]\n",
      "Step 535454  [5.414 sec/step, loss=0.07364, avg_loss=0.07351]\n",
      "Step 535455  [5.414 sec/step, loss=0.07562, avg_loss=0.07353]\n",
      "Step 535456  [5.405 sec/step, loss=0.07063, avg_loss=0.07351]\n",
      "Step 535457  [5.405 sec/step, loss=0.07265, avg_loss=0.07349]\n",
      "Step 535458  [5.390 sec/step, loss=0.07437, avg_loss=0.07350]\n",
      "Step 535459  [5.401 sec/step, loss=0.07609, avg_loss=0.07350]\n",
      "Step 535460  [5.404 sec/step, loss=0.07457, avg_loss=0.07349]\n",
      "Step 535461  [5.362 sec/step, loss=0.07315, avg_loss=0.07357]\n",
      "Step 535462  [5.373 sec/step, loss=0.07238, avg_loss=0.07356]\n",
      "Step 535463  [5.390 sec/step, loss=0.07472, avg_loss=0.07358]\n",
      "Step 535464  [5.392 sec/step, loss=0.07405, avg_loss=0.07357]\n",
      "Step 535465  [5.428 sec/step, loss=0.06650, avg_loss=0.07348]\n",
      "Step 535466  [5.449 sec/step, loss=0.07490, avg_loss=0.07350]\n",
      "Step 535467  [5.455 sec/step, loss=0.07316, avg_loss=0.07352]\n",
      "Step 535468  [5.481 sec/step, loss=0.07277, avg_loss=0.07351]\n",
      "Step 535469  [5.498 sec/step, loss=0.07567, avg_loss=0.07353]\n",
      "Step 535470  [5.472 sec/step, loss=0.07180, avg_loss=0.07350]\n",
      "Step 535471  [5.469 sec/step, loss=0.07008, avg_loss=0.07344]\n",
      "Step 535472  [5.469 sec/step, loss=0.07491, avg_loss=0.07348]\n",
      "Step 535473  [5.466 sec/step, loss=0.07350, avg_loss=0.07348]\n",
      "Step 535474  [5.448 sec/step, loss=0.07495, avg_loss=0.07350]\n",
      "Generated 32 batches of size 32 in 2.407 sec\n",
      "Step 535475  [5.453 sec/step, loss=0.07498, avg_loss=0.07350]\n",
      "Step 535476  [5.456 sec/step, loss=0.07467, avg_loss=0.07351]\n",
      "Step 535477  [5.456 sec/step, loss=0.06569, avg_loss=0.07351]\n",
      "Step 535478  [5.453 sec/step, loss=0.07383, avg_loss=0.07351]\n",
      "Step 535479  [5.450 sec/step, loss=0.07467, avg_loss=0.07352]\n",
      "Step 535480  [5.454 sec/step, loss=0.07350, avg_loss=0.07351]\n",
      "Step 535481  [5.461 sec/step, loss=0.07561, avg_loss=0.07351]\n",
      "Step 535482  [5.444 sec/step, loss=0.07336, avg_loss=0.07352]\n",
      "Step 535483  [5.440 sec/step, loss=0.07458, avg_loss=0.07352]\n",
      "Step 535484  [5.428 sec/step, loss=0.07537, avg_loss=0.07351]\n",
      "Step 535485  [5.403 sec/step, loss=0.07479, avg_loss=0.07353]\n",
      "Step 535486  [5.379 sec/step, loss=0.06495, avg_loss=0.07343]\n",
      "Step 535487  [5.395 sec/step, loss=0.07512, avg_loss=0.07344]\n",
      "Step 535488  [5.399 sec/step, loss=0.07384, avg_loss=0.07347]\n",
      "Step 535489  [5.400 sec/step, loss=0.07042, avg_loss=0.07344]\n",
      "Step 535490  [5.400 sec/step, loss=0.07410, avg_loss=0.07343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 535491  [5.397 sec/step, loss=0.07480, avg_loss=0.07344]\n",
      "Step 535492  [5.412 sec/step, loss=0.07541, avg_loss=0.07346]\n",
      "Step 535493  [5.400 sec/step, loss=0.07161, avg_loss=0.07342]\n",
      "Step 535494  [5.402 sec/step, loss=0.07557, avg_loss=0.07343]\n",
      "Step 535495  [5.407 sec/step, loss=0.07590, avg_loss=0.07343]\n",
      "Step 535496  [5.421 sec/step, loss=0.07497, avg_loss=0.07344]\n",
      "Step 535497  [5.439 sec/step, loss=0.07418, avg_loss=0.07346]\n",
      "Step 535498  [5.437 sec/step, loss=0.07436, avg_loss=0.07346]\n",
      "Step 535499  [5.412 sec/step, loss=0.07082, avg_loss=0.07340]\n",
      "Step 535500  [5.362 sec/step, loss=0.07427, avg_loss=0.07347]\n",
      "Writing summary at step: 535500\n",
      "Step 535501  [5.357 sec/step, loss=0.07363, avg_loss=0.07347]\n",
      "Step 535502  [5.358 sec/step, loss=0.07588, avg_loss=0.07348]\n",
      "Step 535503  [5.368 sec/step, loss=0.07316, avg_loss=0.07351]\n",
      "Step 535504  [5.353 sec/step, loss=0.07515, avg_loss=0.07353]\n",
      "Step 535505  [5.319 sec/step, loss=0.07372, avg_loss=0.07354]\n",
      "Generated 32 batches of size 32 in 2.370 sec\n",
      "Step 535506  [5.329 sec/step, loss=0.07599, avg_loss=0.07355]\n",
      "Step 535507  [5.332 sec/step, loss=0.07513, avg_loss=0.07359]\n",
      "Step 535508  [5.381 sec/step, loss=0.06772, avg_loss=0.07353]\n",
      "Step 535509  [5.366 sec/step, loss=0.07188, avg_loss=0.07350]\n",
      "Step 535510  [5.390 sec/step, loss=0.07400, avg_loss=0.07349]\n",
      "Step 535511  [5.385 sec/step, loss=0.07390, avg_loss=0.07349]\n",
      "Step 535512  [5.366 sec/step, loss=0.07338, avg_loss=0.07347]\n",
      "Step 535513  [5.388 sec/step, loss=0.07452, avg_loss=0.07355]\n",
      "Step 535514  [5.398 sec/step, loss=0.07481, avg_loss=0.07358]\n",
      "Step 535515  [5.415 sec/step, loss=0.07588, avg_loss=0.07359]\n",
      "Step 535516  [5.430 sec/step, loss=0.07135, avg_loss=0.07359]\n",
      "Step 535517  [5.442 sec/step, loss=0.07535, avg_loss=0.07361]\n",
      "Step 535518  [5.437 sec/step, loss=0.07562, avg_loss=0.07361]\n",
      "Step 535519  [5.419 sec/step, loss=0.07347, avg_loss=0.07360]\n",
      "Step 535520  [5.413 sec/step, loss=0.07285, avg_loss=0.07359]\n",
      "Step 535521  [5.423 sec/step, loss=0.07322, avg_loss=0.07360]\n",
      "Step 535522  [5.414 sec/step, loss=0.07322, avg_loss=0.07359]\n",
      "Step 535523  [5.419 sec/step, loss=0.07349, avg_loss=0.07359]\n",
      "Step 535524  [5.402 sec/step, loss=0.07526, avg_loss=0.07359]\n",
      "Step 535525  [5.414 sec/step, loss=0.07600, avg_loss=0.07362]\n",
      "Step 535526  [5.403 sec/step, loss=0.07484, avg_loss=0.07361]\n",
      "Step 535527  [5.405 sec/step, loss=0.07422, avg_loss=0.07361]\n",
      "Step 535528  [5.450 sec/step, loss=0.06668, avg_loss=0.07353]\n",
      "Step 535529  [5.445 sec/step, loss=0.07432, avg_loss=0.07352]\n",
      "Step 535530  [5.457 sec/step, loss=0.07364, avg_loss=0.07356]\n",
      "Step 535531  [5.483 sec/step, loss=0.07338, avg_loss=0.07355]\n",
      "Step 535532  [5.504 sec/step, loss=0.07370, avg_loss=0.07355]\n",
      "Step 535533  [5.506 sec/step, loss=0.07276, avg_loss=0.07355]\n",
      "Step 535534  [5.521 sec/step, loss=0.07492, avg_loss=0.07365]\n",
      "Step 535535  [5.504 sec/step, loss=0.06430, avg_loss=0.07355]\n",
      "Step 535536  [5.500 sec/step, loss=0.07526, avg_loss=0.07356]\n",
      "Step 535537  [5.500 sec/step, loss=0.06995, avg_loss=0.07354]\n",
      "Generated 32 batches of size 32 in 2.442 sec\n",
      "Step 535538  [5.500 sec/step, loss=0.07404, avg_loss=0.07353]\n",
      "Step 535539  [5.504 sec/step, loss=0.07568, avg_loss=0.07353]\n",
      "Step 535540  [5.500 sec/step, loss=0.07337, avg_loss=0.07352]\n",
      "Step 535541  [5.500 sec/step, loss=0.07425, avg_loss=0.07351]\n",
      "Step 535542  [5.464 sec/step, loss=0.07531, avg_loss=0.07358]\n",
      "Step 535543  [5.444 sec/step, loss=0.07062, avg_loss=0.07353]\n",
      "Step 535544  [5.435 sec/step, loss=0.07475, avg_loss=0.07352]\n",
      "Step 535545  [5.434 sec/step, loss=0.07328, avg_loss=0.07354]\n",
      "Step 535546  [5.468 sec/step, loss=0.07391, avg_loss=0.07355]\n",
      "Step 535547  [5.483 sec/step, loss=0.07405, avg_loss=0.07354]\n",
      "Step 535548  [5.443 sec/step, loss=0.07041, avg_loss=0.07352]\n",
      "Step 535549  [5.446 sec/step, loss=0.07572, avg_loss=0.07354]\n",
      "Step 535550  [5.436 sec/step, loss=0.07427, avg_loss=0.07353]\n",
      "Step 535551  [5.423 sec/step, loss=0.07148, avg_loss=0.07349]\n",
      "Step 535552  [5.432 sec/step, loss=0.07500, avg_loss=0.07350]\n",
      "Step 535553  [5.437 sec/step, loss=0.07594, avg_loss=0.07350]\n",
      "Step 535554  [5.440 sec/step, loss=0.07514, avg_loss=0.07352]\n",
      "Step 535555  [5.428 sec/step, loss=0.07286, avg_loss=0.07349]\n",
      "Step 535556  [5.441 sec/step, loss=0.07486, avg_loss=0.07353]\n",
      "Step 535557  [5.450 sec/step, loss=0.07469, avg_loss=0.07355]\n",
      "Step 535558  [5.448 sec/step, loss=0.07323, avg_loss=0.07354]\n",
      "Step 535559  [5.451 sec/step, loss=0.07294, avg_loss=0.07351]\n",
      "Step 535560  [5.428 sec/step, loss=0.07129, avg_loss=0.07348]\n",
      "Step 535561  [5.411 sec/step, loss=0.07366, avg_loss=0.07348]\n",
      "Step 535562  [5.436 sec/step, loss=0.07411, avg_loss=0.07350]\n",
      "Step 535563  [5.474 sec/step, loss=0.06495, avg_loss=0.07340]\n",
      "Step 535564  [5.462 sec/step, loss=0.07283, avg_loss=0.07339]\n",
      "Step 535565  [5.420 sec/step, loss=0.07500, avg_loss=0.07347]\n",
      "Step 535566  [5.413 sec/step, loss=0.07547, avg_loss=0.07348]\n",
      "Step 535567  [5.422 sec/step, loss=0.07500, avg_loss=0.07350]\n",
      "Step 535568  [5.379 sec/step, loss=0.06647, avg_loss=0.07343]\n",
      "Step 535569  [5.376 sec/step, loss=0.07444, avg_loss=0.07342]\n",
      "Generated 32 batches of size 32 in 2.463 sec\n",
      "Step 535570  [5.395 sec/step, loss=0.07460, avg_loss=0.07345]\n",
      "Step 535571  [5.386 sec/step, loss=0.07381, avg_loss=0.07349]\n",
      "Step 535572  [5.406 sec/step, loss=0.07402, avg_loss=0.07348]\n",
      "Step 535573  [5.425 sec/step, loss=0.07577, avg_loss=0.07350]\n",
      "Step 535574  [5.421 sec/step, loss=0.07482, avg_loss=0.07350]\n",
      "Step 535575  [5.421 sec/step, loss=0.07492, avg_loss=0.07350]\n",
      "Step 535576  [5.408 sec/step, loss=0.07059, avg_loss=0.07346]\n",
      "Step 535577  [5.433 sec/step, loss=0.07445, avg_loss=0.07355]\n",
      "Step 535578  [5.427 sec/step, loss=0.07181, avg_loss=0.07353]\n",
      "Step 535579  [5.426 sec/step, loss=0.07482, avg_loss=0.07353]\n",
      "Step 535580  [5.409 sec/step, loss=0.07318, avg_loss=0.07352]\n",
      "Step 535581  [5.406 sec/step, loss=0.07546, avg_loss=0.07352]\n",
      "Step 535582  [5.418 sec/step, loss=0.07431, avg_loss=0.07353]\n",
      "Step 535583  [5.425 sec/step, loss=0.07503, avg_loss=0.07354]\n",
      "Step 535584  [5.439 sec/step, loss=0.07558, avg_loss=0.07354]\n",
      "Step 535585  [5.428 sec/step, loss=0.06974, avg_loss=0.07349]\n",
      "Step 535586  [5.444 sec/step, loss=0.07431, avg_loss=0.07358]\n",
      "Step 535587  [5.414 sec/step, loss=0.07393, avg_loss=0.07357]\n",
      "Step 535588  [5.433 sec/step, loss=0.07454, avg_loss=0.07358]\n",
      "Step 535589  [5.434 sec/step, loss=0.07377, avg_loss=0.07361]\n",
      "Step 535590  [5.425 sec/step, loss=0.07325, avg_loss=0.07360]\n",
      "Step 535591  [5.423 sec/step, loss=0.07165, avg_loss=0.07357]\n",
      "Step 535592  [5.404 sec/step, loss=0.07344, avg_loss=0.07355]\n",
      "Step 535593  [5.400 sec/step, loss=0.07218, avg_loss=0.07356]\n",
      "Step 535594  [5.401 sec/step, loss=0.07573, avg_loss=0.07356]\n",
      "Step 535595  [5.415 sec/step, loss=0.07346, avg_loss=0.07353]\n",
      "Step 535596  [5.453 sec/step, loss=0.06679, avg_loss=0.07345]\n",
      "Step 535597  [5.445 sec/step, loss=0.07557, avg_loss=0.07347]\n",
      "Step 535598  [5.456 sec/step, loss=0.07628, avg_loss=0.07348]\n",
      "Step 535599  [5.490 sec/step, loss=0.07370, avg_loss=0.07351]\n",
      "Step 535600  [5.502 sec/step, loss=0.07386, avg_loss=0.07351]\n",
      "Writing summary at step: 535600\n",
      "Generated 32 batches of size 32 in 2.417 sec\n",
      "Step 535601  [5.524 sec/step, loss=0.07367, avg_loss=0.07351]\n",
      "Step 535602  [5.515 sec/step, loss=0.07453, avg_loss=0.07350]\n",
      "Step 535603  [5.529 sec/step, loss=0.07585, avg_loss=0.07352]\n",
      "Step 535604  [5.513 sec/step, loss=0.07159, avg_loss=0.07349]\n",
      "Step 535605  [5.519 sec/step, loss=0.07475, avg_loss=0.07350]\n",
      "Step 535606  [5.508 sec/step, loss=0.07532, avg_loss=0.07349]\n",
      "Step 535607  [5.512 sec/step, loss=0.07306, avg_loss=0.07347]\n",
      "Step 535608  [5.467 sec/step, loss=0.07052, avg_loss=0.07350]\n",
      "Step 535609  [5.474 sec/step, loss=0.07191, avg_loss=0.07350]\n",
      "Step 535610  [5.456 sec/step, loss=0.07569, avg_loss=0.07352]\n",
      "Step 535611  [5.463 sec/step, loss=0.07602, avg_loss=0.07354]\n",
      "Step 535612  [5.469 sec/step, loss=0.07375, avg_loss=0.07354]\n",
      "Step 535613  [5.460 sec/step, loss=0.07334, avg_loss=0.07353]\n",
      "Step 535614  [5.465 sec/step, loss=0.07547, avg_loss=0.07353]\n",
      "Step 535615  [5.459 sec/step, loss=0.07571, avg_loss=0.07353]\n",
      "Step 535616  [5.460 sec/step, loss=0.07555, avg_loss=0.07358]\n",
      "Step 535617  [5.459 sec/step, loss=0.07468, avg_loss=0.07357]\n",
      "Step 535618  [5.463 sec/step, loss=0.07579, avg_loss=0.07357]\n",
      "Step 535619  [5.491 sec/step, loss=0.07483, avg_loss=0.07358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 535620  [5.506 sec/step, loss=0.07441, avg_loss=0.07360]\n",
      "Step 535621  [5.509 sec/step, loss=0.07536, avg_loss=0.07362]\n",
      "Step 535622  [5.514 sec/step, loss=0.07424, avg_loss=0.07363]\n",
      "Step 535623  [5.505 sec/step, loss=0.07132, avg_loss=0.07361]\n",
      "Step 535624  [5.490 sec/step, loss=0.07136, avg_loss=0.07357]\n",
      "Step 535625  [5.479 sec/step, loss=0.07203, avg_loss=0.07353]\n",
      "Step 535626  [5.480 sec/step, loss=0.07497, avg_loss=0.07353]\n",
      "Step 535627  [5.504 sec/step, loss=0.07293, avg_loss=0.07352]\n",
      "Step 535628  [5.463 sec/step, loss=0.07632, avg_loss=0.07362]\n",
      "Step 535629  [5.459 sec/step, loss=0.07537, avg_loss=0.07363]\n",
      "Step 535630  [5.457 sec/step, loss=0.07446, avg_loss=0.07363]\n",
      "Step 535631  [5.431 sec/step, loss=0.07341, avg_loss=0.07363]\n",
      "Step 535632  [5.410 sec/step, loss=0.07331, avg_loss=0.07363]\n",
      "Generated 32 batches of size 32 in 2.905 sec\n",
      "Step 535633  [5.401 sec/step, loss=0.06565, avg_loss=0.07356]\n",
      "Step 535634  [5.400 sec/step, loss=0.07481, avg_loss=0.07356]\n",
      "Step 535635  [5.411 sec/step, loss=0.07386, avg_loss=0.07365]\n",
      "Step 535636  [5.411 sec/step, loss=0.07345, avg_loss=0.07364]\n",
      "Step 535637  [5.418 sec/step, loss=0.07267, avg_loss=0.07366]\n",
      "Step 535638  [5.415 sec/step, loss=0.07493, avg_loss=0.07367]\n",
      "Step 535639  [5.406 sec/step, loss=0.07370, avg_loss=0.07365]\n",
      "Step 535640  [5.402 sec/step, loss=0.07412, avg_loss=0.07366]\n",
      "Step 535641  [5.446 sec/step, loss=0.06611, avg_loss=0.07358]\n",
      "Step 535642  [5.451 sec/step, loss=0.07574, avg_loss=0.07358]\n",
      "Step 535643  [5.478 sec/step, loss=0.07604, avg_loss=0.07364]\n",
      "Step 535644  [5.483 sec/step, loss=0.07532, avg_loss=0.07364]\n",
      "Step 535645  [5.492 sec/step, loss=0.07475, avg_loss=0.07366]\n",
      "Step 535646  [5.480 sec/step, loss=0.07546, avg_loss=0.07367]\n",
      "Step 535647  [5.465 sec/step, loss=0.07486, avg_loss=0.07368]\n",
      "Step 535648  [5.490 sec/step, loss=0.07399, avg_loss=0.07372]\n",
      "Step 535649  [5.460 sec/step, loss=0.06601, avg_loss=0.07362]\n",
      "Step 535650  [5.461 sec/step, loss=0.07199, avg_loss=0.07360]\n",
      "Step 535651  [5.458 sec/step, loss=0.07490, avg_loss=0.07363]\n",
      "Step 535652  [5.461 sec/step, loss=0.07444, avg_loss=0.07363]\n",
      "Step 535653  [5.478 sec/step, loss=0.07318, avg_loss=0.07360]\n",
      "Step 535654  [5.467 sec/step, loss=0.06977, avg_loss=0.07354]\n",
      "Step 535655  [5.469 sec/step, loss=0.07424, avg_loss=0.07356]\n",
      "Step 535656  [5.473 sec/step, loss=0.07455, avg_loss=0.07355]\n",
      "Step 535657  [5.482 sec/step, loss=0.07558, avg_loss=0.07356]\n",
      "Step 535658  [5.492 sec/step, loss=0.07397, avg_loss=0.07357]\n",
      "Step 535659  [5.475 sec/step, loss=0.07336, avg_loss=0.07358]\n",
      "Step 535660  [5.479 sec/step, loss=0.07327, avg_loss=0.07360]\n",
      "Step 535661  [5.484 sec/step, loss=0.07138, avg_loss=0.07357]\n",
      "Step 535662  [5.448 sec/step, loss=0.07100, avg_loss=0.07354]\n",
      "Step 535663  [5.383 sec/step, loss=0.07127, avg_loss=0.07360]\n",
      "Step 535664  [5.385 sec/step, loss=0.07209, avg_loss=0.07360]\n",
      "Generated 32 batches of size 32 in 2.367 sec\n",
      "Step 535665  [5.382 sec/step, loss=0.07180, avg_loss=0.07357]\n",
      "Step 535666  [5.368 sec/step, loss=0.07341, avg_loss=0.07354]\n",
      "Step 535667  [5.359 sec/step, loss=0.07494, avg_loss=0.07354]\n",
      "Step 535668  [5.425 sec/step, loss=0.06577, avg_loss=0.07354]\n",
      "Step 535669  [5.439 sec/step, loss=0.07337, avg_loss=0.07353]\n",
      "Step 535670  [5.443 sec/step, loss=0.07601, avg_loss=0.07354]\n",
      "Step 535671  [5.464 sec/step, loss=0.07518, avg_loss=0.07355]\n",
      "Step 535672  [5.460 sec/step, loss=0.07377, avg_loss=0.07355]\n",
      "Step 535673  [5.445 sec/step, loss=0.07327, avg_loss=0.07353]\n",
      "Step 535674  [5.444 sec/step, loss=0.07304, avg_loss=0.07351]\n",
      "Step 535675  [5.446 sec/step, loss=0.07633, avg_loss=0.07352]\n",
      "Step 535676  [5.457 sec/step, loss=0.07412, avg_loss=0.07356]\n",
      "Step 535677  [5.446 sec/step, loss=0.07462, avg_loss=0.07356]\n",
      "Step 535678  [5.449 sec/step, loss=0.07503, avg_loss=0.07359]\n",
      "Step 535679  [5.461 sec/step, loss=0.07490, avg_loss=0.07359]\n",
      "Step 535680  [5.493 sec/step, loss=0.07487, avg_loss=0.07361]\n",
      "Step 535681  [5.472 sec/step, loss=0.07343, avg_loss=0.07359]\n",
      "Step 535682  [5.486 sec/step, loss=0.07476, avg_loss=0.07359]\n",
      "Step 535683  [5.491 sec/step, loss=0.07434, avg_loss=0.07359]\n",
      "Step 535684  [5.488 sec/step, loss=0.07648, avg_loss=0.07360]\n",
      "Step 535685  [5.505 sec/step, loss=0.07603, avg_loss=0.07366]\n",
      "Step 535686  [5.499 sec/step, loss=0.07255, avg_loss=0.07364]\n",
      "Step 535687  [5.504 sec/step, loss=0.07006, avg_loss=0.07360]\n",
      "Step 535688  [5.503 sec/step, loss=0.07367, avg_loss=0.07359]\n",
      "Step 535689  [5.507 sec/step, loss=0.07523, avg_loss=0.07361]\n",
      "Step 535690  [5.497 sec/step, loss=0.07091, avg_loss=0.07359]\n",
      "Step 535691  [5.502 sec/step, loss=0.07451, avg_loss=0.07361]\n",
      "Step 535692  [5.512 sec/step, loss=0.07352, avg_loss=0.07362]\n",
      "Step 535693  [5.532 sec/step, loss=0.07501, avg_loss=0.07364]\n",
      "Step 535694  [5.521 sec/step, loss=0.07485, avg_loss=0.07363]\n",
      "Step 535695  [5.488 sec/step, loss=0.07374, avg_loss=0.07364]\n",
      "Step 535696  [5.443 sec/step, loss=0.07407, avg_loss=0.07371]\n",
      "Generated 32 batches of size 32 in 2.472 sec\n",
      "Step 535697  [5.446 sec/step, loss=0.07370, avg_loss=0.07369]\n",
      "Step 535698  [5.440 sec/step, loss=0.07365, avg_loss=0.07367]\n",
      "Step 535699  [5.404 sec/step, loss=0.06490, avg_loss=0.07358]\n",
      "Step 535700  [5.387 sec/step, loss=0.07264, avg_loss=0.07356]\n",
      "Writing summary at step: 535700\n",
      "Step 535701  [5.385 sec/step, loss=0.07439, avg_loss=0.07357]\n",
      "Step 535702  [5.430 sec/step, loss=0.06539, avg_loss=0.07348]\n",
      "Step 535703  [5.427 sec/step, loss=0.07509, avg_loss=0.07347]\n",
      "Step 535704  [5.423 sec/step, loss=0.07099, avg_loss=0.07347]\n",
      "Step 535705  [5.429 sec/step, loss=0.07375, avg_loss=0.07346]\n",
      "Step 535706  [5.435 sec/step, loss=0.07565, avg_loss=0.07346]\n",
      "Step 535707  [5.457 sec/step, loss=0.07363, avg_loss=0.07347]\n",
      "Step 535708  [5.473 sec/step, loss=0.07580, avg_loss=0.07352]\n",
      "Step 535709  [5.476 sec/step, loss=0.07447, avg_loss=0.07354]\n",
      "Step 535710  [5.465 sec/step, loss=0.07524, avg_loss=0.07354]\n",
      "Step 535711  [5.458 sec/step, loss=0.07452, avg_loss=0.07353]\n",
      "Step 535712  [5.509 sec/step, loss=0.06608, avg_loss=0.07345]\n",
      "Step 535713  [5.508 sec/step, loss=0.07343, avg_loss=0.07345]\n",
      "Step 535714  [5.490 sec/step, loss=0.07296, avg_loss=0.07342]\n",
      "Step 535715  [5.486 sec/step, loss=0.07466, avg_loss=0.07341]\n",
      "Step 535716  [5.485 sec/step, loss=0.07181, avg_loss=0.07338]\n",
      "Step 535717  [5.486 sec/step, loss=0.07484, avg_loss=0.07338]\n",
      "Step 535718  [5.453 sec/step, loss=0.06599, avg_loss=0.07328]\n",
      "Step 535719  [5.442 sec/step, loss=0.07436, avg_loss=0.07328]\n",
      "Step 535720  [5.447 sec/step, loss=0.07553, avg_loss=0.07329]\n",
      "Step 535721  [5.431 sec/step, loss=0.07225, avg_loss=0.07326]\n",
      "Step 535722  [5.445 sec/step, loss=0.07357, avg_loss=0.07325]\n",
      "Step 535723  [5.456 sec/step, loss=0.07406, avg_loss=0.07328]\n",
      "Step 535724  [5.493 sec/step, loss=0.07274, avg_loss=0.07329]\n",
      "Step 535725  [5.493 sec/step, loss=0.07248, avg_loss=0.07329]\n",
      "Step 535726  [5.480 sec/step, loss=0.07175, avg_loss=0.07326]\n",
      "Step 535727  [5.454 sec/step, loss=0.07443, avg_loss=0.07328]\n",
      "Generated 32 batches of size 32 in 2.558 sec\n",
      "Step 535728  [5.439 sec/step, loss=0.07156, avg_loss=0.07323]\n",
      "Step 535729  [5.427 sec/step, loss=0.07327, avg_loss=0.07321]\n",
      "Step 535730  [5.426 sec/step, loss=0.07515, avg_loss=0.07322]\n",
      "Step 535731  [5.422 sec/step, loss=0.07444, avg_loss=0.07323]\n",
      "Step 535732  [5.438 sec/step, loss=0.07577, avg_loss=0.07325]\n",
      "Step 535733  [5.461 sec/step, loss=0.07340, avg_loss=0.07333]\n",
      "Step 535734  [5.453 sec/step, loss=0.07359, avg_loss=0.07332]\n",
      "Step 535735  [5.460 sec/step, loss=0.07329, avg_loss=0.07331]\n",
      "Step 535736  [5.466 sec/step, loss=0.07608, avg_loss=0.07334]\n",
      "Step 535737  [5.471 sec/step, loss=0.07502, avg_loss=0.07336]\n",
      "Step 535738  [5.461 sec/step, loss=0.07360, avg_loss=0.07335]\n",
      "Step 535739  [5.448 sec/step, loss=0.07054, avg_loss=0.07331]\n",
      "Step 535740  [5.468 sec/step, loss=0.07557, avg_loss=0.07333]\n",
      "Step 535741  [5.414 sec/step, loss=0.07285, avg_loss=0.07340]\n",
      "Step 535742  [5.427 sec/step, loss=0.07407, avg_loss=0.07338]\n",
      "Step 535743  [5.408 sec/step, loss=0.06975, avg_loss=0.07332]\n",
      "Step 535744  [5.453 sec/step, loss=0.06591, avg_loss=0.07322]\n",
      "Step 535745  [5.462 sec/step, loss=0.07501, avg_loss=0.07323]\n",
      "Step 535746  [5.442 sec/step, loss=0.07161, avg_loss=0.07319]\n",
      "Step 535747  [5.443 sec/step, loss=0.07484, avg_loss=0.07319]\n",
      "Step 535748  [5.415 sec/step, loss=0.06576, avg_loss=0.07310]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 535749  [5.441 sec/step, loss=0.07376, avg_loss=0.07318]\n",
      "Step 535750  [5.449 sec/step, loss=0.07551, avg_loss=0.07322]\n",
      "Step 535751  [5.448 sec/step, loss=0.07490, avg_loss=0.07322]\n",
      "Step 535752  [5.452 sec/step, loss=0.07550, avg_loss=0.07323]\n",
      "Step 535753  [5.442 sec/step, loss=0.07575, avg_loss=0.07325]\n",
      "Step 535754  [5.452 sec/step, loss=0.07382, avg_loss=0.07329]\n",
      "Step 535755  [5.448 sec/step, loss=0.07193, avg_loss=0.07327]\n",
      "Step 535756  [5.446 sec/step, loss=0.07532, avg_loss=0.07328]\n",
      "Step 535757  [5.433 sec/step, loss=0.07110, avg_loss=0.07323]\n",
      "Step 535758  [5.431 sec/step, loss=0.07249, avg_loss=0.07322]\n",
      "Step 535759  [5.442 sec/step, loss=0.07475, avg_loss=0.07323]\n",
      "Generated 32 batches of size 32 in 2.347 sec\n",
      "Step 535760  [5.452 sec/step, loss=0.07413, avg_loss=0.07324]\n",
      "Step 535761  [5.468 sec/step, loss=0.07269, avg_loss=0.07325]\n",
      "Step 535762  [5.477 sec/step, loss=0.07507, avg_loss=0.07329]\n",
      "Step 535763  [5.492 sec/step, loss=0.07431, avg_loss=0.07333]\n",
      "Step 535764  [5.506 sec/step, loss=0.07519, avg_loss=0.07336]\n",
      "Step 535765  [5.487 sec/step, loss=0.07187, avg_loss=0.07336]\n",
      "Step 535766  [5.490 sec/step, loss=0.07354, avg_loss=0.07336]\n",
      "Step 535767  [5.506 sec/step, loss=0.07604, avg_loss=0.07337]\n",
      "Step 535768  [5.467 sec/step, loss=0.07538, avg_loss=0.07347]\n",
      "Step 535769  [5.458 sec/step, loss=0.07607, avg_loss=0.07349]\n",
      "Step 535770  [5.461 sec/step, loss=0.07292, avg_loss=0.07346]\n",
      "Step 535771  [5.477 sec/step, loss=0.07294, avg_loss=0.07344]\n",
      "Step 535772  [5.448 sec/step, loss=0.07075, avg_loss=0.07341]\n",
      "Step 535773  [5.444 sec/step, loss=0.07296, avg_loss=0.07341]\n",
      "Step 535774  [5.458 sec/step, loss=0.07364, avg_loss=0.07341]\n",
      "Step 535775  [5.437 sec/step, loss=0.07362, avg_loss=0.07338]\n",
      "Step 535776  [5.486 sec/step, loss=0.06596, avg_loss=0.07330]\n",
      "Step 535777  [5.491 sec/step, loss=0.07454, avg_loss=0.07330]\n",
      "Step 535778  [5.490 sec/step, loss=0.07500, avg_loss=0.07330]\n",
      "Step 535779  [5.501 sec/step, loss=0.07278, avg_loss=0.07328]\n",
      "Step 535780  [5.473 sec/step, loss=0.07363, avg_loss=0.07327]\n",
      "Step 535781  [5.493 sec/step, loss=0.07578, avg_loss=0.07329]\n",
      "Step 535782  [5.475 sec/step, loss=0.07293, avg_loss=0.07327]\n",
      "Step 535783  [5.462 sec/step, loss=0.07513, avg_loss=0.07328]\n",
      "Step 535784  [5.462 sec/step, loss=0.07430, avg_loss=0.07326]\n",
      "Step 535785  [5.468 sec/step, loss=0.07505, avg_loss=0.07325]\n",
      "Step 535786  [5.481 sec/step, loss=0.07565, avg_loss=0.07328]\n",
      "Step 535787  [5.472 sec/step, loss=0.06928, avg_loss=0.07327]\n",
      "Step 535788  [5.463 sec/step, loss=0.07436, avg_loss=0.07328]\n",
      "Step 535789  [5.461 sec/step, loss=0.07413, avg_loss=0.07327]\n",
      "Step 535790  [5.477 sec/step, loss=0.07338, avg_loss=0.07329]\n",
      "Step 535791  [5.461 sec/step, loss=0.06561, avg_loss=0.07320]\n",
      "Generated 32 batches of size 32 in 2.378 sec\n",
      "Step 535792  [5.465 sec/step, loss=0.07419, avg_loss=0.07321]\n",
      "Step 535793  [5.466 sec/step, loss=0.07431, avg_loss=0.07320]\n",
      "Step 535794  [5.463 sec/step, loss=0.07324, avg_loss=0.07319]\n",
      "Step 535795  [5.476 sec/step, loss=0.07497, avg_loss=0.07320]\n",
      "Step 535796  [5.472 sec/step, loss=0.07415, avg_loss=0.07320]\n",
      "Step 535797  [5.481 sec/step, loss=0.07628, avg_loss=0.07323]\n",
      "Step 535798  [5.469 sec/step, loss=0.07284, avg_loss=0.07322]\n",
      "Step 535799  [5.480 sec/step, loss=0.07165, avg_loss=0.07329]\n",
      "Step 535800  [5.485 sec/step, loss=0.07486, avg_loss=0.07331]\n",
      "Writing summary at step: 535800\n",
      "Step 535801  [5.488 sec/step, loss=0.07525, avg_loss=0.07332]\n",
      "Step 535802  [5.448 sec/step, loss=0.07613, avg_loss=0.07342]\n",
      "Step 535803  [5.441 sec/step, loss=0.07440, avg_loss=0.07342]\n",
      "Step 535804  [5.445 sec/step, loss=0.07140, avg_loss=0.07342]\n",
      "Step 535805  [5.447 sec/step, loss=0.07303, avg_loss=0.07341]\n",
      "Step 535806  [5.432 sec/step, loss=0.07334, avg_loss=0.07339]\n",
      "Step 535807  [5.403 sec/step, loss=0.07483, avg_loss=0.07340]\n",
      "Step 535808  [5.390 sec/step, loss=0.07499, avg_loss=0.07340]\n",
      "Step 535809  [5.401 sec/step, loss=0.07519, avg_loss=0.07340]\n",
      "Step 535810  [5.412 sec/step, loss=0.07660, avg_loss=0.07342]\n",
      "Step 535811  [5.433 sec/step, loss=0.07276, avg_loss=0.07340]\n",
      "Step 535812  [5.391 sec/step, loss=0.07543, avg_loss=0.07349]\n",
      "Step 535813  [5.394 sec/step, loss=0.07428, avg_loss=0.07350]\n",
      "Step 535814  [5.408 sec/step, loss=0.07542, avg_loss=0.07353]\n",
      "Step 535815  [5.407 sec/step, loss=0.07244, avg_loss=0.07350]\n",
      "Step 535816  [5.404 sec/step, loss=0.07481, avg_loss=0.07353]\n",
      "Step 535817  [5.407 sec/step, loss=0.07432, avg_loss=0.07353]\n",
      "Step 535818  [5.422 sec/step, loss=0.07253, avg_loss=0.07359]\n",
      "Step 535819  [5.466 sec/step, loss=0.06592, avg_loss=0.07351]\n",
      "Step 535820  [5.457 sec/step, loss=0.07490, avg_loss=0.07350]\n",
      "Step 535821  [5.459 sec/step, loss=0.07306, avg_loss=0.07351]\n",
      "Step 535822  [5.459 sec/step, loss=0.07518, avg_loss=0.07353]\n",
      "Generated 32 batches of size 32 in 2.380 sec\n",
      "Step 535823  [5.466 sec/step, loss=0.07458, avg_loss=0.07353]\n",
      "Step 535824  [5.455 sec/step, loss=0.07329, avg_loss=0.07354]\n",
      "Step 535825  [5.451 sec/step, loss=0.07275, avg_loss=0.07354]\n",
      "Step 535826  [5.448 sec/step, loss=0.06537, avg_loss=0.07348]\n",
      "Step 535827  [5.437 sec/step, loss=0.07351, avg_loss=0.07347]\n",
      "Step 535828  [5.428 sec/step, loss=0.07188, avg_loss=0.07347]\n",
      "Step 535829  [5.447 sec/step, loss=0.07557, avg_loss=0.07349]\n",
      "Step 535830  [5.444 sec/step, loss=0.07092, avg_loss=0.07345]\n",
      "Step 535831  [5.448 sec/step, loss=0.07418, avg_loss=0.07345]\n",
      "Step 535832  [5.428 sec/step, loss=0.07041, avg_loss=0.07339]\n",
      "Step 535833  [5.435 sec/step, loss=0.07282, avg_loss=0.07339]\n",
      "Step 535834  [5.450 sec/step, loss=0.07430, avg_loss=0.07340]\n",
      "Step 535835  [5.455 sec/step, loss=0.07494, avg_loss=0.07341]\n",
      "Step 535836  [5.446 sec/step, loss=0.07420, avg_loss=0.07339]\n",
      "Step 535837  [5.446 sec/step, loss=0.07460, avg_loss=0.07339]\n",
      "Step 535838  [5.465 sec/step, loss=0.07395, avg_loss=0.07339]\n",
      "Step 535839  [5.486 sec/step, loss=0.07576, avg_loss=0.07345]\n",
      "Step 535840  [5.460 sec/step, loss=0.06520, avg_loss=0.07334]\n",
      "Step 535841  [5.467 sec/step, loss=0.07515, avg_loss=0.07336]\n",
      "Step 535842  [5.466 sec/step, loss=0.07273, avg_loss=0.07335]\n",
      "Step 535843  [5.464 sec/step, loss=0.07377, avg_loss=0.07339]\n",
      "Step 535844  [5.428 sec/step, loss=0.07546, avg_loss=0.07349]\n",
      "Step 535845  [5.416 sec/step, loss=0.07082, avg_loss=0.07345]\n",
      "Step 535846  [5.421 sec/step, loss=0.07327, avg_loss=0.07346]\n",
      "Step 535847  [5.405 sec/step, loss=0.07028, avg_loss=0.07342]\n",
      "Step 535848  [5.420 sec/step, loss=0.07475, avg_loss=0.07351]\n",
      "Step 535849  [5.406 sec/step, loss=0.07376, avg_loss=0.07351]\n",
      "Step 535850  [5.402 sec/step, loss=0.07563, avg_loss=0.07351]\n",
      "Step 535851  [5.402 sec/step, loss=0.07457, avg_loss=0.07350]\n",
      "Step 535852  [5.403 sec/step, loss=0.07563, avg_loss=0.07351]\n",
      "Step 535853  [5.375 sec/step, loss=0.07351, avg_loss=0.07348]\n",
      "Step 535854  [5.386 sec/step, loss=0.07460, avg_loss=0.07349]\n",
      "Generated 32 batches of size 32 in 2.582 sec\n",
      "Step 535855  [5.390 sec/step, loss=0.07174, avg_loss=0.07349]\n",
      "Step 535856  [5.385 sec/step, loss=0.07430, avg_loss=0.07348]\n",
      "Step 535857  [5.434 sec/step, loss=0.06618, avg_loss=0.07343]\n",
      "Step 535858  [5.440 sec/step, loss=0.07448, avg_loss=0.07345]\n",
      "Step 535859  [5.430 sec/step, loss=0.07212, avg_loss=0.07342]\n",
      "Step 535860  [5.431 sec/step, loss=0.07577, avg_loss=0.07344]\n",
      "Step 535861  [5.431 sec/step, loss=0.07582, avg_loss=0.07347]\n",
      "Step 535862  [5.433 sec/step, loss=0.07310, avg_loss=0.07345]\n",
      "Step 535863  [5.437 sec/step, loss=0.07295, avg_loss=0.07344]\n",
      "Step 535864  [5.425 sec/step, loss=0.07181, avg_loss=0.07340]\n",
      "Step 535865  [5.434 sec/step, loss=0.07473, avg_loss=0.07343]\n",
      "Step 535866  [5.447 sec/step, loss=0.07532, avg_loss=0.07345]\n",
      "Step 535867  [5.445 sec/step, loss=0.07532, avg_loss=0.07344]\n",
      "Step 535868  [5.429 sec/step, loss=0.07300, avg_loss=0.07342]\n",
      "Step 535869  [5.420 sec/step, loss=0.07479, avg_loss=0.07341]\n",
      "Step 535870  [5.434 sec/step, loss=0.07256, avg_loss=0.07340]\n",
      "Step 535871  [5.425 sec/step, loss=0.07533, avg_loss=0.07343]\n",
      "Step 535872  [5.452 sec/step, loss=0.07467, avg_loss=0.07347]\n",
      "Step 535873  [5.467 sec/step, loss=0.07415, avg_loss=0.07348]\n",
      "Step 535874  [5.447 sec/step, loss=0.07132, avg_loss=0.07345]\n",
      "Step 535875  [5.465 sec/step, loss=0.07594, avg_loss=0.07348]\n",
      "Step 535876  [5.414 sec/step, loss=0.07368, avg_loss=0.07355]\n",
      "Step 535877  [5.416 sec/step, loss=0.07492, avg_loss=0.07356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 535878  [5.425 sec/step, loss=0.07444, avg_loss=0.07355]\n",
      "Step 535879  [5.395 sec/step, loss=0.07340, avg_loss=0.07356]\n",
      "Step 535880  [5.396 sec/step, loss=0.07435, avg_loss=0.07357]\n",
      "Step 535881  [5.439 sec/step, loss=0.06515, avg_loss=0.07346]\n",
      "Step 535882  [5.454 sec/step, loss=0.07535, avg_loss=0.07348]\n",
      "Step 535883  [5.461 sec/step, loss=0.07588, avg_loss=0.07349]\n",
      "Step 535884  [5.465 sec/step, loss=0.07503, avg_loss=0.07350]\n",
      "Step 535885  [5.447 sec/step, loss=0.07298, avg_loss=0.07348]\n",
      "Step 535886  [5.443 sec/step, loss=0.07462, avg_loss=0.07347]\n",
      "Generated 32 batches of size 32 in 2.791 sec\n",
      "Step 535887  [5.445 sec/step, loss=0.07203, avg_loss=0.07350]\n",
      "Step 535888  [5.440 sec/step, loss=0.07328, avg_loss=0.07348]\n",
      "Step 535889  [5.442 sec/step, loss=0.07070, avg_loss=0.07345]\n",
      "Step 535890  [5.430 sec/step, loss=0.07377, avg_loss=0.07345]\n",
      "Step 535891  [5.430 sec/step, loss=0.06708, avg_loss=0.07347]\n",
      "Step 535892  [5.421 sec/step, loss=0.07478, avg_loss=0.07347]\n",
      "Step 535893  [5.416 sec/step, loss=0.07437, avg_loss=0.07348]\n",
      "Step 535894  [5.426 sec/step, loss=0.07373, avg_loss=0.07348]\n",
      "Step 535895  [5.424 sec/step, loss=0.07494, avg_loss=0.07348]\n",
      "Step 535896  [5.422 sec/step, loss=0.07479, avg_loss=0.07349]\n",
      "Step 535897  [5.414 sec/step, loss=0.07388, avg_loss=0.07346]\n",
      "Step 535898  [5.402 sec/step, loss=0.06525, avg_loss=0.07339]\n",
      "Step 535899  [5.405 sec/step, loss=0.07141, avg_loss=0.07338]\n",
      "Step 535900  [5.406 sec/step, loss=0.07325, avg_loss=0.07337]\n",
      "Writing summary at step: 535900\n",
      "Step 535901  [5.377 sec/step, loss=0.07067, avg_loss=0.07332]\n",
      "Step 535902  [5.353 sec/step, loss=0.07130, avg_loss=0.07327]\n",
      "Step 535903  [5.344 sec/step, loss=0.07210, avg_loss=0.07325]\n",
      "Step 535904  [5.361 sec/step, loss=0.07567, avg_loss=0.07329]\n",
      "Step 535905  [5.347 sec/step, loss=0.07296, avg_loss=0.07329]\n",
      "Step 535906  [5.354 sec/step, loss=0.07388, avg_loss=0.07330]\n",
      "Step 535907  [5.365 sec/step, loss=0.07424, avg_loss=0.07329]\n",
      "Step 535908  [5.379 sec/step, loss=0.07473, avg_loss=0.07329]\n",
      "Step 535909  [5.371 sec/step, loss=0.07558, avg_loss=0.07329]\n",
      "Step 535910  [5.374 sec/step, loss=0.07665, avg_loss=0.07329]\n",
      "Step 535911  [5.348 sec/step, loss=0.07472, avg_loss=0.07331]\n",
      "Step 535912  [5.364 sec/step, loss=0.07440, avg_loss=0.07330]\n",
      "Step 535913  [5.356 sec/step, loss=0.07397, avg_loss=0.07330]\n",
      "Step 535914  [5.359 sec/step, loss=0.07408, avg_loss=0.07329]\n",
      "Step 535915  [5.367 sec/step, loss=0.07314, avg_loss=0.07329]\n",
      "Step 535916  [5.370 sec/step, loss=0.07404, avg_loss=0.07329]\n",
      "Step 535917  [5.363 sec/step, loss=0.07464, avg_loss=0.07329]\n",
      "Generated 32 batches of size 32 in 2.661 sec\n",
      "Step 535918  [5.421 sec/step, loss=0.06543, avg_loss=0.07322]\n",
      "Step 535919  [5.367 sec/step, loss=0.07391, avg_loss=0.07330]\n",
      "Step 535920  [5.370 sec/step, loss=0.07389, avg_loss=0.07329]\n",
      "Step 535921  [5.374 sec/step, loss=0.07473, avg_loss=0.07331]\n",
      "Step 535922  [5.354 sec/step, loss=0.07337, avg_loss=0.07329]\n",
      "Step 535923  [5.358 sec/step, loss=0.07586, avg_loss=0.07330]\n",
      "Step 535924  [5.349 sec/step, loss=0.07550, avg_loss=0.07332]\n",
      "Step 535925  [5.369 sec/step, loss=0.07521, avg_loss=0.07335]\n",
      "Step 535926  [5.386 sec/step, loss=0.07399, avg_loss=0.07343]\n",
      "Step 535927  [5.398 sec/step, loss=0.07537, avg_loss=0.07345]\n",
      "Step 535928  [5.402 sec/step, loss=0.07349, avg_loss=0.07347]\n",
      "Step 535929  [5.379 sec/step, loss=0.07096, avg_loss=0.07342]\n",
      "Step 535930  [5.391 sec/step, loss=0.07545, avg_loss=0.07347]\n",
      "Step 535931  [5.400 sec/step, loss=0.07416, avg_loss=0.07347]\n",
      "Step 535932  [5.415 sec/step, loss=0.07476, avg_loss=0.07351]\n",
      "Step 535933  [5.382 sec/step, loss=0.07084, avg_loss=0.07349]\n",
      "Step 535934  [5.377 sec/step, loss=0.07428, avg_loss=0.07349]\n",
      "Step 535935  [5.423 sec/step, loss=0.06704, avg_loss=0.07341]\n",
      "Step 535936  [5.414 sec/step, loss=0.07245, avg_loss=0.07339]\n",
      "Step 535937  [5.415 sec/step, loss=0.07093, avg_loss=0.07336]\n",
      "Step 535938  [5.401 sec/step, loss=0.07373, avg_loss=0.07335]\n",
      "Step 535939  [5.383 sec/step, loss=0.07301, avg_loss=0.07333]\n",
      "Step 535940  [5.392 sec/step, loss=0.07331, avg_loss=0.07341]\n",
      "Step 535941  [5.389 sec/step, loss=0.07177, avg_loss=0.07337]\n",
      "Step 535942  [5.391 sec/step, loss=0.07305, avg_loss=0.07338]\n",
      "Step 535943  [5.406 sec/step, loss=0.07622, avg_loss=0.07340]\n",
      "Step 535944  [5.398 sec/step, loss=0.07524, avg_loss=0.07340]\n",
      "Step 535945  [5.405 sec/step, loss=0.07542, avg_loss=0.07345]\n",
      "Step 535946  [5.431 sec/step, loss=0.07370, avg_loss=0.07345]\n",
      "Step 535947  [5.451 sec/step, loss=0.07159, avg_loss=0.07346]\n",
      "Step 535948  [5.460 sec/step, loss=0.07523, avg_loss=0.07347]\n",
      "Step 535949  [5.479 sec/step, loss=0.07603, avg_loss=0.07349]\n",
      "Generated 32 batches of size 32 in 2.589 sec\n",
      "Step 535950  [5.492 sec/step, loss=0.07375, avg_loss=0.07347]\n",
      "Step 535951  [5.493 sec/step, loss=0.07141, avg_loss=0.07344]\n",
      "Step 535952  [5.492 sec/step, loss=0.07475, avg_loss=0.07343]\n",
      "Step 535953  [5.484 sec/step, loss=0.06577, avg_loss=0.07335]\n",
      "Step 535954  [5.470 sec/step, loss=0.07490, avg_loss=0.07336]\n",
      "Step 535955  [5.465 sec/step, loss=0.07328, avg_loss=0.07337]\n",
      "Step 535956  [5.466 sec/step, loss=0.07483, avg_loss=0.07338]\n",
      "Step 535957  [5.428 sec/step, loss=0.07362, avg_loss=0.07345]\n",
      "Step 535958  [5.422 sec/step, loss=0.07544, avg_loss=0.07346]\n",
      "Step 535959  [5.435 sec/step, loss=0.07416, avg_loss=0.07348]\n",
      "Step 535960  [5.422 sec/step, loss=0.07484, avg_loss=0.07347]\n",
      "Step 535961  [5.397 sec/step, loss=0.07391, avg_loss=0.07345]\n",
      "Step 535962  [5.396 sec/step, loss=0.07165, avg_loss=0.07344]\n",
      "Step 535963  [5.393 sec/step, loss=0.07452, avg_loss=0.07346]\n",
      "Step 535964  [5.394 sec/step, loss=0.07337, avg_loss=0.07347]\n",
      "Step 535965  [5.402 sec/step, loss=0.07412, avg_loss=0.07347]\n",
      "Step 535966  [5.418 sec/step, loss=0.07481, avg_loss=0.07346]\n",
      "Step 535967  [5.416 sec/step, loss=0.07612, avg_loss=0.07347]\n",
      "Step 535968  [5.433 sec/step, loss=0.07364, avg_loss=0.07347]\n",
      "Step 535969  [5.437 sec/step, loss=0.07417, avg_loss=0.07347]\n",
      "Step 535970  [5.404 sec/step, loss=0.07372, avg_loss=0.07348]\n",
      "Step 535971  [5.379 sec/step, loss=0.07252, avg_loss=0.07345]\n",
      "Step 535972  [5.355 sec/step, loss=0.07071, avg_loss=0.07341]\n",
      "Step 535973  [5.349 sec/step, loss=0.07513, avg_loss=0.07342]\n",
      "Step 535974  [5.360 sec/step, loss=0.07160, avg_loss=0.07342]\n",
      "Step 535975  [5.351 sec/step, loss=0.07418, avg_loss=0.07341]\n",
      "Step 535976  [5.364 sec/step, loss=0.07382, avg_loss=0.07341]\n",
      "Step 535977  [5.369 sec/step, loss=0.07549, avg_loss=0.07341]\n",
      "Step 535978  [5.342 sec/step, loss=0.06463, avg_loss=0.07332]\n",
      "Step 535979  [5.403 sec/step, loss=0.06588, avg_loss=0.07324]\n",
      "Step 535980  [5.409 sec/step, loss=0.07455, avg_loss=0.07324]\n",
      "Step 535981  [5.353 sec/step, loss=0.07296, avg_loss=0.07332]\n",
      "Generated 32 batches of size 32 in 2.406 sec\n",
      "Step 535982  [5.348 sec/step, loss=0.07529, avg_loss=0.07332]\n",
      "Step 535983  [5.358 sec/step, loss=0.07588, avg_loss=0.07332]\n",
      "Step 535984  [5.348 sec/step, loss=0.07529, avg_loss=0.07332]\n",
      "Step 535985  [5.348 sec/step, loss=0.07484, avg_loss=0.07334]\n",
      "Step 535986  [5.344 sec/step, loss=0.07414, avg_loss=0.07334]\n",
      "Step 535987  [5.365 sec/step, loss=0.07591, avg_loss=0.07338]\n",
      "Step 535988  [5.389 sec/step, loss=0.07516, avg_loss=0.07339]\n",
      "Step 535989  [5.400 sec/step, loss=0.07589, avg_loss=0.07345]\n",
      "Step 535990  [5.393 sec/step, loss=0.07105, avg_loss=0.07342]\n",
      "Step 535991  [5.418 sec/step, loss=0.07548, avg_loss=0.07350]\n",
      "Step 535992  [5.431 sec/step, loss=0.07627, avg_loss=0.07352]\n",
      "Step 535993  [5.439 sec/step, loss=0.07559, avg_loss=0.07353]\n",
      "Step 535994  [5.445 sec/step, loss=0.07541, avg_loss=0.07355]\n",
      "Step 535995  [5.447 sec/step, loss=0.07427, avg_loss=0.07354]\n",
      "Step 535996  [5.446 sec/step, loss=0.07469, avg_loss=0.07354]\n",
      "Step 535997  [5.430 sec/step, loss=0.07082, avg_loss=0.07351]\n",
      "Step 535998  [5.430 sec/step, loss=0.06642, avg_loss=0.07352]\n",
      "Step 535999  [5.428 sec/step, loss=0.07052, avg_loss=0.07351]\n",
      "Step 536000  [5.472 sec/step, loss=0.06952, avg_loss=0.07347]\n",
      "Writing summary at step: 536000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-536000\n",
      "Saving audio and alignment...\n",
      "Input: utshtshay or baazuuq muslam muzaffar nay mulayrijaa or indzrii koo parkhaa or foodziioon koo aemardzansii bhaydz ddijaa~______________\n",
      "Step 536001  [5.482 sec/step, loss=0.07385, avg_loss=0.07351]\n",
      "Step 536002  [5.507 sec/step, loss=0.07579, avg_loss=0.07355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 536003  [5.516 sec/step, loss=0.07441, avg_loss=0.07357]\n",
      "Step 536004  [5.504 sec/step, loss=0.07188, avg_loss=0.07354]\n",
      "Step 536005  [5.504 sec/step, loss=0.07370, avg_loss=0.07354]\n",
      "Step 536006  [5.508 sec/step, loss=0.07465, avg_loss=0.07355]\n",
      "Step 536007  [5.500 sec/step, loss=0.07458, avg_loss=0.07355]\n",
      "Step 536008  [5.496 sec/step, loss=0.07337, avg_loss=0.07354]\n",
      "Step 536009  [5.474 sec/step, loss=0.07159, avg_loss=0.07350]\n",
      "Step 536010  [5.464 sec/step, loss=0.07527, avg_loss=0.07349]\n",
      "Step 536011  [5.456 sec/step, loss=0.07310, avg_loss=0.07347]\n",
      "Generated 32 batches of size 32 in 2.388 sec\n",
      "Step 536012  [5.456 sec/step, loss=0.07300, avg_loss=0.07346]\n",
      "Step 536013  [5.480 sec/step, loss=0.07479, avg_loss=0.07347]\n",
      "Step 536014  [5.468 sec/step, loss=0.07261, avg_loss=0.07345]\n",
      "Step 536015  [5.456 sec/step, loss=0.07422, avg_loss=0.07346]\n",
      "Step 536016  [5.445 sec/step, loss=0.07359, avg_loss=0.07346]\n",
      "Step 536017  [5.450 sec/step, loss=0.07542, avg_loss=0.07346]\n",
      "Step 536018  [5.389 sec/step, loss=0.07332, avg_loss=0.07354]\n",
      "Step 536019  [5.401 sec/step, loss=0.07520, avg_loss=0.07356]\n",
      "Step 536020  [5.392 sec/step, loss=0.07489, avg_loss=0.07357]\n",
      "Step 536021  [5.398 sec/step, loss=0.07569, avg_loss=0.07358]\n",
      "Step 536022  [5.397 sec/step, loss=0.07356, avg_loss=0.07358]\n",
      "Step 536023  [5.380 sec/step, loss=0.07481, avg_loss=0.07357]\n",
      "Step 536024  [5.429 sec/step, loss=0.06528, avg_loss=0.07347]\n",
      "Step 536025  [5.416 sec/step, loss=0.07436, avg_loss=0.07346]\n",
      "Step 536026  [5.427 sec/step, loss=0.07424, avg_loss=0.07346]\n",
      "Step 536027  [5.422 sec/step, loss=0.07269, avg_loss=0.07343]\n",
      "Step 536028  [5.450 sec/step, loss=0.07507, avg_loss=0.07345]\n",
      "Step 536029  [5.486 sec/step, loss=0.07439, avg_loss=0.07348]\n",
      "Step 536030  [5.471 sec/step, loss=0.07352, avg_loss=0.07346]\n",
      "Step 536031  [5.455 sec/step, loss=0.07311, avg_loss=0.07345]\n",
      "Step 536032  [5.455 sec/step, loss=0.07499, avg_loss=0.07345]\n",
      "Step 536033  [5.486 sec/step, loss=0.07513, avg_loss=0.07350]\n",
      "Step 536034  [5.485 sec/step, loss=0.07496, avg_loss=0.07350]\n",
      "Step 536035  [5.436 sec/step, loss=0.07496, avg_loss=0.07358]\n",
      "Step 536036  [5.425 sec/step, loss=0.06596, avg_loss=0.07352]\n",
      "Step 536037  [5.428 sec/step, loss=0.07423, avg_loss=0.07355]\n",
      "Step 536038  [5.429 sec/step, loss=0.07401, avg_loss=0.07355]\n",
      "Step 536039  [5.425 sec/step, loss=0.07096, avg_loss=0.07353]\n",
      "Step 536040  [5.442 sec/step, loss=0.07565, avg_loss=0.07356]\n",
      "Step 536041  [5.444 sec/step, loss=0.07477, avg_loss=0.07359]\n",
      "Step 536042  [5.423 sec/step, loss=0.07429, avg_loss=0.07360]\n",
      "Step 536043  [5.415 sec/step, loss=0.07377, avg_loss=0.07358]\n",
      "Generated 32 batches of size 32 in 2.478 sec\n",
      "Step 536044  [5.425 sec/step, loss=0.07555, avg_loss=0.07358]\n",
      "Step 536045  [5.402 sec/step, loss=0.07171, avg_loss=0.07354]\n",
      "Step 536046  [5.392 sec/step, loss=0.07636, avg_loss=0.07357]\n",
      "Step 536047  [5.388 sec/step, loss=0.07473, avg_loss=0.07360]\n",
      "Step 536048  [5.391 sec/step, loss=0.07324, avg_loss=0.07358]\n",
      "Step 536049  [5.368 sec/step, loss=0.07397, avg_loss=0.07356]\n",
      "Step 536050  [5.346 sec/step, loss=0.07097, avg_loss=0.07353]\n",
      "Step 536051  [5.352 sec/step, loss=0.07401, avg_loss=0.07356]\n",
      "Step 536052  [5.359 sec/step, loss=0.07601, avg_loss=0.07357]\n",
      "Step 536053  [5.382 sec/step, loss=0.07124, avg_loss=0.07362]\n",
      "Step 536054  [5.394 sec/step, loss=0.07477, avg_loss=0.07362]\n",
      "Step 536055  [5.384 sec/step, loss=0.07114, avg_loss=0.07360]\n",
      "Step 536056  [5.397 sec/step, loss=0.07550, avg_loss=0.07361]\n",
      "Step 536057  [5.373 sec/step, loss=0.07081, avg_loss=0.07358]\n",
      "Step 536058  [5.384 sec/step, loss=0.07390, avg_loss=0.07357]\n",
      "Step 536059  [5.378 sec/step, loss=0.07566, avg_loss=0.07358]\n",
      "Step 536060  [5.381 sec/step, loss=0.07279, avg_loss=0.07356]\n",
      "Step 536061  [5.403 sec/step, loss=0.07358, avg_loss=0.07356]\n",
      "Step 536062  [5.402 sec/step, loss=0.07484, avg_loss=0.07359]\n",
      "Step 536063  [5.396 sec/step, loss=0.07464, avg_loss=0.07359]\n",
      "Step 536064  [5.414 sec/step, loss=0.07639, avg_loss=0.07362]\n",
      "Step 536065  [5.411 sec/step, loss=0.07145, avg_loss=0.07359]\n",
      "Step 536066  [5.393 sec/step, loss=0.07533, avg_loss=0.07360]\n",
      "Step 536067  [5.389 sec/step, loss=0.07455, avg_loss=0.07358]\n",
      "Step 536068  [5.373 sec/step, loss=0.07180, avg_loss=0.07356]\n",
      "Step 536069  [5.380 sec/step, loss=0.07557, avg_loss=0.07358]\n",
      "Step 536070  [5.386 sec/step, loss=0.07436, avg_loss=0.07358]\n",
      "Step 536071  [5.376 sec/step, loss=0.06522, avg_loss=0.07351]\n",
      "Step 536072  [5.437 sec/step, loss=0.06702, avg_loss=0.07347]\n",
      "Step 536073  [5.440 sec/step, loss=0.07429, avg_loss=0.07347]\n",
      "Step 536074  [5.443 sec/step, loss=0.07524, avg_loss=0.07350]\n",
      "Step 536075  [5.464 sec/step, loss=0.07284, avg_loss=0.07349]\n",
      "Generated 32 batches of size 32 in 2.471 sec\n",
      "Step 536076  [5.463 sec/step, loss=0.07574, avg_loss=0.07351]\n",
      "Step 536077  [5.465 sec/step, loss=0.07452, avg_loss=0.07350]\n",
      "Step 536078  [5.474 sec/step, loss=0.07263, avg_loss=0.07358]\n",
      "Step 536079  [5.422 sec/step, loss=0.07126, avg_loss=0.07363]\n",
      "Step 536080  [5.446 sec/step, loss=0.07317, avg_loss=0.07362]\n",
      "Step 536081  [5.439 sec/step, loss=0.07383, avg_loss=0.07363]\n",
      "Step 536082  [5.432 sec/step, loss=0.07459, avg_loss=0.07362]\n",
      "Step 536083  [5.410 sec/step, loss=0.07361, avg_loss=0.07360]\n",
      "Step 536084  [5.401 sec/step, loss=0.07357, avg_loss=0.07358]\n",
      "Step 536085  [5.406 sec/step, loss=0.07559, avg_loss=0.07359]\n",
      "Step 536086  [5.420 sec/step, loss=0.07554, avg_loss=0.07360]\n",
      "Step 536087  [5.408 sec/step, loss=0.07402, avg_loss=0.07358]\n",
      "Step 536088  [5.413 sec/step, loss=0.07299, avg_loss=0.07356]\n",
      "Step 536089  [5.404 sec/step, loss=0.07403, avg_loss=0.07354]\n",
      "Step 536090  [5.428 sec/step, loss=0.07563, avg_loss=0.07359]\n",
      "Step 536091  [5.410 sec/step, loss=0.07385, avg_loss=0.07357]\n",
      "Step 536092  [5.397 sec/step, loss=0.07392, avg_loss=0.07355]\n",
      "Step 536093  [5.414 sec/step, loss=0.07262, avg_loss=0.07352]\n",
      "Step 536094  [5.407 sec/step, loss=0.07418, avg_loss=0.07351]\n",
      "Step 536095  [5.388 sec/step, loss=0.07203, avg_loss=0.07348]\n",
      "Step 536096  [5.384 sec/step, loss=0.07347, avg_loss=0.07347]\n",
      "Step 536097  [5.404 sec/step, loss=0.07505, avg_loss=0.07351]\n",
      "Step 536098  [5.430 sec/step, loss=0.07512, avg_loss=0.07360]\n",
      "Step 536099  [5.441 sec/step, loss=0.07574, avg_loss=0.07365]\n",
      "Step 536100  [5.403 sec/step, loss=0.07508, avg_loss=0.07371]\n",
      "Writing summary at step: 536100\n",
      "Step 536101  [5.454 sec/step, loss=0.06517, avg_loss=0.07362]\n",
      "Step 536102  [5.459 sec/step, loss=0.07527, avg_loss=0.07362]\n",
      "Step 536103  [5.447 sec/step, loss=0.07021, avg_loss=0.07358]\n",
      "Step 536104  [5.436 sec/step, loss=0.06606, avg_loss=0.07352]\n",
      "Step 536105  [5.455 sec/step, loss=0.07593, avg_loss=0.07354]\n",
      "Step 536106  [5.451 sec/step, loss=0.07227, avg_loss=0.07352]\n",
      "Generated 32 batches of size 32 in 2.423 sec\n",
      "Step 536107  [5.467 sec/step, loss=0.07350, avg_loss=0.07350]\n",
      "Step 536108  [5.455 sec/step, loss=0.07480, avg_loss=0.07352]\n",
      "Step 536109  [5.468 sec/step, loss=0.07471, avg_loss=0.07355]\n",
      "Step 536110  [5.470 sec/step, loss=0.07478, avg_loss=0.07355]\n",
      "Step 536111  [5.480 sec/step, loss=0.07119, avg_loss=0.07353]\n",
      "Step 536112  [5.469 sec/step, loss=0.07378, avg_loss=0.07353]\n",
      "Step 536113  [5.446 sec/step, loss=0.07119, avg_loss=0.07350]\n",
      "Step 536114  [5.444 sec/step, loss=0.07331, avg_loss=0.07350]\n",
      "Step 536115  [5.442 sec/step, loss=0.07288, avg_loss=0.07349]\n",
      "Step 536116  [5.463 sec/step, loss=0.07411, avg_loss=0.07350]\n",
      "Step 536117  [5.466 sec/step, loss=0.07440, avg_loss=0.07349]\n",
      "Step 536118  [5.473 sec/step, loss=0.07504, avg_loss=0.07350]\n",
      "Step 536119  [5.458 sec/step, loss=0.07342, avg_loss=0.07349]\n",
      "Step 536120  [5.475 sec/step, loss=0.07595, avg_loss=0.07350]\n",
      "Step 536121  [5.453 sec/step, loss=0.07365, avg_loss=0.07348]\n",
      "Step 536122  [5.472 sec/step, loss=0.07613, avg_loss=0.07350]\n",
      "Step 536123  [5.475 sec/step, loss=0.07438, avg_loss=0.07350]\n",
      "Step 536124  [5.450 sec/step, loss=0.07332, avg_loss=0.07358]\n",
      "Step 536125  [5.447 sec/step, loss=0.07437, avg_loss=0.07358]\n",
      "Step 536126  [5.425 sec/step, loss=0.06984, avg_loss=0.07353]\n",
      "Step 536127  [5.433 sec/step, loss=0.07587, avg_loss=0.07357]\n",
      "Step 536128  [5.465 sec/step, loss=0.06580, avg_loss=0.07347]\n",
      "Step 536129  [5.453 sec/step, loss=0.07265, avg_loss=0.07346]\n",
      "Step 536130  [5.469 sec/step, loss=0.07322, avg_loss=0.07345]\n",
      "Step 536131  [5.478 sec/step, loss=0.07533, avg_loss=0.07347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 536132  [5.487 sec/step, loss=0.07560, avg_loss=0.07348]\n",
      "Step 536133  [5.469 sec/step, loss=0.07501, avg_loss=0.07348]\n",
      "Step 536134  [5.468 sec/step, loss=0.07334, avg_loss=0.07346]\n",
      "Step 536135  [5.471 sec/step, loss=0.07521, avg_loss=0.07347]\n",
      "Step 536136  [5.488 sec/step, loss=0.07324, avg_loss=0.07354]\n",
      "Step 536137  [5.485 sec/step, loss=0.07454, avg_loss=0.07354]\n",
      "Step 536138  [5.468 sec/step, loss=0.06475, avg_loss=0.07345]\n",
      "Generated 32 batches of size 32 in 2.519 sec\n",
      "Step 536139  [5.478 sec/step, loss=0.07271, avg_loss=0.07347]\n",
      "Step 536140  [5.478 sec/step, loss=0.07474, avg_loss=0.07346]\n",
      "Step 536141  [5.485 sec/step, loss=0.07442, avg_loss=0.07345]\n",
      "Step 536142  [5.500 sec/step, loss=0.07270, avg_loss=0.07344]\n",
      "Step 536143  [5.486 sec/step, loss=0.07070, avg_loss=0.07341]\n",
      "Step 536144  [5.467 sec/step, loss=0.07127, avg_loss=0.07336]\n",
      "Step 536145  [5.489 sec/step, loss=0.07594, avg_loss=0.07341]\n",
      "Step 536146  [5.478 sec/step, loss=0.07163, avg_loss=0.07336]\n",
      "Step 536147  [5.478 sec/step, loss=0.07150, avg_loss=0.07333]\n",
      "Step 536148  [5.455 sec/step, loss=0.07103, avg_loss=0.07331]\n",
      "Step 536149  [5.447 sec/step, loss=0.06634, avg_loss=0.07323]\n",
      "Step 536150  [5.449 sec/step, loss=0.07437, avg_loss=0.07326]\n",
      "Step 536151  [5.447 sec/step, loss=0.07452, avg_loss=0.07327]\n",
      "Step 536152  [5.446 sec/step, loss=0.07563, avg_loss=0.07326]\n",
      "Step 536153  [5.440 sec/step, loss=0.07460, avg_loss=0.07330]\n",
      "Step 536154  [5.423 sec/step, loss=0.07324, avg_loss=0.07328]\n",
      "Step 536155  [5.426 sec/step, loss=0.07378, avg_loss=0.07331]\n",
      "Step 536156  [5.419 sec/step, loss=0.07402, avg_loss=0.07329]\n",
      "Step 536157  [5.430 sec/step, loss=0.07399, avg_loss=0.07333]\n",
      "Step 536158  [5.419 sec/step, loss=0.07506, avg_loss=0.07334]\n",
      "Step 536159  [5.429 sec/step, loss=0.07601, avg_loss=0.07334]\n",
      "Step 536160  [5.434 sec/step, loss=0.07544, avg_loss=0.07337]\n",
      "Step 536161  [5.418 sec/step, loss=0.07188, avg_loss=0.07335]\n",
      "Step 536162  [5.407 sec/step, loss=0.07141, avg_loss=0.07332]\n",
      "Step 536163  [5.420 sec/step, loss=0.07595, avg_loss=0.07333]\n",
      "Step 536164  [5.402 sec/step, loss=0.07510, avg_loss=0.07332]\n",
      "Step 536165  [5.405 sec/step, loss=0.07319, avg_loss=0.07333]\n",
      "Step 536166  [5.397 sec/step, loss=0.07247, avg_loss=0.07330]\n",
      "Step 536167  [5.405 sec/step, loss=0.07490, avg_loss=0.07331]\n",
      "Step 536168  [5.416 sec/step, loss=0.07393, avg_loss=0.07333]\n",
      "Step 536169  [5.416 sec/step, loss=0.07554, avg_loss=0.07333]\n",
      "Step 536170  [5.409 sec/step, loss=0.07348, avg_loss=0.07332]\n",
      "Generated 32 batches of size 32 in 2.360 sec\n",
      "Step 536171  [5.448 sec/step, loss=0.07528, avg_loss=0.07342]\n",
      "Step 536172  [5.406 sec/step, loss=0.07608, avg_loss=0.07351]\n",
      "Step 536173  [5.452 sec/step, loss=0.06596, avg_loss=0.07343]\n",
      "Step 536174  [5.455 sec/step, loss=0.07346, avg_loss=0.07341]\n",
      "Step 536175  [5.463 sec/step, loss=0.07283, avg_loss=0.07341]\n",
      "Step 536176  [5.460 sec/step, loss=0.07528, avg_loss=0.07341]\n",
      "Step 536177  [5.453 sec/step, loss=0.07436, avg_loss=0.07340]\n",
      "Step 536178  [5.458 sec/step, loss=0.07025, avg_loss=0.07338]\n",
      "Step 536179  [5.458 sec/step, loss=0.07341, avg_loss=0.07340]\n",
      "Step 536180  [5.435 sec/step, loss=0.07483, avg_loss=0.07342]\n",
      "Step 536181  [5.449 sec/step, loss=0.07455, avg_loss=0.07343]\n",
      "Step 536182  [5.446 sec/step, loss=0.07315, avg_loss=0.07341]\n",
      "Step 536183  [5.459 sec/step, loss=0.07518, avg_loss=0.07343]\n",
      "Step 536184  [5.471 sec/step, loss=0.07389, avg_loss=0.07343]\n",
      "Step 536185  [5.465 sec/step, loss=0.07351, avg_loss=0.07341]\n",
      "Step 536186  [5.467 sec/step, loss=0.07577, avg_loss=0.07341]\n",
      "Step 536187  [5.461 sec/step, loss=0.07141, avg_loss=0.07339]\n",
      "Step 536188  [5.454 sec/step, loss=0.07557, avg_loss=0.07341]\n",
      "Step 536189  [5.446 sec/step, loss=0.07504, avg_loss=0.07342]\n",
      "Step 536190  [5.455 sec/step, loss=0.07519, avg_loss=0.07342]\n",
      "Step 536191  [5.472 sec/step, loss=0.07492, avg_loss=0.07343]\n",
      "Step 536192  [5.465 sec/step, loss=0.07400, avg_loss=0.07343]\n",
      "Step 536193  [5.441 sec/step, loss=0.07434, avg_loss=0.07345]\n",
      "Step 536194  [5.426 sec/step, loss=0.06980, avg_loss=0.07340]\n",
      "Step 536195  [5.427 sec/step, loss=0.07080, avg_loss=0.07339]\n",
      "Step 536196  [5.436 sec/step, loss=0.07159, avg_loss=0.07337]\n",
      "Step 536197  [5.429 sec/step, loss=0.07461, avg_loss=0.07337]\n",
      "Step 536198  [5.409 sec/step, loss=0.07345, avg_loss=0.07335]\n",
      "Step 536199  [5.425 sec/step, loss=0.07434, avg_loss=0.07334]\n",
      "Step 536200  [5.434 sec/step, loss=0.07346, avg_loss=0.07332]\n",
      "Writing summary at step: 536200\n",
      "Step 536201  [5.381 sec/step, loss=0.07137, avg_loss=0.07338]\n",
      "Generated 32 batches of size 32 in 2.746 sec\n",
      "Step 536202  [5.354 sec/step, loss=0.06566, avg_loss=0.07329]\n",
      "Step 536203  [5.413 sec/step, loss=0.06593, avg_loss=0.07324]\n",
      "Step 536204  [5.428 sec/step, loss=0.07412, avg_loss=0.07332]\n",
      "Step 536205  [5.411 sec/step, loss=0.07251, avg_loss=0.07329]\n",
      "Step 536206  [5.423 sec/step, loss=0.07582, avg_loss=0.07332]\n",
      "Step 536207  [5.417 sec/step, loss=0.07551, avg_loss=0.07334]\n",
      "Step 536208  [5.430 sec/step, loss=0.07385, avg_loss=0.07334]\n",
      "Step 536209  [5.428 sec/step, loss=0.07441, avg_loss=0.07333]\n",
      "Step 536210  [5.427 sec/step, loss=0.07535, avg_loss=0.07334]\n",
      "Step 536211  [5.452 sec/step, loss=0.07264, avg_loss=0.07335]\n",
      "Step 536212  [5.435 sec/step, loss=0.07268, avg_loss=0.07334]\n",
      "Step 536213  [5.457 sec/step, loss=0.07532, avg_loss=0.07338]\n",
      "Step 536214  [5.457 sec/step, loss=0.07429, avg_loss=0.07339]\n",
      "Step 536215  [5.464 sec/step, loss=0.07438, avg_loss=0.07341]\n",
      "Step 536216  [5.439 sec/step, loss=0.07192, avg_loss=0.07339]\n",
      "Step 536217  [5.416 sec/step, loss=0.06494, avg_loss=0.07329]\n",
      "Step 536218  [5.415 sec/step, loss=0.07437, avg_loss=0.07328]\n",
      "Step 536219  [5.423 sec/step, loss=0.07526, avg_loss=0.07330]\n",
      "Step 536220  [5.403 sec/step, loss=0.07354, avg_loss=0.07328]\n",
      "Step 536221  [5.431 sec/step, loss=0.07508, avg_loss=0.07329]\n",
      "Step 536222  [5.430 sec/step, loss=0.07604, avg_loss=0.07329]\n",
      "Step 536223  [5.432 sec/step, loss=0.07361, avg_loss=0.07328]\n",
      "Step 536224  [5.417 sec/step, loss=0.07590, avg_loss=0.07331]\n",
      "Step 536225  [5.419 sec/step, loss=0.07356, avg_loss=0.07330]\n",
      "Step 536226  [5.432 sec/step, loss=0.07499, avg_loss=0.07335]\n",
      "Step 536227  [5.419 sec/step, loss=0.07043, avg_loss=0.07330]\n",
      "Step 536228  [5.367 sec/step, loss=0.07496, avg_loss=0.07339]\n",
      "Step 536229  [5.344 sec/step, loss=0.07163, avg_loss=0.07338]\n",
      "Step 536230  [5.330 sec/step, loss=0.07411, avg_loss=0.07339]\n",
      "Step 536231  [5.379 sec/step, loss=0.06572, avg_loss=0.07329]\n",
      "Step 536232  [5.372 sec/step, loss=0.07540, avg_loss=0.07329]\n",
      "Step 536233  [5.389 sec/step, loss=0.07474, avg_loss=0.07329]\n",
      "Generated 32 batches of size 32 in 2.557 sec\n",
      "Step 536234  [5.389 sec/step, loss=0.07341, avg_loss=0.07329]\n",
      "Step 536235  [5.379 sec/step, loss=0.07302, avg_loss=0.07327]\n",
      "Step 536236  [5.377 sec/step, loss=0.07231, avg_loss=0.07326]\n",
      "Step 536237  [5.381 sec/step, loss=0.07404, avg_loss=0.07325]\n",
      "Step 536238  [5.409 sec/step, loss=0.07606, avg_loss=0.07337]\n",
      "Step 536239  [5.417 sec/step, loss=0.07300, avg_loss=0.07337]\n",
      "Step 536240  [5.412 sec/step, loss=0.07542, avg_loss=0.07338]\n",
      "Step 536241  [5.414 sec/step, loss=0.07356, avg_loss=0.07337]\n",
      "Step 536242  [5.403 sec/step, loss=0.07553, avg_loss=0.07340]\n",
      "Step 536243  [5.412 sec/step, loss=0.07311, avg_loss=0.07342]\n",
      "Step 536244  [5.421 sec/step, loss=0.07447, avg_loss=0.07345]\n",
      "Step 536245  [5.405 sec/step, loss=0.07325, avg_loss=0.07343]\n",
      "Step 536246  [5.391 sec/step, loss=0.06507, avg_loss=0.07336]\n",
      "Step 536247  [5.393 sec/step, loss=0.07440, avg_loss=0.07339]\n",
      "Step 536248  [5.407 sec/step, loss=0.07491, avg_loss=0.07343]\n",
      "Step 536249  [5.432 sec/step, loss=0.07541, avg_loss=0.07352]\n",
      "Step 536250  [5.443 sec/step, loss=0.07527, avg_loss=0.07353]\n",
      "Step 536251  [5.447 sec/step, loss=0.07417, avg_loss=0.07352]\n",
      "Step 536252  [5.450 sec/step, loss=0.07497, avg_loss=0.07352]\n",
      "Step 536253  [5.463 sec/step, loss=0.07373, avg_loss=0.07351]\n",
      "Step 536254  [5.466 sec/step, loss=0.07433, avg_loss=0.07352]\n",
      "Step 536255  [5.475 sec/step, loss=0.07082, avg_loss=0.07349]\n",
      "Step 536256  [5.471 sec/step, loss=0.07457, avg_loss=0.07350]\n",
      "Step 536257  [5.482 sec/step, loss=0.07507, avg_loss=0.07351]\n",
      "Step 536258  [5.507 sec/step, loss=0.07246, avg_loss=0.07348]\n",
      "Step 536259  [5.485 sec/step, loss=0.07389, avg_loss=0.07346]\n",
      "Step 536260  [5.470 sec/step, loss=0.06972, avg_loss=0.07340]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 536261  [5.475 sec/step, loss=0.07482, avg_loss=0.07343]\n",
      "Step 536262  [5.508 sec/step, loss=0.07325, avg_loss=0.07345]\n",
      "Step 536263  [5.486 sec/step, loss=0.07097, avg_loss=0.07340]\n",
      "Step 536264  [5.482 sec/step, loss=0.07335, avg_loss=0.07338]\n",
      "Step 536265  [5.493 sec/step, loss=0.07428, avg_loss=0.07339]\n",
      "Generated 32 batches of size 32 in 2.558 sec\n",
      "Step 536266  [5.498 sec/step, loss=0.07483, avg_loss=0.07342]\n",
      "Step 536267  [5.487 sec/step, loss=0.07450, avg_loss=0.07341]\n",
      "Step 536268  [5.475 sec/step, loss=0.07257, avg_loss=0.07340]\n",
      "Step 536269  [5.463 sec/step, loss=0.07245, avg_loss=0.07337]\n",
      "Step 536270  [5.474 sec/step, loss=0.07360, avg_loss=0.07337]\n",
      "Step 536271  [5.505 sec/step, loss=0.06604, avg_loss=0.07328]\n",
      "Step 536272  [5.505 sec/step, loss=0.07608, avg_loss=0.07328]\n",
      "Step 536273  [5.464 sec/step, loss=0.07521, avg_loss=0.07337]\n",
      "Step 536274  [5.458 sec/step, loss=0.07402, avg_loss=0.07338]\n",
      "Step 536275  [5.450 sec/step, loss=0.07255, avg_loss=0.07337]\n",
      "Step 536276  [5.439 sec/step, loss=0.07165, avg_loss=0.07334]\n",
      "Step 536277  [5.443 sec/step, loss=0.07147, avg_loss=0.07331]\n",
      "Step 536278  [5.445 sec/step, loss=0.07407, avg_loss=0.07335]\n",
      "Step 536279  [5.461 sec/step, loss=0.07382, avg_loss=0.07335]\n",
      "Step 536280  [5.439 sec/step, loss=0.06630, avg_loss=0.07326]\n",
      "Step 536281  [5.431 sec/step, loss=0.07306, avg_loss=0.07325]\n",
      "Step 536282  [5.442 sec/step, loss=0.07529, avg_loss=0.07327]\n",
      "Step 536283  [5.448 sec/step, loss=0.07644, avg_loss=0.07328]\n",
      "Step 536284  [5.440 sec/step, loss=0.07470, avg_loss=0.07329]\n",
      "Step 536285  [5.445 sec/step, loss=0.07200, avg_loss=0.07328]\n",
      "Step 536286  [5.419 sec/step, loss=0.07201, avg_loss=0.07324]\n",
      "Step 536287  [5.419 sec/step, loss=0.07382, avg_loss=0.07326]\n",
      "Step 536288  [5.432 sec/step, loss=0.07292, avg_loss=0.07324]\n",
      "Step 536289  [5.442 sec/step, loss=0.07603, avg_loss=0.07325]\n",
      "Step 536290  [5.419 sec/step, loss=0.07474, avg_loss=0.07324]\n",
      "Step 536291  [5.416 sec/step, loss=0.07430, avg_loss=0.07324]\n",
      "Step 536292  [5.426 sec/step, loss=0.07447, avg_loss=0.07324]\n",
      "Step 536293  [5.414 sec/step, loss=0.07325, avg_loss=0.07323]\n",
      "Step 536294  [5.429 sec/step, loss=0.07143, avg_loss=0.07325]\n",
      "Step 536295  [5.454 sec/step, loss=0.07607, avg_loss=0.07330]\n",
      "Step 536296  [5.461 sec/step, loss=0.07526, avg_loss=0.07333]\n",
      "Step 536297  [5.458 sec/step, loss=0.07372, avg_loss=0.07333]\n",
      "Generated 32 batches of size 32 in 2.642 sec\n",
      "Step 536298  [5.523 sec/step, loss=0.06590, avg_loss=0.07325]\n",
      "Step 536299  [5.507 sec/step, loss=0.07506, avg_loss=0.07326]\n",
      "Step 536300  [5.490 sec/step, loss=0.07192, avg_loss=0.07324]\n",
      "Writing summary at step: 536300\n",
      "Step 536301  [5.496 sec/step, loss=0.07533, avg_loss=0.07328]\n",
      "Step 536302  [5.510 sec/step, loss=0.07489, avg_loss=0.07337]\n",
      "Step 536303  [5.466 sec/step, loss=0.07593, avg_loss=0.07347]\n",
      "Step 536304  [5.460 sec/step, loss=0.07232, avg_loss=0.07346]\n",
      "Step 536305  [5.453 sec/step, loss=0.07137, avg_loss=0.07344]\n",
      "Step 536306  [5.435 sec/step, loss=0.07307, avg_loss=0.07342]\n",
      "Step 536307  [5.421 sec/step, loss=0.07361, avg_loss=0.07340]\n",
      "Step 536308  [5.429 sec/step, loss=0.07357, avg_loss=0.07340]\n",
      "Step 536309  [5.441 sec/step, loss=0.07556, avg_loss=0.07341]\n",
      "Step 536310  [5.450 sec/step, loss=0.07373, avg_loss=0.07339]\n",
      "Step 536311  [5.409 sec/step, loss=0.07117, avg_loss=0.07338]\n",
      "Step 536312  [5.416 sec/step, loss=0.07478, avg_loss=0.07340]\n",
      "Step 536313  [5.412 sec/step, loss=0.07548, avg_loss=0.07340]\n",
      "Step 536314  [5.411 sec/step, loss=0.07221, avg_loss=0.07338]\n",
      "Step 536315  [5.409 sec/step, loss=0.07528, avg_loss=0.07339]\n",
      "Step 536316  [5.434 sec/step, loss=0.07540, avg_loss=0.07342]\n",
      "Step 536317  [5.455 sec/step, loss=0.07172, avg_loss=0.07349]\n",
      "Step 536318  [5.456 sec/step, loss=0.07472, avg_loss=0.07349]\n",
      "Step 536319  [5.444 sec/step, loss=0.07338, avg_loss=0.07347]\n",
      "Step 536320  [5.464 sec/step, loss=0.07630, avg_loss=0.07350]\n",
      "Step 536321  [5.444 sec/step, loss=0.07440, avg_loss=0.07349]\n",
      "Step 536322  [5.424 sec/step, loss=0.07065, avg_loss=0.07344]\n",
      "Step 536323  [5.428 sec/step, loss=0.07413, avg_loss=0.07345]\n",
      "Step 536324  [5.399 sec/step, loss=0.06526, avg_loss=0.07334]\n",
      "Step 536325  [5.418 sec/step, loss=0.07330, avg_loss=0.07334]\n",
      "Step 536326  [5.408 sec/step, loss=0.07350, avg_loss=0.07332]\n",
      "Step 536327  [5.412 sec/step, loss=0.07491, avg_loss=0.07337]\n",
      "Step 536328  [5.464 sec/step, loss=0.06604, avg_loss=0.07328]\n",
      "Generated 32 batches of size 32 in 2.403 sec\n",
      "Step 536329  [5.485 sec/step, loss=0.07452, avg_loss=0.07331]\n",
      "Step 536330  [5.512 sec/step, loss=0.07333, avg_loss=0.07330]\n",
      "Step 536331  [5.476 sec/step, loss=0.07358, avg_loss=0.07338]\n",
      "Step 536332  [5.478 sec/step, loss=0.07492, avg_loss=0.07337]\n",
      "Step 536333  [5.463 sec/step, loss=0.07395, avg_loss=0.07336]\n",
      "Step 536334  [5.465 sec/step, loss=0.07137, avg_loss=0.07334]\n",
      "Step 536335  [5.471 sec/step, loss=0.07409, avg_loss=0.07336]\n",
      "Step 536336  [5.476 sec/step, loss=0.07427, avg_loss=0.07337]\n",
      "Step 536337  [5.475 sec/step, loss=0.07516, avg_loss=0.07339]\n",
      "Step 536338  [5.473 sec/step, loss=0.07402, avg_loss=0.07337]\n",
      "Step 536339  [5.457 sec/step, loss=0.07100, avg_loss=0.07335]\n",
      "Step 536340  [5.466 sec/step, loss=0.07415, avg_loss=0.07333]\n",
      "Step 536341  [5.452 sec/step, loss=0.07478, avg_loss=0.07334]\n",
      "Step 536342  [5.432 sec/step, loss=0.07365, avg_loss=0.07333]\n",
      "Step 536343  [5.448 sec/step, loss=0.07462, avg_loss=0.07334]\n",
      "Step 536344  [5.428 sec/step, loss=0.06702, avg_loss=0.07327]\n",
      "Step 536345  [5.436 sec/step, loss=0.07538, avg_loss=0.07329]\n",
      "Step 536346  [5.500 sec/step, loss=0.06452, avg_loss=0.07328]\n",
      "Step 536347  [5.512 sec/step, loss=0.07516, avg_loss=0.07329]\n",
      "Step 536348  [5.535 sec/step, loss=0.07328, avg_loss=0.07327]\n",
      "Step 536349  [5.525 sec/step, loss=0.07109, avg_loss=0.07323]\n",
      "Step 536350  [5.518 sec/step, loss=0.07460, avg_loss=0.07322]\n",
      "Step 536351  [5.507 sec/step, loss=0.07284, avg_loss=0.07321]\n",
      "Step 536352  [5.505 sec/step, loss=0.07593, avg_loss=0.07322]\n",
      "Step 536353  [5.499 sec/step, loss=0.07387, avg_loss=0.07322]\n",
      "Step 536354  [5.503 sec/step, loss=0.07377, avg_loss=0.07322]\n",
      "Step 536355  [5.506 sec/step, loss=0.07498, avg_loss=0.07326]\n",
      "Step 536356  [5.514 sec/step, loss=0.07586, avg_loss=0.07327]\n",
      "Step 536357  [5.500 sec/step, loss=0.07334, avg_loss=0.07325]\n",
      "Step 536358  [5.482 sec/step, loss=0.07421, avg_loss=0.07327]\n",
      "Step 536359  [5.497 sec/step, loss=0.07348, avg_loss=0.07327]\n",
      "Step 536360  [5.509 sec/step, loss=0.07431, avg_loss=0.07331]\n",
      "Generated 32 batches of size 32 in 2.490 sec\n",
      "Step 536361  [5.513 sec/step, loss=0.07239, avg_loss=0.07329]\n",
      "Step 536362  [5.502 sec/step, loss=0.07527, avg_loss=0.07331]\n",
      "Step 536363  [5.523 sec/step, loss=0.07605, avg_loss=0.07336]\n",
      "Step 536364  [5.516 sec/step, loss=0.07173, avg_loss=0.07334]\n",
      "Step 536365  [5.497 sec/step, loss=0.07499, avg_loss=0.07335]\n",
      "Step 536366  [5.506 sec/step, loss=0.07445, avg_loss=0.07335]\n",
      "Step 536367  [5.497 sec/step, loss=0.07273, avg_loss=0.07333]\n",
      "Step 536368  [5.504 sec/step, loss=0.07420, avg_loss=0.07334]\n",
      "Step 536369  [5.496 sec/step, loss=0.07338, avg_loss=0.07335]\n",
      "Step 536370  [5.496 sec/step, loss=0.07463, avg_loss=0.07336]\n",
      "Step 536371  [5.449 sec/step, loss=0.07393, avg_loss=0.07344]\n",
      "Step 536372  [5.433 sec/step, loss=0.07342, avg_loss=0.07342]\n",
      "Step 536373  [5.419 sec/step, loss=0.07453, avg_loss=0.07341]\n",
      "Step 536374  [5.403 sec/step, loss=0.06491, avg_loss=0.07332]\n",
      "Step 536375  [5.383 sec/step, loss=0.07444, avg_loss=0.07334]\n",
      "Step 536376  [5.394 sec/step, loss=0.07560, avg_loss=0.07338]\n",
      "Step 536377  [5.407 sec/step, loss=0.07245, avg_loss=0.07339]\n",
      "Step 536378  [5.401 sec/step, loss=0.07353, avg_loss=0.07338]\n",
      "Step 536379  [5.438 sec/step, loss=0.06714, avg_loss=0.07331]\n",
      "Step 536380  [5.470 sec/step, loss=0.07602, avg_loss=0.07341]\n",
      "Step 536381  [5.486 sec/step, loss=0.07581, avg_loss=0.07344]\n",
      "Step 536382  [5.474 sec/step, loss=0.07084, avg_loss=0.07340]\n",
      "Step 536383  [5.470 sec/step, loss=0.07414, avg_loss=0.07337]\n",
      "Step 536384  [5.469 sec/step, loss=0.07269, avg_loss=0.07335]\n",
      "Step 536385  [5.473 sec/step, loss=0.07512, avg_loss=0.07338]\n",
      "Step 536386  [5.496 sec/step, loss=0.07573, avg_loss=0.07342]\n",
      "Step 536387  [5.508 sec/step, loss=0.07470, avg_loss=0.07343]\n",
      "Step 536388  [5.493 sec/step, loss=0.07359, avg_loss=0.07344]\n",
      "Step 536389  [5.486 sec/step, loss=0.07377, avg_loss=0.07341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 536390  [5.490 sec/step, loss=0.07366, avg_loss=0.07340]\n",
      "Step 536391  [5.479 sec/step, loss=0.07374, avg_loss=0.07340]\n",
      "Step 536392  [5.477 sec/step, loss=0.07468, avg_loss=0.07340]\n",
      "Generated 32 batches of size 32 in 2.625 sec\n",
      "Step 536393  [5.491 sec/step, loss=0.07472, avg_loss=0.07341]\n",
      "Step 536394  [5.490 sec/step, loss=0.07494, avg_loss=0.07345]\n",
      "Step 536395  [5.490 sec/step, loss=0.07355, avg_loss=0.07342]\n",
      "Step 536396  [5.491 sec/step, loss=0.07501, avg_loss=0.07342]\n",
      "Step 536397  [5.479 sec/step, loss=0.07170, avg_loss=0.07340]\n",
      "Step 536398  [5.424 sec/step, loss=0.07514, avg_loss=0.07349]\n",
      "Step 536399  [5.406 sec/step, loss=0.07411, avg_loss=0.07348]\n",
      "Step 536400  [5.437 sec/step, loss=0.07333, avg_loss=0.07350]\n",
      "Writing summary at step: 536400\n",
      "Step 536401  [5.483 sec/step, loss=0.06535, avg_loss=0.07340]\n",
      "Step 536402  [5.470 sec/step, loss=0.07115, avg_loss=0.07336]\n",
      "Step 536403  [5.468 sec/step, loss=0.07444, avg_loss=0.07335]\n",
      "Step 536404  [5.486 sec/step, loss=0.07356, avg_loss=0.07336]\n",
      "Step 536405  [5.515 sec/step, loss=0.07555, avg_loss=0.07340]\n",
      "Step 536406  [5.542 sec/step, loss=0.07306, avg_loss=0.07340]\n",
      "Step 536407  [5.559 sec/step, loss=0.07332, avg_loss=0.07340]\n",
      "Step 536408  [5.537 sec/step, loss=0.07333, avg_loss=0.07339]\n",
      "Step 536409  [5.513 sec/step, loss=0.07199, avg_loss=0.07336]\n",
      "Step 536410  [5.530 sec/step, loss=0.07270, avg_loss=0.07335]\n",
      "Step 536411  [5.544 sec/step, loss=0.07409, avg_loss=0.07338]\n",
      "Step 536412  [5.536 sec/step, loss=0.07260, avg_loss=0.07336]\n",
      "Step 536413  [5.519 sec/step, loss=0.07009, avg_loss=0.07330]\n",
      "Step 536414  [5.538 sec/step, loss=0.07556, avg_loss=0.07334]\n",
      "Step 536415  [5.535 sec/step, loss=0.07305, avg_loss=0.07331]\n",
      "Step 536416  [5.527 sec/step, loss=0.07429, avg_loss=0.07330]\n",
      "Step 536417  [5.523 sec/step, loss=0.07479, avg_loss=0.07333]\n",
      "Step 536418  [5.518 sec/step, loss=0.07015, avg_loss=0.07329]\n",
      "Step 536419  [5.532 sec/step, loss=0.07420, avg_loss=0.07330]\n",
      "Step 536420  [5.527 sec/step, loss=0.07569, avg_loss=0.07329]\n",
      "Step 536421  [5.541 sec/step, loss=0.07428, avg_loss=0.07329]\n",
      "Step 536422  [5.534 sec/step, loss=0.06780, avg_loss=0.07326]\n",
      "Step 536423  [5.523 sec/step, loss=0.07338, avg_loss=0.07325]\n",
      "Generated 32 batches of size 32 in 2.428 sec\n",
      "Step 536424  [5.550 sec/step, loss=0.07379, avg_loss=0.07334]\n",
      "Step 536425  [5.543 sec/step, loss=0.07537, avg_loss=0.07336]\n",
      "Step 536426  [5.553 sec/step, loss=0.07578, avg_loss=0.07338]\n",
      "Step 536427  [5.560 sec/step, loss=0.07449, avg_loss=0.07338]\n",
      "Step 536428  [5.515 sec/step, loss=0.07424, avg_loss=0.07346]\n",
      "Step 536429  [5.504 sec/step, loss=0.07472, avg_loss=0.07346]\n",
      "Step 536430  [5.478 sec/step, loss=0.07469, avg_loss=0.07347]\n",
      "Step 536431  [5.457 sec/step, loss=0.07319, avg_loss=0.07347]\n",
      "Step 536432  [5.456 sec/step, loss=0.07554, avg_loss=0.07348]\n",
      "Step 536433  [5.460 sec/step, loss=0.07439, avg_loss=0.07348]\n",
      "Step 536434  [5.488 sec/step, loss=0.07220, avg_loss=0.07349]\n",
      "Step 536435  [5.489 sec/step, loss=0.07544, avg_loss=0.07350]\n",
      "Step 536436  [5.479 sec/step, loss=0.07333, avg_loss=0.07349]\n",
      "Step 536437  [5.457 sec/step, loss=0.06439, avg_loss=0.07339]\n",
      "Step 536438  [5.443 sec/step, loss=0.07337, avg_loss=0.07338]\n",
      "Step 536439  [5.462 sec/step, loss=0.07548, avg_loss=0.07342]\n",
      "Step 536440  [5.455 sec/step, loss=0.07515, avg_loss=0.07343]\n",
      "Step 536441  [5.459 sec/step, loss=0.07471, avg_loss=0.07343]\n",
      "Step 536442  [5.475 sec/step, loss=0.07477, avg_loss=0.07344]\n",
      "Step 536443  [5.475 sec/step, loss=0.07576, avg_loss=0.07346]\n",
      "Step 536444  [5.500 sec/step, loss=0.07585, avg_loss=0.07354]\n",
      "Step 536445  [5.504 sec/step, loss=0.07471, avg_loss=0.07354]\n",
      "Step 536446  [5.456 sec/step, loss=0.07405, avg_loss=0.07363]\n",
      "Step 536447  [5.436 sec/step, loss=0.07323, avg_loss=0.07361]\n",
      "Step 536448  [5.423 sec/step, loss=0.07316, avg_loss=0.07361]\n",
      "Step 536449  [5.423 sec/step, loss=0.07484, avg_loss=0.07365]\n",
      "Step 536450  [5.431 sec/step, loss=0.07545, avg_loss=0.07366]\n",
      "Step 536451  [5.436 sec/step, loss=0.07416, avg_loss=0.07367]\n",
      "Step 536452  [5.428 sec/step, loss=0.07176, avg_loss=0.07363]\n",
      "Step 536453  [5.410 sec/step, loss=0.07125, avg_loss=0.07360]\n",
      "Step 536454  [5.407 sec/step, loss=0.07458, avg_loss=0.07361]\n",
      "Step 536455  [5.419 sec/step, loss=0.07552, avg_loss=0.07362]\n",
      "Generated 32 batches of size 32 in 2.614 sec\n",
      "Step 536456  [5.466 sec/step, loss=0.06604, avg_loss=0.07352]\n",
      "Step 536457  [5.461 sec/step, loss=0.07275, avg_loss=0.07351]\n",
      "Step 536458  [5.462 sec/step, loss=0.07584, avg_loss=0.07353]\n",
      "Step 536459  [5.454 sec/step, loss=0.07267, avg_loss=0.07352]\n",
      "Step 536460  [5.443 sec/step, loss=0.07367, avg_loss=0.07351]\n",
      "Step 536461  [5.428 sec/step, loss=0.07082, avg_loss=0.07350]\n",
      "Step 536462  [5.428 sec/step, loss=0.07442, avg_loss=0.07349]\n",
      "Step 536463  [5.439 sec/step, loss=0.07553, avg_loss=0.07349]\n",
      "Step 536464  [5.455 sec/step, loss=0.07323, avg_loss=0.07350]\n",
      "Step 536465  [5.469 sec/step, loss=0.07308, avg_loss=0.07348]\n",
      "Step 536466  [5.459 sec/step, loss=0.07491, avg_loss=0.07349]\n",
      "Step 536467  [5.464 sec/step, loss=0.07477, avg_loss=0.07351]\n",
      "Step 536468  [5.461 sec/step, loss=0.07449, avg_loss=0.07351]\n",
      "Step 536469  [5.458 sec/step, loss=0.07067, avg_loss=0.07348]\n",
      "Step 536470  [5.450 sec/step, loss=0.07180, avg_loss=0.07345]\n",
      "Step 536471  [5.460 sec/step, loss=0.07480, avg_loss=0.07346]\n",
      "Step 536472  [5.519 sec/step, loss=0.06596, avg_loss=0.07339]\n",
      "Step 536473  [5.532 sec/step, loss=0.07513, avg_loss=0.07339]\n",
      "Step 536474  [5.554 sec/step, loss=0.07259, avg_loss=0.07347]\n",
      "Step 536475  [5.551 sec/step, loss=0.07504, avg_loss=0.07348]\n",
      "Step 536476  [5.543 sec/step, loss=0.07418, avg_loss=0.07346]\n",
      "Step 536477  [5.533 sec/step, loss=0.07450, avg_loss=0.07348]\n",
      "Step 536478  [5.540 sec/step, loss=0.07533, avg_loss=0.07350]\n",
      "Step 536479  [5.480 sec/step, loss=0.07389, avg_loss=0.07357]\n",
      "Step 536480  [5.448 sec/step, loss=0.06541, avg_loss=0.07346]\n",
      "Step 536481  [5.437 sec/step, loss=0.07317, avg_loss=0.07344]\n",
      "Step 536482  [5.435 sec/step, loss=0.07359, avg_loss=0.07346]\n",
      "Step 536483  [5.433 sec/step, loss=0.07581, avg_loss=0.07348]\n",
      "Step 536484  [5.436 sec/step, loss=0.07036, avg_loss=0.07346]\n",
      "Step 536485  [5.436 sec/step, loss=0.07485, avg_loss=0.07345]\n",
      "Step 536486  [5.432 sec/step, loss=0.07673, avg_loss=0.07346]\n",
      "Step 536487  [5.426 sec/step, loss=0.07290, avg_loss=0.07345]\n",
      "Generated 32 batches of size 32 in 2.419 sec\n",
      "Step 536488  [5.432 sec/step, loss=0.07544, avg_loss=0.07347]\n",
      "Step 536489  [5.449 sec/step, loss=0.07513, avg_loss=0.07348]\n",
      "Step 536490  [5.435 sec/step, loss=0.07143, avg_loss=0.07346]\n",
      "Step 536491  [5.434 sec/step, loss=0.07420, avg_loss=0.07346]\n",
      "Step 536492  [5.444 sec/step, loss=0.07604, avg_loss=0.07347]\n",
      "Step 536493  [5.434 sec/step, loss=0.07342, avg_loss=0.07346]\n",
      "Step 536494  [5.436 sec/step, loss=0.07531, avg_loss=0.07347]\n",
      "Step 536495  [5.452 sec/step, loss=0.07360, avg_loss=0.07347]\n",
      "Step 536496  [5.453 sec/step, loss=0.07615, avg_loss=0.07348]\n",
      "Step 536497  [5.453 sec/step, loss=0.07139, avg_loss=0.07347]\n",
      "Step 536498  [5.463 sec/step, loss=0.07607, avg_loss=0.07348]\n",
      "Step 536499  [5.457 sec/step, loss=0.06763, avg_loss=0.07342]\n",
      "Step 536500  [5.481 sec/step, loss=0.06772, avg_loss=0.07336]\n",
      "Writing summary at step: 536500\n",
      "Step 536501  [5.446 sec/step, loss=0.07372, avg_loss=0.07345]\n",
      "Step 536502  [5.447 sec/step, loss=0.07437, avg_loss=0.07348]\n",
      "Step 536503  [5.450 sec/step, loss=0.07679, avg_loss=0.07350]\n",
      "Step 536504  [5.430 sec/step, loss=0.07194, avg_loss=0.07349]\n",
      "Step 536505  [5.426 sec/step, loss=0.07512, avg_loss=0.07348]\n",
      "Step 536506  [5.411 sec/step, loss=0.07302, avg_loss=0.07348]\n",
      "Step 536507  [5.400 sec/step, loss=0.07558, avg_loss=0.07350]\n",
      "Step 536508  [5.400 sec/step, loss=0.07364, avg_loss=0.07351]\n",
      "Step 536509  [5.427 sec/step, loss=0.07528, avg_loss=0.07354]\n",
      "Step 536510  [5.404 sec/step, loss=0.07527, avg_loss=0.07357]\n",
      "Step 536511  [5.413 sec/step, loss=0.07634, avg_loss=0.07359]\n",
      "Step 536512  [5.411 sec/step, loss=0.07457, avg_loss=0.07361]\n",
      "Step 536513  [5.415 sec/step, loss=0.07447, avg_loss=0.07365]\n",
      "Step 536514  [5.405 sec/step, loss=0.07580, avg_loss=0.07365]\n",
      "Step 536515  [5.424 sec/step, loss=0.07380, avg_loss=0.07366]\n",
      "Step 536516  [5.422 sec/step, loss=0.07503, avg_loss=0.07367]\n",
      "Step 536517  [5.421 sec/step, loss=0.07582, avg_loss=0.07368]\n",
      "Step 536518  [5.450 sec/step, loss=0.07382, avg_loss=0.07372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 32 batches of size 32 in 2.535 sec\n",
      "Step 536519  [5.450 sec/step, loss=0.07519, avg_loss=0.07373]\n",
      "Step 536520  [5.444 sec/step, loss=0.07583, avg_loss=0.07373]\n",
      "Step 536521  [5.440 sec/step, loss=0.07621, avg_loss=0.07375]\n",
      "Step 536522  [5.467 sec/step, loss=0.07604, avg_loss=0.07383]\n",
      "Step 536523  [5.472 sec/step, loss=0.07165, avg_loss=0.07381]\n",
      "Step 536524  [5.454 sec/step, loss=0.07443, avg_loss=0.07382]\n",
      "Step 536525  [5.445 sec/step, loss=0.07637, avg_loss=0.07383]\n",
      "Step 536526  [5.444 sec/step, loss=0.07396, avg_loss=0.07381]\n",
      "Step 536527  [5.456 sec/step, loss=0.07721, avg_loss=0.07384]\n",
      "Step 536528  [5.444 sec/step, loss=0.07254, avg_loss=0.07382]\n",
      "Step 536529  [5.427 sec/step, loss=0.06590, avg_loss=0.07373]\n",
      "Step 536530  [5.425 sec/step, loss=0.07231, avg_loss=0.07371]\n",
      "Step 536531  [5.427 sec/step, loss=0.07214, avg_loss=0.07370]\n",
      "Step 536532  [5.436 sec/step, loss=0.07494, avg_loss=0.07369]\n",
      "Step 536533  [5.421 sec/step, loss=0.07397, avg_loss=0.07369]\n",
      "Step 536534  [5.398 sec/step, loss=0.07486, avg_loss=0.07371]\n",
      "Step 536535  [5.405 sec/step, loss=0.07588, avg_loss=0.07372]\n",
      "Step 536536  [5.413 sec/step, loss=0.07222, avg_loss=0.07371]\n",
      "Step 536537  [5.429 sec/step, loss=0.07467, avg_loss=0.07381]\n",
      "Step 536538  [5.427 sec/step, loss=0.07337, avg_loss=0.07381]\n",
      "Step 536539  [5.424 sec/step, loss=0.07304, avg_loss=0.07379]\n",
      "Step 536540  [5.434 sec/step, loss=0.07499, avg_loss=0.07378]\n",
      "Step 536541  [5.421 sec/step, loss=0.07119, avg_loss=0.07375]\n",
      "Step 536542  [5.441 sec/step, loss=0.07492, avg_loss=0.07375]\n",
      "Step 536543  [5.442 sec/step, loss=0.07572, avg_loss=0.07375]\n",
      "Step 536544  [5.430 sec/step, loss=0.07522, avg_loss=0.07374]\n",
      "Step 536545  [5.433 sec/step, loss=0.07604, avg_loss=0.07376]\n",
      "Step 536546  [5.429 sec/step, loss=0.07362, avg_loss=0.07375]\n",
      "Step 536547  [5.428 sec/step, loss=0.07425, avg_loss=0.07376]\n",
      "Step 536548  [5.422 sec/step, loss=0.07293, avg_loss=0.07376]\n",
      "Step 536549  [5.418 sec/step, loss=0.07351, avg_loss=0.07375]\n",
      "Step 536550  [5.418 sec/step, loss=0.07553, avg_loss=0.07375]\n",
      "Generated 32 batches of size 32 in 2.395 sec\n",
      "Step 536551  [5.433 sec/step, loss=0.07562, avg_loss=0.07376]\n",
      "Step 536552  [5.479 sec/step, loss=0.06577, avg_loss=0.07370]\n",
      "Step 536553  [5.494 sec/step, loss=0.07409, avg_loss=0.07373]\n",
      "Step 536554  [5.498 sec/step, loss=0.07570, avg_loss=0.07374]\n",
      "Step 536555  [5.490 sec/step, loss=0.07511, avg_loss=0.07374]\n",
      "Step 536556  [5.435 sec/step, loss=0.07528, avg_loss=0.07383]\n",
      "Step 536557  [5.433 sec/step, loss=0.07027, avg_loss=0.07381]\n",
      "Step 536558  [5.427 sec/step, loss=0.07598, avg_loss=0.07381]\n",
      "Step 536559  [5.439 sec/step, loss=0.07621, avg_loss=0.07384]\n",
      "Step 536560  [5.463 sec/step, loss=0.07567, avg_loss=0.07386]\n",
      "Step 536561  [5.463 sec/step, loss=0.07139, avg_loss=0.07387]\n",
      "Step 536562  [5.440 sec/step, loss=0.07224, avg_loss=0.07385]\n",
      "Step 536563  [5.430 sec/step, loss=0.07581, avg_loss=0.07385]\n",
      "Step 536564  [5.425 sec/step, loss=0.07356, avg_loss=0.07385]\n",
      "Step 536565  [5.465 sec/step, loss=0.06664, avg_loss=0.07379]\n",
      "Step 536566  [5.464 sec/step, loss=0.07480, avg_loss=0.07379]\n",
      "Step 536567  [5.486 sec/step, loss=0.07399, avg_loss=0.07378]\n",
      "Step 536568  [5.491 sec/step, loss=0.07456, avg_loss=0.07378]\n",
      "Step 536569  [5.499 sec/step, loss=0.07310, avg_loss=0.07380]\n",
      "Step 536570  [5.502 sec/step, loss=0.07491, avg_loss=0.07384]\n",
      "Step 536571  [5.487 sec/step, loss=0.07433, avg_loss=0.07383]\n",
      "Step 536572  [5.429 sec/step, loss=0.07411, avg_loss=0.07391]\n",
      "Step 536573  [5.412 sec/step, loss=0.07294, avg_loss=0.07389]\n",
      "Step 536574  [5.421 sec/step, loss=0.07636, avg_loss=0.07393]\n",
      "Step 536575  [5.426 sec/step, loss=0.07343, avg_loss=0.07391]\n",
      "Step 536576  [5.429 sec/step, loss=0.07492, avg_loss=0.07392]\n",
      "Step 536577  [5.433 sec/step, loss=0.07429, avg_loss=0.07392]\n",
      "Step 536578  [5.440 sec/step, loss=0.07544, avg_loss=0.07392]\n",
      "Step 536579  [5.462 sec/step, loss=0.07572, avg_loss=0.07394]\n",
      "Step 536580  [5.483 sec/step, loss=0.07431, avg_loss=0.07403]\n",
      "Step 536581  [5.490 sec/step, loss=0.07625, avg_loss=0.07406]\n",
      "Step 536582  [5.505 sec/step, loss=0.07407, avg_loss=0.07406]\n",
      "Generated 32 batches of size 32 in 2.844 sec\n",
      "Step 536583  [5.485 sec/step, loss=0.06663, avg_loss=0.07397]\n",
      "Step 536584  [5.485 sec/step, loss=0.07119, avg_loss=0.07398]\n",
      "Step 536585  [5.481 sec/step, loss=0.07462, avg_loss=0.07398]\n",
      "Step 536586  [5.475 sec/step, loss=0.07236, avg_loss=0.07393]\n",
      "Step 536587  [5.477 sec/step, loss=0.07481, avg_loss=0.07395]\n",
      "Step 536588  [5.467 sec/step, loss=0.07495, avg_loss=0.07395]\n",
      "Step 536589  [5.477 sec/step, loss=0.07299, avg_loss=0.07392]\n",
      "Step 536590  [5.481 sec/step, loss=0.07386, avg_loss=0.07395]\n",
      "Step 536591  [5.486 sec/step, loss=0.07336, avg_loss=0.07394]\n",
      "Step 536592  [5.489 sec/step, loss=0.07559, avg_loss=0.07394]\n",
      "Step 536593  [5.505 sec/step, loss=0.07555, avg_loss=0.07396]\n",
      "Step 536594  [5.497 sec/step, loss=0.07361, avg_loss=0.07394]\n",
      "Step 536595  [5.468 sec/step, loss=0.07268, avg_loss=0.07393]\n",
      "Step 536596  [5.451 sec/step, loss=0.07354, avg_loss=0.07390]\n",
      "Step 536597  [5.460 sec/step, loss=0.07334, avg_loss=0.07392]\n",
      "Step 536598  [5.432 sec/step, loss=0.06626, avg_loss=0.07383]\n",
      "Step 536599  [5.445 sec/step, loss=0.07485, avg_loss=0.07390]\n",
      "Step 536600  [5.409 sec/step, loss=0.07521, avg_loss=0.07397]\n",
      "Writing summary at step: 536600\n",
      "Step 536601  [5.395 sec/step, loss=0.07479, avg_loss=0.07398]\n",
      "Step 536602  [5.409 sec/step, loss=0.07460, avg_loss=0.07399]\n",
      "Step 536603  [5.402 sec/step, loss=0.07502, avg_loss=0.07397]\n",
      "Step 536604  [5.428 sec/step, loss=0.07446, avg_loss=0.07399]\n",
      "Step 536605  [5.424 sec/step, loss=0.07359, avg_loss=0.07398]\n",
      "Step 536606  [5.432 sec/step, loss=0.07323, avg_loss=0.07398]\n",
      "Step 536607  [5.437 sec/step, loss=0.07528, avg_loss=0.07398]\n",
      "Step 536608  [5.492 sec/step, loss=0.06565, avg_loss=0.07390]\n",
      "Step 536609  [5.475 sec/step, loss=0.07462, avg_loss=0.07389]\n",
      "Step 536610  [5.474 sec/step, loss=0.07143, avg_loss=0.07385]\n",
      "Step 536611  [5.465 sec/step, loss=0.07418, avg_loss=0.07383]\n",
      "Step 536612  [5.463 sec/step, loss=0.07095, avg_loss=0.07379]\n",
      "Step 536613  [5.468 sec/step, loss=0.07196, avg_loss=0.07377]\n",
      "Generated 32 batches of size 32 in 2.417 sec\n",
      "Step 536614  [5.478 sec/step, loss=0.07312, avg_loss=0.07374]\n",
      "Step 536615  [5.485 sec/step, loss=0.07450, avg_loss=0.07375]\n",
      "Step 536616  [5.490 sec/step, loss=0.07375, avg_loss=0.07374]\n",
      "Step 536617  [5.481 sec/step, loss=0.07156, avg_loss=0.07369]\n",
      "Step 536618  [5.466 sec/step, loss=0.07597, avg_loss=0.07372]\n",
      "Step 536619  [5.464 sec/step, loss=0.07388, avg_loss=0.07370]\n",
      "Step 536620  [5.458 sec/step, loss=0.07029, avg_loss=0.07365]\n",
      "Step 536621  [5.458 sec/step, loss=0.07612, avg_loss=0.07365]\n",
      "Step 536622  [5.435 sec/step, loss=0.07196, avg_loss=0.07361]\n",
      "Step 536623  [5.443 sec/step, loss=0.07539, avg_loss=0.07364]\n",
      "Step 536624  [5.462 sec/step, loss=0.07566, avg_loss=0.07366]\n",
      "Step 536625  [5.449 sec/step, loss=0.06999, avg_loss=0.07359]\n",
      "Step 536626  [5.447 sec/step, loss=0.07389, avg_loss=0.07359]\n",
      "Step 536627  [5.433 sec/step, loss=0.07481, avg_loss=0.07357]\n",
      "Step 536628  [5.453 sec/step, loss=0.07574, avg_loss=0.07360]\n",
      "Step 536629  [5.497 sec/step, loss=0.07347, avg_loss=0.07367]\n",
      "Step 536630  [5.506 sec/step, loss=0.07457, avg_loss=0.07370]\n",
      "Step 536631  [5.497 sec/step, loss=0.07167, avg_loss=0.07369]\n",
      "Step 536632  [5.489 sec/step, loss=0.07572, avg_loss=0.07370]\n",
      "Step 536633  [5.504 sec/step, loss=0.07466, avg_loss=0.07371]\n",
      "Step 536634  [5.551 sec/step, loss=0.06646, avg_loss=0.07362]\n",
      "Step 536635  [5.543 sec/step, loss=0.07514, avg_loss=0.07362]\n",
      "Step 536636  [5.543 sec/step, loss=0.07096, avg_loss=0.07360]\n",
      "Step 536637  [5.540 sec/step, loss=0.07116, avg_loss=0.07357]\n",
      "Step 536638  [5.553 sec/step, loss=0.07502, avg_loss=0.07358]\n",
      "Step 536639  [5.544 sec/step, loss=0.07290, avg_loss=0.07358]\n",
      "Step 536640  [5.523 sec/step, loss=0.07453, avg_loss=0.07358]\n",
      "Step 536641  [5.540 sec/step, loss=0.07412, avg_loss=0.07361]\n",
      "Step 536642  [5.535 sec/step, loss=0.07382, avg_loss=0.07360]\n",
      "Step 536643  [5.516 sec/step, loss=0.07355, avg_loss=0.07358]\n",
      "Step 536644  [5.534 sec/step, loss=0.07526, avg_loss=0.07358]\n",
      "Step 536645  [5.516 sec/step, loss=0.07391, avg_loss=0.07355]\n",
      "Generated 32 batches of size 32 in 2.876 sec\n",
      "Step 536646  [5.508 sec/step, loss=0.06500, avg_loss=0.07347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 536647  [5.522 sec/step, loss=0.07319, avg_loss=0.07346]\n",
      "Step 536648  [5.529 sec/step, loss=0.07544, avg_loss=0.07348]\n",
      "Step 536649  [5.534 sec/step, loss=0.07484, avg_loss=0.07350]\n",
      "Step 536650  [5.522 sec/step, loss=0.07505, avg_loss=0.07349]\n",
      "Step 536651  [5.500 sec/step, loss=0.07152, avg_loss=0.07345]\n",
      "Step 536652  [5.446 sec/step, loss=0.07142, avg_loss=0.07351]\n",
      "Step 536653  [5.449 sec/step, loss=0.07464, avg_loss=0.07351]\n",
      "Step 536654  [5.447 sec/step, loss=0.07396, avg_loss=0.07349]\n",
      "Step 536655  [5.454 sec/step, loss=0.07555, avg_loss=0.07350]\n",
      "Step 536656  [5.462 sec/step, loss=0.07534, avg_loss=0.07350]\n",
      "Step 536657  [5.483 sec/step, loss=0.07557, avg_loss=0.07355]\n",
      "Step 536658  [5.461 sec/step, loss=0.06565, avg_loss=0.07345]\n",
      "Step 536659  [5.464 sec/step, loss=0.07574, avg_loss=0.07345]\n",
      "Step 536660  [5.478 sec/step, loss=0.07182, avg_loss=0.07341]\n",
      "Step 536661  [5.487 sec/step, loss=0.07482, avg_loss=0.07344]\n",
      "Step 536662  [5.498 sec/step, loss=0.07487, avg_loss=0.07347]\n",
      "Step 536663  [5.492 sec/step, loss=0.07497, avg_loss=0.07346]\n",
      "Step 536664  [5.487 sec/step, loss=0.07018, avg_loss=0.07343]\n",
      "Step 536665  [5.434 sec/step, loss=0.07340, avg_loss=0.07349]\n",
      "Step 536666  [5.433 sec/step, loss=0.07479, avg_loss=0.07349]\n",
      "Step 536667  [5.418 sec/step, loss=0.07468, avg_loss=0.07350]\n",
      "Step 536668  [5.408 sec/step, loss=0.07291, avg_loss=0.07348]\n",
      "Step 536669  [5.404 sec/step, loss=0.07429, avg_loss=0.07349]\n",
      "Step 536670  [5.393 sec/step, loss=0.07141, avg_loss=0.07346]\n",
      "Step 536671  [5.392 sec/step, loss=0.07469, avg_loss=0.07346]\n",
      "Step 536672  [5.413 sec/step, loss=0.07499, avg_loss=0.07347]\n",
      "Step 536673  [5.421 sec/step, loss=0.07517, avg_loss=0.07349]\n",
      "Step 536674  [5.457 sec/step, loss=0.06571, avg_loss=0.07339]\n",
      "Step 536675  [5.463 sec/step, loss=0.07441, avg_loss=0.07340]\n",
      "Step 536676  [5.468 sec/step, loss=0.07577, avg_loss=0.07341]\n",
      "Step 536677  [5.455 sec/step, loss=0.07430, avg_loss=0.07341]\n",
      "Generated 32 batches of size 32 in 2.597 sec\n",
      "Step 536678  [5.442 sec/step, loss=0.07281, avg_loss=0.07338]\n",
      "Step 536679  [5.428 sec/step, loss=0.07048, avg_loss=0.07333]\n",
      "Step 536680  [5.429 sec/step, loss=0.07490, avg_loss=0.07333]\n",
      "Step 536681  [5.440 sec/step, loss=0.07435, avg_loss=0.07331]\n",
      "Step 536682  [5.439 sec/step, loss=0.07337, avg_loss=0.07331]\n",
      "Step 536683  [5.454 sec/step, loss=0.07441, avg_loss=0.07339]\n",
      "Step 536684  [5.460 sec/step, loss=0.07476, avg_loss=0.07342]\n",
      "Step 536685  [5.450 sec/step, loss=0.07114, avg_loss=0.07339]\n",
      "Step 536686  [5.446 sec/step, loss=0.07273, avg_loss=0.07339]\n",
      "Step 536687  [5.458 sec/step, loss=0.07646, avg_loss=0.07341]\n",
      "Step 536688  [5.433 sec/step, loss=0.06600, avg_loss=0.07332]\n",
      "Step 536689  [5.395 sec/step, loss=0.07359, avg_loss=0.07332]\n",
      "Step 536690  [5.418 sec/step, loss=0.07322, avg_loss=0.07332]\n",
      "Step 536691  [5.424 sec/step, loss=0.07425, avg_loss=0.07333]\n",
      "Step 536692  [5.412 sec/step, loss=0.07503, avg_loss=0.07332]\n",
      "Step 536693  [5.429 sec/step, loss=0.07333, avg_loss=0.07330]\n",
      "Step 536694  [5.441 sec/step, loss=0.07545, avg_loss=0.07332]\n",
      "Step 536695  [5.444 sec/step, loss=0.07543, avg_loss=0.07334]\n",
      "Step 536696  [5.456 sec/step, loss=0.07384, avg_loss=0.07335]\n",
      "Step 536697  [5.473 sec/step, loss=0.07562, avg_loss=0.07337]\n",
      "Step 536698  [5.489 sec/step, loss=0.07420, avg_loss=0.07345]\n",
      "Step 536699  [5.490 sec/step, loss=0.07413, avg_loss=0.07344]\n",
      "Step 536700  [5.462 sec/step, loss=0.07140, avg_loss=0.07340]\n",
      "Writing summary at step: 536700\n",
      "Step 536701  [5.474 sec/step, loss=0.07577, avg_loss=0.07341]\n",
      "Step 536702  [5.522 sec/step, loss=0.06599, avg_loss=0.07333]\n",
      "Step 536703  [5.513 sec/step, loss=0.07413, avg_loss=0.07332]\n",
      "Step 536704  [5.505 sec/step, loss=0.07441, avg_loss=0.07332]\n",
      "Step 536705  [5.492 sec/step, loss=0.07219, avg_loss=0.07330]\n",
      "Step 536706  [5.499 sec/step, loss=0.07239, avg_loss=0.07329]\n",
      "Step 536707  [5.492 sec/step, loss=0.07121, avg_loss=0.07325]\n",
      "Step 536708  [5.446 sec/step, loss=0.07475, avg_loss=0.07335]\n",
      "Generated 32 batches of size 32 in 2.446 sec\n",
      "Step 536709  [5.464 sec/step, loss=0.07580, avg_loss=0.07336]\n",
      "Step 536710  [5.458 sec/step, loss=0.07336, avg_loss=0.07338]\n",
      "Step 536711  [5.451 sec/step, loss=0.07279, avg_loss=0.07336]\n",
      "Step 536712  [5.457 sec/step, loss=0.07486, avg_loss=0.07340]\n",
      "Step 536713  [5.464 sec/step, loss=0.07564, avg_loss=0.07344]\n",
      "Step 536714  [5.438 sec/step, loss=0.07071, avg_loss=0.07341]\n",
      "Step 536715  [5.414 sec/step, loss=0.07462, avg_loss=0.07342]\n",
      "Step 536716  [5.407 sec/step, loss=0.07076, avg_loss=0.07339]\n",
      "Step 536717  [5.429 sec/step, loss=0.07316, avg_loss=0.07340]\n",
      "Step 536718  [5.418 sec/step, loss=0.07125, avg_loss=0.07335]\n",
      "Step 536719  [5.410 sec/step, loss=0.07371, avg_loss=0.07335]\n",
      "Step 536720  [5.418 sec/step, loss=0.07500, avg_loss=0.07340]\n",
      "Step 536721  [5.411 sec/step, loss=0.07512, avg_loss=0.07339]\n",
      "Step 536722  [5.436 sec/step, loss=0.07537, avg_loss=0.07342]\n",
      "Step 536723  [5.426 sec/step, loss=0.07307, avg_loss=0.07340]\n",
      "Step 536724  [5.421 sec/step, loss=0.07487, avg_loss=0.07339]\n",
      "Step 536725  [5.446 sec/step, loss=0.07535, avg_loss=0.07345]\n",
      "Step 536726  [5.443 sec/step, loss=0.07188, avg_loss=0.07343]\n",
      "Step 536727  [5.433 sec/step, loss=0.07067, avg_loss=0.07338]\n",
      "Step 536728  [5.471 sec/step, loss=0.06568, avg_loss=0.07328]\n",
      "Step 536729  [5.431 sec/step, loss=0.07183, avg_loss=0.07327]\n",
      "Step 536730  [5.419 sec/step, loss=0.07457, avg_loss=0.07327]\n",
      "Step 536731  [5.436 sec/step, loss=0.07439, avg_loss=0.07329]\n",
      "Step 536732  [5.453 sec/step, loss=0.07230, avg_loss=0.07326]\n",
      "Step 536733  [5.457 sec/step, loss=0.07587, avg_loss=0.07327]\n",
      "Step 536734  [5.415 sec/step, loss=0.07508, avg_loss=0.07336]\n",
      "Step 536735  [5.421 sec/step, loss=0.07583, avg_loss=0.07337]\n",
      "Step 536736  [5.419 sec/step, loss=0.07485, avg_loss=0.07340]\n",
      "Step 536737  [5.437 sec/step, loss=0.07572, avg_loss=0.07345]\n",
      "Step 536738  [5.428 sec/step, loss=0.07249, avg_loss=0.07343]\n",
      "Step 536739  [5.422 sec/step, loss=0.07107, avg_loss=0.07341]\n",
      "Step 536740  [5.436 sec/step, loss=0.07264, avg_loss=0.07339]\n",
      "Generated 32 batches of size 32 in 2.558 sec\n",
      "Step 536741  [5.429 sec/step, loss=0.07353, avg_loss=0.07338]\n",
      "Step 536742  [5.410 sec/step, loss=0.07499, avg_loss=0.07339]\n",
      "Step 536743  [5.419 sec/step, loss=0.07468, avg_loss=0.07341]\n",
      "Step 536744  [5.400 sec/step, loss=0.07359, avg_loss=0.07339]\n",
      "Step 536745  [5.424 sec/step, loss=0.07366, avg_loss=0.07339]\n",
      "Step 536746  [5.433 sec/step, loss=0.07468, avg_loss=0.07348]\n",
      "Step 536747  [5.407 sec/step, loss=0.06575, avg_loss=0.07341]\n",
      "Step 536748  [5.413 sec/step, loss=0.07558, avg_loss=0.07341]\n",
      "Step 536749  [5.420 sec/step, loss=0.07432, avg_loss=0.07340]\n",
      "Step 536750  [5.431 sec/step, loss=0.07523, avg_loss=0.07341]\n",
      "Step 536751  [5.430 sec/step, loss=0.07254, avg_loss=0.07342]\n",
      "Step 536752  [5.431 sec/step, loss=0.07405, avg_loss=0.07344]\n",
      "Step 536753  [5.423 sec/step, loss=0.07391, avg_loss=0.07344]\n",
      "Step 536754  [5.408 sec/step, loss=0.06586, avg_loss=0.07335]\n",
      "Step 536755  [5.395 sec/step, loss=0.07345, avg_loss=0.07333]\n",
      "Step 536756  [5.386 sec/step, loss=0.07231, avg_loss=0.07330]\n",
      "Step 536757  [5.425 sec/step, loss=0.06438, avg_loss=0.07319]\n",
      "Step 536758  [5.439 sec/step, loss=0.07491, avg_loss=0.07328]\n",
      "Step 536759  [5.436 sec/step, loss=0.07570, avg_loss=0.07328]\n",
      "Step 536760  [5.404 sec/step, loss=0.07040, avg_loss=0.07327]\n",
      "Step 536761  [5.421 sec/step, loss=0.07433, avg_loss=0.07326]\n",
      "Step 536762  [5.449 sec/step, loss=0.07462, avg_loss=0.07326]\n",
      "Step 536763  [5.449 sec/step, loss=0.07483, avg_loss=0.07326]\n",
      "Step 536764  [5.467 sec/step, loss=0.07638, avg_loss=0.07332]\n",
      "Step 536765  [5.456 sec/step, loss=0.07102, avg_loss=0.07330]\n",
      "Step 536766  [5.460 sec/step, loss=0.07507, avg_loss=0.07330]\n",
      "Step 536767  [5.452 sec/step, loss=0.07308, avg_loss=0.07329]\n",
      "Step 536768  [5.449 sec/step, loss=0.07362, avg_loss=0.07329]\n",
      "Step 536769  [5.468 sec/step, loss=0.07504, avg_loss=0.07330]\n",
      "Step 536770  [5.471 sec/step, loss=0.06976, avg_loss=0.07328]\n",
      "Step 536771  [5.479 sec/step, loss=0.07227, avg_loss=0.07326]\n",
      "Step 536772  [5.482 sec/step, loss=0.07421, avg_loss=0.07325]\n",
      "Generated 32 batches of size 32 in 2.616 sec\n",
      "Step 536773  [5.482 sec/step, loss=0.07351, avg_loss=0.07323]\n",
      "Step 536774  [5.441 sec/step, loss=0.07594, avg_loss=0.07334]\n",
      "Step 536775  [5.433 sec/step, loss=0.07397, avg_loss=0.07333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 536776  [5.435 sec/step, loss=0.07283, avg_loss=0.07330]\n",
      "Step 536777  [5.440 sec/step, loss=0.07253, avg_loss=0.07329]\n",
      "Step 536778  [5.444 sec/step, loss=0.07563, avg_loss=0.07331]\n",
      "Step 536779  [5.436 sec/step, loss=0.07353, avg_loss=0.07334]\n",
      "Step 536780  [5.436 sec/step, loss=0.07458, avg_loss=0.07334]\n",
      "Step 536781  [5.429 sec/step, loss=0.07579, avg_loss=0.07336]\n",
      "Step 536782  [5.414 sec/step, loss=0.07272, avg_loss=0.07335]\n",
      "Step 536783  [5.410 sec/step, loss=0.07405, avg_loss=0.07335]\n",
      "Step 536784  [5.406 sec/step, loss=0.07500, avg_loss=0.07335]\n",
      "Step 536785  [5.467 sec/step, loss=0.06694, avg_loss=0.07331]\n",
      "Step 536786  [5.469 sec/step, loss=0.07210, avg_loss=0.07330]\n",
      "Step 536787  [5.483 sec/step, loss=0.07314, avg_loss=0.07327]\n",
      "Step 536788  [5.496 sec/step, loss=0.07281, avg_loss=0.07333]\n",
      "Step 536789  [5.499 sec/step, loss=0.07312, avg_loss=0.07333]\n",
      "Step 536790  [5.493 sec/step, loss=0.07586, avg_loss=0.07336]\n",
      "Step 536791  [5.503 sec/step, loss=0.07386, avg_loss=0.07335]\n",
      "Step 536792  [5.505 sec/step, loss=0.07228, avg_loss=0.07333]\n",
      "Step 536793  [5.464 sec/step, loss=0.07146, avg_loss=0.07331]\n",
      "Step 536794  [5.459 sec/step, loss=0.07350, avg_loss=0.07329]\n",
      "Step 536795  [5.462 sec/step, loss=0.07455, avg_loss=0.07328]\n",
      "Step 536796  [5.466 sec/step, loss=0.07527, avg_loss=0.07329]\n",
      "Step 536797  [5.460 sec/step, loss=0.07182, avg_loss=0.07325]\n",
      "Step 536798  [5.459 sec/step, loss=0.07467, avg_loss=0.07326]\n",
      "Step 536799  [5.451 sec/step, loss=0.07348, avg_loss=0.07325]\n",
      "Step 536800  [5.463 sec/step, loss=0.07077, avg_loss=0.07325]\n",
      "Writing summary at step: 536800\n",
      "Step 536801  [5.433 sec/step, loss=0.06459, avg_loss=0.07313]\n",
      "Step 536802  [5.392 sec/step, loss=0.07615, avg_loss=0.07324]\n",
      "Step 536803  [5.413 sec/step, loss=0.07656, avg_loss=0.07326]\n",
      "Generated 32 batches of size 32 in 2.357 sec\n",
      "Step 536804  [5.420 sec/step, loss=0.07411, avg_loss=0.07326]\n",
      "Step 536805  [5.417 sec/step, loss=0.07331, avg_loss=0.07327]\n",
      "Step 536806  [5.408 sec/step, loss=0.07554, avg_loss=0.07330]\n",
      "Step 536807  [5.413 sec/step, loss=0.07533, avg_loss=0.07334]\n",
      "Step 536808  [5.407 sec/step, loss=0.07453, avg_loss=0.07334]\n",
      "Step 536809  [5.390 sec/step, loss=0.07305, avg_loss=0.07331]\n",
      "Step 536810  [5.397 sec/step, loss=0.07491, avg_loss=0.07333]\n",
      "Step 536811  [5.423 sec/step, loss=0.07296, avg_loss=0.07333]\n",
      "Step 536812  [5.439 sec/step, loss=0.07567, avg_loss=0.07334]\n",
      "Step 536813  [5.428 sec/step, loss=0.07477, avg_loss=0.07333]\n",
      "Step 536814  [5.453 sec/step, loss=0.07518, avg_loss=0.07337]\n",
      "Step 536815  [5.501 sec/step, loss=0.06629, avg_loss=0.07329]\n",
      "Step 536816  [5.502 sec/step, loss=0.07398, avg_loss=0.07332]\n",
      "Step 536817  [5.508 sec/step, loss=0.07579, avg_loss=0.07335]\n",
      "Step 536818  [5.512 sec/step, loss=0.07409, avg_loss=0.07338]\n",
      "Step 536819  [5.530 sec/step, loss=0.07482, avg_loss=0.07339]\n",
      "Step 536820  [5.527 sec/step, loss=0.07527, avg_loss=0.07339]\n",
      "Step 536821  [5.513 sec/step, loss=0.07196, avg_loss=0.07336]\n",
      "Step 536822  [5.502 sec/step, loss=0.07388, avg_loss=0.07334]\n",
      "Step 536823  [5.497 sec/step, loss=0.07368, avg_loss=0.07335]\n",
      "Step 536824  [5.501 sec/step, loss=0.07579, avg_loss=0.07336]\n",
      "Step 536825  [5.471 sec/step, loss=0.06595, avg_loss=0.07327]\n",
      "Step 536826  [5.483 sec/step, loss=0.07532, avg_loss=0.07330]\n",
      "Step 536827  [5.500 sec/step, loss=0.07571, avg_loss=0.07335]\n",
      "Step 536828  [5.447 sec/step, loss=0.07364, avg_loss=0.07343]\n",
      "Step 536829  [5.451 sec/step, loss=0.07097, avg_loss=0.07342]\n",
      "Step 536830  [5.466 sec/step, loss=0.07567, avg_loss=0.07343]\n",
      "Step 536831  [5.460 sec/step, loss=0.07066, avg_loss=0.07339]\n",
      "Step 536832  [5.441 sec/step, loss=0.07450, avg_loss=0.07342]\n",
      "Step 536833  [5.429 sec/step, loss=0.07314, avg_loss=0.07339]\n",
      "Step 536834  [5.427 sec/step, loss=0.07346, avg_loss=0.07337]\n",
      "Step 536835  [5.420 sec/step, loss=0.07472, avg_loss=0.07336]\n",
      "Generated 32 batches of size 32 in 2.460 sec\n",
      "Step 536836  [5.428 sec/step, loss=0.07477, avg_loss=0.07336]\n",
      "Step 536837  [5.444 sec/step, loss=0.07279, avg_loss=0.07333]\n",
      "Step 536838  [5.439 sec/step, loss=0.07259, avg_loss=0.07333]\n",
      "Step 536839  [5.456 sec/step, loss=0.07425, avg_loss=0.07336]\n",
      "Step 536840  [5.442 sec/step, loss=0.07475, avg_loss=0.07339]\n",
      "Step 536841  [5.438 sec/step, loss=0.07362, avg_loss=0.07339]\n",
      "Step 536842  [5.450 sec/step, loss=0.07547, avg_loss=0.07339]\n",
      "Step 536843  [5.454 sec/step, loss=0.07470, avg_loss=0.07339]\n",
      "Step 536844  [5.447 sec/step, loss=0.07358, avg_loss=0.07339]\n",
      "Step 536845  [5.432 sec/step, loss=0.07495, avg_loss=0.07340]\n",
      "Step 536846  [5.439 sec/step, loss=0.07550, avg_loss=0.07341]\n",
      "Step 536847  [5.483 sec/step, loss=0.07331, avg_loss=0.07349]\n",
      "Step 536848  [5.462 sec/step, loss=0.07515, avg_loss=0.07348]\n",
      "Step 536849  [5.457 sec/step, loss=0.07421, avg_loss=0.07348]\n",
      "Step 536850  [5.426 sec/step, loss=0.06538, avg_loss=0.07338]\n",
      "Step 536851  [5.435 sec/step, loss=0.07483, avg_loss=0.07341]\n",
      "Step 536852  [5.423 sec/step, loss=0.07076, avg_loss=0.07337]\n",
      "Step 536853  [5.443 sec/step, loss=0.07506, avg_loss=0.07339]\n",
      "Step 536854  [5.467 sec/step, loss=0.07480, avg_loss=0.07348]\n",
      "Step 536855  [5.468 sec/step, loss=0.07170, avg_loss=0.07346]\n",
      "Step 536856  [5.472 sec/step, loss=0.07427, avg_loss=0.07348]\n",
      "Step 536857  [5.434 sec/step, loss=0.07540, avg_loss=0.07359]\n",
      "Step 536858  [5.451 sec/step, loss=0.07591, avg_loss=0.07360]\n",
      "Step 536859  [5.446 sec/step, loss=0.07437, avg_loss=0.07358]\n",
      "Step 536860  [5.459 sec/step, loss=0.07545, avg_loss=0.07363]\n",
      "Step 536861  [5.442 sec/step, loss=0.07326, avg_loss=0.07362]\n",
      "Step 536862  [5.419 sec/step, loss=0.07524, avg_loss=0.07363]\n",
      "Step 536863  [5.409 sec/step, loss=0.07354, avg_loss=0.07362]\n",
      "Step 536864  [5.450 sec/step, loss=0.06585, avg_loss=0.07351]\n",
      "Step 536865  [5.477 sec/step, loss=0.07406, avg_loss=0.07354]\n",
      "Step 536866  [5.483 sec/step, loss=0.07395, avg_loss=0.07353]\n",
      "Step 536867  [5.493 sec/step, loss=0.07546, avg_loss=0.07355]\n",
      "Generated 32 batches of size 32 in 2.460 sec\n",
      "Step 536868  [5.517 sec/step, loss=0.07582, avg_loss=0.07358]\n",
      "Step 536869  [5.495 sec/step, loss=0.07412, avg_loss=0.07357]\n",
      "Step 536870  [5.514 sec/step, loss=0.07592, avg_loss=0.07363]\n",
      "Step 536871  [5.498 sec/step, loss=0.07082, avg_loss=0.07361]\n",
      "Step 536872  [5.477 sec/step, loss=0.07361, avg_loss=0.07361]\n",
      "Step 536873  [5.473 sec/step, loss=0.07218, avg_loss=0.07360]\n",
      "Step 536874  [5.458 sec/step, loss=0.07295, avg_loss=0.07357]\n",
      "Step 536875  [5.460 sec/step, loss=0.07500, avg_loss=0.07358]\n",
      "Step 536876  [5.446 sec/step, loss=0.07170, avg_loss=0.07356]\n",
      "Step 536877  [5.492 sec/step, loss=0.06594, avg_loss=0.07350]\n",
      "Step 536878  [5.491 sec/step, loss=0.07514, avg_loss=0.07349]\n",
      "Step 536879  [5.491 sec/step, loss=0.07028, avg_loss=0.07346]\n",
      "Step 536880  [5.495 sec/step, loss=0.07587, avg_loss=0.07347]\n",
      "Step 536881  [5.487 sec/step, loss=0.07406, avg_loss=0.07346]\n",
      "Step 536882  [5.489 sec/step, loss=0.07117, avg_loss=0.07344]\n",
      "Step 536883  [5.490 sec/step, loss=0.07456, avg_loss=0.07345]\n",
      "Step 536884  [5.487 sec/step, loss=0.07474, avg_loss=0.07344]\n",
      "Step 536885  [5.438 sec/step, loss=0.07163, avg_loss=0.07349]\n",
      "Step 536886  [5.449 sec/step, loss=0.07325, avg_loss=0.07350]\n",
      "Step 536887  [5.427 sec/step, loss=0.07449, avg_loss=0.07352]\n",
      "Step 536888  [5.440 sec/step, loss=0.07556, avg_loss=0.07354]\n",
      "Step 536889  [5.436 sec/step, loss=0.07380, avg_loss=0.07355]\n",
      "Step 536890  [5.435 sec/step, loss=0.07507, avg_loss=0.07354]\n",
      "Step 536891  [5.403 sec/step, loss=0.06514, avg_loss=0.07345]\n",
      "Step 536892  [5.407 sec/step, loss=0.07432, avg_loss=0.07348]\n",
      "Step 536893  [5.418 sec/step, loss=0.07462, avg_loss=0.07351]\n",
      "Step 536894  [5.414 sec/step, loss=0.07423, avg_loss=0.07351]\n",
      "Step 536895  [5.412 sec/step, loss=0.07433, avg_loss=0.07351]\n",
      "Step 536896  [5.389 sec/step, loss=0.07033, avg_loss=0.07346]\n",
      "Step 536897  [5.393 sec/step, loss=0.07341, avg_loss=0.07348]\n",
      "Step 536898  [5.405 sec/step, loss=0.07577, avg_loss=0.07349]\n",
      "Step 536899  [5.411 sec/step, loss=0.07319, avg_loss=0.07349]\n",
      "Generated 32 batches of size 32 in 2.381 sec\n",
      "Step 536900  [5.421 sec/step, loss=0.07488, avg_loss=0.07353]\n",
      "Writing summary at step: 536900\n",
      "Step 536901  [5.437 sec/step, loss=0.07370, avg_loss=0.07362]\n",
      "Step 536902  [5.420 sec/step, loss=0.07336, avg_loss=0.07359]\n",
      "Step 536903  [5.423 sec/step, loss=0.07507, avg_loss=0.07358]\n",
      "Step 536904  [5.435 sec/step, loss=0.07311, avg_loss=0.07357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 536905  [5.457 sec/step, loss=0.07583, avg_loss=0.07359]\n",
      "Step 536906  [5.440 sec/step, loss=0.07386, avg_loss=0.07357]\n",
      "Step 536907  [5.449 sec/step, loss=0.07535, avg_loss=0.07357]\n",
      "Step 536908  [5.448 sec/step, loss=0.07388, avg_loss=0.07357]\n",
      "Step 536909  [5.450 sec/step, loss=0.07069, avg_loss=0.07354]\n",
      "Step 536910  [5.461 sec/step, loss=0.07464, avg_loss=0.07354]\n",
      "Step 536911  [5.455 sec/step, loss=0.07542, avg_loss=0.07357]\n",
      "Step 536912  [5.453 sec/step, loss=0.07417, avg_loss=0.07355]\n",
      "Step 536913  [5.449 sec/step, loss=0.07273, avg_loss=0.07353]\n",
      "Step 536914  [5.436 sec/step, loss=0.07448, avg_loss=0.07352]\n",
      "Step 536915  [5.406 sec/step, loss=0.07224, avg_loss=0.07358]\n",
      "Step 536916  [5.412 sec/step, loss=0.07535, avg_loss=0.07360]\n",
      "Step 536917  [5.407 sec/step, loss=0.07537, avg_loss=0.07359]\n",
      "Step 536918  [5.392 sec/step, loss=0.07367, avg_loss=0.07359]\n",
      "Step 536919  [5.379 sec/step, loss=0.07482, avg_loss=0.07359]\n",
      "Step 536920  [5.360 sec/step, loss=0.06625, avg_loss=0.07350]\n",
      "Step 536921  [5.359 sec/step, loss=0.07134, avg_loss=0.07349]\n",
      "Step 536922  [5.363 sec/step, loss=0.07430, avg_loss=0.07350]\n",
      "Step 536923  [5.383 sec/step, loss=0.07537, avg_loss=0.07351]\n",
      "Step 536924  [5.382 sec/step, loss=0.07635, avg_loss=0.07352]\n",
      "Step 536925  [5.448 sec/step, loss=0.06577, avg_loss=0.07352]\n",
      "Step 536926  [5.443 sec/step, loss=0.07502, avg_loss=0.07351]\n",
      "Step 536927  [5.432 sec/step, loss=0.07202, avg_loss=0.07348]\n",
      "Step 536928  [5.433 sec/step, loss=0.07380, avg_loss=0.07348]\n",
      "Step 536929  [5.439 sec/step, loss=0.07310, avg_loss=0.07350]\n",
      "Step 536930  [5.432 sec/step, loss=0.07328, avg_loss=0.07348]\n",
      "Generated 32 batches of size 32 in 2.388 sec\n",
      "Step 536931  [5.449 sec/step, loss=0.07291, avg_loss=0.07350]\n",
      "Step 536932  [5.432 sec/step, loss=0.07067, avg_loss=0.07346]\n",
      "Step 536933  [5.428 sec/step, loss=0.07350, avg_loss=0.07346]\n",
      "Step 536934  [5.423 sec/step, loss=0.07273, avg_loss=0.07346]\n",
      "Step 536935  [5.424 sec/step, loss=0.07526, avg_loss=0.07346]\n",
      "Step 536936  [5.423 sec/step, loss=0.07542, avg_loss=0.07347]\n",
      "Step 536937  [5.422 sec/step, loss=0.07274, avg_loss=0.07347]\n",
      "Step 536938  [5.425 sec/step, loss=0.07489, avg_loss=0.07349]\n",
      "Step 536939  [5.429 sec/step, loss=0.07457, avg_loss=0.07349]\n",
      "Step 536940  [5.432 sec/step, loss=0.07154, avg_loss=0.07346]\n",
      "Step 536941  [5.426 sec/step, loss=0.07153, avg_loss=0.07344]\n",
      "Step 536942  [5.420 sec/step, loss=0.07359, avg_loss=0.07342]\n",
      "Step 536943  [5.415 sec/step, loss=0.07447, avg_loss=0.07342]\n",
      "Step 536944  [5.420 sec/step, loss=0.07364, avg_loss=0.07342]\n",
      "Step 536945  [5.426 sec/step, loss=0.07396, avg_loss=0.07341]\n",
      "Step 536946  [5.431 sec/step, loss=0.07268, avg_loss=0.07338]\n",
      "Step 536947  [5.421 sec/step, loss=0.07528, avg_loss=0.07340]\n",
      "Step 536948  [5.427 sec/step, loss=0.07560, avg_loss=0.07341]\n",
      "Step 536949  [5.425 sec/step, loss=0.07438, avg_loss=0.07341]\n",
      "Step 536950  [5.454 sec/step, loss=0.07550, avg_loss=0.07351]\n",
      "Step 536951  [5.455 sec/step, loss=0.07514, avg_loss=0.07351]\n",
      "Step 536952  [5.519 sec/step, loss=0.06487, avg_loss=0.07345]\n",
      "Step 536953  [5.500 sec/step, loss=0.07213, avg_loss=0.07342]\n",
      "Step 536954  [5.508 sec/step, loss=0.07531, avg_loss=0.07343]\n",
      "Step 536955  [5.511 sec/step, loss=0.07442, avg_loss=0.07346]\n",
      "Step 536956  [5.521 sec/step, loss=0.07572, avg_loss=0.07347]\n",
      "Step 536957  [5.492 sec/step, loss=0.06611, avg_loss=0.07338]\n",
      "Step 536958  [5.470 sec/step, loss=0.07025, avg_loss=0.07332]\n",
      "Step 536959  [5.471 sec/step, loss=0.07478, avg_loss=0.07333]\n",
      "Step 536960  [5.474 sec/step, loss=0.07588, avg_loss=0.07333]\n",
      "Step 536961  [5.468 sec/step, loss=0.07340, avg_loss=0.07333]\n",
      "Step 536962  [5.462 sec/step, loss=0.07003, avg_loss=0.07328]\n",
      "Generated 32 batches of size 32 in 2.602 sec\n",
      "Step 536963  [5.471 sec/step, loss=0.07479, avg_loss=0.07329]\n",
      "Step 536964  [5.430 sec/step, loss=0.07585, avg_loss=0.07339]\n",
      "Step 536965  [5.422 sec/step, loss=0.07479, avg_loss=0.07340]\n",
      "Step 536966  [5.419 sec/step, loss=0.07577, avg_loss=0.07342]\n",
      "Step 536967  [5.400 sec/step, loss=0.07172, avg_loss=0.07338]\n",
      "Step 536968  [5.408 sec/step, loss=0.07467, avg_loss=0.07337]\n",
      "Step 536969  [5.419 sec/step, loss=0.07390, avg_loss=0.07337]\n",
      "Step 536970  [5.406 sec/step, loss=0.07323, avg_loss=0.07334]\n",
      "Step 536971  [5.416 sec/step, loss=0.07440, avg_loss=0.07338]\n",
      "Step 536972  [5.409 sec/step, loss=0.07350, avg_loss=0.07337]\n",
      "Step 536973  [5.462 sec/step, loss=0.06699, avg_loss=0.07332]\n",
      "Step 536974  [5.473 sec/step, loss=0.07460, avg_loss=0.07334]\n",
      "Step 536975  [5.469 sec/step, loss=0.07428, avg_loss=0.07333]\n",
      "Step 536976  [5.455 sec/step, loss=0.06566, avg_loss=0.07327]\n",
      "Step 536977  [5.410 sec/step, loss=0.07415, avg_loss=0.07335]\n",
      "Step 536978  [5.410 sec/step, loss=0.07488, avg_loss=0.07335]\n",
      "Step 536979  [5.422 sec/step, loss=0.07444, avg_loss=0.07339]\n",
      "Step 536980  [5.412 sec/step, loss=0.07451, avg_loss=0.07338]\n",
      "Step 536981  [5.434 sec/step, loss=0.07347, avg_loss=0.07337]\n",
      "Step 536982  [5.430 sec/step, loss=0.07208, avg_loss=0.07338]\n",
      "Step 536983  [5.427 sec/step, loss=0.06950, avg_loss=0.07333]\n",
      "Step 536984  [5.415 sec/step, loss=0.07106, avg_loss=0.07329]\n",
      "Step 536985  [5.426 sec/step, loss=0.07537, avg_loss=0.07333]\n",
      "Step 536986  [5.424 sec/step, loss=0.07414, avg_loss=0.07334]\n",
      "Step 536987  [5.418 sec/step, loss=0.07461, avg_loss=0.07334]\n",
      "Step 536988  [5.418 sec/step, loss=0.07452, avg_loss=0.07333]\n",
      "Step 536989  [5.417 sec/step, loss=0.06962, avg_loss=0.07329]\n",
      "Step 536990  [5.410 sec/step, loss=0.07415, avg_loss=0.07328]\n",
      "Step 536991  [5.436 sec/step, loss=0.07487, avg_loss=0.07338]\n",
      "Step 536992  [5.430 sec/step, loss=0.07396, avg_loss=0.07337]\n",
      "Step 536993  [5.431 sec/step, loss=0.07208, avg_loss=0.07335]\n",
      "Step 536994  [5.442 sec/step, loss=0.07544, avg_loss=0.07336]\n",
      "Generated 32 batches of size 32 in 2.391 sec\n",
      "Step 536995  [5.468 sec/step, loss=0.07307, avg_loss=0.07335]\n",
      "Step 536996  [5.496 sec/step, loss=0.07391, avg_loss=0.07338]\n",
      "Step 536997  [5.496 sec/step, loss=0.07563, avg_loss=0.07341]\n",
      "Step 536998  [5.482 sec/step, loss=0.07326, avg_loss=0.07338]\n",
      "Step 536999  [5.492 sec/step, loss=0.07431, avg_loss=0.07339]\n",
      "Step 537000  [5.491 sec/step, loss=0.07539, avg_loss=0.07340]\n",
      "Writing summary at step: 537000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-537000\n",
      "Saving audio and alignment...\n",
      "Input: kunrr kay gilriiz xaan kay taraylar pir maxsuus tsamrraa or loohaa ddhuraa hae~________________________________\n",
      "Step 537001  [5.506 sec/step, loss=0.07631, avg_loss=0.07342]\n",
      "Step 537002  [5.525 sec/step, loss=0.07320, avg_loss=0.07342]\n",
      "Step 537003  [5.495 sec/step, loss=0.07012, avg_loss=0.07337]\n",
      "Step 537004  [5.454 sec/step, loss=0.07076, avg_loss=0.07335]\n",
      "Step 537005  [5.451 sec/step, loss=0.07518, avg_loss=0.07334]\n",
      "Step 537006  [5.448 sec/step, loss=0.07336, avg_loss=0.07334]\n",
      "Step 537007  [5.425 sec/step, loss=0.07256, avg_loss=0.07331]\n",
      "Step 537008  [5.422 sec/step, loss=0.07190, avg_loss=0.07329]\n",
      "Step 537009  [5.426 sec/step, loss=0.07526, avg_loss=0.07334]\n",
      "Step 537010  [5.392 sec/step, loss=0.06444, avg_loss=0.07323]\n",
      "Step 537011  [5.390 sec/step, loss=0.07556, avg_loss=0.07323]\n",
      "Step 537012  [5.379 sec/step, loss=0.07461, avg_loss=0.07324]\n",
      "Step 537013  [5.382 sec/step, loss=0.07344, avg_loss=0.07325]\n",
      "Step 537014  [5.380 sec/step, loss=0.07451, avg_loss=0.07325]\n",
      "Step 537015  [5.355 sec/step, loss=0.07316, avg_loss=0.07326]\n",
      "Step 537016  [5.355 sec/step, loss=0.07234, avg_loss=0.07323]\n",
      "Step 537017  [5.347 sec/step, loss=0.07300, avg_loss=0.07320]\n",
      "Step 537018  [5.365 sec/step, loss=0.07526, avg_loss=0.07322]\n",
      "Step 537019  [5.364 sec/step, loss=0.07189, avg_loss=0.07319]\n",
      "Step 537020  [5.374 sec/step, loss=0.07325, avg_loss=0.07326]\n",
      "Step 537021  [5.402 sec/step, loss=0.07498, avg_loss=0.07330]\n",
      "Step 537022  [5.405 sec/step, loss=0.07565, avg_loss=0.07331]\n",
      "Step 537023  [5.397 sec/step, loss=0.07516, avg_loss=0.07331]\n",
      "Step 537024  [5.405 sec/step, loss=0.07510, avg_loss=0.07329]\n",
      "Generated 32 batches of size 32 in 2.374 sec\n",
      "Step 537025  [5.371 sec/step, loss=0.07513, avg_loss=0.07339]\n",
      "Step 537026  [5.375 sec/step, loss=0.07637, avg_loss=0.07340]\n",
      "Step 537027  [5.373 sec/step, loss=0.07448, avg_loss=0.07343]\n",
      "Step 537028  [5.400 sec/step, loss=0.07229, avg_loss=0.07341]\n",
      "Step 537029  [5.403 sec/step, loss=0.07351, avg_loss=0.07341]\n",
      "Step 537030  [5.400 sec/step, loss=0.07452, avg_loss=0.07343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 537031  [5.388 sec/step, loss=0.07422, avg_loss=0.07344]\n",
      "Step 537032  [5.394 sec/step, loss=0.07487, avg_loss=0.07348]\n",
      "Step 537033  [5.451 sec/step, loss=0.06521, avg_loss=0.07340]\n",
      "Step 537034  [5.452 sec/step, loss=0.07498, avg_loss=0.07342]\n",
      "Step 537035  [5.446 sec/step, loss=0.07197, avg_loss=0.07339]\n",
      "Step 537036  [5.449 sec/step, loss=0.07493, avg_loss=0.07338]\n",
      "Step 537037  [5.419 sec/step, loss=0.07492, avg_loss=0.07341]\n",
      "Step 537038  [5.431 sec/step, loss=0.07567, avg_loss=0.07341]\n",
      "Step 537039  [5.415 sec/step, loss=0.07334, avg_loss=0.07340]\n",
      "Step 537040  [5.417 sec/step, loss=0.07465, avg_loss=0.07343]\n",
      "Step 537041  [5.447 sec/step, loss=0.07508, avg_loss=0.07347]\n",
      "Step 537042  [5.438 sec/step, loss=0.07439, avg_loss=0.07348]\n",
      "Step 537043  [5.447 sec/step, loss=0.07534, avg_loss=0.07348]\n",
      "Step 537044  [5.442 sec/step, loss=0.07195, avg_loss=0.07347]\n",
      "Step 537045  [5.449 sec/step, loss=0.07370, avg_loss=0.07347]\n",
      "Step 537046  [5.437 sec/step, loss=0.07396, avg_loss=0.07348]\n",
      "Step 537047  [5.413 sec/step, loss=0.06923, avg_loss=0.07342]\n",
      "Step 537048  [5.402 sec/step, loss=0.07057, avg_loss=0.07337]\n",
      "Step 537049  [5.455 sec/step, loss=0.06609, avg_loss=0.07328]\n",
      "Step 537050  [5.448 sec/step, loss=0.07503, avg_loss=0.07328]\n",
      "Step 537051  [5.442 sec/step, loss=0.07349, avg_loss=0.07326]\n",
      "Step 537052  [5.394 sec/step, loss=0.07113, avg_loss=0.07333]\n",
      "Step 537053  [5.380 sec/step, loss=0.06661, avg_loss=0.07327]\n",
      "Step 537054  [5.350 sec/step, loss=0.07257, avg_loss=0.07324]\n",
      "Step 537055  [5.355 sec/step, loss=0.07547, avg_loss=0.07325]\n",
      "Step 537056  [5.347 sec/step, loss=0.07539, avg_loss=0.07325]\n",
      "Generated 32 batches of size 32 in 2.496 sec\n",
      "Step 537057  [5.369 sec/step, loss=0.07441, avg_loss=0.07333]\n",
      "Step 537058  [5.403 sec/step, loss=0.07228, avg_loss=0.07335]\n",
      "Step 537059  [5.406 sec/step, loss=0.07524, avg_loss=0.07336]\n",
      "Step 537060  [5.406 sec/step, loss=0.07544, avg_loss=0.07335]\n",
      "Step 537061  [5.407 sec/step, loss=0.07278, avg_loss=0.07335]\n",
      "Step 537062  [5.411 sec/step, loss=0.07278, avg_loss=0.07338]\n",
      "Step 537063  [5.413 sec/step, loss=0.07430, avg_loss=0.07337]\n",
      "Step 537064  [5.401 sec/step, loss=0.07448, avg_loss=0.07336]\n",
      "Step 537065  [5.410 sec/step, loss=0.07513, avg_loss=0.07336]\n",
      "Step 537066  [5.406 sec/step, loss=0.07431, avg_loss=0.07335]\n",
      "Step 537067  [5.428 sec/step, loss=0.07558, avg_loss=0.07338]\n",
      "Step 537068  [5.453 sec/step, loss=0.06552, avg_loss=0.07329]\n",
      "Step 537069  [5.459 sec/step, loss=0.07515, avg_loss=0.07331]\n",
      "Step 537070  [5.472 sec/step, loss=0.07481, avg_loss=0.07332]\n",
      "Step 537071  [5.470 sec/step, loss=0.07404, avg_loss=0.07332]\n",
      "Step 537072  [5.488 sec/step, loss=0.07551, avg_loss=0.07334]\n",
      "Step 537073  [5.424 sec/step, loss=0.07138, avg_loss=0.07338]\n",
      "Step 537074  [5.413 sec/step, loss=0.07369, avg_loss=0.07337]\n",
      "Step 537075  [5.411 sec/step, loss=0.07466, avg_loss=0.07338]\n",
      "Step 537076  [5.435 sec/step, loss=0.07550, avg_loss=0.07347]\n",
      "Step 537077  [5.435 sec/step, loss=0.07461, avg_loss=0.07348]\n",
      "Step 537078  [5.435 sec/step, loss=0.07547, avg_loss=0.07349]\n",
      "Step 537079  [5.437 sec/step, loss=0.07423, avg_loss=0.07348]\n",
      "Step 537080  [5.436 sec/step, loss=0.07328, avg_loss=0.07347]\n",
      "Step 537081  [5.409 sec/step, loss=0.07140, avg_loss=0.07345]\n",
      "Step 537082  [5.438 sec/step, loss=0.07315, avg_loss=0.07346]\n",
      "Step 537083  [5.435 sec/step, loss=0.07330, avg_loss=0.07350]\n",
      "Step 537084  [5.460 sec/step, loss=0.07382, avg_loss=0.07353]\n",
      "Step 537085  [5.443 sec/step, loss=0.07137, avg_loss=0.07349]\n",
      "Step 537086  [5.434 sec/step, loss=0.07494, avg_loss=0.07349]\n",
      "Step 537087  [5.427 sec/step, loss=0.07345, avg_loss=0.07348]\n",
      "Step 537088  [5.432 sec/step, loss=0.07583, avg_loss=0.07350]\n",
      "Generated 32 batches of size 32 in 2.725 sec\n",
      "Step 537089  [5.437 sec/step, loss=0.07367, avg_loss=0.07354]\n",
      "Step 537090  [5.429 sec/step, loss=0.07127, avg_loss=0.07351]\n",
      "Step 537091  [5.420 sec/step, loss=0.07298, avg_loss=0.07349]\n",
      "Step 537092  [5.432 sec/step, loss=0.07402, avg_loss=0.07349]\n",
      "Step 537093  [5.416 sec/step, loss=0.06610, avg_loss=0.07343]\n",
      "Step 537094  [5.419 sec/step, loss=0.07623, avg_loss=0.07344]\n",
      "Step 537095  [5.392 sec/step, loss=0.07496, avg_loss=0.07346]\n",
      "Step 537096  [5.372 sec/step, loss=0.07469, avg_loss=0.07346]\n",
      "Step 537097  [5.389 sec/step, loss=0.07300, avg_loss=0.07344]\n",
      "Step 537098  [5.400 sec/step, loss=0.07509, avg_loss=0.07346]\n",
      "Step 537099  [5.387 sec/step, loss=0.07337, avg_loss=0.07345]\n",
      "Step 537100  [5.374 sec/step, loss=0.07404, avg_loss=0.07343]\n",
      "Writing summary at step: 537100\n",
      "Step 537101  [5.366 sec/step, loss=0.07521, avg_loss=0.07342]\n",
      "Step 537102  [5.340 sec/step, loss=0.06998, avg_loss=0.07339]\n",
      "Step 537103  [5.369 sec/step, loss=0.07453, avg_loss=0.07343]\n",
      "Step 537104  [5.396 sec/step, loss=0.07361, avg_loss=0.07346]\n",
      "Step 537105  [5.388 sec/step, loss=0.07457, avg_loss=0.07346]\n",
      "Step 537106  [5.398 sec/step, loss=0.07329, avg_loss=0.07346]\n",
      "Step 537107  [5.415 sec/step, loss=0.07364, avg_loss=0.07347]\n",
      "Step 537108  [5.431 sec/step, loss=0.07509, avg_loss=0.07350]\n",
      "Step 537109  [5.419 sec/step, loss=0.06980, avg_loss=0.07344]\n",
      "Step 537110  [5.434 sec/step, loss=0.07122, avg_loss=0.07351]\n",
      "Step 537111  [5.430 sec/step, loss=0.07527, avg_loss=0.07351]\n",
      "Step 537112  [5.424 sec/step, loss=0.07170, avg_loss=0.07348]\n",
      "Step 537113  [5.424 sec/step, loss=0.07302, avg_loss=0.07348]\n",
      "Step 537114  [5.474 sec/step, loss=0.06558, avg_loss=0.07339]\n",
      "Step 537115  [5.480 sec/step, loss=0.07375, avg_loss=0.07339]\n",
      "Step 537116  [5.478 sec/step, loss=0.07431, avg_loss=0.07341]\n",
      "Step 537117  [5.475 sec/step, loss=0.07560, avg_loss=0.07344]\n",
      "Step 537118  [5.469 sec/step, loss=0.07429, avg_loss=0.07343]\n",
      "Step 537119  [5.470 sec/step, loss=0.07366, avg_loss=0.07345]\n",
      "Generated 32 batches of size 32 in 2.400 sec\n",
      "Step 537120  [5.486 sec/step, loss=0.07391, avg_loss=0.07345]\n",
      "Step 537121  [5.469 sec/step, loss=0.07530, avg_loss=0.07346]\n",
      "Step 537122  [5.453 sec/step, loss=0.07386, avg_loss=0.07344]\n",
      "Step 537123  [5.453 sec/step, loss=0.07140, avg_loss=0.07340]\n",
      "Step 537124  [5.421 sec/step, loss=0.06468, avg_loss=0.07330]\n",
      "Step 537125  [5.412 sec/step, loss=0.07470, avg_loss=0.07329]\n",
      "Step 537126  [5.416 sec/step, loss=0.07530, avg_loss=0.07328]\n",
      "Step 537127  [5.413 sec/step, loss=0.07235, avg_loss=0.07326]\n",
      "Step 537128  [5.412 sec/step, loss=0.07282, avg_loss=0.07326]\n",
      "Step 537129  [5.423 sec/step, loss=0.07256, avg_loss=0.07326]\n",
      "Step 537130  [5.419 sec/step, loss=0.07328, avg_loss=0.07324]\n",
      "Step 537131  [5.430 sec/step, loss=0.07602, avg_loss=0.07326]\n",
      "Step 537132  [5.441 sec/step, loss=0.07551, avg_loss=0.07327]\n",
      "Step 537133  [5.385 sec/step, loss=0.07274, avg_loss=0.07334]\n",
      "Step 537134  [5.384 sec/step, loss=0.07433, avg_loss=0.07334]\n",
      "Step 537135  [5.379 sec/step, loss=0.07376, avg_loss=0.07335]\n",
      "Step 537136  [5.374 sec/step, loss=0.07469, avg_loss=0.07335]\n",
      "Step 537137  [5.377 sec/step, loss=0.07491, avg_loss=0.07335]\n",
      "Step 537138  [5.353 sec/step, loss=0.07177, avg_loss=0.07331]\n",
      "Step 537139  [5.342 sec/step, loss=0.06595, avg_loss=0.07324]\n",
      "Step 537140  [5.388 sec/step, loss=0.06600, avg_loss=0.07315]\n",
      "Step 537141  [5.383 sec/step, loss=0.07548, avg_loss=0.07316]\n",
      "Step 537142  [5.411 sec/step, loss=0.07246, avg_loss=0.07314]\n",
      "Step 537143  [5.394 sec/step, loss=0.07008, avg_loss=0.07308]\n",
      "Step 537144  [5.406 sec/step, loss=0.07392, avg_loss=0.07310]\n",
      "Step 537145  [5.394 sec/step, loss=0.07212, avg_loss=0.07309]\n",
      "Step 537146  [5.403 sec/step, loss=0.07586, avg_loss=0.07311]\n",
      "Step 537147  [5.410 sec/step, loss=0.07327, avg_loss=0.07315]\n",
      "Step 537148  [5.423 sec/step, loss=0.07419, avg_loss=0.07318]\n",
      "Step 537149  [5.378 sec/step, loss=0.07501, avg_loss=0.07327]\n",
      "Step 537150  [5.384 sec/step, loss=0.07446, avg_loss=0.07327]\n",
      "Step 537151  [5.378 sec/step, loss=0.07177, avg_loss=0.07325]\n",
      "Generated 32 batches of size 32 in 2.524 sec\n",
      "Step 537152  [5.381 sec/step, loss=0.07439, avg_loss=0.07328]\n",
      "Step 537153  [5.395 sec/step, loss=0.07466, avg_loss=0.07336]\n",
      "Step 537154  [5.402 sec/step, loss=0.07284, avg_loss=0.07337]\n",
      "Step 537155  [5.396 sec/step, loss=0.07441, avg_loss=0.07335]\n",
      "Step 537156  [5.403 sec/step, loss=0.07584, avg_loss=0.07336]\n",
      "Step 537157  [5.394 sec/step, loss=0.07478, avg_loss=0.07336]\n",
      "Step 537158  [5.386 sec/step, loss=0.07495, avg_loss=0.07339]\n",
      "Step 537159  [5.379 sec/step, loss=0.07289, avg_loss=0.07337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 537160  [5.380 sec/step, loss=0.07581, avg_loss=0.07337]\n",
      "Step 537161  [5.377 sec/step, loss=0.07308, avg_loss=0.07337]\n",
      "Step 537162  [5.362 sec/step, loss=0.07207, avg_loss=0.07337]\n",
      "Step 537163  [5.371 sec/step, loss=0.07516, avg_loss=0.07337]\n",
      "Step 537164  [5.383 sec/step, loss=0.07599, avg_loss=0.07339]\n",
      "Step 537165  [5.358 sec/step, loss=0.07093, avg_loss=0.07335]\n",
      "Step 537166  [5.372 sec/step, loss=0.07556, avg_loss=0.07336]\n",
      "Step 537167  [5.354 sec/step, loss=0.07302, avg_loss=0.07333]\n",
      "Step 537168  [5.354 sec/step, loss=0.06545, avg_loss=0.07333]\n",
      "Step 537169  [5.344 sec/step, loss=0.07268, avg_loss=0.07331]\n",
      "Step 537170  [5.346 sec/step, loss=0.07559, avg_loss=0.07332]\n",
      "Step 537171  [5.357 sec/step, loss=0.07302, avg_loss=0.07331]\n",
      "Step 537172  [5.349 sec/step, loss=0.07407, avg_loss=0.07329]\n",
      "Step 537173  [5.359 sec/step, loss=0.07453, avg_loss=0.07332]\n",
      "Step 537174  [5.372 sec/step, loss=0.07450, avg_loss=0.07333]\n",
      "Step 537175  [5.380 sec/step, loss=0.07506, avg_loss=0.07334]\n",
      "Step 537176  [5.375 sec/step, loss=0.07133, avg_loss=0.07329]\n",
      "Step 537177  [5.373 sec/step, loss=0.07461, avg_loss=0.07329]\n",
      "Step 537178  [5.380 sec/step, loss=0.07598, avg_loss=0.07330]\n",
      "Step 537179  [5.404 sec/step, loss=0.07240, avg_loss=0.07328]\n",
      "Step 537180  [5.409 sec/step, loss=0.07349, avg_loss=0.07328]\n",
      "Step 537181  [5.412 sec/step, loss=0.07190, avg_loss=0.07329]\n",
      "Step 537182  [5.405 sec/step, loss=0.07616, avg_loss=0.07332]\n",
      "Step 537183  [5.393 sec/step, loss=0.06580, avg_loss=0.07324]\n",
      "Generated 32 batches of size 32 in 2.368 sec\n",
      "Step 537184  [5.395 sec/step, loss=0.07431, avg_loss=0.07325]\n",
      "Step 537185  [5.396 sec/step, loss=0.07447, avg_loss=0.07328]\n",
      "Step 537186  [5.399 sec/step, loss=0.07419, avg_loss=0.07327]\n",
      "Step 537187  [5.403 sec/step, loss=0.07349, avg_loss=0.07327]\n",
      "Step 537188  [5.383 sec/step, loss=0.07329, avg_loss=0.07325]\n",
      "Step 537189  [5.387 sec/step, loss=0.07187, avg_loss=0.07323]\n",
      "Step 537190  [5.402 sec/step, loss=0.07366, avg_loss=0.07325]\n",
      "Step 537191  [5.415 sec/step, loss=0.07342, avg_loss=0.07326]\n",
      "Step 537192  [5.405 sec/step, loss=0.07491, avg_loss=0.07327]\n",
      "Step 537193  [5.422 sec/step, loss=0.07353, avg_loss=0.07334]\n",
      "Step 537194  [5.405 sec/step, loss=0.07262, avg_loss=0.07330]\n",
      "Step 537195  [5.415 sec/step, loss=0.07578, avg_loss=0.07331]\n",
      "Step 537196  [5.422 sec/step, loss=0.07456, avg_loss=0.07331]\n",
      "Step 537197  [5.396 sec/step, loss=0.07455, avg_loss=0.07333]\n",
      "Step 537198  [5.387 sec/step, loss=0.07339, avg_loss=0.07331]\n",
      "Step 537199  [5.391 sec/step, loss=0.07502, avg_loss=0.07333]\n",
      "Step 537200  [5.424 sec/step, loss=0.07323, avg_loss=0.07332]\n",
      "Writing summary at step: 537200\n",
      "Step 537201  [5.401 sec/step, loss=0.06506, avg_loss=0.07322]\n",
      "Step 537202  [5.465 sec/step, loss=0.06640, avg_loss=0.07318]\n",
      "Step 537203  [5.462 sec/step, loss=0.07618, avg_loss=0.07320]\n",
      "Step 537204  [5.439 sec/step, loss=0.07091, avg_loss=0.07317]\n",
      "Step 537205  [5.441 sec/step, loss=0.07441, avg_loss=0.07317]\n",
      "Step 537206  [5.434 sec/step, loss=0.07336, avg_loss=0.07317]\n",
      "Step 537207  [5.428 sec/step, loss=0.07445, avg_loss=0.07318]\n",
      "Step 537208  [5.420 sec/step, loss=0.07445, avg_loss=0.07317]\n",
      "Step 537209  [5.441 sec/step, loss=0.07283, avg_loss=0.07320]\n",
      "Step 537210  [5.443 sec/step, loss=0.07479, avg_loss=0.07324]\n",
      "Step 537211  [5.451 sec/step, loss=0.07456, avg_loss=0.07323]\n",
      "Step 537212  [5.465 sec/step, loss=0.07612, avg_loss=0.07327]\n",
      "Step 537213  [5.456 sec/step, loss=0.07089, avg_loss=0.07325]\n",
      "Step 537214  [5.408 sec/step, loss=0.07462, avg_loss=0.07334]\n",
      "Generated 32 batches of size 32 in 2.545 sec\n",
      "Step 537215  [5.408 sec/step, loss=0.07288, avg_loss=0.07333]\n",
      "Step 537216  [5.416 sec/step, loss=0.07527, avg_loss=0.07334]\n",
      "Step 537217  [5.413 sec/step, loss=0.07097, avg_loss=0.07330]\n",
      "Step 537218  [5.405 sec/step, loss=0.07345, avg_loss=0.07329]\n",
      "Step 537219  [5.410 sec/step, loss=0.07411, avg_loss=0.07329]\n",
      "Step 537220  [5.411 sec/step, loss=0.07559, avg_loss=0.07331]\n",
      "Step 537221  [5.404 sec/step, loss=0.07350, avg_loss=0.07329]\n",
      "Step 537222  [5.423 sec/step, loss=0.07325, avg_loss=0.07329]\n",
      "Step 537223  [5.440 sec/step, loss=0.07265, avg_loss=0.07330]\n",
      "Step 537224  [5.440 sec/step, loss=0.06484, avg_loss=0.07330]\n",
      "Step 537225  [5.430 sec/step, loss=0.07110, avg_loss=0.07326]\n",
      "Step 537226  [5.413 sec/step, loss=0.07388, avg_loss=0.07325]\n",
      "Step 537227  [5.428 sec/step, loss=0.07443, avg_loss=0.07327]\n",
      "Step 537228  [5.393 sec/step, loss=0.07356, avg_loss=0.07328]\n",
      "Step 537229  [5.384 sec/step, loss=0.07394, avg_loss=0.07329]\n",
      "Step 537230  [5.399 sec/step, loss=0.07537, avg_loss=0.07331]\n",
      "Step 537231  [5.388 sec/step, loss=0.07508, avg_loss=0.07330]\n",
      "Step 537232  [5.384 sec/step, loss=0.07139, avg_loss=0.07326]\n",
      "Step 537233  [5.404 sec/step, loss=0.07486, avg_loss=0.07328]\n",
      "Step 537234  [5.414 sec/step, loss=0.07616, avg_loss=0.07330]\n",
      "Step 537235  [5.408 sec/step, loss=0.07003, avg_loss=0.07327]\n",
      "Step 537236  [5.413 sec/step, loss=0.07631, avg_loss=0.07328]\n",
      "Step 537237  [5.415 sec/step, loss=0.07517, avg_loss=0.07328]\n",
      "Step 537238  [5.430 sec/step, loss=0.07470, avg_loss=0.07331]\n",
      "Step 537239  [5.446 sec/step, loss=0.07490, avg_loss=0.07340]\n",
      "Step 537240  [5.409 sec/step, loss=0.07569, avg_loss=0.07350]\n",
      "Step 537241  [5.392 sec/step, loss=0.07249, avg_loss=0.07347]\n",
      "Step 537242  [5.361 sec/step, loss=0.07291, avg_loss=0.07347]\n",
      "Step 537243  [5.373 sec/step, loss=0.07408, avg_loss=0.07351]\n",
      "Step 537244  [5.382 sec/step, loss=0.07566, avg_loss=0.07353]\n",
      "Step 537245  [5.377 sec/step, loss=0.07301, avg_loss=0.07354]\n",
      "Step 537246  [5.378 sec/step, loss=0.07200, avg_loss=0.07350]\n",
      "Generated 32 batches of size 32 in 2.565 sec\n",
      "Step 537247  [5.380 sec/step, loss=0.07443, avg_loss=0.07351]\n",
      "Step 537248  [5.403 sec/step, loss=0.07246, avg_loss=0.07350]\n",
      "Step 537249  [5.404 sec/step, loss=0.07446, avg_loss=0.07349]\n",
      "Step 537250  [5.444 sec/step, loss=0.06579, avg_loss=0.07340]\n",
      "Step 537251  [5.457 sec/step, loss=0.07505, avg_loss=0.07344]\n",
      "Step 537252  [5.471 sec/step, loss=0.07539, avg_loss=0.07345]\n",
      "Step 537253  [5.463 sec/step, loss=0.07041, avg_loss=0.07340]\n",
      "Step 537254  [5.472 sec/step, loss=0.07292, avg_loss=0.07341]\n",
      "Step 537255  [5.474 sec/step, loss=0.07366, avg_loss=0.07340]\n",
      "Step 537256  [5.460 sec/step, loss=0.07465, avg_loss=0.07339]\n",
      "Step 537257  [5.468 sec/step, loss=0.07431, avg_loss=0.07338]\n",
      "Step 537258  [5.445 sec/step, loss=0.07111, avg_loss=0.07334]\n",
      "Step 537259  [5.458 sec/step, loss=0.07605, avg_loss=0.07337]\n",
      "Step 537260  [5.449 sec/step, loss=0.07454, avg_loss=0.07336]\n",
      "Step 537261  [5.464 sec/step, loss=0.07342, avg_loss=0.07337]\n",
      "Step 537262  [5.462 sec/step, loss=0.06692, avg_loss=0.07331]\n",
      "Step 537263  [5.442 sec/step, loss=0.07382, avg_loss=0.07330]\n",
      "Step 537264  [5.429 sec/step, loss=0.07325, avg_loss=0.07327]\n",
      "Step 537265  [5.453 sec/step, loss=0.07544, avg_loss=0.07332]\n",
      "Step 537266  [5.424 sec/step, loss=0.07113, avg_loss=0.07327]\n",
      "Step 537267  [5.484 sec/step, loss=0.06559, avg_loss=0.07320]\n",
      "Step 537268  [5.434 sec/step, loss=0.07434, avg_loss=0.07329]\n",
      "Step 537269  [5.440 sec/step, loss=0.07598, avg_loss=0.07332]\n",
      "Step 537270  [5.416 sec/step, loss=0.07215, avg_loss=0.07329]\n",
      "Step 537271  [5.413 sec/step, loss=0.07603, avg_loss=0.07332]\n",
      "Step 537272  [5.416 sec/step, loss=0.07384, avg_loss=0.07332]\n",
      "Step 537273  [5.421 sec/step, loss=0.07428, avg_loss=0.07331]\n",
      "Step 537274  [5.420 sec/step, loss=0.07554, avg_loss=0.07332]\n",
      "Step 537275  [5.424 sec/step, loss=0.07507, avg_loss=0.07332]\n",
      "Step 537276  [5.420 sec/step, loss=0.07356, avg_loss=0.07335]\n",
      "Step 537277  [5.415 sec/step, loss=0.07112, avg_loss=0.07331]\n",
      "Step 537278  [5.396 sec/step, loss=0.07342, avg_loss=0.07328]\n",
      "Generated 32 batches of size 32 in 2.384 sec\n",
      "Step 537279  [5.392 sec/step, loss=0.07210, avg_loss=0.07328]\n",
      "Step 537280  [5.405 sec/step, loss=0.07355, avg_loss=0.07328]\n",
      "Step 537281  [5.414 sec/step, loss=0.07381, avg_loss=0.07330]\n",
      "Step 537282  [5.409 sec/step, loss=0.07581, avg_loss=0.07330]\n",
      "Step 537283  [5.419 sec/step, loss=0.07283, avg_loss=0.07337]\n",
      "Step 537284  [5.432 sec/step, loss=0.07516, avg_loss=0.07338]\n",
      "Step 537285  [5.435 sec/step, loss=0.07349, avg_loss=0.07337]\n",
      "Step 537286  [5.439 sec/step, loss=0.07475, avg_loss=0.07337]\n",
      "Step 537287  [5.441 sec/step, loss=0.07476, avg_loss=0.07339]\n",
      "Step 537288  [5.456 sec/step, loss=0.07437, avg_loss=0.07340]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 537289  [5.457 sec/step, loss=0.07361, avg_loss=0.07341]\n",
      "Step 537290  [5.449 sec/step, loss=0.07347, avg_loss=0.07341]\n",
      "Step 537291  [5.433 sec/step, loss=0.07441, avg_loss=0.07342]\n",
      "Step 537292  [5.430 sec/step, loss=0.07203, avg_loss=0.07339]\n",
      "Step 537293  [5.429 sec/step, loss=0.07113, avg_loss=0.07337]\n",
      "Step 537294  [5.438 sec/step, loss=0.07493, avg_loss=0.07339]\n",
      "Step 537295  [5.441 sec/step, loss=0.07560, avg_loss=0.07339]\n",
      "Step 537296  [5.449 sec/step, loss=0.07529, avg_loss=0.07340]\n",
      "Step 537297  [5.457 sec/step, loss=0.07594, avg_loss=0.07341]\n",
      "Step 537298  [5.451 sec/step, loss=0.07415, avg_loss=0.07342]\n",
      "Step 537299  [5.455 sec/step, loss=0.07530, avg_loss=0.07342]\n",
      "Step 537300  [5.439 sec/step, loss=0.07520, avg_loss=0.07344]\n",
      "Writing summary at step: 537300\n",
      "Step 537301  [5.457 sec/step, loss=0.07446, avg_loss=0.07354]\n",
      "Step 537302  [5.415 sec/step, loss=0.07574, avg_loss=0.07363]\n",
      "Step 537303  [5.452 sec/step, loss=0.06671, avg_loss=0.07353]\n",
      "Step 537304  [5.480 sec/step, loss=0.07534, avg_loss=0.07358]\n",
      "Step 537305  [5.466 sec/step, loss=0.07363, avg_loss=0.07357]\n",
      "Step 537306  [5.469 sec/step, loss=0.07309, avg_loss=0.07357]\n",
      "Step 537307  [5.493 sec/step, loss=0.07270, avg_loss=0.07355]\n",
      "Step 537308  [5.501 sec/step, loss=0.07550, avg_loss=0.07356]\n",
      "Step 537309  [5.489 sec/step, loss=0.07449, avg_loss=0.07358]\n",
      "Generated 32 batches of size 32 in 2.409 sec\n",
      "Step 537310  [5.504 sec/step, loss=0.07385, avg_loss=0.07357]\n",
      "Step 537311  [5.493 sec/step, loss=0.07459, avg_loss=0.07357]\n",
      "Step 537312  [5.477 sec/step, loss=0.07051, avg_loss=0.07351]\n",
      "Step 537313  [5.503 sec/step, loss=0.07528, avg_loss=0.07356]\n",
      "Step 537314  [5.485 sec/step, loss=0.06510, avg_loss=0.07346]\n",
      "Step 537315  [5.474 sec/step, loss=0.07078, avg_loss=0.07344]\n",
      "Step 537316  [5.463 sec/step, loss=0.07461, avg_loss=0.07343]\n",
      "Step 537317  [5.463 sec/step, loss=0.07463, avg_loss=0.07347]\n",
      "Step 537318  [5.474 sec/step, loss=0.07417, avg_loss=0.07348]\n",
      "Step 537319  [5.465 sec/step, loss=0.07471, avg_loss=0.07348]\n",
      "Step 537320  [5.466 sec/step, loss=0.07550, avg_loss=0.07348]\n",
      "Step 537321  [5.477 sec/step, loss=0.07512, avg_loss=0.07350]\n",
      "Step 537322  [5.477 sec/step, loss=0.07571, avg_loss=0.07352]\n",
      "Step 537323  [5.482 sec/step, loss=0.07173, avg_loss=0.07351]\n",
      "Step 537324  [5.540 sec/step, loss=0.06845, avg_loss=0.07355]\n",
      "Step 537325  [5.541 sec/step, loss=0.07341, avg_loss=0.07357]\n",
      "Step 537326  [5.565 sec/step, loss=0.07499, avg_loss=0.07358]\n",
      "Step 537327  [5.554 sec/step, loss=0.07459, avg_loss=0.07359]\n",
      "Step 537328  [5.562 sec/step, loss=0.07097, avg_loss=0.07356]\n",
      "Step 537329  [5.567 sec/step, loss=0.07455, avg_loss=0.07357]\n",
      "Step 537330  [5.566 sec/step, loss=0.07272, avg_loss=0.07354]\n",
      "Step 537331  [5.563 sec/step, loss=0.07502, avg_loss=0.07354]\n",
      "Step 537332  [5.569 sec/step, loss=0.07374, avg_loss=0.07356]\n",
      "Step 537333  [5.570 sec/step, loss=0.07496, avg_loss=0.07356]\n",
      "Step 537334  [5.572 sec/step, loss=0.07545, avg_loss=0.07356]\n",
      "Step 537335  [5.591 sec/step, loss=0.07318, avg_loss=0.07359]\n",
      "Step 537336  [5.592 sec/step, loss=0.07615, avg_loss=0.07359]\n",
      "Step 537337  [5.579 sec/step, loss=0.07141, avg_loss=0.07355]\n",
      "Step 537338  [5.579 sec/step, loss=0.07452, avg_loss=0.07355]\n",
      "Step 537339  [5.587 sec/step, loss=0.07398, avg_loss=0.07354]\n",
      "Step 537340  [5.585 sec/step, loss=0.07597, avg_loss=0.07354]\n",
      "Step 537341  [5.592 sec/step, loss=0.07397, avg_loss=0.07355]\n",
      "Generated 32 batches of size 32 in 2.619 sec\n",
      "Step 537342  [5.590 sec/step, loss=0.07356, avg_loss=0.07356]\n",
      "Step 537343  [5.593 sec/step, loss=0.07442, avg_loss=0.07356]\n",
      "Step 537344  [5.575 sec/step, loss=0.07323, avg_loss=0.07354]\n",
      "Step 537345  [5.565 sec/step, loss=0.06644, avg_loss=0.07347]\n",
      "Step 537346  [5.542 sec/step, loss=0.07265, avg_loss=0.07348]\n",
      "Step 537347  [5.537 sec/step, loss=0.07332, avg_loss=0.07347]\n",
      "Step 537348  [5.501 sec/step, loss=0.07362, avg_loss=0.07348]\n",
      "Step 537349  [5.495 sec/step, loss=0.07406, avg_loss=0.07348]\n",
      "Step 537350  [5.440 sec/step, loss=0.07305, avg_loss=0.07355]\n",
      "Step 537351  [5.461 sec/step, loss=0.07443, avg_loss=0.07354]\n",
      "Step 537352  [5.447 sec/step, loss=0.07496, avg_loss=0.07354]\n",
      "Step 537353  [5.454 sec/step, loss=0.07477, avg_loss=0.07358]\n",
      "Step 537354  [5.449 sec/step, loss=0.07483, avg_loss=0.07360]\n",
      "Step 537355  [5.444 sec/step, loss=0.07220, avg_loss=0.07359]\n",
      "Step 537356  [5.442 sec/step, loss=0.07198, avg_loss=0.07356]\n",
      "Step 537357  [5.445 sec/step, loss=0.07581, avg_loss=0.07358]\n",
      "Step 537358  [5.467 sec/step, loss=0.07530, avg_loss=0.07362]\n",
      "Step 537359  [5.441 sec/step, loss=0.07091, avg_loss=0.07357]\n",
      "Step 537360  [5.486 sec/step, loss=0.06558, avg_loss=0.07348]\n",
      "Step 537361  [5.479 sec/step, loss=0.07347, avg_loss=0.07348]\n",
      "Step 537362  [5.509 sec/step, loss=0.07600, avg_loss=0.07357]\n",
      "Step 537363  [5.533 sec/step, loss=0.07515, avg_loss=0.07358]\n",
      "Step 537364  [5.547 sec/step, loss=0.07493, avg_loss=0.07360]\n",
      "Step 537365  [5.518 sec/step, loss=0.06490, avg_loss=0.07349]\n",
      "Step 537366  [5.530 sec/step, loss=0.07528, avg_loss=0.07353]\n",
      "Step 537367  [5.490 sec/step, loss=0.07547, avg_loss=0.07363]\n",
      "Step 537368  [5.499 sec/step, loss=0.07484, avg_loss=0.07364]\n",
      "Step 537369  [5.486 sec/step, loss=0.07349, avg_loss=0.07361]\n",
      "Step 537370  [5.501 sec/step, loss=0.07449, avg_loss=0.07364]\n",
      "Step 537371  [5.486 sec/step, loss=0.07345, avg_loss=0.07361]\n",
      "Step 537372  [5.495 sec/step, loss=0.07375, avg_loss=0.07361]\n",
      "Step 537373  [5.498 sec/step, loss=0.07216, avg_loss=0.07359]\n",
      "Generated 32 batches of size 32 in 2.470 sec\n",
      "Step 537374  [5.506 sec/step, loss=0.07597, avg_loss=0.07359]\n",
      "Step 537375  [5.504 sec/step, loss=0.07418, avg_loss=0.07358]\n",
      "Step 537376  [5.503 sec/step, loss=0.07112, avg_loss=0.07356]\n",
      "Step 537377  [5.495 sec/step, loss=0.07018, avg_loss=0.07355]\n",
      "Step 537378  [5.510 sec/step, loss=0.07422, avg_loss=0.07356]\n",
      "Step 537379  [5.488 sec/step, loss=0.07354, avg_loss=0.07357]\n",
      "Step 537380  [5.467 sec/step, loss=0.07285, avg_loss=0.07357]\n",
      "Step 537381  [5.468 sec/step, loss=0.07312, avg_loss=0.07356]\n",
      "Step 537382  [5.456 sec/step, loss=0.07362, avg_loss=0.07354]\n",
      "Step 537383  [5.514 sec/step, loss=0.06662, avg_loss=0.07347]\n",
      "Step 537384  [5.488 sec/step, loss=0.07232, avg_loss=0.07345]\n",
      "Step 537385  [5.503 sec/step, loss=0.07576, avg_loss=0.07347]\n",
      "Step 537386  [5.495 sec/step, loss=0.07451, avg_loss=0.07347]\n",
      "Step 537387  [5.508 sec/step, loss=0.07530, avg_loss=0.07347]\n",
      "Step 537388  [5.485 sec/step, loss=0.07060, avg_loss=0.07343]\n",
      "Step 537389  [5.493 sec/step, loss=0.07542, avg_loss=0.07345]\n",
      "Step 537390  [5.493 sec/step, loss=0.07493, avg_loss=0.07347]\n",
      "Step 537391  [5.507 sec/step, loss=0.07338, avg_loss=0.07346]\n",
      "Step 537392  [5.498 sec/step, loss=0.07339, avg_loss=0.07347]\n",
      "Step 537393  [5.506 sec/step, loss=0.07610, avg_loss=0.07352]\n",
      "Step 537394  [5.505 sec/step, loss=0.07465, avg_loss=0.07352]\n",
      "Step 537395  [5.493 sec/step, loss=0.07523, avg_loss=0.07351]\n",
      "Step 537396  [5.473 sec/step, loss=0.07081, avg_loss=0.07347]\n",
      "Step 537397  [5.474 sec/step, loss=0.07539, avg_loss=0.07346]\n",
      "Step 537398  [5.482 sec/step, loss=0.07378, avg_loss=0.07346]\n",
      "Step 537399  [5.494 sec/step, loss=0.07572, avg_loss=0.07346]\n",
      "Step 537400  [5.478 sec/step, loss=0.07234, avg_loss=0.07344]\n",
      "Writing summary at step: 537400\n",
      "Step 537401  [5.504 sec/step, loss=0.07306, avg_loss=0.07342]\n",
      "Step 537402  [5.497 sec/step, loss=0.07390, avg_loss=0.07340]\n",
      "Step 537403  [5.439 sec/step, loss=0.07335, avg_loss=0.07347]\n",
      "Step 537404  [5.405 sec/step, loss=0.06609, avg_loss=0.07338]\n",
      "Generated 32 batches of size 32 in 2.604 sec\n",
      "Step 537405  [5.415 sec/step, loss=0.07272, avg_loss=0.07337]\n",
      "Step 537406  [5.411 sec/step, loss=0.07287, avg_loss=0.07337]\n",
      "Step 537407  [5.381 sec/step, loss=0.06960, avg_loss=0.07333]\n",
      "Step 537408  [5.375 sec/step, loss=0.07410, avg_loss=0.07332]\n",
      "Step 537409  [5.378 sec/step, loss=0.07413, avg_loss=0.07332]\n",
      "Step 537410  [5.384 sec/step, loss=0.07293, avg_loss=0.07331]\n",
      "Step 537411  [5.384 sec/step, loss=0.07261, avg_loss=0.07329]\n",
      "Step 537412  [5.390 sec/step, loss=0.07474, avg_loss=0.07333]\n",
      "Step 537413  [5.384 sec/step, loss=0.07436, avg_loss=0.07332]\n",
      "Step 537414  [5.401 sec/step, loss=0.07521, avg_loss=0.07342]\n",
      "Step 537415  [5.416 sec/step, loss=0.07402, avg_loss=0.07345]\n",
      "Step 537416  [5.422 sec/step, loss=0.07185, avg_loss=0.07343]\n",
      "Step 537417  [5.424 sec/step, loss=0.07068, avg_loss=0.07339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 537418  [5.428 sec/step, loss=0.07548, avg_loss=0.07340]\n",
      "Step 537419  [5.444 sec/step, loss=0.07547, avg_loss=0.07341]\n",
      "Step 537420  [5.424 sec/step, loss=0.07319, avg_loss=0.07339]\n",
      "Step 537421  [5.428 sec/step, loss=0.07131, avg_loss=0.07335]\n",
      "Step 537422  [5.422 sec/step, loss=0.07427, avg_loss=0.07333]\n",
      "Step 537423  [5.408 sec/step, loss=0.07601, avg_loss=0.07338]\n",
      "Step 537424  [5.356 sec/step, loss=0.07352, avg_loss=0.07343]\n",
      "Step 537425  [5.406 sec/step, loss=0.06523, avg_loss=0.07334]\n",
      "Step 537426  [5.400 sec/step, loss=0.07434, avg_loss=0.07334]\n",
      "Step 537427  [5.412 sec/step, loss=0.07545, avg_loss=0.07335]\n",
      "Step 537428  [5.408 sec/step, loss=0.07331, avg_loss=0.07337]\n",
      "Step 537429  [5.390 sec/step, loss=0.07134, avg_loss=0.07334]\n",
      "Step 537430  [5.386 sec/step, loss=0.07548, avg_loss=0.07337]\n",
      "Step 537431  [5.384 sec/step, loss=0.07420, avg_loss=0.07336]\n",
      "Step 537432  [5.389 sec/step, loss=0.07313, avg_loss=0.07335]\n",
      "Step 537433  [5.362 sec/step, loss=0.07091, avg_loss=0.07331]\n",
      "Step 537434  [5.364 sec/step, loss=0.07585, avg_loss=0.07331]\n",
      "Step 537435  [5.368 sec/step, loss=0.07544, avg_loss=0.07334]\n",
      "Step 537436  [5.357 sec/step, loss=0.07161, avg_loss=0.07329]\n",
      "Generated 32 batches of size 32 in 2.556 sec\n",
      "Step 537437  [5.368 sec/step, loss=0.07440, avg_loss=0.07332]\n",
      "Step 537438  [5.394 sec/step, loss=0.07267, avg_loss=0.07330]\n",
      "Step 537439  [5.381 sec/step, loss=0.07308, avg_loss=0.07329]\n",
      "Step 537440  [5.374 sec/step, loss=0.07484, avg_loss=0.07328]\n",
      "Step 537441  [5.370 sec/step, loss=0.07509, avg_loss=0.07329]\n",
      "Step 537442  [5.360 sec/step, loss=0.06561, avg_loss=0.07321]\n",
      "Step 537443  [5.348 sec/step, loss=0.07241, avg_loss=0.07319]\n",
      "Step 537444  [5.375 sec/step, loss=0.07258, avg_loss=0.07319]\n",
      "Step 537445  [5.391 sec/step, loss=0.07415, avg_loss=0.07327]\n",
      "Step 537446  [5.391 sec/step, loss=0.07059, avg_loss=0.07324]\n",
      "Step 537447  [5.395 sec/step, loss=0.07545, avg_loss=0.07327]\n",
      "Step 537448  [5.402 sec/step, loss=0.07353, avg_loss=0.07327]\n",
      "Step 537449  [5.396 sec/step, loss=0.07341, avg_loss=0.07326]\n",
      "Step 537450  [5.454 sec/step, loss=0.06570, avg_loss=0.07319]\n",
      "Step 537451  [5.440 sec/step, loss=0.07376, avg_loss=0.07318]\n",
      "Step 537452  [5.463 sec/step, loss=0.07277, avg_loss=0.07316]\n",
      "Step 537453  [5.475 sec/step, loss=0.07529, avg_loss=0.07316]\n",
      "Step 537454  [5.475 sec/step, loss=0.07310, avg_loss=0.07314]\n",
      "Step 537455  [5.488 sec/step, loss=0.07565, avg_loss=0.07318]\n",
      "Step 537456  [5.505 sec/step, loss=0.07544, avg_loss=0.07321]\n",
      "Step 537457  [5.495 sec/step, loss=0.07469, avg_loss=0.07320]\n",
      "Step 537458  [5.483 sec/step, loss=0.07385, avg_loss=0.07319]\n",
      "Step 537459  [5.510 sec/step, loss=0.07334, avg_loss=0.07321]\n",
      "Step 537460  [5.457 sec/step, loss=0.07452, avg_loss=0.07330]\n",
      "Step 537461  [5.449 sec/step, loss=0.07061, avg_loss=0.07327]\n",
      "Step 537462  [5.461 sec/step, loss=0.07320, avg_loss=0.07324]\n",
      "Step 537463  [5.460 sec/step, loss=0.07549, avg_loss=0.07325]\n",
      "Step 537464  [5.448 sec/step, loss=0.07101, avg_loss=0.07321]\n",
      "Step 537465  [5.473 sec/step, loss=0.07389, avg_loss=0.07330]\n",
      "Step 537466  [5.462 sec/step, loss=0.07347, avg_loss=0.07328]\n",
      "Step 537467  [5.456 sec/step, loss=0.07402, avg_loss=0.07327]\n",
      "Step 537468  [5.443 sec/step, loss=0.07267, avg_loss=0.07324]\n",
      "Generated 32 batches of size 32 in 2.526 sec\n",
      "Step 537469  [5.461 sec/step, loss=0.07412, avg_loss=0.07325]\n",
      "Step 537470  [5.453 sec/step, loss=0.07358, avg_loss=0.07324]\n",
      "Step 537471  [5.463 sec/step, loss=0.07481, avg_loss=0.07326]\n",
      "Step 537472  [5.453 sec/step, loss=0.07463, avg_loss=0.07326]\n",
      "Step 537473  [5.433 sec/step, loss=0.06577, avg_loss=0.07320]\n",
      "Step 537474  [5.421 sec/step, loss=0.07488, avg_loss=0.07319]\n",
      "Step 537475  [5.425 sec/step, loss=0.07575, avg_loss=0.07320]\n",
      "Step 537476  [5.432 sec/step, loss=0.07442, avg_loss=0.07324]\n",
      "Step 537477  [5.453 sec/step, loss=0.07434, avg_loss=0.07328]\n",
      "Step 537478  [5.451 sec/step, loss=0.07486, avg_loss=0.07329]\n",
      "Step 537479  [5.464 sec/step, loss=0.07528, avg_loss=0.07330]\n",
      "Step 537480  [5.483 sec/step, loss=0.07446, avg_loss=0.07332]\n",
      "Step 537481  [5.463 sec/step, loss=0.07318, avg_loss=0.07332]\n",
      "Step 537482  [5.465 sec/step, loss=0.07200, avg_loss=0.07330]\n",
      "Step 537483  [5.429 sec/step, loss=0.07564, avg_loss=0.07339]\n",
      "Step 537484  [5.422 sec/step, loss=0.07331, avg_loss=0.07340]\n",
      "Step 537485  [5.414 sec/step, loss=0.07571, avg_loss=0.07340]\n",
      "Step 537486  [5.422 sec/step, loss=0.07546, avg_loss=0.07341]\n",
      "Step 537487  [5.409 sec/step, loss=0.07182, avg_loss=0.07338]\n",
      "Step 537488  [5.435 sec/step, loss=0.07551, avg_loss=0.07343]\n",
      "Step 537489  [5.434 sec/step, loss=0.07227, avg_loss=0.07340]\n",
      "Step 537490  [5.436 sec/step, loss=0.07501, avg_loss=0.07340]\n",
      "Step 537491  [5.444 sec/step, loss=0.07464, avg_loss=0.07341]\n",
      "Step 537492  [5.466 sec/step, loss=0.07322, avg_loss=0.07341]\n",
      "Step 537493  [5.467 sec/step, loss=0.07627, avg_loss=0.07341]\n",
      "Step 537494  [5.469 sec/step, loss=0.07307, avg_loss=0.07339]\n",
      "Step 537495  [5.495 sec/step, loss=0.07399, avg_loss=0.07338]\n",
      "Step 537496  [5.495 sec/step, loss=0.07081, avg_loss=0.07338]\n",
      "Step 537497  [5.483 sec/step, loss=0.07247, avg_loss=0.07335]\n",
      "Step 537498  [5.488 sec/step, loss=0.07440, avg_loss=0.07336]\n",
      "Step 537499  [5.488 sec/step, loss=0.07597, avg_loss=0.07336]\n",
      "Step 537500  [5.474 sec/step, loss=0.06555, avg_loss=0.07329]\n",
      "Writing summary at step: 537500\n",
      "Generated 32 batches of size 32 in 2.431 sec\n",
      "Step 537501  [5.445 sec/step, loss=0.07453, avg_loss=0.07331]\n",
      "Step 537502  [5.435 sec/step, loss=0.07373, avg_loss=0.07331]\n",
      "Step 537503  [5.440 sec/step, loss=0.07350, avg_loss=0.07331]\n",
      "Step 537504  [5.442 sec/step, loss=0.07178, avg_loss=0.07336]\n",
      "Step 537505  [5.443 sec/step, loss=0.07508, avg_loss=0.07339]\n",
      "Step 537506  [5.452 sec/step, loss=0.07365, avg_loss=0.07340]\n",
      "Step 537507  [5.505 sec/step, loss=0.06606, avg_loss=0.07336]\n",
      "Step 537508  [5.506 sec/step, loss=0.07410, avg_loss=0.07336]\n",
      "Step 537509  [5.510 sec/step, loss=0.07383, avg_loss=0.07336]\n",
      "Step 537510  [5.483 sec/step, loss=0.07253, avg_loss=0.07335]\n",
      "Step 537511  [5.474 sec/step, loss=0.07205, avg_loss=0.07335]\n",
      "Step 537512  [5.484 sec/step, loss=0.07510, avg_loss=0.07335]\n",
      "Step 537513  [5.481 sec/step, loss=0.07548, avg_loss=0.07336]\n",
      "Step 537514  [5.477 sec/step, loss=0.07449, avg_loss=0.07335]\n",
      "Step 537515  [5.492 sec/step, loss=0.07304, avg_loss=0.07334]\n",
      "Step 537516  [5.493 sec/step, loss=0.07545, avg_loss=0.07338]\n",
      "Step 537517  [5.494 sec/step, loss=0.07128, avg_loss=0.07339]\n",
      "Step 537518  [5.496 sec/step, loss=0.07359, avg_loss=0.07337]\n",
      "Step 537519  [5.497 sec/step, loss=0.07590, avg_loss=0.07337]\n",
      "Step 537520  [5.507 sec/step, loss=0.07358, avg_loss=0.07338]\n",
      "Step 537521  [5.506 sec/step, loss=0.07458, avg_loss=0.07341]\n",
      "Step 537522  [5.508 sec/step, loss=0.07460, avg_loss=0.07341]\n",
      "Step 537523  [5.476 sec/step, loss=0.06608, avg_loss=0.07331]\n",
      "Step 537524  [5.538 sec/step, loss=0.06518, avg_loss=0.07323]\n",
      "Step 537525  [5.496 sec/step, loss=0.07466, avg_loss=0.07332]\n",
      "Step 537526  [5.500 sec/step, loss=0.07490, avg_loss=0.07333]\n",
      "Step 537527  [5.489 sec/step, loss=0.07331, avg_loss=0.07331]\n",
      "Step 537528  [5.487 sec/step, loss=0.07396, avg_loss=0.07331]\n",
      "Step 537529  [5.500 sec/step, loss=0.07491, avg_loss=0.07335]\n",
      "Step 537530  [5.506 sec/step, loss=0.07384, avg_loss=0.07333]\n",
      "Step 537531  [5.504 sec/step, loss=0.07166, avg_loss=0.07331]\n",
      "Generated 32 batches of size 32 in 2.639 sec\n",
      "Step 537532  [5.485 sec/step, loss=0.07072, avg_loss=0.07328]\n",
      "Step 537533  [5.503 sec/step, loss=0.07437, avg_loss=0.07332]\n",
      "Step 537534  [5.489 sec/step, loss=0.07373, avg_loss=0.07330]\n",
      "Step 537535  [5.490 sec/step, loss=0.07600, avg_loss=0.07330]\n",
      "Step 537536  [5.482 sec/step, loss=0.07367, avg_loss=0.07332]\n",
      "Step 537537  [5.493 sec/step, loss=0.07534, avg_loss=0.07333]\n",
      "Step 537538  [5.453 sec/step, loss=0.07145, avg_loss=0.07332]\n",
      "Step 537539  [5.487 sec/step, loss=0.07321, avg_loss=0.07332]\n",
      "Step 537540  [5.497 sec/step, loss=0.07582, avg_loss=0.07333]\n",
      "Step 537541  [5.494 sec/step, loss=0.07332, avg_loss=0.07331]\n",
      "Step 537542  [5.508 sec/step, loss=0.07024, avg_loss=0.07336]\n",
      "Step 537543  [5.522 sec/step, loss=0.07457, avg_loss=0.07338]\n",
      "Step 537544  [5.501 sec/step, loss=0.07460, avg_loss=0.07340]\n",
      "Step 537545  [5.506 sec/step, loss=0.07442, avg_loss=0.07341]\n",
      "Step 537546  [5.545 sec/step, loss=0.07395, avg_loss=0.07344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 537547  [5.553 sec/step, loss=0.07459, avg_loss=0.07343]\n",
      "Step 537548  [5.559 sec/step, loss=0.07398, avg_loss=0.07343]\n",
      "Step 537549  [5.580 sec/step, loss=0.07525, avg_loss=0.07345]\n",
      "Step 537550  [5.529 sec/step, loss=0.07439, avg_loss=0.07354]\n",
      "Step 537551  [5.570 sec/step, loss=0.06597, avg_loss=0.07346]\n",
      "Step 537552  [5.529 sec/step, loss=0.07122, avg_loss=0.07345]\n",
      "Step 537553  [5.521 sec/step, loss=0.07390, avg_loss=0.07343]\n",
      "Step 537554  [5.528 sec/step, loss=0.07539, avg_loss=0.07346]\n",
      "Step 537555  [5.524 sec/step, loss=0.07610, avg_loss=0.07346]\n",
      "Step 537556  [5.509 sec/step, loss=0.07276, avg_loss=0.07343]\n",
      "Step 537557  [5.525 sec/step, loss=0.07506, avg_loss=0.07344]\n",
      "Step 537558  [5.510 sec/step, loss=0.07372, avg_loss=0.07344]\n",
      "Step 537559  [5.492 sec/step, loss=0.07372, avg_loss=0.07344]\n",
      "Step 537560  [5.492 sec/step, loss=0.07466, avg_loss=0.07344]\n",
      "Step 537561  [5.516 sec/step, loss=0.07350, avg_loss=0.07347]\n",
      "Step 537562  [5.494 sec/step, loss=0.07473, avg_loss=0.07349]\n",
      "Step 537563  [5.481 sec/step, loss=0.07288, avg_loss=0.07346]\n",
      "Generated 32 batches of size 32 in 2.595 sec\n",
      "Step 537564  [5.494 sec/step, loss=0.07443, avg_loss=0.07349]\n",
      "Step 537565  [5.497 sec/step, loss=0.07573, avg_loss=0.07351]\n",
      "Step 537566  [5.501 sec/step, loss=0.07352, avg_loss=0.07351]\n",
      "Step 537567  [5.509 sec/step, loss=0.07326, avg_loss=0.07350]\n",
      "Step 537568  [5.502 sec/step, loss=0.07062, avg_loss=0.07348]\n",
      "Step 537569  [5.497 sec/step, loss=0.07409, avg_loss=0.07348]\n",
      "Step 537570  [5.488 sec/step, loss=0.06614, avg_loss=0.07341]\n",
      "Step 537571  [5.504 sec/step, loss=0.07519, avg_loss=0.07341]\n",
      "Step 537572  [5.514 sec/step, loss=0.07562, avg_loss=0.07342]\n",
      "Step 537573  [5.583 sec/step, loss=0.06603, avg_loss=0.07343]\n",
      "Step 537574  [5.593 sec/step, loss=0.07393, avg_loss=0.07342]\n",
      "Step 537575  [5.597 sec/step, loss=0.07517, avg_loss=0.07341]\n",
      "Step 537576  [5.596 sec/step, loss=0.07154, avg_loss=0.07338]\n",
      "Step 537577  [5.584 sec/step, loss=0.07254, avg_loss=0.07336]\n",
      "Step 537578  [5.578 sec/step, loss=0.07243, avg_loss=0.07334]\n",
      "Step 537579  [5.567 sec/step, loss=0.07562, avg_loss=0.07334]\n",
      "Step 537580  [5.557 sec/step, loss=0.07399, avg_loss=0.07334]\n",
      "Step 537581  [5.565 sec/step, loss=0.07446, avg_loss=0.07335]\n",
      "Step 537582  [5.568 sec/step, loss=0.07588, avg_loss=0.07339]\n",
      "Step 537583  [5.557 sec/step, loss=0.07577, avg_loss=0.07339]\n",
      "Step 537584  [5.574 sec/step, loss=0.07529, avg_loss=0.07341]\n",
      "Step 537585  [5.557 sec/step, loss=0.07415, avg_loss=0.07340]\n",
      "Step 537586  [5.564 sec/step, loss=0.07621, avg_loss=0.07340]\n",
      "Step 537587  [5.571 sec/step, loss=0.07434, avg_loss=0.07343]\n",
      "Step 537588  [5.552 sec/step, loss=0.07363, avg_loss=0.07341]\n",
      "Step 537589  [5.544 sec/step, loss=0.07548, avg_loss=0.07344]\n",
      "Step 537590  [5.543 sec/step, loss=0.07523, avg_loss=0.07344]\n",
      "Step 537591  [5.507 sec/step, loss=0.06635, avg_loss=0.07336]\n",
      "Step 537592  [5.485 sec/step, loss=0.07235, avg_loss=0.07335]\n",
      "Step 537593  [5.482 sec/step, loss=0.07563, avg_loss=0.07335]\n",
      "Step 537594  [5.467 sec/step, loss=0.07433, avg_loss=0.07336]\n",
      "Step 537595  [5.435 sec/step, loss=0.07411, avg_loss=0.07336]\n",
      "Generated 32 batches of size 32 in 2.499 sec\n",
      "Step 537596  [5.460 sec/step, loss=0.07595, avg_loss=0.07341]\n",
      "Step 537597  [5.469 sec/step, loss=0.07614, avg_loss=0.07345]\n",
      "Step 537598  [5.471 sec/step, loss=0.07642, avg_loss=0.07347]\n",
      "Step 537599  [5.472 sec/step, loss=0.07367, avg_loss=0.07344]\n",
      "Step 537600  [5.505 sec/step, loss=0.07609, avg_loss=0.07355]\n",
      "Writing summary at step: 537600\n",
      "Step 537601  [5.492 sec/step, loss=0.07180, avg_loss=0.07352]\n",
      "Step 537602  [5.523 sec/step, loss=0.07359, avg_loss=0.07352]\n",
      "Step 537603  [5.555 sec/step, loss=0.07404, avg_loss=0.07353]\n",
      "Step 537604  [5.555 sec/step, loss=0.07152, avg_loss=0.07352]\n",
      "Step 537605  [5.553 sec/step, loss=0.07447, avg_loss=0.07352]\n",
      "Step 537606  [5.566 sec/step, loss=0.07583, avg_loss=0.07354]\n",
      "Step 537607  [5.507 sec/step, loss=0.07366, avg_loss=0.07362]\n",
      "Step 537608  [5.497 sec/step, loss=0.07310, avg_loss=0.07361]\n",
      "Step 537609  [5.490 sec/step, loss=0.07227, avg_loss=0.07359]\n",
      "Step 537610  [5.507 sec/step, loss=0.07644, avg_loss=0.07363]\n",
      "Step 537611  [5.501 sec/step, loss=0.07376, avg_loss=0.07365]\n",
      "Step 537612  [5.496 sec/step, loss=0.07605, avg_loss=0.07366]\n",
      "Step 537613  [5.492 sec/step, loss=0.07516, avg_loss=0.07365]\n",
      "Step 537614  [5.501 sec/step, loss=0.07518, avg_loss=0.07366]\n",
      "Step 537615  [5.479 sec/step, loss=0.07378, avg_loss=0.07367]\n",
      "Step 537616  [5.478 sec/step, loss=0.07452, avg_loss=0.07366]\n",
      "Step 537617  [5.486 sec/step, loss=0.07514, avg_loss=0.07370]\n",
      "Step 537618  [5.473 sec/step, loss=0.07492, avg_loss=0.07371]\n",
      "Step 537619  [5.474 sec/step, loss=0.07546, avg_loss=0.07371]\n",
      "Step 537620  [5.478 sec/step, loss=0.07330, avg_loss=0.07370]\n",
      "Step 537621  [5.494 sec/step, loss=0.07503, avg_loss=0.07371]\n",
      "Step 537622  [5.484 sec/step, loss=0.07374, avg_loss=0.07370]\n",
      "Step 537623  [5.492 sec/step, loss=0.07037, avg_loss=0.07374]\n",
      "Step 537624  [5.447 sec/step, loss=0.07567, avg_loss=0.07385]\n",
      "Step 537625  [5.451 sec/step, loss=0.07426, avg_loss=0.07384]\n",
      "Step 537626  [5.420 sec/step, loss=0.06614, avg_loss=0.07375]\n",
      "Generated 32 batches of size 32 in 2.436 sec\n",
      "Step 537627  [5.440 sec/step, loss=0.07395, avg_loss=0.07376]\n",
      "Step 537628  [5.450 sec/step, loss=0.07520, avg_loss=0.07377]\n",
      "Step 537629  [5.453 sec/step, loss=0.07461, avg_loss=0.07377]\n",
      "Step 537630  [5.442 sec/step, loss=0.07436, avg_loss=0.07378]\n",
      "Step 537631  [5.445 sec/step, loss=0.07239, avg_loss=0.07378]\n",
      "Step 537632  [5.479 sec/step, loss=0.07305, avg_loss=0.07381]\n",
      "Step 537633  [5.524 sec/step, loss=0.06683, avg_loss=0.07373]\n",
      "Step 537634  [5.540 sec/step, loss=0.07688, avg_loss=0.07376]\n",
      "Step 537635  [5.541 sec/step, loss=0.07351, avg_loss=0.07374]\n",
      "Step 537636  [5.541 sec/step, loss=0.07395, avg_loss=0.07374]\n",
      "Step 537637  [5.526 sec/step, loss=0.07540, avg_loss=0.07374]\n",
      "Step 537638  [5.539 sec/step, loss=0.07017, avg_loss=0.07373]\n",
      "Step 537639  [5.525 sec/step, loss=0.07590, avg_loss=0.07375]\n",
      "Step 537640  [5.503 sec/step, loss=0.07204, avg_loss=0.07372]\n",
      "Step 537641  [5.515 sec/step, loss=0.07447, avg_loss=0.07373]\n",
      "Step 537642  [5.526 sec/step, loss=0.07623, avg_loss=0.07379]\n",
      "Step 537643  [5.513 sec/step, loss=0.07088, avg_loss=0.07375]\n",
      "Step 537644  [5.523 sec/step, loss=0.07526, avg_loss=0.07376]\n",
      "Step 537645  [5.535 sec/step, loss=0.07484, avg_loss=0.07376]\n",
      "Step 537646  [5.503 sec/step, loss=0.07371, avg_loss=0.07376]\n",
      "Step 537647  [5.500 sec/step, loss=0.07509, avg_loss=0.07377]\n",
      "Step 537648  [5.502 sec/step, loss=0.07543, avg_loss=0.07378]\n",
      "Step 537649  [5.472 sec/step, loss=0.06465, avg_loss=0.07367]\n",
      "Step 537650  [5.468 sec/step, loss=0.07471, avg_loss=0.07368]\n",
      "Step 537651  [5.426 sec/step, loss=0.07325, avg_loss=0.07375]\n",
      "Step 537652  [5.442 sec/step, loss=0.07530, avg_loss=0.07379]\n",
      "Step 537653  [5.454 sec/step, loss=0.07566, avg_loss=0.07381]\n",
      "Step 537654  [5.436 sec/step, loss=0.07086, avg_loss=0.07376]\n",
      "Step 537655  [5.427 sec/step, loss=0.07435, avg_loss=0.07375]\n",
      "Step 537656  [5.439 sec/step, loss=0.07576, avg_loss=0.07378]\n",
      "Step 537657  [5.475 sec/step, loss=0.06641, avg_loss=0.07369]\n",
      "Step 537658  [5.481 sec/step, loss=0.07428, avg_loss=0.07369]\n",
      "Generated 32 batches of size 32 in 2.515 sec\n",
      "Step 537659  [5.502 sec/step, loss=0.07587, avg_loss=0.07372]\n",
      "Step 537660  [5.506 sec/step, loss=0.07354, avg_loss=0.07370]\n",
      "Step 537661  [5.522 sec/step, loss=0.07273, avg_loss=0.07370]\n",
      "Step 537662  [5.537 sec/step, loss=0.07554, avg_loss=0.07370]\n",
      "Step 537663  [5.538 sec/step, loss=0.07203, avg_loss=0.07370]\n",
      "Step 537664  [5.515 sec/step, loss=0.07009, avg_loss=0.07365]\n",
      "Step 537665  [5.509 sec/step, loss=0.07217, avg_loss=0.07362]\n",
      "Step 537666  [5.523 sec/step, loss=0.07446, avg_loss=0.07363]\n",
      "Step 537667  [5.512 sec/step, loss=0.07525, avg_loss=0.07365]\n",
      "Step 537668  [5.531 sec/step, loss=0.07467, avg_loss=0.07369]\n",
      "Step 537669  [5.527 sec/step, loss=0.07334, avg_loss=0.07368]\n",
      "Step 537670  [5.540 sec/step, loss=0.07515, avg_loss=0.07377]\n",
      "Step 537671  [5.532 sec/step, loss=0.07383, avg_loss=0.07376]\n",
      "Step 537672  [5.503 sec/step, loss=0.06566, avg_loss=0.07366]\n",
      "Step 537673  [5.461 sec/step, loss=0.07618, avg_loss=0.07376]\n",
      "Step 537674  [5.434 sec/step, loss=0.07154, avg_loss=0.07373]\n",
      "Step 537675  [5.432 sec/step, loss=0.07581, avg_loss=0.07374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 537676  [5.424 sec/step, loss=0.07338, avg_loss=0.07376]\n",
      "Step 537677  [5.427 sec/step, loss=0.07398, avg_loss=0.07377]\n",
      "Step 537678  [5.473 sec/step, loss=0.06906, avg_loss=0.07374]\n",
      "Step 537679  [5.465 sec/step, loss=0.06967, avg_loss=0.07368]\n",
      "Step 537680  [5.492 sec/step, loss=0.07245, avg_loss=0.07366]\n",
      "Step 537681  [5.491 sec/step, loss=0.07435, avg_loss=0.07366]\n",
      "Step 537682  [5.488 sec/step, loss=0.07554, avg_loss=0.07366]\n",
      "Step 537683  [5.500 sec/step, loss=0.07543, avg_loss=0.07366]\n",
      "Step 537684  [5.501 sec/step, loss=0.07608, avg_loss=0.07366]\n",
      "Step 537685  [5.514 sec/step, loss=0.07493, avg_loss=0.07367]\n",
      "Step 537686  [5.505 sec/step, loss=0.07491, avg_loss=0.07366]\n",
      "Step 537687  [5.501 sec/step, loss=0.07201, avg_loss=0.07364]\n",
      "Step 537688  [5.500 sec/step, loss=0.07336, avg_loss=0.07363]\n",
      "Step 537689  [5.507 sec/step, loss=0.07359, avg_loss=0.07361]\n",
      "Step 537690  [5.528 sec/step, loss=0.07498, avg_loss=0.07361]\n",
      "Generated 32 batches of size 32 in 2.676 sec\n",
      "Step 537691  [5.550 sec/step, loss=0.07377, avg_loss=0.07369]\n",
      "Step 537692  [5.564 sec/step, loss=0.07429, avg_loss=0.07371]\n",
      "Step 537693  [5.562 sec/step, loss=0.07481, avg_loss=0.07370]\n",
      "Step 537694  [5.565 sec/step, loss=0.07102, avg_loss=0.07366]\n",
      "Step 537695  [5.584 sec/step, loss=0.07522, avg_loss=0.07368]\n",
      "Step 537696  [5.583 sec/step, loss=0.07585, avg_loss=0.07367]\n",
      "Step 537697  [5.586 sec/step, loss=0.07538, avg_loss=0.07367]\n",
      "Step 537698  [5.567 sec/step, loss=0.07188, avg_loss=0.07362]\n",
      "Step 537699  [5.555 sec/step, loss=0.07466, avg_loss=0.07363]\n",
      "Step 537700  [5.543 sec/step, loss=0.07458, avg_loss=0.07362]\n",
      "Writing summary at step: 537700\n",
      "Step 537701  [5.571 sec/step, loss=0.07318, avg_loss=0.07363]\n",
      "Step 537702  [5.543 sec/step, loss=0.07207, avg_loss=0.07361]\n",
      "Step 537703  [5.512 sec/step, loss=0.07114, avg_loss=0.07359]\n",
      "Step 537704  [5.512 sec/step, loss=0.07139, avg_loss=0.07358]\n",
      "Step 537705  [5.520 sec/step, loss=0.07626, avg_loss=0.07360]\n",
      "Step 537706  [5.506 sec/step, loss=0.07385, avg_loss=0.07358]\n",
      "Step 537707  [5.520 sec/step, loss=0.07480, avg_loss=0.07359]\n",
      "Step 537708  [5.517 sec/step, loss=0.07320, avg_loss=0.07359]\n",
      "Step 537709  [5.521 sec/step, loss=0.07194, avg_loss=0.07359]\n",
      "Step 537710  [5.527 sec/step, loss=0.07567, avg_loss=0.07358]\n",
      "Step 537711  [5.535 sec/step, loss=0.07472, avg_loss=0.07359]\n",
      "Step 537712  [5.562 sec/step, loss=0.07259, avg_loss=0.07356]\n",
      "Step 537713  [5.573 sec/step, loss=0.07574, avg_loss=0.07356]\n",
      "Step 537714  [5.586 sec/step, loss=0.07481, avg_loss=0.07356]\n",
      "Step 537715  [5.573 sec/step, loss=0.06656, avg_loss=0.07349]\n",
      "Step 537716  [5.573 sec/step, loss=0.07430, avg_loss=0.07349]\n",
      "Step 537717  [5.553 sec/step, loss=0.07061, avg_loss=0.07344]\n",
      "Step 537718  [5.567 sec/step, loss=0.07566, avg_loss=0.07345]\n",
      "Step 537719  [5.552 sec/step, loss=0.07313, avg_loss=0.07343]\n",
      "Step 537720  [5.542 sec/step, loss=0.07151, avg_loss=0.07341]\n",
      "Step 537721  [5.523 sec/step, loss=0.07261, avg_loss=0.07338]\n",
      "Generated 32 batches of size 32 in 2.466 sec\n",
      "Step 537722  [5.546 sec/step, loss=0.07344, avg_loss=0.07338]\n",
      "Step 537723  [5.566 sec/step, loss=0.07398, avg_loss=0.07342]\n",
      "Step 537724  [5.574 sec/step, loss=0.07565, avg_loss=0.07342]\n",
      "Step 537725  [5.567 sec/step, loss=0.07446, avg_loss=0.07342]\n",
      "Step 537726  [5.581 sec/step, loss=0.07465, avg_loss=0.07350]\n",
      "Step 537727  [5.569 sec/step, loss=0.07509, avg_loss=0.07351]\n",
      "Step 537728  [5.618 sec/step, loss=0.06607, avg_loss=0.07342]\n",
      "Step 537729  [5.606 sec/step, loss=0.07379, avg_loss=0.07342]\n",
      "Step 537730  [5.612 sec/step, loss=0.07398, avg_loss=0.07341]\n",
      "Step 537731  [5.625 sec/step, loss=0.07439, avg_loss=0.07343]\n",
      "Step 537732  [5.595 sec/step, loss=0.07285, avg_loss=0.07343]\n",
      "Step 537733  [5.536 sec/step, loss=0.07381, avg_loss=0.07350]\n",
      "Step 537734  [5.530 sec/step, loss=0.07395, avg_loss=0.07347]\n",
      "Step 537735  [5.524 sec/step, loss=0.07387, avg_loss=0.07347]\n",
      "Step 537736  [5.533 sec/step, loss=0.07452, avg_loss=0.07348]\n",
      "Step 537737  [5.531 sec/step, loss=0.07290, avg_loss=0.07345]\n",
      "Step 537738  [5.532 sec/step, loss=0.07444, avg_loss=0.07350]\n",
      "Step 537739  [5.530 sec/step, loss=0.07536, avg_loss=0.07349]\n",
      "Step 537740  [5.552 sec/step, loss=0.07622, avg_loss=0.07353]\n",
      "Step 537741  [5.560 sec/step, loss=0.07593, avg_loss=0.07355]\n",
      "Step 537742  [5.557 sec/step, loss=0.07486, avg_loss=0.07353]\n",
      "Step 537743  [5.615 sec/step, loss=0.06604, avg_loss=0.07349]\n",
      "Step 537744  [5.610 sec/step, loss=0.07537, avg_loss=0.07349]\n",
      "Step 537745  [5.587 sec/step, loss=0.07249, avg_loss=0.07346]\n",
      "Step 537746  [5.583 sec/step, loss=0.07383, avg_loss=0.07346]\n",
      "Step 537747  [5.582 sec/step, loss=0.07550, avg_loss=0.07347]\n",
      "Step 537748  [5.561 sec/step, loss=0.07012, avg_loss=0.07342]\n",
      "Step 537749  [5.579 sec/step, loss=0.07362, avg_loss=0.07351]\n",
      "Step 537750  [5.586 sec/step, loss=0.07411, avg_loss=0.07350]\n",
      "Step 537751  [5.588 sec/step, loss=0.07586, avg_loss=0.07353]\n",
      "Step 537752  [5.583 sec/step, loss=0.07111, avg_loss=0.07348]\n",
      "Step 537753  [5.593 sec/step, loss=0.07262, avg_loss=0.07345]\n",
      "Generated 32 batches of size 32 in 2.660 sec\n",
      "Step 537754  [5.611 sec/step, loss=0.07281, avg_loss=0.07347]\n",
      "Step 537755  [5.608 sec/step, loss=0.07493, avg_loss=0.07348]\n",
      "Step 537756  [5.604 sec/step, loss=0.07491, avg_loss=0.07347]\n",
      "Step 537757  [5.555 sec/step, loss=0.07354, avg_loss=0.07354]\n",
      "Step 537758  [5.569 sec/step, loss=0.07533, avg_loss=0.07355]\n",
      "Step 537759  [5.542 sec/step, loss=0.07058, avg_loss=0.07350]\n",
      "Step 537760  [5.569 sec/step, loss=0.07262, avg_loss=0.07349]\n",
      "Step 537761  [5.522 sec/step, loss=0.06487, avg_loss=0.07341]\n",
      "Step 537762  [5.516 sec/step, loss=0.07265, avg_loss=0.07338]\n",
      "Step 537763  [5.527 sec/step, loss=0.07522, avg_loss=0.07341]\n",
      "Step 537764  [5.553 sec/step, loss=0.07458, avg_loss=0.07346]\n",
      "Step 537765  [5.576 sec/step, loss=0.07191, avg_loss=0.07346]\n",
      "Step 537766  [5.572 sec/step, loss=0.07250, avg_loss=0.07344]\n",
      "Step 537767  [5.564 sec/step, loss=0.07196, avg_loss=0.07340]\n",
      "Step 537768  [5.568 sec/step, loss=0.07547, avg_loss=0.07341]\n",
      "Step 537769  [5.580 sec/step, loss=0.07430, avg_loss=0.07342]\n",
      "Step 537770  [5.574 sec/step, loss=0.07395, avg_loss=0.07341]\n",
      "Step 537771  [5.560 sec/step, loss=0.07328, avg_loss=0.07340]\n",
      "Step 537772  [5.588 sec/step, loss=0.07477, avg_loss=0.07350]\n",
      "Step 537773  [5.582 sec/step, loss=0.07459, avg_loss=0.07348]\n",
      "Step 537774  [5.585 sec/step, loss=0.07117, avg_loss=0.07348]\n",
      "Step 537775  [5.586 sec/step, loss=0.07585, avg_loss=0.07348]\n",
      "Step 537776  [5.591 sec/step, loss=0.07359, avg_loss=0.07348]\n",
      "Step 537777  [5.602 sec/step, loss=0.07565, avg_loss=0.07349]\n",
      "Step 537778  [5.558 sec/step, loss=0.07493, avg_loss=0.07355]\n",
      "Step 537779  [5.546 sec/step, loss=0.06629, avg_loss=0.07352]\n",
      "Step 537780  [5.510 sec/step, loss=0.07368, avg_loss=0.07353]\n",
      "Step 537781  [5.508 sec/step, loss=0.07421, avg_loss=0.07353]\n",
      "Step 537782  [5.496 sec/step, loss=0.07002, avg_loss=0.07348]\n",
      "Step 537783  [5.479 sec/step, loss=0.07108, avg_loss=0.07343]\n",
      "Step 537784  [5.475 sec/step, loss=0.07445, avg_loss=0.07342]\n",
      "Step 537785  [5.473 sec/step, loss=0.07501, avg_loss=0.07342]\n",
      "Generated 32 batches of size 32 in 2.424 sec\n",
      "Step 537786  [5.477 sec/step, loss=0.07472, avg_loss=0.07341]\n",
      "Step 537787  [5.484 sec/step, loss=0.07375, avg_loss=0.07343]\n",
      "Step 537788  [5.503 sec/step, loss=0.07319, avg_loss=0.07343]\n",
      "Step 537789  [5.505 sec/step, loss=0.07492, avg_loss=0.07344]\n",
      "Step 537790  [5.473 sec/step, loss=0.07171, avg_loss=0.07341]\n",
      "Step 537791  [5.487 sec/step, loss=0.07518, avg_loss=0.07343]\n",
      "Step 537792  [5.485 sec/step, loss=0.07404, avg_loss=0.07342]\n",
      "Step 537793  [5.484 sec/step, loss=0.07454, avg_loss=0.07342]\n",
      "Step 537794  [5.543 sec/step, loss=0.06539, avg_loss=0.07336]\n",
      "Step 537795  [5.513 sec/step, loss=0.07129, avg_loss=0.07332]\n",
      "Step 537796  [5.505 sec/step, loss=0.07458, avg_loss=0.07331]\n",
      "Step 537797  [5.507 sec/step, loss=0.07328, avg_loss=0.07329]\n",
      "Step 537798  [5.507 sec/step, loss=0.07087, avg_loss=0.07328]\n",
      "Step 537799  [5.510 sec/step, loss=0.07454, avg_loss=0.07328]\n",
      "Step 537800  [5.506 sec/step, loss=0.07466, avg_loss=0.07328]\n",
      "Writing summary at step: 537800\n",
      "Step 537801  [5.487 sec/step, loss=0.06910, avg_loss=0.07324]\n",
      "Step 537802  [5.484 sec/step, loss=0.07349, avg_loss=0.07325]\n",
      "Step 537803  [5.500 sec/step, loss=0.07274, avg_loss=0.07327]\n",
      "Step 537804  [5.510 sec/step, loss=0.07383, avg_loss=0.07329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 537805  [5.509 sec/step, loss=0.07485, avg_loss=0.07328]\n",
      "Step 537806  [5.519 sec/step, loss=0.07569, avg_loss=0.07330]\n",
      "Step 537807  [5.514 sec/step, loss=0.07442, avg_loss=0.07329]\n",
      "Step 537808  [5.504 sec/step, loss=0.06673, avg_loss=0.07323]\n",
      "Step 537809  [5.514 sec/step, loss=0.07578, avg_loss=0.07327]\n",
      "Step 537810  [5.492 sec/step, loss=0.07347, avg_loss=0.07325]\n",
      "Step 537811  [5.503 sec/step, loss=0.07424, avg_loss=0.07324]\n",
      "Step 537812  [5.479 sec/step, loss=0.07405, avg_loss=0.07326]\n",
      "Step 537813  [5.483 sec/step, loss=0.07554, avg_loss=0.07325]\n",
      "Step 537814  [5.491 sec/step, loss=0.07292, avg_loss=0.07324]\n",
      "Step 537815  [5.521 sec/step, loss=0.07594, avg_loss=0.07333]\n",
      "Step 537816  [5.517 sec/step, loss=0.07546, avg_loss=0.07334]\n",
      "Generated 32 batches of size 32 in 2.614 sec\n",
      "Step 537817  [5.533 sec/step, loss=0.07386, avg_loss=0.07337]\n",
      "Step 537818  [5.517 sec/step, loss=0.07284, avg_loss=0.07334]\n",
      "Step 537819  [5.519 sec/step, loss=0.07212, avg_loss=0.07333]\n",
      "Step 537820  [5.534 sec/step, loss=0.07615, avg_loss=0.07338]\n",
      "Step 537821  [5.530 sec/step, loss=0.07500, avg_loss=0.07341]\n",
      "Step 537822  [5.518 sec/step, loss=0.07220, avg_loss=0.07339]\n",
      "Step 537823  [5.559 sec/step, loss=0.06653, avg_loss=0.07332]\n",
      "Step 537824  [5.549 sec/step, loss=0.07490, avg_loss=0.07331]\n",
      "Step 537825  [5.551 sec/step, loss=0.07419, avg_loss=0.07331]\n",
      "Step 537826  [5.560 sec/step, loss=0.07539, avg_loss=0.07332]\n",
      "Step 537827  [5.545 sec/step, loss=0.07372, avg_loss=0.07330]\n",
      "Step 537828  [5.498 sec/step, loss=0.07429, avg_loss=0.07338]\n",
      "Step 537829  [5.501 sec/step, loss=0.07351, avg_loss=0.07338]\n",
      "Step 537830  [5.491 sec/step, loss=0.07423, avg_loss=0.07338]\n",
      "Step 537831  [5.482 sec/step, loss=0.07497, avg_loss=0.07339]\n",
      "Step 537832  [5.499 sec/step, loss=0.07355, avg_loss=0.07340]\n",
      "Step 537833  [5.534 sec/step, loss=0.07273, avg_loss=0.07339]\n",
      "Step 537834  [5.514 sec/step, loss=0.07129, avg_loss=0.07336]\n",
      "Step 537835  [5.502 sec/step, loss=0.07315, avg_loss=0.07335]\n",
      "Step 537836  [5.519 sec/step, loss=0.07555, avg_loss=0.07336]\n",
      "Step 537837  [5.525 sec/step, loss=0.07453, avg_loss=0.07338]\n",
      "Step 537838  [5.535 sec/step, loss=0.07510, avg_loss=0.07339]\n",
      "Step 537839  [5.522 sec/step, loss=0.07121, avg_loss=0.07334]\n",
      "Step 537840  [5.509 sec/step, loss=0.07471, avg_loss=0.07333]\n",
      "Step 537841  [5.497 sec/step, loss=0.07085, avg_loss=0.07328]\n",
      "Step 537842  [5.534 sec/step, loss=0.06839, avg_loss=0.07321]\n",
      "Step 537843  [5.494 sec/step, loss=0.07544, avg_loss=0.07331]\n",
      "Step 537844  [5.476 sec/step, loss=0.07171, avg_loss=0.07327]\n",
      "Step 537845  [5.478 sec/step, loss=0.07146, avg_loss=0.07326]\n",
      "Step 537846  [5.493 sec/step, loss=0.07098, avg_loss=0.07323]\n",
      "Step 537847  [5.500 sec/step, loss=0.07391, avg_loss=0.07322]\n",
      "Step 537848  [5.535 sec/step, loss=0.07294, avg_loss=0.07324]\n",
      "Generated 32 batches of size 32 in 2.620 sec\n",
      "Step 537849  [5.535 sec/step, loss=0.07326, avg_loss=0.07324]\n",
      "Step 537850  [5.542 sec/step, loss=0.07533, avg_loss=0.07325]\n",
      "Step 537851  [5.528 sec/step, loss=0.07463, avg_loss=0.07324]\n",
      "Step 537852  [5.515 sec/step, loss=0.06530, avg_loss=0.07318]\n",
      "Step 537853  [5.496 sec/step, loss=0.07323, avg_loss=0.07319]\n",
      "Step 537854  [5.501 sec/step, loss=0.07477, avg_loss=0.07321]\n",
      "Step 537855  [5.509 sec/step, loss=0.07448, avg_loss=0.07320]\n",
      "Step 537856  [5.507 sec/step, loss=0.07509, avg_loss=0.07321]\n",
      "Step 537857  [5.515 sec/step, loss=0.07468, avg_loss=0.07322]\n",
      "Step 537858  [5.487 sec/step, loss=0.06567, avg_loss=0.07312]\n",
      "Step 537859  [5.509 sec/step, loss=0.07256, avg_loss=0.07314]\n",
      "Step 537860  [5.478 sec/step, loss=0.07465, avg_loss=0.07316]\n",
      "Step 537861  [5.492 sec/step, loss=0.07117, avg_loss=0.07322]\n",
      "Step 537862  [5.482 sec/step, loss=0.07474, avg_loss=0.07324]\n",
      "Step 537863  [5.480 sec/step, loss=0.07355, avg_loss=0.07323]\n",
      "Step 537864  [5.474 sec/step, loss=0.07391, avg_loss=0.07322]\n",
      "Step 537865  [5.459 sec/step, loss=0.07525, avg_loss=0.07325]\n",
      "Step 537866  [5.468 sec/step, loss=0.07544, avg_loss=0.07328]\n",
      "Step 537867  [5.476 sec/step, loss=0.07369, avg_loss=0.07330]\n",
      "Step 537868  [5.454 sec/step, loss=0.07374, avg_loss=0.07328]\n",
      "Step 537869  [5.446 sec/step, loss=0.07373, avg_loss=0.07328]\n",
      "Step 537870  [5.443 sec/step, loss=0.07013, avg_loss=0.07324]\n",
      "Step 537871  [5.451 sec/step, loss=0.07259, avg_loss=0.07323]\n",
      "Step 537872  [5.457 sec/step, loss=0.07550, avg_loss=0.07324]\n",
      "Step 537873  [5.466 sec/step, loss=0.07559, avg_loss=0.07325]\n",
      "Step 537874  [5.471 sec/step, loss=0.07338, avg_loss=0.07327]\n",
      "Step 537875  [5.479 sec/step, loss=0.07467, avg_loss=0.07326]\n",
      "Step 537876  [5.485 sec/step, loss=0.07557, avg_loss=0.07328]\n",
      "Step 537877  [5.474 sec/step, loss=0.07268, avg_loss=0.07325]\n",
      "Step 537878  [5.482 sec/step, loss=0.07450, avg_loss=0.07325]\n",
      "Step 537879  [5.496 sec/step, loss=0.07325, avg_loss=0.07332]\n",
      "Step 537880  [5.531 sec/step, loss=0.07281, avg_loss=0.07331]\n",
      "Generated 32 batches of size 32 in 2.634 sec\n",
      "Step 537881  [5.538 sec/step, loss=0.06964, avg_loss=0.07326]\n",
      "Step 537882  [5.533 sec/step, loss=0.07078, avg_loss=0.07327]\n",
      "Step 537883  [5.537 sec/step, loss=0.07460, avg_loss=0.07330]\n",
      "Step 537884  [5.536 sec/step, loss=0.07504, avg_loss=0.07331]\n",
      "Step 537885  [5.543 sec/step, loss=0.07631, avg_loss=0.07332]\n",
      "Step 537886  [5.524 sec/step, loss=0.07274, avg_loss=0.07330]\n",
      "Step 537887  [5.516 sec/step, loss=0.07507, avg_loss=0.07332]\n",
      "Step 537888  [5.556 sec/step, loss=0.06595, avg_loss=0.07324]\n",
      "Step 537889  [5.548 sec/step, loss=0.07424, avg_loss=0.07324]\n",
      "Step 537890  [5.554 sec/step, loss=0.07261, avg_loss=0.07325]\n",
      "Step 537891  [5.544 sec/step, loss=0.07576, avg_loss=0.07325]\n",
      "Step 537892  [5.549 sec/step, loss=0.07574, avg_loss=0.07327]\n",
      "Step 537893  [5.557 sec/step, loss=0.07486, avg_loss=0.07327]\n",
      "Step 537894  [5.495 sec/step, loss=0.07393, avg_loss=0.07336]\n",
      "Step 537895  [5.514 sec/step, loss=0.07439, avg_loss=0.07339]\n",
      "Step 537896  [5.538 sec/step, loss=0.07308, avg_loss=0.07337]\n",
      "Step 537897  [5.530 sec/step, loss=0.07369, avg_loss=0.07338]\n",
      "Step 537898  [5.527 sec/step, loss=0.07167, avg_loss=0.07339]\n",
      "Step 537899  [5.529 sec/step, loss=0.07147, avg_loss=0.07335]\n",
      "Step 537900  [5.514 sec/step, loss=0.06481, avg_loss=0.07326]\n",
      "Writing summary at step: 537900\n",
      "Step 537901  [5.525 sec/step, loss=0.07428, avg_loss=0.07331]\n",
      "Step 537902  [5.535 sec/step, loss=0.07385, avg_loss=0.07331]\n",
      "Step 537903  [5.521 sec/step, loss=0.07106, avg_loss=0.07329]\n",
      "Step 537904  [5.519 sec/step, loss=0.07330, avg_loss=0.07329]\n",
      "Step 537905  [5.511 sec/step, loss=0.07298, avg_loss=0.07327]\n",
      "Step 537906  [5.511 sec/step, loss=0.07530, avg_loss=0.07327]\n",
      "Step 537907  [5.524 sec/step, loss=0.07360, avg_loss=0.07326]\n",
      "Step 537908  [5.543 sec/step, loss=0.07522, avg_loss=0.07334]\n",
      "Step 537909  [5.541 sec/step, loss=0.07523, avg_loss=0.07334]\n",
      "Step 537910  [5.543 sec/step, loss=0.07336, avg_loss=0.07334]\n",
      "Step 537911  [5.555 sec/step, loss=0.07252, avg_loss=0.07332]\n",
      "Generated 32 batches of size 32 in 2.763 sec\n",
      "Step 537912  [5.542 sec/step, loss=0.07067, avg_loss=0.07329]\n",
      "Step 537913  [5.521 sec/step, loss=0.07326, avg_loss=0.07326]\n",
      "Step 537914  [5.499 sec/step, loss=0.07271, avg_loss=0.07326]\n",
      "Step 537915  [5.482 sec/step, loss=0.07423, avg_loss=0.07324]\n",
      "Step 537916  [5.494 sec/step, loss=0.07612, avg_loss=0.07325]\n",
      "Step 537917  [5.503 sec/step, loss=0.07574, avg_loss=0.07327]\n",
      "Step 537918  [5.510 sec/step, loss=0.07469, avg_loss=0.07329]\n",
      "Step 537919  [5.563 sec/step, loss=0.06705, avg_loss=0.07324]\n",
      "Step 537920  [5.565 sec/step, loss=0.07563, avg_loss=0.07323]\n",
      "Step 537921  [5.579 sec/step, loss=0.07291, avg_loss=0.07321]\n",
      "Step 537922  [5.579 sec/step, loss=0.07435, avg_loss=0.07323]\n",
      "Step 537923  [5.520 sec/step, loss=0.07333, avg_loss=0.07330]\n",
      "Step 537924  [5.498 sec/step, loss=0.06578, avg_loss=0.07321]\n",
      "Step 537925  [5.520 sec/step, loss=0.07256, avg_loss=0.07319]\n",
      "Step 537926  [5.520 sec/step, loss=0.07411, avg_loss=0.07318]\n",
      "Step 537927  [5.526 sec/step, loss=0.07249, avg_loss=0.07317]\n",
      "Step 537928  [5.507 sec/step, loss=0.07129, avg_loss=0.07314]\n",
      "Step 537929  [5.565 sec/step, loss=0.06501, avg_loss=0.07305]\n",
      "Step 537930  [5.568 sec/step, loss=0.07359, avg_loss=0.07305]\n",
      "Step 537931  [5.578 sec/step, loss=0.07304, avg_loss=0.07303]\n",
      "Step 537932  [5.560 sec/step, loss=0.07483, avg_loss=0.07304]\n",
      "Step 537933  [5.553 sec/step, loss=0.07338, avg_loss=0.07305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 537934  [5.564 sec/step, loss=0.07274, avg_loss=0.07306]\n",
      "Step 537935  [5.569 sec/step, loss=0.07348, avg_loss=0.07306]\n",
      "Step 537936  [5.554 sec/step, loss=0.07444, avg_loss=0.07305]\n",
      "Step 537937  [5.559 sec/step, loss=0.07541, avg_loss=0.07306]\n",
      "Step 537938  [5.547 sec/step, loss=0.07435, avg_loss=0.07305]\n",
      "Step 537939  [5.551 sec/step, loss=0.07453, avg_loss=0.07309]\n",
      "Step 537940  [5.560 sec/step, loss=0.07563, avg_loss=0.07310]\n",
      "Step 537941  [5.563 sec/step, loss=0.07459, avg_loss=0.07313]\n",
      "Step 537942  [5.526 sec/step, loss=0.07419, avg_loss=0.07319]\n",
      "Step 537943  [5.522 sec/step, loss=0.07592, avg_loss=0.07320]\n",
      "Generated 32 batches of size 32 in 2.583 sec\n",
      "Step 537944  [5.531 sec/step, loss=0.07023, avg_loss=0.07318]\n",
      "Step 537945  [5.534 sec/step, loss=0.07106, avg_loss=0.07318]\n",
      "Step 537946  [5.547 sec/step, loss=0.07525, avg_loss=0.07322]\n",
      "Step 537947  [5.550 sec/step, loss=0.07560, avg_loss=0.07324]\n",
      "Step 537948  [5.540 sec/step, loss=0.07428, avg_loss=0.07325]\n",
      "Step 537949  [5.544 sec/step, loss=0.07419, avg_loss=0.07326]\n",
      "Step 537950  [5.521 sec/step, loss=0.07092, avg_loss=0.07322]\n",
      "Step 537951  [5.526 sec/step, loss=0.07552, avg_loss=0.07323]\n",
      "Step 537952  [5.536 sec/step, loss=0.07332, avg_loss=0.07331]\n",
      "Step 537953  [5.546 sec/step, loss=0.07554, avg_loss=0.07333]\n",
      "Step 537954  [5.540 sec/step, loss=0.07436, avg_loss=0.07332]\n",
      "Step 537955  [5.537 sec/step, loss=0.07358, avg_loss=0.07332]\n",
      "Step 537956  [5.531 sec/step, loss=0.07338, avg_loss=0.07330]\n",
      "Step 537957  [5.511 sec/step, loss=0.07374, avg_loss=0.07329]\n",
      "Step 537958  [5.534 sec/step, loss=0.07400, avg_loss=0.07337]\n",
      "Step 537959  [5.514 sec/step, loss=0.07133, avg_loss=0.07336]\n",
      "Step 537960  [5.513 sec/step, loss=0.07113, avg_loss=0.07332]\n",
      "Step 537961  [5.511 sec/step, loss=0.07342, avg_loss=0.07335]\n",
      "Step 537962  [5.519 sec/step, loss=0.07321, avg_loss=0.07333]\n",
      "Step 537963  [5.504 sec/step, loss=0.07487, avg_loss=0.07335]\n",
      "Step 537964  [5.505 sec/step, loss=0.07347, avg_loss=0.07334]\n",
      "Step 537965  [5.495 sec/step, loss=0.07398, avg_loss=0.07333]\n",
      "Step 537966  [5.534 sec/step, loss=0.06654, avg_loss=0.07324]\n",
      "Step 537967  [5.539 sec/step, loss=0.07568, avg_loss=0.07326]\n",
      "Step 537968  [5.569 sec/step, loss=0.07452, avg_loss=0.07327]\n",
      "Step 537969  [5.570 sec/step, loss=0.07504, avg_loss=0.07328]\n",
      "Step 537970  [5.581 sec/step, loss=0.07074, avg_loss=0.07329]\n",
      "Step 537971  [5.591 sec/step, loss=0.07610, avg_loss=0.07332]\n",
      "Step 537972  [5.603 sec/step, loss=0.07340, avg_loss=0.07330]\n",
      "Step 537973  [5.593 sec/step, loss=0.07453, avg_loss=0.07329]\n",
      "Step 537974  [5.611 sec/step, loss=0.07568, avg_loss=0.07331]\n",
      "Step 537975  [5.581 sec/step, loss=0.06933, avg_loss=0.07326]\n",
      "Generated 32 batches of size 32 in 2.477 sec\n",
      "Step 537976  [5.587 sec/step, loss=0.07543, avg_loss=0.07326]\n",
      "Step 537977  [5.574 sec/step, loss=0.07157, avg_loss=0.07325]\n",
      "Step 537978  [5.551 sec/step, loss=0.06433, avg_loss=0.07314]\n",
      "Step 537979  [5.554 sec/step, loss=0.07307, avg_loss=0.07314]\n",
      "Step 537980  [5.530 sec/step, loss=0.07371, avg_loss=0.07315]\n",
      "Step 537981  [5.537 sec/step, loss=0.07443, avg_loss=0.07320]\n",
      "Step 537982  [5.555 sec/step, loss=0.07498, avg_loss=0.07324]\n",
      "Step 537983  [5.551 sec/step, loss=0.07468, avg_loss=0.07324]\n",
      "Step 537984  [5.561 sec/step, loss=0.07555, avg_loss=0.07325]\n",
      "Step 537985  [5.560 sec/step, loss=0.07397, avg_loss=0.07322]\n",
      "Step 537986  [5.569 sec/step, loss=0.07426, avg_loss=0.07324]\n",
      "Step 537987  [5.597 sec/step, loss=0.07474, avg_loss=0.07324]\n",
      "Step 537988  [5.539 sec/step, loss=0.07047, avg_loss=0.07328]\n",
      "Step 537989  [5.589 sec/step, loss=0.06480, avg_loss=0.07319]\n",
      "Step 537990  [5.601 sec/step, loss=0.07468, avg_loss=0.07321]\n",
      "Step 537991  [5.598 sec/step, loss=0.07510, avg_loss=0.07320]\n",
      "Step 537992  [5.584 sec/step, loss=0.07344, avg_loss=0.07318]\n",
      "Step 537993  [5.583 sec/step, loss=0.07268, avg_loss=0.07316]\n",
      "Step 537994  [5.606 sec/step, loss=0.07538, avg_loss=0.07317]\n",
      "Step 537995  [5.607 sec/step, loss=0.07419, avg_loss=0.07317]\n",
      "Step 537996  [5.584 sec/step, loss=0.07502, avg_loss=0.07319]\n",
      "Step 537997  [5.570 sec/step, loss=0.07380, avg_loss=0.07319]\n",
      "Step 537998  [5.581 sec/step, loss=0.07355, avg_loss=0.07321]\n",
      "Step 537999  [5.555 sec/step, loss=0.06748, avg_loss=0.07317]\n",
      "Step 538000  [5.583 sec/step, loss=0.07573, avg_loss=0.07328]\n",
      "Writing summary at step: 538000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-538000\n",
      "Saving audio and alignment...\n",
      "Input: barsarayaqtday ahl ddaanish sae qisam jaa half kii gathrrii uthvaa loo tdoo farz oolii tshup dzaatdaa hae~____________________________________________\n",
      "Step 538001  [5.594 sec/step, loss=0.07416, avg_loss=0.07328]\n",
      "Step 538002  [5.599 sec/step, loss=0.07442, avg_loss=0.07328]\n",
      "Step 538003  [5.601 sec/step, loss=0.07472, avg_loss=0.07332]\n",
      "Step 538004  [5.592 sec/step, loss=0.07057, avg_loss=0.07329]\n",
      "Step 538005  [5.591 sec/step, loss=0.07482, avg_loss=0.07331]\n",
      "Generated 32 batches of size 32 in 2.454 sec\n",
      "Step 538006  [5.587 sec/step, loss=0.07376, avg_loss=0.07329]\n",
      "Step 538007  [5.592 sec/step, loss=0.07508, avg_loss=0.07331]\n",
      "Step 538008  [5.597 sec/step, loss=0.07512, avg_loss=0.07331]\n",
      "Step 538009  [5.577 sec/step, loss=0.07164, avg_loss=0.07327]\n",
      "Step 538010  [5.579 sec/step, loss=0.07095, avg_loss=0.07325]\n",
      "Step 538011  [5.554 sec/step, loss=0.07253, avg_loss=0.07325]\n",
      "Step 538012  [5.570 sec/step, loss=0.07620, avg_loss=0.07330]\n",
      "Step 538013  [5.579 sec/step, loss=0.07476, avg_loss=0.07332]\n",
      "Step 538014  [5.572 sec/step, loss=0.07458, avg_loss=0.07334]\n",
      "Step 538015  [5.579 sec/step, loss=0.07448, avg_loss=0.07334]\n",
      "Step 538016  [5.567 sec/step, loss=0.07343, avg_loss=0.07331]\n",
      "Step 538017  [5.549 sec/step, loss=0.07223, avg_loss=0.07328]\n",
      "Step 538018  [5.543 sec/step, loss=0.07480, avg_loss=0.07328]\n",
      "Step 538019  [5.493 sec/step, loss=0.07560, avg_loss=0.07336]\n",
      "Step 538020  [5.496 sec/step, loss=0.07449, avg_loss=0.07335]\n",
      "Step 538021  [5.469 sec/step, loss=0.06634, avg_loss=0.07329]\n",
      "Step 538022  [5.518 sec/step, loss=0.06548, avg_loss=0.07320]\n",
      "Step 538023  [5.544 sec/step, loss=0.07501, avg_loss=0.07321]\n",
      "Step 538024  [5.570 sec/step, loss=0.07625, avg_loss=0.07332]\n",
      "Step 538025  [5.543 sec/step, loss=0.07429, avg_loss=0.07334]\n",
      "Step 538026  [5.542 sec/step, loss=0.07318, avg_loss=0.07333]\n",
      "Step 538027  [5.539 sec/step, loss=0.07430, avg_loss=0.07335]\n",
      "Step 538028  [5.558 sec/step, loss=0.07481, avg_loss=0.07338]\n",
      "Step 538029  [5.512 sec/step, loss=0.07204, avg_loss=0.07345]\n",
      "Step 538030  [5.517 sec/step, loss=0.07618, avg_loss=0.07348]\n",
      "Step 538031  [5.499 sec/step, loss=0.07365, avg_loss=0.07348]\n",
      "Step 538032  [5.502 sec/step, loss=0.07284, avg_loss=0.07346]\n",
      "Step 538033  [5.492 sec/step, loss=0.07611, avg_loss=0.07349]\n",
      "Step 538034  [5.504 sec/step, loss=0.07535, avg_loss=0.07352]\n",
      "Step 538035  [5.518 sec/step, loss=0.07347, avg_loss=0.07352]\n",
      "Step 538036  [5.509 sec/step, loss=0.07315, avg_loss=0.07350]\n",
      "Step 538037  [5.504 sec/step, loss=0.07459, avg_loss=0.07350]\n",
      "Generated 32 batches of size 32 in 2.529 sec\n",
      "Step 538038  [5.509 sec/step, loss=0.07194, avg_loss=0.07347]\n",
      "Step 538039  [5.509 sec/step, loss=0.07529, avg_loss=0.07348]\n",
      "Step 538040  [5.507 sec/step, loss=0.07571, avg_loss=0.07348]\n",
      "Step 538041  [5.489 sec/step, loss=0.07118, avg_loss=0.07345]\n",
      "Step 538042  [5.472 sec/step, loss=0.07033, avg_loss=0.07341]\n",
      "Step 538043  [5.453 sec/step, loss=0.07376, avg_loss=0.07339]\n",
      "Step 538044  [5.485 sec/step, loss=0.07278, avg_loss=0.07341]\n",
      "Step 538045  [5.483 sec/step, loss=0.07449, avg_loss=0.07344]\n",
      "Step 538046  [5.481 sec/step, loss=0.07516, avg_loss=0.07344]\n",
      "Step 538047  [5.464 sec/step, loss=0.07467, avg_loss=0.07343]\n",
      "Step 538048  [5.458 sec/step, loss=0.07442, avg_loss=0.07344]\n",
      "Step 538049  [5.451 sec/step, loss=0.07356, avg_loss=0.07343]\n",
      "Step 538050  [5.475 sec/step, loss=0.07557, avg_loss=0.07348]\n",
      "Step 538051  [5.461 sec/step, loss=0.07343, avg_loss=0.07346]\n",
      "Step 538052  [5.485 sec/step, loss=0.07542, avg_loss=0.07348]\n",
      "Step 538053  [5.474 sec/step, loss=0.07517, avg_loss=0.07347]\n",
      "Step 538054  [5.476 sec/step, loss=0.07548, avg_loss=0.07348]\n",
      "Step 538055  [5.479 sec/step, loss=0.07117, avg_loss=0.07346]\n",
      "Step 538056  [5.488 sec/step, loss=0.07472, avg_loss=0.07347]\n",
      "Step 538057  [5.522 sec/step, loss=0.07483, avg_loss=0.07348]\n",
      "Step 538058  [5.513 sec/step, loss=0.07242, avg_loss=0.07347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 538059  [5.503 sec/step, loss=0.06477, avg_loss=0.07340]\n",
      "Step 538060  [5.518 sec/step, loss=0.07360, avg_loss=0.07343]\n",
      "Step 538061  [5.538 sec/step, loss=0.07367, avg_loss=0.07343]\n",
      "Step 538062  [5.539 sec/step, loss=0.07557, avg_loss=0.07345]\n",
      "Step 538063  [5.551 sec/step, loss=0.07583, avg_loss=0.07346]\n",
      "Step 538064  [5.552 sec/step, loss=0.07444, avg_loss=0.07347]\n",
      "Step 538065  [5.553 sec/step, loss=0.07425, avg_loss=0.07348]\n",
      "Step 538066  [5.489 sec/step, loss=0.07162, avg_loss=0.07353]\n",
      "Step 538067  [5.489 sec/step, loss=0.07572, avg_loss=0.07353]\n",
      "Step 538068  [5.472 sec/step, loss=0.07258, avg_loss=0.07351]\n",
      "Step 538069  [5.470 sec/step, loss=0.07437, avg_loss=0.07350]\n",
      "Generated 32 batches of size 32 in 2.637 sec\n",
      "Step 538070  [5.473 sec/step, loss=0.07302, avg_loss=0.07352]\n",
      "Step 538071  [5.453 sec/step, loss=0.07455, avg_loss=0.07351]\n",
      "Step 538072  [5.418 sec/step, loss=0.07324, avg_loss=0.07351]\n",
      "Step 538073  [5.418 sec/step, loss=0.07525, avg_loss=0.07351]\n",
      "Step 538074  [5.395 sec/step, loss=0.07069, avg_loss=0.07346]\n",
      "Step 538075  [5.397 sec/step, loss=0.07265, avg_loss=0.07350]\n",
      "Step 538076  [5.434 sec/step, loss=0.06530, avg_loss=0.07340]\n",
      "Step 538077  [5.445 sec/step, loss=0.07178, avg_loss=0.07340]\n",
      "Step 538078  [5.475 sec/step, loss=0.07637, avg_loss=0.07352]\n",
      "Step 538079  [5.503 sec/step, loss=0.07206, avg_loss=0.07351]\n",
      "Step 538080  [5.550 sec/step, loss=0.06770, avg_loss=0.07345]\n",
      "Step 538081  [5.544 sec/step, loss=0.07519, avg_loss=0.07346]\n",
      "Step 538082  [5.542 sec/step, loss=0.07320, avg_loss=0.07344]\n",
      "Step 538083  [5.548 sec/step, loss=0.07365, avg_loss=0.07343]\n",
      "Step 538084  [5.534 sec/step, loss=0.07370, avg_loss=0.07341]\n",
      "Step 538085  [5.529 sec/step, loss=0.07497, avg_loss=0.07342]\n",
      "Step 538086  [5.530 sec/step, loss=0.07160, avg_loss=0.07339]\n",
      "Step 538087  [5.502 sec/step, loss=0.07278, avg_loss=0.07337]\n",
      "Step 538088  [5.498 sec/step, loss=0.07311, avg_loss=0.07340]\n",
      "Step 538089  [5.459 sec/step, loss=0.07583, avg_loss=0.07351]\n",
      "Step 538090  [5.454 sec/step, loss=0.07139, avg_loss=0.07348]\n",
      "Step 538091  [5.450 sec/step, loss=0.07464, avg_loss=0.07347]\n",
      "Step 538092  [5.441 sec/step, loss=0.06610, avg_loss=0.07340]\n",
      "Step 538093  [5.431 sec/step, loss=0.07469, avg_loss=0.07342]\n",
      "Step 538094  [5.413 sec/step, loss=0.07333, avg_loss=0.07340]\n",
      "Step 538095  [5.400 sec/step, loss=0.07323, avg_loss=0.07339]\n",
      "Step 538096  [5.401 sec/step, loss=0.07538, avg_loss=0.07339]\n",
      "Step 538097  [5.421 sec/step, loss=0.07524, avg_loss=0.07341]\n",
      "Step 538098  [5.427 sec/step, loss=0.07161, avg_loss=0.07339]\n",
      "Step 538099  [5.456 sec/step, loss=0.07412, avg_loss=0.07345]\n",
      "Step 538100  [5.452 sec/step, loss=0.07590, avg_loss=0.07346]\n",
      "Writing summary at step: 538100\n",
      "Generated 32 batches of size 32 in 2.364 sec\n",
      "Step 538101  [5.452 sec/step, loss=0.07410, avg_loss=0.07345]\n",
      "Step 538102  [5.460 sec/step, loss=0.07573, avg_loss=0.07347]\n",
      "Step 538103  [5.445 sec/step, loss=0.07140, avg_loss=0.07343]\n",
      "Step 538104  [5.467 sec/step, loss=0.07575, avg_loss=0.07349]\n",
      "Step 538105  [5.464 sec/step, loss=0.07298, avg_loss=0.07347]\n",
      "Step 538106  [5.466 sec/step, loss=0.07148, avg_loss=0.07345]\n",
      "Step 538107  [5.463 sec/step, loss=0.07624, avg_loss=0.07346]\n",
      "Step 538108  [5.444 sec/step, loss=0.07094, avg_loss=0.07342]\n",
      "Step 538109  [5.475 sec/step, loss=0.07303, avg_loss=0.07343]\n",
      "Step 538110  [5.474 sec/step, loss=0.07268, avg_loss=0.07345]\n",
      "Step 538111  [5.496 sec/step, loss=0.07457, avg_loss=0.07347]\n",
      "Step 538112  [5.493 sec/step, loss=0.07452, avg_loss=0.07345]\n",
      "Step 538113  [5.499 sec/step, loss=0.07567, avg_loss=0.07346]\n",
      "Step 538114  [5.499 sec/step, loss=0.07100, avg_loss=0.07342]\n",
      "Step 538115  [5.495 sec/step, loss=0.07448, avg_loss=0.07342]\n",
      "Step 538116  [5.491 sec/step, loss=0.07520, avg_loss=0.07344]\n",
      "Step 538117  [5.497 sec/step, loss=0.07486, avg_loss=0.07347]\n",
      "Step 538118  [5.499 sec/step, loss=0.07260, avg_loss=0.07345]\n",
      "Step 538119  [5.487 sec/step, loss=0.07378, avg_loss=0.07343]\n",
      "Step 538120  [5.472 sec/step, loss=0.07377, avg_loss=0.07342]\n",
      "Step 538121  [5.474 sec/step, loss=0.07131, avg_loss=0.07347]\n",
      "Step 538122  [5.430 sec/step, loss=0.07369, avg_loss=0.07355]\n",
      "Step 538123  [5.426 sec/step, loss=0.07465, avg_loss=0.07355]\n",
      "Step 538124  [5.420 sec/step, loss=0.07433, avg_loss=0.07353]\n",
      "Step 538125  [5.413 sec/step, loss=0.07320, avg_loss=0.07352]\n",
      "Step 538126  [5.412 sec/step, loss=0.07463, avg_loss=0.07353]\n",
      "Step 538127  [5.432 sec/step, loss=0.07428, avg_loss=0.07353]\n",
      "Step 538128  [5.423 sec/step, loss=0.07479, avg_loss=0.07353]\n",
      "Step 538129  [5.427 sec/step, loss=0.07544, avg_loss=0.07357]\n",
      "Step 538130  [5.471 sec/step, loss=0.06353, avg_loss=0.07344]\n",
      "Step 538131  [5.469 sec/step, loss=0.07311, avg_loss=0.07343]\n",
      "Step 538132  [5.481 sec/step, loss=0.07560, avg_loss=0.07346]\n",
      "Generated 32 batches of size 32 in 2.885 sec\n",
      "Step 538133  [5.455 sec/step, loss=0.06477, avg_loss=0.07335]\n",
      "Step 538134  [5.432 sec/step, loss=0.07108, avg_loss=0.07331]\n",
      "Step 538135  [5.430 sec/step, loss=0.07574, avg_loss=0.07333]\n",
      "Step 538136  [5.448 sec/step, loss=0.07507, avg_loss=0.07335]\n",
      "Step 538137  [5.452 sec/step, loss=0.07426, avg_loss=0.07334]\n",
      "Step 538138  [5.454 sec/step, loss=0.07388, avg_loss=0.07336]\n",
      "Step 538139  [5.476 sec/step, loss=0.07434, avg_loss=0.07335]\n",
      "Step 538140  [5.469 sec/step, loss=0.07520, avg_loss=0.07335]\n",
      "Step 538141  [5.480 sec/step, loss=0.07300, avg_loss=0.07337]\n",
      "Step 538142  [5.488 sec/step, loss=0.07501, avg_loss=0.07341]\n",
      "Step 538143  [5.511 sec/step, loss=0.07448, avg_loss=0.07342]\n",
      "Step 538144  [5.485 sec/step, loss=0.07390, avg_loss=0.07343]\n",
      "Step 538145  [5.484 sec/step, loss=0.07325, avg_loss=0.07342]\n",
      "Step 538146  [5.471 sec/step, loss=0.07529, avg_loss=0.07342]\n",
      "Step 538147  [5.477 sec/step, loss=0.07465, avg_loss=0.07342]\n",
      "Step 538148  [5.476 sec/step, loss=0.07477, avg_loss=0.07342]\n",
      "Step 538149  [5.489 sec/step, loss=0.07510, avg_loss=0.07344]\n",
      "Step 538150  [5.481 sec/step, loss=0.07423, avg_loss=0.07343]\n",
      "Step 538151  [5.477 sec/step, loss=0.07118, avg_loss=0.07340]\n",
      "Step 538152  [5.473 sec/step, loss=0.07544, avg_loss=0.07340]\n",
      "Step 538153  [5.468 sec/step, loss=0.07291, avg_loss=0.07338]\n",
      "Step 538154  [5.468 sec/step, loss=0.07378, avg_loss=0.07337]\n",
      "Step 538155  [5.463 sec/step, loss=0.07385, avg_loss=0.07339]\n",
      "Step 538156  [5.456 sec/step, loss=0.07197, avg_loss=0.07336]\n",
      "Step 538157  [5.428 sec/step, loss=0.07478, avg_loss=0.07336]\n",
      "Step 538158  [5.456 sec/step, loss=0.07397, avg_loss=0.07338]\n",
      "Step 538159  [5.466 sec/step, loss=0.07331, avg_loss=0.07346]\n",
      "Step 538160  [5.438 sec/step, loss=0.06528, avg_loss=0.07338]\n",
      "Step 538161  [5.413 sec/step, loss=0.06995, avg_loss=0.07334]\n",
      "Step 538162  [5.428 sec/step, loss=0.07276, avg_loss=0.07332]\n",
      "Step 538163  [5.429 sec/step, loss=0.07623, avg_loss=0.07332]\n",
      "Step 538164  [5.408 sec/step, loss=0.07356, avg_loss=0.07331]\n",
      "Generated 32 batches of size 32 in 2.485 sec\n",
      "Step 538165  [5.406 sec/step, loss=0.07410, avg_loss=0.07331]\n",
      "Step 538166  [5.413 sec/step, loss=0.07351, avg_loss=0.07333]\n",
      "Step 538167  [5.405 sec/step, loss=0.07219, avg_loss=0.07329]\n",
      "Step 538168  [5.454 sec/step, loss=0.06699, avg_loss=0.07324]\n",
      "Step 538169  [5.464 sec/step, loss=0.07413, avg_loss=0.07324]\n",
      "Step 538170  [5.478 sec/step, loss=0.07637, avg_loss=0.07327]\n",
      "Step 538171  [5.485 sec/step, loss=0.07463, avg_loss=0.07327]\n",
      "Step 538172  [5.501 sec/step, loss=0.07589, avg_loss=0.07330]\n",
      "Step 538173  [5.509 sec/step, loss=0.07442, avg_loss=0.07329]\n",
      "Step 538174  [5.570 sec/step, loss=0.06620, avg_loss=0.07324]\n",
      "Step 538175  [5.584 sec/step, loss=0.07410, avg_loss=0.07326]\n",
      "Step 538176  [5.530 sec/step, loss=0.07357, avg_loss=0.07334]\n",
      "Step 538177  [5.536 sec/step, loss=0.07432, avg_loss=0.07337]\n",
      "Step 538178  [5.514 sec/step, loss=0.07221, avg_loss=0.07332]\n",
      "Step 538179  [5.487 sec/step, loss=0.07436, avg_loss=0.07335]\n",
      "Step 538180  [5.447 sec/step, loss=0.07297, avg_loss=0.07340]\n",
      "Step 538181  [5.444 sec/step, loss=0.07481, avg_loss=0.07340]\n",
      "Step 538182  [5.433 sec/step, loss=0.07177, avg_loss=0.07338]\n",
      "Step 538183  [5.432 sec/step, loss=0.07358, avg_loss=0.07338]\n",
      "Step 538184  [5.457 sec/step, loss=0.07317, avg_loss=0.07338]\n",
      "Step 538185  [5.462 sec/step, loss=0.07518, avg_loss=0.07338]\n",
      "Step 538186  [5.456 sec/step, loss=0.07432, avg_loss=0.07341]\n",
      "Step 538187  [5.467 sec/step, loss=0.07453, avg_loss=0.07342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 538188  [5.470 sec/step, loss=0.07324, avg_loss=0.07342]\n",
      "Step 538189  [5.476 sec/step, loss=0.07502, avg_loss=0.07342]\n",
      "Step 538190  [5.486 sec/step, loss=0.07648, avg_loss=0.07347]\n",
      "Step 538191  [5.489 sec/step, loss=0.07499, avg_loss=0.07347]\n",
      "Step 538192  [5.506 sec/step, loss=0.07364, avg_loss=0.07355]\n",
      "Step 538193  [5.519 sec/step, loss=0.07544, avg_loss=0.07355]\n",
      "Step 538194  [5.535 sec/step, loss=0.07575, avg_loss=0.07358]\n",
      "Step 538195  [5.558 sec/step, loss=0.07502, avg_loss=0.07360]\n",
      "Step 538196  [5.551 sec/step, loss=0.07463, avg_loss=0.07359]\n",
      "Generated 32 batches of size 32 in 2.590 sec\n",
      "Step 538197  [5.545 sec/step, loss=0.07205, avg_loss=0.07356]\n",
      "Step 538198  [5.528 sec/step, loss=0.07130, avg_loss=0.07355]\n",
      "Step 538199  [5.513 sec/step, loss=0.07140, avg_loss=0.07353]\n",
      "Step 538200  [5.502 sec/step, loss=0.07368, avg_loss=0.07350]\n",
      "Writing summary at step: 538200\n",
      "Step 538201  [5.476 sec/step, loss=0.07374, avg_loss=0.07350]\n",
      "Step 538202  [5.446 sec/step, loss=0.06572, avg_loss=0.07340]\n",
      "Step 538203  [5.465 sec/step, loss=0.07472, avg_loss=0.07343]\n",
      "Step 538204  [5.460 sec/step, loss=0.07443, avg_loss=0.07342]\n",
      "Step 538205  [5.465 sec/step, loss=0.07409, avg_loss=0.07343]\n",
      "Step 538206  [5.465 sec/step, loss=0.07588, avg_loss=0.07347]\n",
      "Step 538207  [5.439 sec/step, loss=0.07388, avg_loss=0.07345]\n",
      "Step 538208  [5.453 sec/step, loss=0.07506, avg_loss=0.07349]\n",
      "Step 538209  [5.449 sec/step, loss=0.07416, avg_loss=0.07350]\n",
      "Step 538210  [5.444 sec/step, loss=0.07343, avg_loss=0.07351]\n",
      "Step 538211  [5.428 sec/step, loss=0.07468, avg_loss=0.07351]\n",
      "Step 538212  [5.406 sec/step, loss=0.06618, avg_loss=0.07343]\n",
      "Step 538213  [5.397 sec/step, loss=0.07415, avg_loss=0.07341]\n",
      "Step 538214  [5.412 sec/step, loss=0.07571, avg_loss=0.07346]\n",
      "Step 538215  [5.420 sec/step, loss=0.07555, avg_loss=0.07347]\n",
      "Step 538216  [5.431 sec/step, loss=0.07357, avg_loss=0.07346]\n",
      "Step 538217  [5.415 sec/step, loss=0.07067, avg_loss=0.07341]\n",
      "Step 538218  [5.411 sec/step, loss=0.07425, avg_loss=0.07343]\n",
      "Step 538219  [5.434 sec/step, loss=0.07576, avg_loss=0.07345]\n",
      "Step 538220  [5.438 sec/step, loss=0.07394, avg_loss=0.07345]\n",
      "Step 538221  [5.453 sec/step, loss=0.07528, avg_loss=0.07349]\n",
      "Step 538222  [5.463 sec/step, loss=0.07485, avg_loss=0.07350]\n",
      "Step 538223  [5.452 sec/step, loss=0.07466, avg_loss=0.07350]\n",
      "Step 538224  [5.501 sec/step, loss=0.06493, avg_loss=0.07341]\n",
      "Step 538225  [5.520 sec/step, loss=0.07495, avg_loss=0.07343]\n",
      "Step 538226  [5.514 sec/step, loss=0.07134, avg_loss=0.07339]\n",
      "Step 538227  [5.513 sec/step, loss=0.07386, avg_loss=0.07339]\n",
      "Generated 32 batches of size 32 in 2.399 sec\n",
      "Step 538228  [5.525 sec/step, loss=0.07308, avg_loss=0.07337]\n",
      "Step 538229  [5.511 sec/step, loss=0.07323, avg_loss=0.07335]\n",
      "Step 538230  [5.456 sec/step, loss=0.07292, avg_loss=0.07344]\n",
      "Step 538231  [5.451 sec/step, loss=0.06956, avg_loss=0.07341]\n",
      "Step 538232  [5.465 sec/step, loss=0.07401, avg_loss=0.07339]\n",
      "Step 538233  [5.474 sec/step, loss=0.07238, avg_loss=0.07347]\n",
      "Step 538234  [5.482 sec/step, loss=0.07509, avg_loss=0.07351]\n",
      "Step 538235  [5.474 sec/step, loss=0.07318, avg_loss=0.07348]\n",
      "Step 538236  [5.472 sec/step, loss=0.07582, avg_loss=0.07349]\n",
      "Step 538237  [5.454 sec/step, loss=0.07083, avg_loss=0.07346]\n",
      "Step 538238  [5.457 sec/step, loss=0.07585, avg_loss=0.07348]\n",
      "Step 538239  [5.434 sec/step, loss=0.07038, avg_loss=0.07344]\n",
      "Step 538240  [5.438 sec/step, loss=0.07456, avg_loss=0.07343]\n",
      "Step 538241  [5.435 sec/step, loss=0.07381, avg_loss=0.07344]\n",
      "Step 538242  [5.437 sec/step, loss=0.07272, avg_loss=0.07342]\n",
      "Step 538243  [5.409 sec/step, loss=0.06589, avg_loss=0.07333]\n",
      "Step 538244  [5.411 sec/step, loss=0.07170, avg_loss=0.07331]\n",
      "Step 538245  [5.420 sec/step, loss=0.07438, avg_loss=0.07332]\n",
      "Step 538246  [5.431 sec/step, loss=0.07577, avg_loss=0.07332]\n",
      "Step 538247  [5.422 sec/step, loss=0.07354, avg_loss=0.07331]\n",
      "Step 538248  [5.418 sec/step, loss=0.07417, avg_loss=0.07331]\n",
      "Step 538249  [5.403 sec/step, loss=0.07043, avg_loss=0.07326]\n",
      "Step 538250  [5.412 sec/step, loss=0.07532, avg_loss=0.07327]\n",
      "Step 538251  [5.438 sec/step, loss=0.07509, avg_loss=0.07331]\n",
      "Step 538252  [5.427 sec/step, loss=0.07142, avg_loss=0.07327]\n",
      "Step 538253  [5.419 sec/step, loss=0.07383, avg_loss=0.07328]\n",
      "Step 538254  [5.434 sec/step, loss=0.07263, avg_loss=0.07327]\n",
      "Step 538255  [5.438 sec/step, loss=0.07522, avg_loss=0.07328]\n",
      "Step 538256  [5.434 sec/step, loss=0.07208, avg_loss=0.07328]\n",
      "Step 538257  [5.488 sec/step, loss=0.06616, avg_loss=0.07320]\n",
      "Step 538258  [5.475 sec/step, loss=0.07444, avg_loss=0.07320]\n",
      "Step 538259  [5.480 sec/step, loss=0.07491, avg_loss=0.07322]\n",
      "Generated 32 batches of size 32 in 2.391 sec\n",
      "Step 538260  [5.505 sec/step, loss=0.07431, avg_loss=0.07331]\n",
      "Step 538261  [5.509 sec/step, loss=0.07293, avg_loss=0.07334]\n",
      "Step 538262  [5.472 sec/step, loss=0.07133, avg_loss=0.07332]\n",
      "Step 538263  [5.460 sec/step, loss=0.07458, avg_loss=0.07331]\n",
      "Step 538264  [5.483 sec/step, loss=0.07550, avg_loss=0.07333]\n",
      "Step 538265  [5.487 sec/step, loss=0.07553, avg_loss=0.07334]\n",
      "Step 538266  [5.521 sec/step, loss=0.07242, avg_loss=0.07333]\n",
      "Step 538267  [5.531 sec/step, loss=0.07263, avg_loss=0.07333]\n",
      "Step 538268  [5.483 sec/step, loss=0.07551, avg_loss=0.07342]\n",
      "Step 538269  [5.485 sec/step, loss=0.07354, avg_loss=0.07341]\n",
      "Step 538270  [5.481 sec/step, loss=0.07551, avg_loss=0.07340]\n",
      "Step 538271  [5.480 sec/step, loss=0.07561, avg_loss=0.07341]\n",
      "Step 538272  [5.469 sec/step, loss=0.07271, avg_loss=0.07338]\n",
      "Step 538273  [5.459 sec/step, loss=0.07378, avg_loss=0.07338]\n",
      "Step 538274  [5.413 sec/step, loss=0.07449, avg_loss=0.07346]\n",
      "Step 538275  [5.409 sec/step, loss=0.07180, avg_loss=0.07344]\n",
      "Step 538276  [5.402 sec/step, loss=0.07353, avg_loss=0.07343]\n",
      "Step 538277  [5.404 sec/step, loss=0.07460, avg_loss=0.07344]\n",
      "Step 538278  [5.420 sec/step, loss=0.07553, avg_loss=0.07347]\n",
      "Step 538279  [5.430 sec/step, loss=0.07425, avg_loss=0.07347]\n",
      "Step 538280  [5.421 sec/step, loss=0.07136, avg_loss=0.07345]\n",
      "Step 538281  [5.412 sec/step, loss=0.07332, avg_loss=0.07344]\n",
      "Step 538282  [5.448 sec/step, loss=0.07259, avg_loss=0.07345]\n",
      "Step 538283  [5.441 sec/step, loss=0.07273, avg_loss=0.07344]\n",
      "Step 538284  [5.401 sec/step, loss=0.07051, avg_loss=0.07341]\n",
      "Step 538285  [5.407 sec/step, loss=0.07592, avg_loss=0.07342]\n",
      "Step 538286  [5.417 sec/step, loss=0.07541, avg_loss=0.07343]\n",
      "Step 538287  [5.391 sec/step, loss=0.06516, avg_loss=0.07334]\n",
      "Step 538288  [5.386 sec/step, loss=0.07006, avg_loss=0.07330]\n",
      "Step 538289  [5.418 sec/step, loss=0.06508, avg_loss=0.07321]\n",
      "Step 538290  [5.411 sec/step, loss=0.07460, avg_loss=0.07319]\n",
      "Step 538291  [5.424 sec/step, loss=0.07545, avg_loss=0.07319]\n",
      "Generated 32 batches of size 32 in 2.446 sec\n",
      "Step 538292  [5.439 sec/step, loss=0.07431, avg_loss=0.07320]\n",
      "Step 538293  [5.439 sec/step, loss=0.07515, avg_loss=0.07319]\n",
      "Step 538294  [5.423 sec/step, loss=0.07321, avg_loss=0.07317]\n",
      "Step 538295  [5.401 sec/step, loss=0.07342, avg_loss=0.07315]\n",
      "Step 538296  [5.409 sec/step, loss=0.07449, avg_loss=0.07315]\n",
      "Step 538297  [5.401 sec/step, loss=0.07479, avg_loss=0.07318]\n",
      "Step 538298  [5.415 sec/step, loss=0.07091, avg_loss=0.07318]\n",
      "Step 538299  [5.419 sec/step, loss=0.07430, avg_loss=0.07320]\n",
      "Step 538300  [5.427 sec/step, loss=0.07434, avg_loss=0.07321]\n",
      "Writing summary at step: 538300\n",
      "Step 538301  [5.443 sec/step, loss=0.07179, avg_loss=0.07319]\n",
      "Step 538302  [5.468 sec/step, loss=0.07298, avg_loss=0.07326]\n",
      "Step 538303  [5.466 sec/step, loss=0.07400, avg_loss=0.07326]\n",
      "Step 538304  [5.479 sec/step, loss=0.07523, avg_loss=0.07326]\n",
      "Step 538305  [5.502 sec/step, loss=0.07306, avg_loss=0.07325]\n",
      "Step 538306  [5.492 sec/step, loss=0.07010, avg_loss=0.07320]\n",
      "Step 538307  [5.495 sec/step, loss=0.07263, avg_loss=0.07318]\n",
      "Step 538308  [5.493 sec/step, loss=0.07125, avg_loss=0.07315]\n",
      "Step 538309  [5.479 sec/step, loss=0.07492, avg_loss=0.07315]\n",
      "Step 538310  [5.496 sec/step, loss=0.07552, avg_loss=0.07317]\n",
      "Step 538311  [5.544 sec/step, loss=0.06690, avg_loss=0.07310]\n",
      "Step 538312  [5.550 sec/step, loss=0.07047, avg_loss=0.07314]\n",
      "Step 538313  [5.537 sec/step, loss=0.07128, avg_loss=0.07311]\n",
      "Step 538314  [5.537 sec/step, loss=0.07348, avg_loss=0.07309]\n",
      "Step 538315  [5.537 sec/step, loss=0.07511, avg_loss=0.07308]\n",
      "Step 538316  [5.535 sec/step, loss=0.07450, avg_loss=0.07309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 538317  [5.546 sec/step, loss=0.07351, avg_loss=0.07312]\n",
      "Step 538318  [5.546 sec/step, loss=0.07474, avg_loss=0.07313]\n",
      "Step 538319  [5.524 sec/step, loss=0.07368, avg_loss=0.07311]\n",
      "Step 538320  [5.521 sec/step, loss=0.07349, avg_loss=0.07310]\n",
      "Step 538321  [5.515 sec/step, loss=0.07214, avg_loss=0.07307]\n",
      "Step 538322  [5.496 sec/step, loss=0.07469, avg_loss=0.07307]\n",
      "Generated 32 batches of size 32 in 2.440 sec\n",
      "Step 538323  [5.499 sec/step, loss=0.07497, avg_loss=0.07307]\n",
      "Step 538324  [5.432 sec/step, loss=0.06549, avg_loss=0.07308]\n",
      "Step 538325  [5.424 sec/step, loss=0.07403, avg_loss=0.07307]\n",
      "Step 538326  [5.438 sec/step, loss=0.07544, avg_loss=0.07311]\n",
      "Step 538327  [5.435 sec/step, loss=0.07518, avg_loss=0.07312]\n",
      "Step 538328  [5.435 sec/step, loss=0.07533, avg_loss=0.07314]\n",
      "Step 538329  [5.434 sec/step, loss=0.07374, avg_loss=0.07315]\n",
      "Step 538330  [5.452 sec/step, loss=0.07542, avg_loss=0.07317]\n",
      "Step 538331  [5.464 sec/step, loss=0.07402, avg_loss=0.07322]\n",
      "Step 538332  [5.437 sec/step, loss=0.07194, avg_loss=0.07320]\n",
      "Step 538333  [5.435 sec/step, loss=0.07296, avg_loss=0.07320]\n",
      "Step 538334  [5.442 sec/step, loss=0.07417, avg_loss=0.07320]\n",
      "Step 538335  [5.452 sec/step, loss=0.07343, avg_loss=0.07320]\n",
      "Step 538336  [5.494 sec/step, loss=0.06469, avg_loss=0.07309]\n",
      "Step 538337  [5.513 sec/step, loss=0.07503, avg_loss=0.07313]\n",
      "Step 538338  [5.496 sec/step, loss=0.07007, avg_loss=0.07307]\n",
      "Step 538339  [5.502 sec/step, loss=0.07563, avg_loss=0.07312]\n",
      "Step 538340  [5.496 sec/step, loss=0.07380, avg_loss=0.07312]\n",
      "Step 538341  [5.489 sec/step, loss=0.07101, avg_loss=0.07309]\n",
      "Step 538342  [5.479 sec/step, loss=0.07187, avg_loss=0.07308]\n",
      "Step 538343  [5.502 sec/step, loss=0.07391, avg_loss=0.07316]\n",
      "Step 538344  [5.497 sec/step, loss=0.07309, avg_loss=0.07317]\n",
      "Step 538345  [5.507 sec/step, loss=0.07486, avg_loss=0.07318]\n",
      "Step 538346  [5.497 sec/step, loss=0.07381, avg_loss=0.07316]\n",
      "Step 538347  [5.510 sec/step, loss=0.07516, avg_loss=0.07317]\n",
      "Step 538348  [5.512 sec/step, loss=0.07504, avg_loss=0.07318]\n",
      "Step 538349  [5.520 sec/step, loss=0.07457, avg_loss=0.07322]\n",
      "Step 538350  [5.489 sec/step, loss=0.06597, avg_loss=0.07313]\n",
      "Step 538351  [5.479 sec/step, loss=0.07429, avg_loss=0.07312]\n",
      "Step 538352  [5.494 sec/step, loss=0.07479, avg_loss=0.07316]\n",
      "Step 538353  [5.514 sec/step, loss=0.07396, avg_loss=0.07316]\n",
      "Step 538354  [5.519 sec/step, loss=0.07379, avg_loss=0.07317]\n",
      "Generated 32 batches of size 32 in 2.594 sec\n",
      "Step 538355  [5.515 sec/step, loss=0.07330, avg_loss=0.07315]\n",
      "Step 538356  [5.534 sec/step, loss=0.07253, avg_loss=0.07315]\n",
      "Step 538357  [5.476 sec/step, loss=0.07361, avg_loss=0.07323]\n",
      "Step 538358  [5.464 sec/step, loss=0.07474, avg_loss=0.07323]\n",
      "Step 538359  [5.463 sec/step, loss=0.07315, avg_loss=0.07321]\n",
      "Step 538360  [5.466 sec/step, loss=0.07545, avg_loss=0.07323]\n",
      "Step 538361  [5.473 sec/step, loss=0.07532, avg_loss=0.07325]\n",
      "Step 538362  [5.481 sec/step, loss=0.07467, avg_loss=0.07328]\n",
      "Step 538363  [5.496 sec/step, loss=0.07656, avg_loss=0.07330]\n",
      "Step 538364  [5.468 sec/step, loss=0.06424, avg_loss=0.07319]\n",
      "Step 538365  [5.486 sec/step, loss=0.07328, avg_loss=0.07317]\n",
      "Step 538366  [5.472 sec/step, loss=0.07560, avg_loss=0.07320]\n",
      "Step 538367  [5.471 sec/step, loss=0.07315, avg_loss=0.07321]\n",
      "Step 538368  [5.469 sec/step, loss=0.07337, avg_loss=0.07318]\n",
      "Step 538369  [5.445 sec/step, loss=0.07332, avg_loss=0.07318]\n",
      "Step 538370  [5.438 sec/step, loss=0.07389, avg_loss=0.07317]\n",
      "Step 538371  [5.438 sec/step, loss=0.07508, avg_loss=0.07316]\n",
      "Step 538372  [5.452 sec/step, loss=0.07546, avg_loss=0.07319]\n",
      "Step 538373  [5.442 sec/step, loss=0.07006, avg_loss=0.07315]\n",
      "Step 538374  [5.487 sec/step, loss=0.06603, avg_loss=0.07307]\n",
      "Step 538375  [5.481 sec/step, loss=0.07139, avg_loss=0.07306]\n",
      "Step 538376  [5.488 sec/step, loss=0.07320, avg_loss=0.07306]\n",
      "Step 538377  [5.482 sec/step, loss=0.07605, avg_loss=0.07307]\n",
      "Step 538378  [5.489 sec/step, loss=0.07526, avg_loss=0.07307]\n",
      "Step 538379  [5.471 sec/step, loss=0.07374, avg_loss=0.07307]\n",
      "Step 538380  [5.477 sec/step, loss=0.07646, avg_loss=0.07312]\n",
      "Step 538381  [5.488 sec/step, loss=0.07417, avg_loss=0.07312]\n",
      "Step 538382  [5.470 sec/step, loss=0.07545, avg_loss=0.07315]\n",
      "Step 538383  [5.472 sec/step, loss=0.07340, avg_loss=0.07316]\n",
      "Step 538384  [5.506 sec/step, loss=0.07259, avg_loss=0.07318]\n",
      "Step 538385  [5.501 sec/step, loss=0.07427, avg_loss=0.07316]\n",
      "Step 538386  [5.502 sec/step, loss=0.07548, avg_loss=0.07316]\n",
      "Generated 32 batches of size 32 in 2.622 sec\n",
      "Step 538387  [5.511 sec/step, loss=0.07062, avg_loss=0.07322]\n",
      "Step 538388  [5.521 sec/step, loss=0.07490, avg_loss=0.07327]\n",
      "Step 538389  [5.462 sec/step, loss=0.07189, avg_loss=0.07334]\n",
      "Step 538390  [5.465 sec/step, loss=0.07383, avg_loss=0.07333]\n",
      "Step 538391  [5.448 sec/step, loss=0.07152, avg_loss=0.07329]\n",
      "Step 538392  [5.435 sec/step, loss=0.07507, avg_loss=0.07330]\n",
      "Step 538393  [5.425 sec/step, loss=0.07449, avg_loss=0.07329]\n",
      "Step 538394  [5.440 sec/step, loss=0.07564, avg_loss=0.07331]\n",
      "Step 538395  [5.443 sec/step, loss=0.07328, avg_loss=0.07331]\n",
      "Step 538396  [5.442 sec/step, loss=0.07535, avg_loss=0.07332]\n",
      "Step 538397  [5.442 sec/step, loss=0.07461, avg_loss=0.07332]\n",
      "Step 538398  [5.448 sec/step, loss=0.07151, avg_loss=0.07333]\n",
      "Step 538399  [5.453 sec/step, loss=0.07406, avg_loss=0.07332]\n",
      "Step 538400  [5.449 sec/step, loss=0.07456, avg_loss=0.07333]\n",
      "Writing summary at step: 538400\n",
      "Step 538401  [5.428 sec/step, loss=0.06755, avg_loss=0.07328]\n",
      "Step 538402  [5.415 sec/step, loss=0.07286, avg_loss=0.07328]\n",
      "Step 538403  [5.411 sec/step, loss=0.07614, avg_loss=0.07330]\n",
      "Step 538404  [5.410 sec/step, loss=0.07347, avg_loss=0.07329]\n",
      "Step 538405  [5.384 sec/step, loss=0.07199, avg_loss=0.07327]\n",
      "Step 538406  [5.395 sec/step, loss=0.07560, avg_loss=0.07333]\n",
      "Step 538407  [5.413 sec/step, loss=0.07329, avg_loss=0.07334]\n",
      "Step 538408  [5.437 sec/step, loss=0.07248, avg_loss=0.07335]\n",
      "Step 538409  [5.430 sec/step, loss=0.07312, avg_loss=0.07333]\n",
      "Step 538410  [5.422 sec/step, loss=0.07474, avg_loss=0.07332]\n",
      "Step 538411  [5.368 sec/step, loss=0.07407, avg_loss=0.07339]\n",
      "Step 538412  [5.380 sec/step, loss=0.07219, avg_loss=0.07341]\n",
      "Step 538413  [5.412 sec/step, loss=0.07528, avg_loss=0.07345]\n",
      "Step 538414  [5.450 sec/step, loss=0.06569, avg_loss=0.07337]\n",
      "Step 538415  [5.454 sec/step, loss=0.07565, avg_loss=0.07338]\n",
      "Step 538416  [5.443 sec/step, loss=0.06973, avg_loss=0.07333]\n",
      "Step 538417  [5.439 sec/step, loss=0.07327, avg_loss=0.07333]\n",
      "Generated 32 batches of size 32 in 2.411 sec\n",
      "Step 538418  [5.459 sec/step, loss=0.07516, avg_loss=0.07333]\n",
      "Step 538419  [5.479 sec/step, loss=0.07503, avg_loss=0.07335]\n",
      "Step 538420  [5.487 sec/step, loss=0.07586, avg_loss=0.07337]\n",
      "Step 538421  [5.495 sec/step, loss=0.07284, avg_loss=0.07338]\n",
      "Step 538422  [5.483 sec/step, loss=0.07216, avg_loss=0.07335]\n",
      "Step 538423  [5.468 sec/step, loss=0.07092, avg_loss=0.07331]\n",
      "Step 538424  [5.490 sec/step, loss=0.07453, avg_loss=0.07340]\n",
      "Step 538425  [5.486 sec/step, loss=0.07721, avg_loss=0.07343]\n",
      "Step 538426  [5.463 sec/step, loss=0.07256, avg_loss=0.07341]\n",
      "Step 538427  [5.449 sec/step, loss=0.07233, avg_loss=0.07338]\n",
      "Step 538428  [5.443 sec/step, loss=0.07428, avg_loss=0.07337]\n",
      "Step 538429  [5.447 sec/step, loss=0.07357, avg_loss=0.07336]\n",
      "Step 538430  [5.431 sec/step, loss=0.07477, avg_loss=0.07336]\n",
      "Step 538431  [5.433 sec/step, loss=0.07491, avg_loss=0.07337]\n",
      "Step 538432  [5.443 sec/step, loss=0.07476, avg_loss=0.07339]\n",
      "Step 538433  [5.450 sec/step, loss=0.07370, avg_loss=0.07340]\n",
      "Step 538434  [5.455 sec/step, loss=0.07280, avg_loss=0.07339]\n",
      "Step 538435  [5.450 sec/step, loss=0.07424, avg_loss=0.07340]\n",
      "Step 538436  [5.406 sec/step, loss=0.07478, avg_loss=0.07350]\n",
      "Step 538437  [5.386 sec/step, loss=0.07057, avg_loss=0.07345]\n",
      "Step 538438  [5.384 sec/step, loss=0.07354, avg_loss=0.07349]\n",
      "Step 538439  [5.401 sec/step, loss=0.07326, avg_loss=0.07346]\n",
      "Step 538440  [5.393 sec/step, loss=0.07348, avg_loss=0.07346]\n",
      "Step 538441  [5.411 sec/step, loss=0.07506, avg_loss=0.07350]\n",
      "Step 538442  [5.429 sec/step, loss=0.07603, avg_loss=0.07354]\n",
      "Step 538443  [5.414 sec/step, loss=0.07263, avg_loss=0.07353]\n",
      "Step 538444  [5.420 sec/step, loss=0.07404, avg_loss=0.07354]\n",
      "Step 538445  [5.416 sec/step, loss=0.07572, avg_loss=0.07355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 538446  [5.429 sec/step, loss=0.07602, avg_loss=0.07357]\n",
      "Step 538447  [5.421 sec/step, loss=0.07513, avg_loss=0.07357]\n",
      "Step 538448  [5.440 sec/step, loss=0.07209, avg_loss=0.07354]\n",
      "Step 538449  [5.434 sec/step, loss=0.07309, avg_loss=0.07353]\n",
      "Generated 32 batches of size 32 in 2.398 sec\n",
      "Step 538450  [5.465 sec/step, loss=0.07575, avg_loss=0.07362]\n",
      "Step 538451  [5.464 sec/step, loss=0.07638, avg_loss=0.07364]\n",
      "Step 538452  [5.444 sec/step, loss=0.07479, avg_loss=0.07364]\n",
      "Step 538453  [5.424 sec/step, loss=0.07005, avg_loss=0.07361]\n",
      "Step 538454  [5.397 sec/step, loss=0.07080, avg_loss=0.07358]\n",
      "Step 538455  [5.377 sec/step, loss=0.06348, avg_loss=0.07348]\n",
      "Step 538456  [5.414 sec/step, loss=0.06605, avg_loss=0.07341]\n",
      "Step 538457  [5.435 sec/step, loss=0.07582, avg_loss=0.07343]\n",
      "Step 538458  [5.449 sec/step, loss=0.07586, avg_loss=0.07345]\n",
      "Step 538459  [5.449 sec/step, loss=0.07463, avg_loss=0.07346]\n",
      "Step 538460  [5.465 sec/step, loss=0.07309, avg_loss=0.07344]\n",
      "Step 538461  [5.466 sec/step, loss=0.07561, avg_loss=0.07344]\n",
      "Step 538462  [5.489 sec/step, loss=0.07349, avg_loss=0.07343]\n",
      "Step 538463  [5.474 sec/step, loss=0.07145, avg_loss=0.07338]\n",
      "Step 538464  [5.486 sec/step, loss=0.07215, avg_loss=0.07346]\n",
      "Step 538465  [5.464 sec/step, loss=0.07427, avg_loss=0.07347]\n",
      "Step 538466  [5.440 sec/step, loss=0.07319, avg_loss=0.07344]\n",
      "Step 538467  [5.421 sec/step, loss=0.07326, avg_loss=0.07344]\n",
      "Step 538468  [5.435 sec/step, loss=0.07646, avg_loss=0.07347]\n",
      "Step 538469  [5.459 sec/step, loss=0.07530, avg_loss=0.07349]\n",
      "Step 538470  [5.470 sec/step, loss=0.07330, avg_loss=0.07349]\n",
      "Step 538471  [5.457 sec/step, loss=0.07020, avg_loss=0.07344]\n",
      "Step 538472  [5.454 sec/step, loss=0.07498, avg_loss=0.07343]\n",
      "Step 538473  [5.457 sec/step, loss=0.07272, avg_loss=0.07346]\n",
      "Step 538474  [5.408 sec/step, loss=0.07450, avg_loss=0.07355]\n",
      "Step 538475  [5.412 sec/step, loss=0.07421, avg_loss=0.07357]\n",
      "Step 538476  [5.415 sec/step, loss=0.07430, avg_loss=0.07358]\n",
      "Step 538477  [5.427 sec/step, loss=0.07531, avg_loss=0.07358]\n",
      "Step 538478  [5.414 sec/step, loss=0.07444, avg_loss=0.07357]\n",
      "Step 538479  [5.426 sec/step, loss=0.07445, avg_loss=0.07358]\n",
      "Step 538480  [5.427 sec/step, loss=0.07372, avg_loss=0.07355]\n",
      "Step 538481  [5.409 sec/step, loss=0.07069, avg_loss=0.07351]\n",
      "Generated 32 batches of size 32 in 2.446 sec\n",
      "Step 538482  [5.414 sec/step, loss=0.07528, avg_loss=0.07351]\n",
      "Step 538483  [5.467 sec/step, loss=0.06577, avg_loss=0.07344]\n",
      "Step 538484  [5.431 sec/step, loss=0.06583, avg_loss=0.07337]\n",
      "Step 538485  [5.429 sec/step, loss=0.07554, avg_loss=0.07338]\n",
      "Step 538486  [5.431 sec/step, loss=0.07605, avg_loss=0.07339]\n",
      "Step 538487  [5.447 sec/step, loss=0.07465, avg_loss=0.07343]\n",
      "Step 538488  [5.448 sec/step, loss=0.07460, avg_loss=0.07342]\n",
      "Step 538489  [5.453 sec/step, loss=0.07289, avg_loss=0.07343]\n",
      "Step 538490  [5.451 sec/step, loss=0.07502, avg_loss=0.07345]\n",
      "Step 538491  [5.465 sec/step, loss=0.07556, avg_loss=0.07349]\n",
      "Step 538492  [5.450 sec/step, loss=0.07227, avg_loss=0.07346]\n",
      "Step 538493  [5.450 sec/step, loss=0.07385, avg_loss=0.07345]\n",
      "Step 538494  [5.436 sec/step, loss=0.07356, avg_loss=0.07343]\n",
      "Step 538495  [5.432 sec/step, loss=0.07334, avg_loss=0.07343]\n",
      "Step 538496  [5.426 sec/step, loss=0.07600, avg_loss=0.07344]\n",
      "Step 538497  [5.426 sec/step, loss=0.07487, avg_loss=0.07344]\n",
      "Step 538498  [5.422 sec/step, loss=0.07454, avg_loss=0.07347]\n",
      "Step 538499  [5.406 sec/step, loss=0.07221, avg_loss=0.07345]\n",
      "Step 538500  [5.389 sec/step, loss=0.06634, avg_loss=0.07337]\n",
      "Writing summary at step: 538500\n",
      "Step 538501  [5.414 sec/step, loss=0.07529, avg_loss=0.07345]\n",
      "Step 538502  [5.428 sec/step, loss=0.07539, avg_loss=0.07347]\n",
      "Step 538503  [5.422 sec/step, loss=0.07346, avg_loss=0.07345]\n",
      "Step 538504  [5.410 sec/step, loss=0.07513, avg_loss=0.07346]\n",
      "Step 538505  [5.421 sec/step, loss=0.07355, avg_loss=0.07348]\n",
      "Step 538506  [5.430 sec/step, loss=0.07442, avg_loss=0.07347]\n",
      "Step 538507  [5.432 sec/step, loss=0.07352, avg_loss=0.07347]\n",
      "Step 538508  [5.409 sec/step, loss=0.07303, avg_loss=0.07347]\n",
      "Step 538509  [5.425 sec/step, loss=0.07388, avg_loss=0.07348]\n",
      "Step 538510  [5.426 sec/step, loss=0.07503, avg_loss=0.07348]\n",
      "Step 538511  [5.425 sec/step, loss=0.07271, avg_loss=0.07347]\n",
      "Step 538512  [5.433 sec/step, loss=0.07221, avg_loss=0.07347]\n",
      "Generated 32 batches of size 32 in 2.498 sec\n",
      "Step 538513  [5.420 sec/step, loss=0.07573, avg_loss=0.07348]\n",
      "Step 538514  [5.360 sec/step, loss=0.07040, avg_loss=0.07352]\n",
      "Step 538515  [5.350 sec/step, loss=0.07115, avg_loss=0.07348]\n",
      "Step 538516  [5.404 sec/step, loss=0.06500, avg_loss=0.07343]\n",
      "Step 538517  [5.411 sec/step, loss=0.07488, avg_loss=0.07345]\n",
      "Step 538518  [5.398 sec/step, loss=0.07494, avg_loss=0.07344]\n",
      "Step 538519  [5.386 sec/step, loss=0.07137, avg_loss=0.07341]\n",
      "Step 538520  [5.384 sec/step, loss=0.07534, avg_loss=0.07340]\n",
      "Step 538521  [5.394 sec/step, loss=0.07562, avg_loss=0.07343]\n",
      "Step 538522  [5.411 sec/step, loss=0.07409, avg_loss=0.07345]\n",
      "Step 538523  [5.410 sec/step, loss=0.07377, avg_loss=0.07348]\n",
      "Step 538524  [5.405 sec/step, loss=0.07662, avg_loss=0.07350]\n",
      "Step 538525  [5.400 sec/step, loss=0.07297, avg_loss=0.07346]\n",
      "Step 538526  [5.420 sec/step, loss=0.07591, avg_loss=0.07349]\n",
      "Step 538527  [5.436 sec/step, loss=0.07484, avg_loss=0.07352]\n",
      "Step 538528  [5.415 sec/step, loss=0.06492, avg_loss=0.07342]\n",
      "Step 538529  [5.432 sec/step, loss=0.07553, avg_loss=0.07344]\n",
      "Step 538530  [5.435 sec/step, loss=0.07522, avg_loss=0.07345]\n",
      "Step 538531  [5.430 sec/step, loss=0.07439, avg_loss=0.07344]\n",
      "Step 538532  [5.420 sec/step, loss=0.07362, avg_loss=0.07343]\n",
      "Step 538533  [5.424 sec/step, loss=0.07127, avg_loss=0.07341]\n",
      "Step 538534  [5.403 sec/step, loss=0.06970, avg_loss=0.07337]\n",
      "Step 538535  [5.398 sec/step, loss=0.07255, avg_loss=0.07336]\n",
      "Step 538536  [5.418 sec/step, loss=0.07267, avg_loss=0.07334]\n",
      "Step 538537  [5.427 sec/step, loss=0.07305, avg_loss=0.07336]\n",
      "Step 538538  [5.449 sec/step, loss=0.07554, avg_loss=0.07338]\n",
      "Step 538539  [5.426 sec/step, loss=0.07481, avg_loss=0.07340]\n",
      "Step 538540  [5.439 sec/step, loss=0.07440, avg_loss=0.07341]\n",
      "Step 538541  [5.424 sec/step, loss=0.07359, avg_loss=0.07339]\n",
      "Step 538542  [5.403 sec/step, loss=0.07066, avg_loss=0.07334]\n",
      "Step 538543  [5.428 sec/step, loss=0.07548, avg_loss=0.07337]\n",
      "Step 538544  [5.418 sec/step, loss=0.07106, avg_loss=0.07334]\n",
      "Generated 32 batches of size 32 in 2.402 sec\n",
      "Step 538545  [5.425 sec/step, loss=0.07548, avg_loss=0.07333]\n",
      "Step 538546  [5.461 sec/step, loss=0.06586, avg_loss=0.07323]\n",
      "Step 538547  [5.475 sec/step, loss=0.07484, avg_loss=0.07323]\n",
      "Step 538548  [5.447 sec/step, loss=0.07177, avg_loss=0.07323]\n",
      "Step 538549  [5.450 sec/step, loss=0.07480, avg_loss=0.07324]\n",
      "Step 538550  [5.441 sec/step, loss=0.07424, avg_loss=0.07323]\n",
      "Step 538551  [5.452 sec/step, loss=0.07336, avg_loss=0.07320]\n",
      "Step 538552  [5.462 sec/step, loss=0.07585, avg_loss=0.07321]\n",
      "Step 538553  [5.480 sec/step, loss=0.07595, avg_loss=0.07327]\n",
      "Step 538554  [5.479 sec/step, loss=0.07348, avg_loss=0.07329]\n",
      "Step 538555  [5.512 sec/step, loss=0.07563, avg_loss=0.07342]\n",
      "Step 538556  [5.461 sec/step, loss=0.07633, avg_loss=0.07352]\n",
      "Step 538557  [5.443 sec/step, loss=0.07296, avg_loss=0.07349]\n",
      "Step 538558  [5.433 sec/step, loss=0.07498, avg_loss=0.07348]\n",
      "Step 538559  [5.447 sec/step, loss=0.07576, avg_loss=0.07349]\n",
      "Step 538560  [5.412 sec/step, loss=0.07391, avg_loss=0.07350]\n",
      "Step 538561  [5.407 sec/step, loss=0.07460, avg_loss=0.07349]\n",
      "Step 538562  [5.388 sec/step, loss=0.07509, avg_loss=0.07351]\n",
      "Step 538563  [5.416 sec/step, loss=0.07307, avg_loss=0.07352]\n",
      "Step 538564  [5.430 sec/step, loss=0.07295, avg_loss=0.07353]\n",
      "Step 538565  [5.425 sec/step, loss=0.07390, avg_loss=0.07353]\n",
      "Step 538566  [5.487 sec/step, loss=0.06614, avg_loss=0.07346]\n",
      "Step 538567  [5.481 sec/step, loss=0.07145, avg_loss=0.07344]\n",
      "Step 538568  [5.477 sec/step, loss=0.07557, avg_loss=0.07343]\n",
      "Step 538569  [5.454 sec/step, loss=0.07230, avg_loss=0.07340]\n",
      "Step 538570  [5.459 sec/step, loss=0.07240, avg_loss=0.07339]\n",
      "Step 538571  [5.467 sec/step, loss=0.07366, avg_loss=0.07343]\n",
      "Step 538572  [5.467 sec/step, loss=0.07377, avg_loss=0.07341]\n",
      "Step 538573  [5.479 sec/step, loss=0.07432, avg_loss=0.07343]\n",
      "Step 538574  [5.461 sec/step, loss=0.06622, avg_loss=0.07335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 538575  [5.466 sec/step, loss=0.07443, avg_loss=0.07335]\n",
      "Step 538576  [5.469 sec/step, loss=0.07453, avg_loss=0.07335]\n",
      "Generated 32 batches of size 32 in 2.375 sec\n",
      "Step 538577  [5.468 sec/step, loss=0.07383, avg_loss=0.07334]\n",
      "Step 538578  [5.476 sec/step, loss=0.07574, avg_loss=0.07335]\n",
      "Step 538579  [5.471 sec/step, loss=0.07361, avg_loss=0.07334]\n",
      "Step 538580  [5.458 sec/step, loss=0.07500, avg_loss=0.07335]\n",
      "Step 538581  [5.480 sec/step, loss=0.07542, avg_loss=0.07340]\n",
      "Step 538582  [5.472 sec/step, loss=0.07369, avg_loss=0.07338]\n",
      "Step 538583  [5.416 sec/step, loss=0.06906, avg_loss=0.07342]\n",
      "Step 538584  [5.424 sec/step, loss=0.07028, avg_loss=0.07346]\n",
      "Step 538585  [5.431 sec/step, loss=0.07502, avg_loss=0.07346]\n",
      "Step 538586  [5.430 sec/step, loss=0.07573, avg_loss=0.07345]\n",
      "Step 538587  [5.410 sec/step, loss=0.07058, avg_loss=0.07341]\n",
      "Step 538588  [5.439 sec/step, loss=0.07249, avg_loss=0.07339]\n",
      "Step 538589  [5.443 sec/step, loss=0.07529, avg_loss=0.07342]\n",
      "Step 538590  [5.440 sec/step, loss=0.07508, avg_loss=0.07342]\n",
      "Step 538591  [5.439 sec/step, loss=0.07445, avg_loss=0.07341]\n",
      "Step 538592  [5.461 sec/step, loss=0.07386, avg_loss=0.07342]\n",
      "Step 538593  [5.463 sec/step, loss=0.07532, avg_loss=0.07344]\n",
      "Step 538594  [5.477 sec/step, loss=0.07543, avg_loss=0.07345]\n",
      "Step 538595  [5.486 sec/step, loss=0.07370, avg_loss=0.07346]\n",
      "Step 538596  [5.489 sec/step, loss=0.07468, avg_loss=0.07345]\n",
      "Step 538597  [5.481 sec/step, loss=0.07334, avg_loss=0.07343]\n",
      "Step 538598  [5.490 sec/step, loss=0.07346, avg_loss=0.07342]\n",
      "Step 538599  [5.518 sec/step, loss=0.07496, avg_loss=0.07345]\n",
      "Step 538600  [5.549 sec/step, loss=0.07484, avg_loss=0.07353]\n",
      "Writing summary at step: 538600\n",
      "Step 538601  [5.532 sec/step, loss=0.07350, avg_loss=0.07351]\n",
      "Step 538602  [5.533 sec/step, loss=0.07453, avg_loss=0.07351]\n",
      "Step 538603  [5.529 sec/step, loss=0.06986, avg_loss=0.07347]\n",
      "Step 538604  [5.524 sec/step, loss=0.07331, avg_loss=0.07345]\n",
      "Step 538605  [5.511 sec/step, loss=0.07127, avg_loss=0.07343]\n",
      "Step 538606  [5.495 sec/step, loss=0.07599, avg_loss=0.07344]\n",
      "Step 538607  [5.479 sec/step, loss=0.07223, avg_loss=0.07343]\n",
      "Generated 32 batches of size 32 in 2.549 sec\n",
      "Step 538608  [5.483 sec/step, loss=0.07415, avg_loss=0.07344]\n",
      "Step 538609  [5.478 sec/step, loss=0.07294, avg_loss=0.07343]\n",
      "Step 538610  [5.476 sec/step, loss=0.07460, avg_loss=0.07343]\n",
      "Step 538611  [5.486 sec/step, loss=0.07433, avg_loss=0.07344]\n",
      "Step 538612  [5.473 sec/step, loss=0.07458, avg_loss=0.07347]\n",
      "Step 538613  [5.452 sec/step, loss=0.06509, avg_loss=0.07336]\n",
      "Step 538614  [5.469 sec/step, loss=0.07542, avg_loss=0.07341]\n",
      "Step 538615  [5.461 sec/step, loss=0.07265, avg_loss=0.07343]\n",
      "Step 538616  [5.411 sec/step, loss=0.07518, avg_loss=0.07353]\n",
      "Step 538617  [5.422 sec/step, loss=0.07452, avg_loss=0.07353]\n",
      "Step 538618  [5.418 sec/step, loss=0.07285, avg_loss=0.07350]\n",
      "Step 538619  [5.424 sec/step, loss=0.07482, avg_loss=0.07354]\n",
      "Step 538620  [5.415 sec/step, loss=0.07443, avg_loss=0.07353]\n",
      "Step 538621  [5.412 sec/step, loss=0.07576, avg_loss=0.07353]\n",
      "Step 538622  [5.405 sec/step, loss=0.07316, avg_loss=0.07352]\n",
      "Step 538623  [5.428 sec/step, loss=0.07295, avg_loss=0.07351]\n",
      "Step 538624  [5.419 sec/step, loss=0.07378, avg_loss=0.07349]\n",
      "Step 538625  [5.429 sec/step, loss=0.07384, avg_loss=0.07349]\n",
      "Step 538626  [5.420 sec/step, loss=0.07461, avg_loss=0.07348]\n",
      "Step 538627  [5.410 sec/step, loss=0.07271, avg_loss=0.07346]\n",
      "Step 538628  [5.480 sec/step, loss=0.06515, avg_loss=0.07346]\n",
      "Step 538629  [5.475 sec/step, loss=0.07605, avg_loss=0.07347]\n",
      "Step 538630  [5.470 sec/step, loss=0.07150, avg_loss=0.07343]\n",
      "Step 538631  [5.479 sec/step, loss=0.07550, avg_loss=0.07344]\n",
      "Step 538632  [5.496 sec/step, loss=0.07428, avg_loss=0.07345]\n",
      "Step 538633  [5.485 sec/step, loss=0.07273, avg_loss=0.07346]\n",
      "Step 538634  [5.507 sec/step, loss=0.07569, avg_loss=0.07352]\n",
      "Step 538635  [5.498 sec/step, loss=0.07395, avg_loss=0.07354]\n",
      "Step 538636  [5.468 sec/step, loss=0.07097, avg_loss=0.07352]\n",
      "Step 538637  [5.469 sec/step, loss=0.07462, avg_loss=0.07353]\n",
      "Step 538638  [5.462 sec/step, loss=0.07474, avg_loss=0.07353]\n",
      "Step 538639  [5.448 sec/step, loss=0.07076, avg_loss=0.07349]\n",
      "Generated 32 batches of size 32 in 2.774 sec\n",
      "Step 538640  [5.432 sec/step, loss=0.06634, avg_loss=0.07341]\n",
      "Step 538641  [5.445 sec/step, loss=0.07132, avg_loss=0.07338]\n",
      "Step 538642  [5.473 sec/step, loss=0.07387, avg_loss=0.07342]\n",
      "Step 538643  [5.456 sec/step, loss=0.07527, avg_loss=0.07341]\n",
      "Step 538644  [5.466 sec/step, loss=0.07544, avg_loss=0.07346]\n",
      "Step 538645  [5.477 sec/step, loss=0.07262, avg_loss=0.07343]\n",
      "Step 538646  [5.449 sec/step, loss=0.07357, avg_loss=0.07351]\n",
      "Step 538647  [5.434 sec/step, loss=0.07584, avg_loss=0.07352]\n",
      "Step 538648  [5.427 sec/step, loss=0.07174, avg_loss=0.07351]\n",
      "Step 538649  [5.423 sec/step, loss=0.07138, avg_loss=0.07348]\n",
      "Step 538650  [5.427 sec/step, loss=0.07576, avg_loss=0.07350]\n",
      "Step 538651  [5.419 sec/step, loss=0.07467, avg_loss=0.07351]\n",
      "Step 538652  [5.411 sec/step, loss=0.07563, avg_loss=0.07351]\n",
      "Step 538653  [5.410 sec/step, loss=0.07406, avg_loss=0.07349]\n",
      "Step 538654  [5.428 sec/step, loss=0.07542, avg_loss=0.07351]\n",
      "Step 538655  [5.432 sec/step, loss=0.07560, avg_loss=0.07351]\n",
      "Step 538656  [5.441 sec/step, loss=0.07623, avg_loss=0.07351]\n",
      "Step 538657  [5.428 sec/step, loss=0.06515, avg_loss=0.07343]\n",
      "Step 538658  [5.425 sec/step, loss=0.07425, avg_loss=0.07342]\n",
      "Step 538659  [5.404 sec/step, loss=0.07355, avg_loss=0.07340]\n",
      "Step 538660  [5.398 sec/step, loss=0.07112, avg_loss=0.07337]\n",
      "Step 538661  [5.394 sec/step, loss=0.07373, avg_loss=0.07336]\n",
      "Step 538662  [5.405 sec/step, loss=0.07351, avg_loss=0.07335]\n",
      "Step 538663  [5.391 sec/step, loss=0.07370, avg_loss=0.07335]\n",
      "Step 538664  [5.377 sec/step, loss=0.07468, avg_loss=0.07337]\n",
      "Step 538665  [5.384 sec/step, loss=0.07448, avg_loss=0.07338]\n",
      "Step 538666  [5.329 sec/step, loss=0.07353, avg_loss=0.07345]\n",
      "Step 538667  [5.342 sec/step, loss=0.07518, avg_loss=0.07349]\n",
      "Step 538668  [5.320 sec/step, loss=0.07062, avg_loss=0.07344]\n",
      "Step 538669  [5.356 sec/step, loss=0.07273, avg_loss=0.07344]\n",
      "Step 538670  [5.342 sec/step, loss=0.07423, avg_loss=0.07346]\n",
      "Step 538671  [5.355 sec/step, loss=0.07269, avg_loss=0.07345]\n",
      "Generated 32 batches of size 32 in 2.528 sec\n",
      "Step 538672  [5.343 sec/step, loss=0.07234, avg_loss=0.07344]\n",
      "Step 538673  [5.348 sec/step, loss=0.07575, avg_loss=0.07345]\n",
      "Step 538674  [5.359 sec/step, loss=0.07139, avg_loss=0.07350]\n",
      "Step 538675  [5.406 sec/step, loss=0.06529, avg_loss=0.07341]\n",
      "Step 538676  [5.405 sec/step, loss=0.07488, avg_loss=0.07341]\n",
      "Step 538677  [5.395 sec/step, loss=0.07452, avg_loss=0.07342]\n",
      "Step 538678  [5.390 sec/step, loss=0.07391, avg_loss=0.07340]\n",
      "Step 538679  [5.405 sec/step, loss=0.07587, avg_loss=0.07343]\n",
      "Step 538680  [5.409 sec/step, loss=0.07380, avg_loss=0.07341]\n",
      "Step 538681  [5.391 sec/step, loss=0.07022, avg_loss=0.07336]\n",
      "Step 538682  [5.383 sec/step, loss=0.07307, avg_loss=0.07336]\n",
      "Step 538683  [5.438 sec/step, loss=0.06581, avg_loss=0.07332]\n",
      "Step 538684  [5.453 sec/step, loss=0.07594, avg_loss=0.07338]\n",
      "Step 538685  [5.449 sec/step, loss=0.07527, avg_loss=0.07338]\n",
      "Step 538686  [5.447 sec/step, loss=0.07416, avg_loss=0.07337]\n",
      "Step 538687  [5.456 sec/step, loss=0.07320, avg_loss=0.07339]\n",
      "Step 538688  [5.415 sec/step, loss=0.07125, avg_loss=0.07338]\n",
      "Step 538689  [5.414 sec/step, loss=0.07418, avg_loss=0.07337]\n",
      "Step 538690  [5.420 sec/step, loss=0.07591, avg_loss=0.07338]\n",
      "Step 538691  [5.405 sec/step, loss=0.07464, avg_loss=0.07338]\n",
      "Step 538692  [5.423 sec/step, loss=0.07260, avg_loss=0.07337]\n",
      "Step 538693  [5.418 sec/step, loss=0.07338, avg_loss=0.07335]\n",
      "Step 538694  [5.420 sec/step, loss=0.07544, avg_loss=0.07335]\n",
      "Step 538695  [5.403 sec/step, loss=0.06618, avg_loss=0.07327]\n",
      "Step 538696  [5.390 sec/step, loss=0.07414, avg_loss=0.07327]\n",
      "Step 538697  [5.391 sec/step, loss=0.07303, avg_loss=0.07326]\n",
      "Step 538698  [5.396 sec/step, loss=0.07907, avg_loss=0.07332]\n",
      "Step 538699  [5.385 sec/step, loss=0.07680, avg_loss=0.07334]\n",
      "Step 538700  [5.383 sec/step, loss=0.07621, avg_loss=0.07335]\n",
      "Writing summary at step: 538700\n",
      "Step 538701  [5.396 sec/step, loss=0.07625, avg_loss=0.07338]\n",
      "Step 538702  [5.380 sec/step, loss=0.07673, avg_loss=0.07340]\n",
      "Generated 32 batches of size 32 in 2.482 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 538703  [5.414 sec/step, loss=0.07823, avg_loss=0.07348]\n",
      "Step 538704  [5.415 sec/step, loss=0.07319, avg_loss=0.07348]\n",
      "Step 538705  [5.421 sec/step, loss=0.07633, avg_loss=0.07353]\n",
      "Step 538706  [5.420 sec/step, loss=0.07547, avg_loss=0.07353]\n",
      "Step 538707  [5.417 sec/step, loss=0.07537, avg_loss=0.07356]\n",
      "Step 538708  [5.411 sec/step, loss=0.07726, avg_loss=0.07359]\n",
      "Step 538709  [5.416 sec/step, loss=0.07535, avg_loss=0.07362]\n",
      "Step 538710  [5.423 sec/step, loss=0.07640, avg_loss=0.07363]\n",
      "Step 538711  [5.429 sec/step, loss=0.07681, avg_loss=0.07366]\n",
      "Step 538712  [5.429 sec/step, loss=0.07470, avg_loss=0.07366]\n",
      "Step 538713  [5.449 sec/step, loss=0.07562, avg_loss=0.07376]\n",
      "Step 538714  [5.449 sec/step, loss=0.07698, avg_loss=0.07378]\n",
      "Step 538715  [5.449 sec/step, loss=0.07463, avg_loss=0.07380]\n",
      "Step 538716  [5.457 sec/step, loss=0.07552, avg_loss=0.07380]\n",
      "Step 538717  [5.451 sec/step, loss=0.07566, avg_loss=0.07381]\n",
      "Step 538718  [5.454 sec/step, loss=0.07621, avg_loss=0.07385]\n",
      "Step 538719  [5.464 sec/step, loss=0.07657, avg_loss=0.07387]\n",
      "Step 538720  [5.456 sec/step, loss=0.07231, avg_loss=0.07384]\n",
      "Step 538721  [5.441 sec/step, loss=0.07450, avg_loss=0.07383]\n",
      "Step 538722  [5.453 sec/step, loss=0.07662, avg_loss=0.07387]\n",
      "Step 538723  [5.422 sec/step, loss=0.06592, avg_loss=0.07380]\n",
      "Step 538724  [5.428 sec/step, loss=0.07552, avg_loss=0.07381]\n",
      "Step 538725  [5.425 sec/step, loss=0.07564, avg_loss=0.07383]\n",
      "Step 538726  [5.434 sec/step, loss=0.07649, avg_loss=0.07385]\n",
      "Step 538727  [5.433 sec/step, loss=0.07526, avg_loss=0.07388]\n",
      "Step 538728  [5.375 sec/step, loss=0.07362, avg_loss=0.07396]\n",
      "Step 538729  [5.366 sec/step, loss=0.07227, avg_loss=0.07392]\n",
      "Step 538730  [5.366 sec/step, loss=0.07536, avg_loss=0.07396]\n",
      "Step 538731  [5.386 sec/step, loss=0.07368, avg_loss=0.07394]\n",
      "Step 538732  [5.368 sec/step, loss=0.07392, avg_loss=0.07394]\n",
      "Step 538733  [5.376 sec/step, loss=0.07590, avg_loss=0.07397]\n",
      "Step 538734  [5.373 sec/step, loss=0.07646, avg_loss=0.07398]\n",
      "Generated 32 batches of size 32 in 2.619 sec\n",
      "Step 538735  [5.439 sec/step, loss=0.06632, avg_loss=0.07390]\n",
      "Step 538736  [5.447 sec/step, loss=0.07382, avg_loss=0.07393]\n",
      "Step 538737  [5.440 sec/step, loss=0.07461, avg_loss=0.07393]\n",
      "Step 538738  [5.447 sec/step, loss=0.07374, avg_loss=0.07392]\n",
      "Step 538739  [5.482 sec/step, loss=0.07341, avg_loss=0.07395]\n",
      "Step 538740  [5.505 sec/step, loss=0.07637, avg_loss=0.07405]\n",
      "Step 538741  [5.488 sec/step, loss=0.07137, avg_loss=0.07405]\n",
      "Step 538742  [5.491 sec/step, loss=0.07627, avg_loss=0.07407]\n",
      "Step 538743  [5.488 sec/step, loss=0.07245, avg_loss=0.07404]\n",
      "Step 538744  [5.485 sec/step, loss=0.07472, avg_loss=0.07404]\n",
      "Step 538745  [5.446 sec/step, loss=0.06576, avg_loss=0.07397]\n",
      "Step 538746  [5.418 sec/step, loss=0.07388, avg_loss=0.07397]\n",
      "Step 538747  [5.432 sec/step, loss=0.07570, avg_loss=0.07397]\n",
      "Step 538748  [5.441 sec/step, loss=0.07364, avg_loss=0.07399]\n",
      "Step 538749  [5.454 sec/step, loss=0.07590, avg_loss=0.07403]\n",
      "Step 538750  [5.446 sec/step, loss=0.07614, avg_loss=0.07404]\n",
      "Step 538751  [5.494 sec/step, loss=0.06639, avg_loss=0.07396]\n",
      "Step 538752  [5.485 sec/step, loss=0.07250, avg_loss=0.07392]\n",
      "Step 538753  [5.487 sec/step, loss=0.07427, avg_loss=0.07393]\n",
      "Step 538754  [5.477 sec/step, loss=0.07589, avg_loss=0.07393]\n",
      "Step 538755  [5.456 sec/step, loss=0.07503, avg_loss=0.07393]\n",
      "Step 538756  [5.458 sec/step, loss=0.07591, avg_loss=0.07392]\n",
      "Step 538757  [5.475 sec/step, loss=0.07369, avg_loss=0.07401]\n",
      "Step 538758  [5.483 sec/step, loss=0.07586, avg_loss=0.07402]\n",
      "Step 538759  [5.512 sec/step, loss=0.07571, avg_loss=0.07405]\n",
      "Step 538760  [5.515 sec/step, loss=0.07386, avg_loss=0.07407]\n",
      "Step 538761  [5.520 sec/step, loss=0.07096, avg_loss=0.07404]\n",
      "Step 538762  [5.503 sec/step, loss=0.07416, avg_loss=0.07405]\n",
      "Step 538763  [5.495 sec/step, loss=0.07476, avg_loss=0.07406]\n",
      "Step 538764  [5.505 sec/step, loss=0.07606, avg_loss=0.07408]\n",
      "Step 538765  [5.504 sec/step, loss=0.07448, avg_loss=0.07408]\n",
      "Step 538766  [5.512 sec/step, loss=0.07382, avg_loss=0.07408]\n",
      "Generated 32 batches of size 32 in 2.462 sec\n",
      "Step 538767  [5.519 sec/step, loss=0.07281, avg_loss=0.07406]\n",
      "Step 538768  [5.516 sec/step, loss=0.07119, avg_loss=0.07406]\n",
      "Step 538769  [5.502 sec/step, loss=0.07567, avg_loss=0.07409]\n",
      "Step 538770  [5.498 sec/step, loss=0.07460, avg_loss=0.07409]\n",
      "Step 538771  [5.513 sec/step, loss=0.07454, avg_loss=0.07411]\n",
      "Step 538772  [5.519 sec/step, loss=0.07397, avg_loss=0.07413]\n",
      "Step 538773  [5.498 sec/step, loss=0.06967, avg_loss=0.07407]\n",
      "Step 538774  [5.515 sec/step, loss=0.07366, avg_loss=0.07409]\n",
      "Step 538775  [5.461 sec/step, loss=0.07534, avg_loss=0.07419]\n",
      "Step 538776  [5.474 sec/step, loss=0.07561, avg_loss=0.07420]\n",
      "Step 538777  [5.474 sec/step, loss=0.07374, avg_loss=0.07419]\n",
      "Step 538778  [5.468 sec/step, loss=0.07510, avg_loss=0.07420]\n",
      "Step 538779  [5.438 sec/step, loss=0.07185, avg_loss=0.07416]\n",
      "Step 538780  [5.438 sec/step, loss=0.07412, avg_loss=0.07417]\n",
      "Step 538781  [5.458 sec/step, loss=0.07589, avg_loss=0.07422]\n",
      "Step 538782  [5.458 sec/step, loss=0.07201, avg_loss=0.07421]\n",
      "Step 538783  [5.420 sec/step, loss=0.07394, avg_loss=0.07429]\n",
      "Step 538784  [5.404 sec/step, loss=0.07124, avg_loss=0.07425]\n",
      "Step 538785  [5.386 sec/step, loss=0.07394, avg_loss=0.07423]\n",
      "Step 538786  [5.377 sec/step, loss=0.07478, avg_loss=0.07424]\n",
      "Step 538787  [5.380 sec/step, loss=0.07079, avg_loss=0.07421]\n",
      "Step 538788  [5.405 sec/step, loss=0.07498, avg_loss=0.07425]\n",
      "Step 538789  [5.398 sec/step, loss=0.07407, avg_loss=0.07425]\n",
      "Step 538790  [5.391 sec/step, loss=0.07523, avg_loss=0.07424]\n",
      "Step 538791  [5.397 sec/step, loss=0.07598, avg_loss=0.07426]\n",
      "Step 538792  [5.372 sec/step, loss=0.07557, avg_loss=0.07429]\n",
      "Step 538793  [5.372 sec/step, loss=0.07419, avg_loss=0.07430]\n",
      "Step 538794  [5.354 sec/step, loss=0.07379, avg_loss=0.07428]\n",
      "Step 538795  [5.374 sec/step, loss=0.07411, avg_loss=0.07436]\n",
      "Step 538796  [5.391 sec/step, loss=0.07444, avg_loss=0.07436]\n",
      "Step 538797  [5.414 sec/step, loss=0.07582, avg_loss=0.07439]\n",
      "Step 538798  [5.450 sec/step, loss=0.06607, avg_loss=0.07426]\n",
      "Generated 32 batches of size 32 in 2.530 sec\n",
      "Step 538799  [5.437 sec/step, loss=0.07386, avg_loss=0.07423]\n",
      "Step 538800  [5.409 sec/step, loss=0.06667, avg_loss=0.07413]\n",
      "Writing summary at step: 538800\n",
      "Step 538801  [5.423 sec/step, loss=0.07542, avg_loss=0.07413]\n",
      "Step 538802  [5.436 sec/step, loss=0.07584, avg_loss=0.07412]\n",
      "Step 538803  [5.417 sec/step, loss=0.07476, avg_loss=0.07408]\n",
      "Step 538804  [5.426 sec/step, loss=0.07385, avg_loss=0.07409]\n",
      "Step 538805  [5.425 sec/step, loss=0.07422, avg_loss=0.07407]\n",
      "Step 538806  [5.436 sec/step, loss=0.07398, avg_loss=0.07405]\n",
      "Step 538807  [5.447 sec/step, loss=0.07149, avg_loss=0.07401]\n",
      "Step 538808  [5.444 sec/step, loss=0.07459, avg_loss=0.07399]\n",
      "Step 538809  [5.419 sec/step, loss=0.07006, avg_loss=0.07393]\n",
      "Step 538810  [5.397 sec/step, loss=0.06481, avg_loss=0.07382]\n",
      "Step 538811  [5.379 sec/step, loss=0.07313, avg_loss=0.07378]\n",
      "Step 538812  [5.392 sec/step, loss=0.07508, avg_loss=0.07379]\n",
      "Step 538813  [5.383 sec/step, loss=0.07255, avg_loss=0.07375]\n",
      "Step 538814  [5.383 sec/step, loss=0.07606, avg_loss=0.07375]\n",
      "Step 538815  [5.403 sec/step, loss=0.07566, avg_loss=0.07376]\n",
      "Step 538816  [5.399 sec/step, loss=0.07493, avg_loss=0.07375]\n",
      "Step 538817  [5.396 sec/step, loss=0.07169, avg_loss=0.07371]\n",
      "Step 538818  [5.386 sec/step, loss=0.07329, avg_loss=0.07368]\n",
      "Step 538819  [5.389 sec/step, loss=0.07611, avg_loss=0.07368]\n",
      "Step 538820  [5.404 sec/step, loss=0.07454, avg_loss=0.07370]\n",
      "Step 538821  [5.416 sec/step, loss=0.07585, avg_loss=0.07371]\n",
      "Step 538822  [5.420 sec/step, loss=0.07561, avg_loss=0.07370]\n",
      "Step 538823  [5.446 sec/step, loss=0.07310, avg_loss=0.07377]\n",
      "Step 538824  [5.448 sec/step, loss=0.07420, avg_loss=0.07376]\n",
      "Step 538825  [5.447 sec/step, loss=0.07540, avg_loss=0.07376]\n",
      "Step 538826  [5.461 sec/step, loss=0.07483, avg_loss=0.07374]\n",
      "Step 538827  [5.460 sec/step, loss=0.07475, avg_loss=0.07374]\n",
      "Step 538828  [5.472 sec/step, loss=0.07474, avg_loss=0.07375]\n",
      "Step 538829  [5.522 sec/step, loss=0.06564, avg_loss=0.07368]\n",
      "Generated 32 batches of size 32 in 2.491 sec\n",
      "Step 538830  [5.519 sec/step, loss=0.07403, avg_loss=0.07367]\n",
      "Step 538831  [5.489 sec/step, loss=0.07434, avg_loss=0.07368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 538832  [5.493 sec/step, loss=0.07399, avg_loss=0.07368]\n",
      "Step 538833  [5.508 sec/step, loss=0.07531, avg_loss=0.07367]\n",
      "Step 538834  [5.508 sec/step, loss=0.07420, avg_loss=0.07365]\n",
      "Step 538835  [5.464 sec/step, loss=0.07606, avg_loss=0.07374]\n",
      "Step 538836  [5.448 sec/step, loss=0.07037, avg_loss=0.07371]\n",
      "Step 538837  [5.461 sec/step, loss=0.07518, avg_loss=0.07372]\n",
      "Step 538838  [5.449 sec/step, loss=0.07127, avg_loss=0.07369]\n",
      "Step 538839  [5.435 sec/step, loss=0.07488, avg_loss=0.07371]\n",
      "Step 538840  [5.421 sec/step, loss=0.07482, avg_loss=0.07369]\n",
      "Step 538841  [5.421 sec/step, loss=0.07186, avg_loss=0.07370]\n",
      "Step 538842  [5.408 sec/step, loss=0.07534, avg_loss=0.07369]\n",
      "Step 538843  [5.407 sec/step, loss=0.07096, avg_loss=0.07367]\n",
      "Step 538844  [5.396 sec/step, loss=0.07359, avg_loss=0.07366]\n",
      "Step 538845  [5.417 sec/step, loss=0.07533, avg_loss=0.07376]\n",
      "Step 538846  [5.417 sec/step, loss=0.07164, avg_loss=0.07373]\n",
      "Step 538847  [5.398 sec/step, loss=0.07490, avg_loss=0.07373]\n",
      "Step 538848  [5.414 sec/step, loss=0.07536, avg_loss=0.07374]\n",
      "Step 538849  [5.391 sec/step, loss=0.07134, avg_loss=0.07370]\n",
      "Step 538850  [5.374 sec/step, loss=0.06677, avg_loss=0.07360]\n",
      "Step 538851  [5.336 sec/step, loss=0.07583, avg_loss=0.07370]\n",
      "Step 538852  [5.357 sec/step, loss=0.07561, avg_loss=0.07373]\n",
      "Step 538853  [5.369 sec/step, loss=0.07310, avg_loss=0.07372]\n",
      "Step 538854  [5.365 sec/step, loss=0.07488, avg_loss=0.07371]\n",
      "Step 538855  [5.366 sec/step, loss=0.07276, avg_loss=0.07368]\n",
      "Step 538856  [5.404 sec/step, loss=0.06589, avg_loss=0.07358]\n",
      "Step 538857  [5.430 sec/step, loss=0.07295, avg_loss=0.07358]\n",
      "Step 538858  [5.431 sec/step, loss=0.07376, avg_loss=0.07356]\n",
      "Step 538859  [5.420 sec/step, loss=0.07544, avg_loss=0.07355]\n",
      "Step 538860  [5.419 sec/step, loss=0.07392, avg_loss=0.07355]\n",
      "Step 538861  [5.426 sec/step, loss=0.07499, avg_loss=0.07359]\n",
      "Generated 32 batches of size 32 in 2.539 sec\n",
      "Step 538862  [5.438 sec/step, loss=0.07385, avg_loss=0.07359]\n",
      "Step 538863  [5.445 sec/step, loss=0.07545, avg_loss=0.07360]\n",
      "Step 538864  [5.440 sec/step, loss=0.07171, avg_loss=0.07355]\n",
      "Step 538865  [5.431 sec/step, loss=0.07349, avg_loss=0.07354]\n",
      "Step 538866  [5.436 sec/step, loss=0.07585, avg_loss=0.07356]\n",
      "Step 538867  [5.423 sec/step, loss=0.07276, avg_loss=0.07356]\n",
      "Step 538868  [5.442 sec/step, loss=0.07453, avg_loss=0.07360]\n",
      "Step 538869  [5.432 sec/step, loss=0.07427, avg_loss=0.07358]\n",
      "Step 538870  [5.435 sec/step, loss=0.07319, avg_loss=0.07357]\n",
      "Step 538871  [5.411 sec/step, loss=0.07473, avg_loss=0.07357]\n",
      "Step 538872  [5.413 sec/step, loss=0.07505, avg_loss=0.07358]\n",
      "Step 538873  [5.436 sec/step, loss=0.07335, avg_loss=0.07362]\n",
      "Step 538874  [5.429 sec/step, loss=0.07457, avg_loss=0.07363]\n",
      "Step 538875  [5.439 sec/step, loss=0.07603, avg_loss=0.07363]\n",
      "Step 538876  [5.421 sec/step, loss=0.07455, avg_loss=0.07362]\n",
      "Step 538877  [5.429 sec/step, loss=0.07285, avg_loss=0.07362]\n",
      "Step 538878  [5.448 sec/step, loss=0.07523, avg_loss=0.07362]\n",
      "Step 538879  [5.459 sec/step, loss=0.07256, avg_loss=0.07362]\n",
      "Step 538880  [5.460 sec/step, loss=0.07519, avg_loss=0.07363]\n",
      "Step 538881  [5.455 sec/step, loss=0.07535, avg_loss=0.07363]\n",
      "Step 538882  [5.473 sec/step, loss=0.07624, avg_loss=0.07367]\n",
      "Step 538883  [5.461 sec/step, loss=0.07368, avg_loss=0.07367]\n",
      "Step 538884  [5.483 sec/step, loss=0.07593, avg_loss=0.07372]\n",
      "Step 538885  [5.505 sec/step, loss=0.07585, avg_loss=0.07373]\n",
      "Step 538886  [5.494 sec/step, loss=0.07153, avg_loss=0.07370]\n",
      "Step 538887  [5.494 sec/step, loss=0.07468, avg_loss=0.07374]\n",
      "Step 538888  [5.475 sec/step, loss=0.07337, avg_loss=0.07373]\n",
      "Step 538889  [5.491 sec/step, loss=0.07510, avg_loss=0.07374]\n",
      "Step 538890  [5.496 sec/step, loss=0.07188, avg_loss=0.07370]\n",
      "Step 538891  [5.492 sec/step, loss=0.07200, avg_loss=0.07366]\n",
      "Step 538892  [5.479 sec/step, loss=0.07051, avg_loss=0.07361]\n",
      "Step 538893  [5.475 sec/step, loss=0.07302, avg_loss=0.07360]\n",
      "Generated 32 batches of size 32 in 2.534 sec\n",
      "Step 538894  [5.481 sec/step, loss=0.07354, avg_loss=0.07360]\n",
      "Step 538895  [5.468 sec/step, loss=0.07037, avg_loss=0.07356]\n",
      "Step 538896  [5.471 sec/step, loss=0.07398, avg_loss=0.07356]\n",
      "Step 538897  [5.485 sec/step, loss=0.07283, avg_loss=0.07353]\n",
      "Step 538898  [5.430 sec/step, loss=0.06965, avg_loss=0.07356]\n",
      "Step 538899  [5.435 sec/step, loss=0.07414, avg_loss=0.07356]\n",
      "Step 538900  [5.501 sec/step, loss=0.06664, avg_loss=0.07356]\n",
      "Writing summary at step: 538900\n",
      "Step 538901  [5.467 sec/step, loss=0.06537, avg_loss=0.07346]\n",
      "Step 538902  [5.459 sec/step, loss=0.07509, avg_loss=0.07346]\n",
      "Step 538903  [5.457 sec/step, loss=0.07305, avg_loss=0.07344]\n",
      "Step 538904  [5.457 sec/step, loss=0.07589, avg_loss=0.07346]\n",
      "Step 538905  [5.465 sec/step, loss=0.07327, avg_loss=0.07345]\n",
      "Step 538906  [5.504 sec/step, loss=0.06579, avg_loss=0.07337]\n",
      "Step 538907  [5.517 sec/step, loss=0.07492, avg_loss=0.07340]\n",
      "Step 538908  [5.512 sec/step, loss=0.07325, avg_loss=0.07339]\n",
      "Step 538909  [5.529 sec/step, loss=0.07543, avg_loss=0.07344]\n",
      "Step 538910  [5.542 sec/step, loss=0.07096, avg_loss=0.07350]\n",
      "Step 538911  [5.562 sec/step, loss=0.07535, avg_loss=0.07353]\n",
      "Step 538912  [5.548 sec/step, loss=0.07233, avg_loss=0.07350]\n",
      "Step 538913  [5.546 sec/step, loss=0.07335, avg_loss=0.07351]\n",
      "Step 538914  [5.540 sec/step, loss=0.07418, avg_loss=0.07349]\n",
      "Step 538915  [5.516 sec/step, loss=0.07355, avg_loss=0.07347]\n",
      "Step 538916  [5.512 sec/step, loss=0.07352, avg_loss=0.07345]\n",
      "Step 538917  [5.507 sec/step, loss=0.07371, avg_loss=0.07347]\n",
      "Step 538918  [5.523 sec/step, loss=0.07337, avg_loss=0.07347]\n",
      "Step 538919  [5.496 sec/step, loss=0.07075, avg_loss=0.07342]\n",
      "Step 538920  [5.488 sec/step, loss=0.07459, avg_loss=0.07342]\n",
      "Step 538921  [5.490 sec/step, loss=0.07315, avg_loss=0.07339]\n",
      "Step 538922  [5.483 sec/step, loss=0.07421, avg_loss=0.07338]\n",
      "Step 538923  [5.491 sec/step, loss=0.07286, avg_loss=0.07338]\n",
      "Step 538924  [5.490 sec/step, loss=0.07474, avg_loss=0.07338]\n",
      "Generated 32 batches of size 32 in 2.409 sec\n",
      "Step 538925  [5.502 sec/step, loss=0.07579, avg_loss=0.07339]\n",
      "Step 538926  [5.478 sec/step, loss=0.07361, avg_loss=0.07337]\n",
      "Step 538927  [5.492 sec/step, loss=0.07559, avg_loss=0.07338]\n",
      "Step 538928  [5.515 sec/step, loss=0.07306, avg_loss=0.07337]\n",
      "Step 538929  [5.470 sec/step, loss=0.07477, avg_loss=0.07346]\n",
      "Step 538930  [5.488 sec/step, loss=0.07615, avg_loss=0.07348]\n",
      "Step 538931  [5.473 sec/step, loss=0.06656, avg_loss=0.07340]\n",
      "Step 538932  [5.474 sec/step, loss=0.07490, avg_loss=0.07341]\n",
      "Step 538933  [5.445 sec/step, loss=0.07193, avg_loss=0.07338]\n",
      "Step 538934  [5.439 sec/step, loss=0.07179, avg_loss=0.07335]\n",
      "Step 538935  [5.416 sec/step, loss=0.07167, avg_loss=0.07331]\n",
      "Step 538936  [5.434 sec/step, loss=0.07552, avg_loss=0.07336]\n",
      "Step 538937  [5.435 sec/step, loss=0.07420, avg_loss=0.07335]\n",
      "Step 538938  [5.461 sec/step, loss=0.07430, avg_loss=0.07338]\n",
      "Step 538939  [5.439 sec/step, loss=0.07369, avg_loss=0.07337]\n",
      "Step 538940  [5.459 sec/step, loss=0.07505, avg_loss=0.07337]\n",
      "Step 538941  [5.483 sec/step, loss=0.07589, avg_loss=0.07341]\n",
      "Step 538942  [5.490 sec/step, loss=0.07576, avg_loss=0.07341]\n",
      "Step 538943  [5.497 sec/step, loss=0.07433, avg_loss=0.07345]\n",
      "Step 538944  [5.518 sec/step, loss=0.07523, avg_loss=0.07346]\n",
      "Step 538945  [5.509 sec/step, loss=0.07142, avg_loss=0.07343]\n",
      "Step 538946  [5.506 sec/step, loss=0.07355, avg_loss=0.07344]\n",
      "Step 538947  [5.512 sec/step, loss=0.07112, avg_loss=0.07341]\n",
      "Step 538948  [5.512 sec/step, loss=0.07272, avg_loss=0.07338]\n",
      "Step 538949  [5.524 sec/step, loss=0.07346, avg_loss=0.07340]\n",
      "Step 538950  [5.540 sec/step, loss=0.07404, avg_loss=0.07347]\n",
      "Step 538951  [5.530 sec/step, loss=0.07465, avg_loss=0.07346]\n",
      "Step 538952  [5.522 sec/step, loss=0.07460, avg_loss=0.07345]\n",
      "Step 538953  [5.500 sec/step, loss=0.07468, avg_loss=0.07347]\n",
      "Step 538954  [5.486 sec/step, loss=0.07147, avg_loss=0.07343]\n",
      "Step 538955  [5.484 sec/step, loss=0.07463, avg_loss=0.07345]\n",
      "Step 538956  [5.448 sec/step, loss=0.07491, avg_loss=0.07354]\n",
      "Generated 32 batches of size 32 in 2.384 sec\n",
      "Step 538957  [5.435 sec/step, loss=0.07344, avg_loss=0.07355]\n",
      "Step 538958  [5.431 sec/step, loss=0.07469, avg_loss=0.07356]\n",
      "Step 538959  [5.420 sec/step, loss=0.07248, avg_loss=0.07353]\n",
      "Step 538960  [5.441 sec/step, loss=0.07594, avg_loss=0.07355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 538961  [5.429 sec/step, loss=0.07296, avg_loss=0.07353]\n",
      "Step 538962  [5.407 sec/step, loss=0.06569, avg_loss=0.07345]\n",
      "Step 538963  [5.446 sec/step, loss=0.06505, avg_loss=0.07334]\n",
      "Step 538964  [5.456 sec/step, loss=0.07578, avg_loss=0.07338]\n",
      "Step 538965  [5.456 sec/step, loss=0.07309, avg_loss=0.07338]\n",
      "Step 538966  [5.459 sec/step, loss=0.07540, avg_loss=0.07337]\n",
      "Step 538967  [5.465 sec/step, loss=0.07324, avg_loss=0.07338]\n",
      "Step 538968  [5.446 sec/step, loss=0.07172, avg_loss=0.07335]\n",
      "Step 538969  [5.458 sec/step, loss=0.07377, avg_loss=0.07335]\n",
      "Step 538970  [5.457 sec/step, loss=0.07429, avg_loss=0.07336]\n",
      "Step 538971  [5.448 sec/step, loss=0.07284, avg_loss=0.07334]\n",
      "Step 538972  [5.427 sec/step, loss=0.06532, avg_loss=0.07324]\n",
      "Step 538973  [5.465 sec/step, loss=0.06708, avg_loss=0.07318]\n",
      "Step 538974  [5.461 sec/step, loss=0.07408, avg_loss=0.07317]\n",
      "Step 538975  [5.464 sec/step, loss=0.07329, avg_loss=0.07315]\n",
      "Step 538976  [5.482 sec/step, loss=0.07679, avg_loss=0.07317]\n",
      "Step 538977  [5.463 sec/step, loss=0.07018, avg_loss=0.07314]\n",
      "Step 538978  [5.435 sec/step, loss=0.07380, avg_loss=0.07313]\n",
      "Step 538979  [5.438 sec/step, loss=0.07401, avg_loss=0.07314]\n",
      "Step 538980  [5.443 sec/step, loss=0.07428, avg_loss=0.07313]\n",
      "Step 538981  [5.432 sec/step, loss=0.07379, avg_loss=0.07312]\n",
      "Step 538982  [5.427 sec/step, loss=0.07544, avg_loss=0.07311]\n",
      "Step 538983  [5.429 sec/step, loss=0.07532, avg_loss=0.07313]\n",
      "Step 538984  [5.419 sec/step, loss=0.07498, avg_loss=0.07312]\n",
      "Step 538985  [5.416 sec/step, loss=0.07614, avg_loss=0.07312]\n",
      "Step 538986  [5.438 sec/step, loss=0.07558, avg_loss=0.07316]\n",
      "Step 538987  [5.438 sec/step, loss=0.07053, avg_loss=0.07312]\n",
      "Step 538988  [5.441 sec/step, loss=0.07448, avg_loss=0.07313]\n",
      "Generated 32 batches of size 32 in 2.399 sec\n",
      "Step 538989  [5.443 sec/step, loss=0.07378, avg_loss=0.07312]\n",
      "Step 538990  [5.447 sec/step, loss=0.07408, avg_loss=0.07314]\n",
      "Step 538991  [5.441 sec/step, loss=0.07191, avg_loss=0.07314]\n",
      "Step 538992  [5.445 sec/step, loss=0.07330, avg_loss=0.07316]\n",
      "Step 538993  [5.445 sec/step, loss=0.07470, avg_loss=0.07318]\n",
      "Step 538994  [5.470 sec/step, loss=0.07270, avg_loss=0.07317]\n",
      "Step 538995  [5.487 sec/step, loss=0.07505, avg_loss=0.07322]\n",
      "Step 538996  [5.495 sec/step, loss=0.07320, avg_loss=0.07321]\n",
      "Step 538997  [5.470 sec/step, loss=0.07504, avg_loss=0.07323]\n",
      "Step 538998  [5.487 sec/step, loss=0.07567, avg_loss=0.07329]\n",
      "Step 538999  [5.494 sec/step, loss=0.07580, avg_loss=0.07331]\n",
      "Step 539000  [5.448 sec/step, loss=0.07487, avg_loss=0.07339]\n",
      "Writing summary at step: 539000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-539000\n",
      "Saving audio and alignment...\n",
      "Input: tdaehhziibii tdashbiihaatd mudddduan bajaan karnay kaa dzohrii natdiidzaa hae~______________________________________________________\n",
      "Step 539001  [5.468 sec/step, loss=0.07173, avg_loss=0.07346]\n",
      "Step 539002  [5.473 sec/step, loss=0.07527, avg_loss=0.07346]\n",
      "Step 539003  [5.471 sec/step, loss=0.07432, avg_loss=0.07347]\n",
      "Step 539004  [5.473 sec/step, loss=0.07528, avg_loss=0.07346]\n",
      "Step 539005  [5.466 sec/step, loss=0.07471, avg_loss=0.07348]\n",
      "Step 539006  [5.408 sec/step, loss=0.07322, avg_loss=0.07355]\n",
      "Step 539007  [5.407 sec/step, loss=0.07501, avg_loss=0.07355]\n",
      "Step 539008  [5.426 sec/step, loss=0.07541, avg_loss=0.07358]\n",
      "Step 539009  [5.422 sec/step, loss=0.07391, avg_loss=0.07356]\n",
      "Step 539010  [5.438 sec/step, loss=0.07584, avg_loss=0.07361]\n",
      "Step 539011  [5.415 sec/step, loss=0.07041, avg_loss=0.07356]\n",
      "Step 539012  [5.414 sec/step, loss=0.07359, avg_loss=0.07357]\n",
      "Step 539013  [5.470 sec/step, loss=0.06583, avg_loss=0.07350]\n",
      "Step 539014  [5.475 sec/step, loss=0.07364, avg_loss=0.07349]\n",
      "Step 539015  [5.467 sec/step, loss=0.06609, avg_loss=0.07342]\n",
      "Step 539016  [5.479 sec/step, loss=0.07526, avg_loss=0.07344]\n",
      "Step 539017  [5.478 sec/step, loss=0.07281, avg_loss=0.07343]\n",
      "Step 539018  [5.471 sec/step, loss=0.07438, avg_loss=0.07344]\n",
      "Generated 32 batches of size 32 in 2.406 sec\n",
      "Step 539019  [5.512 sec/step, loss=0.07267, avg_loss=0.07346]\n",
      "Step 539020  [5.513 sec/step, loss=0.07312, avg_loss=0.07344]\n",
      "Step 539021  [5.513 sec/step, loss=0.07288, avg_loss=0.07344]\n",
      "Step 539022  [5.498 sec/step, loss=0.07149, avg_loss=0.07341]\n",
      "Step 539023  [5.466 sec/step, loss=0.07197, avg_loss=0.07340]\n",
      "Step 539024  [5.466 sec/step, loss=0.07490, avg_loss=0.07340]\n",
      "Step 539025  [5.453 sec/step, loss=0.07398, avg_loss=0.07339]\n",
      "Step 539026  [5.448 sec/step, loss=0.07457, avg_loss=0.07339]\n",
      "Step 539027  [5.437 sec/step, loss=0.07195, avg_loss=0.07336]\n",
      "Step 539028  [5.406 sec/step, loss=0.07307, avg_loss=0.07336]\n",
      "Step 539029  [5.400 sec/step, loss=0.07271, avg_loss=0.07334]\n",
      "Step 539030  [5.383 sec/step, loss=0.07083, avg_loss=0.07328]\n",
      "Step 539031  [5.412 sec/step, loss=0.07549, avg_loss=0.07337]\n",
      "Step 539032  [5.413 sec/step, loss=0.07491, avg_loss=0.07337]\n",
      "Step 539033  [5.419 sec/step, loss=0.07112, avg_loss=0.07337]\n",
      "Step 539034  [5.434 sec/step, loss=0.07473, avg_loss=0.07340]\n",
      "Step 539035  [5.440 sec/step, loss=0.07331, avg_loss=0.07341]\n",
      "Step 539036  [5.432 sec/step, loss=0.07396, avg_loss=0.07340]\n",
      "Step 539037  [5.430 sec/step, loss=0.07456, avg_loss=0.07340]\n",
      "Step 539038  [5.392 sec/step, loss=0.06952, avg_loss=0.07335]\n",
      "Step 539039  [5.412 sec/step, loss=0.07570, avg_loss=0.07337]\n",
      "Step 539040  [5.399 sec/step, loss=0.07385, avg_loss=0.07336]\n",
      "Step 539041  [5.385 sec/step, loss=0.07449, avg_loss=0.07335]\n",
      "Step 539042  [5.384 sec/step, loss=0.07474, avg_loss=0.07334]\n",
      "Step 539043  [5.378 sec/step, loss=0.07411, avg_loss=0.07333]\n",
      "Step 539044  [5.366 sec/step, loss=0.07478, avg_loss=0.07333]\n",
      "Step 539045  [5.417 sec/step, loss=0.06554, avg_loss=0.07327]\n",
      "Step 539046  [5.432 sec/step, loss=0.07576, avg_loss=0.07329]\n",
      "Step 539047  [5.427 sec/step, loss=0.07369, avg_loss=0.07332]\n",
      "Step 539048  [5.400 sec/step, loss=0.06468, avg_loss=0.07324]\n",
      "Step 539049  [5.391 sec/step, loss=0.07348, avg_loss=0.07324]\n",
      "Step 539050  [5.402 sec/step, loss=0.07532, avg_loss=0.07325]\n",
      "Generated 32 batches of size 32 in 2.353 sec\n",
      "Step 539051  [5.416 sec/step, loss=0.07574, avg_loss=0.07326]\n",
      "Step 539052  [5.412 sec/step, loss=0.07476, avg_loss=0.07326]\n",
      "Step 539053  [5.426 sec/step, loss=0.07367, avg_loss=0.07325]\n",
      "Step 539054  [5.453 sec/step, loss=0.07480, avg_loss=0.07329]\n",
      "Step 539055  [5.445 sec/step, loss=0.06900, avg_loss=0.07323]\n",
      "Step 539056  [5.434 sec/step, loss=0.07361, avg_loss=0.07322]\n",
      "Step 539057  [5.448 sec/step, loss=0.07222, avg_loss=0.07321]\n",
      "Step 539058  [5.434 sec/step, loss=0.07392, avg_loss=0.07320]\n",
      "Step 539059  [5.447 sec/step, loss=0.07460, avg_loss=0.07322]\n",
      "Step 539060  [5.487 sec/step, loss=0.06573, avg_loss=0.07312]\n",
      "Step 539061  [5.489 sec/step, loss=0.07299, avg_loss=0.07312]\n",
      "Step 539062  [5.512 sec/step, loss=0.07331, avg_loss=0.07319]\n",
      "Step 539063  [5.471 sec/step, loss=0.07492, avg_loss=0.07329]\n",
      "Step 539064  [5.466 sec/step, loss=0.07556, avg_loss=0.07329]\n",
      "Step 539065  [5.481 sec/step, loss=0.07540, avg_loss=0.07331]\n",
      "Step 539066  [5.474 sec/step, loss=0.07382, avg_loss=0.07330]\n",
      "Step 539067  [5.475 sec/step, loss=0.07388, avg_loss=0.07330]\n",
      "Step 539068  [5.491 sec/step, loss=0.07430, avg_loss=0.07333]\n",
      "Step 539069  [5.470 sec/step, loss=0.07289, avg_loss=0.07332]\n",
      "Step 539070  [5.477 sec/step, loss=0.07565, avg_loss=0.07333]\n",
      "Step 539071  [5.510 sec/step, loss=0.07237, avg_loss=0.07333]\n",
      "Step 539072  [5.524 sec/step, loss=0.07425, avg_loss=0.07342]\n",
      "Step 539073  [5.480 sec/step, loss=0.07266, avg_loss=0.07347]\n",
      "Step 539074  [5.470 sec/step, loss=0.07112, avg_loss=0.07344]\n",
      "Step 539075  [5.452 sec/step, loss=0.07247, avg_loss=0.07344]\n",
      "Step 539076  [5.437 sec/step, loss=0.07216, avg_loss=0.07339]\n",
      "Step 539077  [5.444 sec/step, loss=0.07222, avg_loss=0.07341]\n",
      "Step 539078  [5.449 sec/step, loss=0.07335, avg_loss=0.07341]\n",
      "Step 539079  [5.457 sec/step, loss=0.07522, avg_loss=0.07342]\n",
      "Step 539080  [5.464 sec/step, loss=0.07553, avg_loss=0.07343]\n",
      "Step 539081  [5.472 sec/step, loss=0.07419, avg_loss=0.07343]\n",
      "Step 539082  [5.448 sec/step, loss=0.07226, avg_loss=0.07340]\n",
      "Generated 32 batches of size 32 in 2.360 sec\n",
      "Step 539083  [5.464 sec/step, loss=0.07470, avg_loss=0.07340]\n",
      "Step 539084  [5.462 sec/step, loss=0.07433, avg_loss=0.07339]\n",
      "Step 539085  [5.450 sec/step, loss=0.07447, avg_loss=0.07337]\n",
      "Step 539086  [5.426 sec/step, loss=0.06594, avg_loss=0.07328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 539087  [5.446 sec/step, loss=0.07485, avg_loss=0.07332]\n",
      "Step 539088  [5.440 sec/step, loss=0.07367, avg_loss=0.07331]\n",
      "Step 539089  [5.427 sec/step, loss=0.07495, avg_loss=0.07332]\n",
      "Step 539090  [5.419 sec/step, loss=0.07520, avg_loss=0.07334]\n",
      "Step 539091  [5.428 sec/step, loss=0.07480, avg_loss=0.07336]\n",
      "Step 539092  [5.431 sec/step, loss=0.07509, avg_loss=0.07338]\n",
      "Step 539093  [5.420 sec/step, loss=0.07075, avg_loss=0.07334]\n",
      "Step 539094  [5.406 sec/step, loss=0.07346, avg_loss=0.07335]\n",
      "Step 539095  [5.394 sec/step, loss=0.07284, avg_loss=0.07333]\n",
      "Step 539096  [5.382 sec/step, loss=0.07416, avg_loss=0.07334]\n",
      "Step 539097  [5.380 sec/step, loss=0.07214, avg_loss=0.07331]\n",
      "Step 539098  [5.391 sec/step, loss=0.07498, avg_loss=0.07330]\n",
      "Step 539099  [5.396 sec/step, loss=0.07373, avg_loss=0.07328]\n",
      "Step 539100  [5.401 sec/step, loss=0.07573, avg_loss=0.07329]\n",
      "Writing summary at step: 539100\n",
      "Step 539101  [5.394 sec/step, loss=0.07458, avg_loss=0.07332]\n",
      "Step 539102  [5.391 sec/step, loss=0.07447, avg_loss=0.07331]\n",
      "Step 539103  [5.392 sec/step, loss=0.07479, avg_loss=0.07331]\n",
      "Step 539104  [5.377 sec/step, loss=0.07386, avg_loss=0.07330]\n",
      "Step 539105  [5.385 sec/step, loss=0.07605, avg_loss=0.07331]\n",
      "Step 539106  [5.385 sec/step, loss=0.07324, avg_loss=0.07331]\n",
      "Step 539107  [5.380 sec/step, loss=0.07557, avg_loss=0.07332]\n",
      "Step 539108  [5.368 sec/step, loss=0.07414, avg_loss=0.07331]\n",
      "Step 539109  [5.386 sec/step, loss=0.07508, avg_loss=0.07332]\n",
      "Step 539110  [5.379 sec/step, loss=0.07158, avg_loss=0.07328]\n",
      "Step 539111  [5.395 sec/step, loss=0.07560, avg_loss=0.07333]\n",
      "Step 539112  [5.391 sec/step, loss=0.07023, avg_loss=0.07329]\n",
      "Step 539113  [5.391 sec/step, loss=0.06583, avg_loss=0.07329]\n",
      "Generated 32 batches of size 32 in 2.409 sec\n",
      "Step 539114  [5.401 sec/step, loss=0.07516, avg_loss=0.07331]\n",
      "Step 539115  [5.423 sec/step, loss=0.07431, avg_loss=0.07339]\n",
      "Step 539116  [5.411 sec/step, loss=0.07369, avg_loss=0.07338]\n",
      "Step 539117  [5.399 sec/step, loss=0.06504, avg_loss=0.07330]\n",
      "Step 539118  [5.396 sec/step, loss=0.07066, avg_loss=0.07326]\n",
      "Step 539119  [5.355 sec/step, loss=0.07099, avg_loss=0.07324]\n",
      "Step 539120  [5.353 sec/step, loss=0.07289, avg_loss=0.07324]\n",
      "Step 539121  [5.343 sec/step, loss=0.07525, avg_loss=0.07327]\n",
      "Step 539122  [5.353 sec/step, loss=0.07356, avg_loss=0.07329]\n",
      "Step 539123  [5.366 sec/step, loss=0.07058, avg_loss=0.07327]\n",
      "Step 539124  [5.377 sec/step, loss=0.07604, avg_loss=0.07328]\n",
      "Step 539125  [5.382 sec/step, loss=0.07518, avg_loss=0.07330]\n",
      "Step 539126  [5.384 sec/step, loss=0.07318, avg_loss=0.07328]\n",
      "Step 539127  [5.396 sec/step, loss=0.07511, avg_loss=0.07331]\n",
      "Step 539128  [5.407 sec/step, loss=0.07552, avg_loss=0.07334]\n",
      "Step 539129  [5.411 sec/step, loss=0.07467, avg_loss=0.07336]\n",
      "Step 539130  [5.466 sec/step, loss=0.06538, avg_loss=0.07330]\n",
      "Step 539131  [5.444 sec/step, loss=0.07343, avg_loss=0.07328]\n",
      "Step 539132  [5.441 sec/step, loss=0.07381, avg_loss=0.07327]\n",
      "Step 539133  [5.459 sec/step, loss=0.07498, avg_loss=0.07331]\n",
      "Step 539134  [5.468 sec/step, loss=0.07336, avg_loss=0.07330]\n",
      "Step 539135  [5.477 sec/step, loss=0.07388, avg_loss=0.07330]\n",
      "Step 539136  [5.477 sec/step, loss=0.07143, avg_loss=0.07328]\n",
      "Step 539137  [5.494 sec/step, loss=0.07218, avg_loss=0.07325]\n",
      "Step 539138  [5.516 sec/step, loss=0.07512, avg_loss=0.07331]\n",
      "Step 539139  [5.503 sec/step, loss=0.07328, avg_loss=0.07328]\n",
      "Step 539140  [5.480 sec/step, loss=0.06623, avg_loss=0.07321]\n",
      "Step 539141  [5.480 sec/step, loss=0.07260, avg_loss=0.07319]\n",
      "Step 539142  [5.473 sec/step, loss=0.07439, avg_loss=0.07319]\n",
      "Step 539143  [5.459 sec/step, loss=0.07096, avg_loss=0.07315]\n",
      "Step 539144  [5.459 sec/step, loss=0.07485, avg_loss=0.07316]\n",
      "Step 539145  [5.411 sec/step, loss=0.07515, avg_loss=0.07325]\n",
      "Generated 32 batches of size 32 in 2.583 sec\n",
      "Step 539146  [5.404 sec/step, loss=0.07485, avg_loss=0.07324]\n",
      "Step 539147  [5.422 sec/step, loss=0.07571, avg_loss=0.07326]\n",
      "Step 539148  [5.427 sec/step, loss=0.07020, avg_loss=0.07332]\n",
      "Step 539149  [5.428 sec/step, loss=0.07340, avg_loss=0.07332]\n",
      "Step 539150  [5.426 sec/step, loss=0.07561, avg_loss=0.07332]\n",
      "Step 539151  [5.402 sec/step, loss=0.07265, avg_loss=0.07329]\n",
      "Step 539152  [5.405 sec/step, loss=0.07454, avg_loss=0.07329]\n",
      "Step 539153  [5.406 sec/step, loss=0.07287, avg_loss=0.07328]\n",
      "Step 539154  [5.388 sec/step, loss=0.07427, avg_loss=0.07327]\n",
      "Step 539155  [5.386 sec/step, loss=0.07124, avg_loss=0.07330]\n",
      "Step 539156  [5.389 sec/step, loss=0.07398, avg_loss=0.07330]\n",
      "Step 539157  [5.362 sec/step, loss=0.07392, avg_loss=0.07332]\n",
      "Step 539158  [5.384 sec/step, loss=0.07502, avg_loss=0.07333]\n",
      "Step 539159  [5.379 sec/step, loss=0.07455, avg_loss=0.07333]\n",
      "Step 539160  [5.321 sec/step, loss=0.07324, avg_loss=0.07340]\n",
      "Step 539161  [5.324 sec/step, loss=0.07397, avg_loss=0.07341]\n",
      "Step 539162  [5.315 sec/step, loss=0.07387, avg_loss=0.07342]\n",
      "Step 539163  [5.332 sec/step, loss=0.07256, avg_loss=0.07339]\n",
      "Step 539164  [5.342 sec/step, loss=0.07473, avg_loss=0.07339]\n",
      "Step 539165  [5.331 sec/step, loss=0.07531, avg_loss=0.07338]\n",
      "Step 539166  [5.323 sec/step, loss=0.07339, avg_loss=0.07338]\n",
      "Step 539167  [5.331 sec/step, loss=0.07516, avg_loss=0.07339]\n",
      "Step 539168  [5.342 sec/step, loss=0.07536, avg_loss=0.07340]\n",
      "Step 539169  [5.345 sec/step, loss=0.07494, avg_loss=0.07342]\n",
      "Step 539170  [5.340 sec/step, loss=0.07372, avg_loss=0.07340]\n",
      "Step 539171  [5.307 sec/step, loss=0.07318, avg_loss=0.07341]\n",
      "Step 539172  [5.308 sec/step, loss=0.07394, avg_loss=0.07341]\n",
      "Step 539173  [5.291 sec/step, loss=0.07335, avg_loss=0.07342]\n",
      "Step 539174  [5.285 sec/step, loss=0.06599, avg_loss=0.07337]\n",
      "Step 539175  [5.305 sec/step, loss=0.07554, avg_loss=0.07340]\n",
      "Step 539176  [5.309 sec/step, loss=0.07433, avg_loss=0.07342]\n",
      "Step 539177  [5.321 sec/step, loss=0.07591, avg_loss=0.07345]\n",
      "Generated 32 batches of size 32 in 2.376 sec\n",
      "Step 539178  [5.340 sec/step, loss=0.07562, avg_loss=0.07348]\n",
      "Step 539179  [5.332 sec/step, loss=0.07281, avg_loss=0.07345]\n",
      "Step 539180  [5.328 sec/step, loss=0.07586, avg_loss=0.07346]\n",
      "Step 539181  [5.313 sec/step, loss=0.07117, avg_loss=0.07343]\n",
      "Step 539182  [5.325 sec/step, loss=0.07032, avg_loss=0.07341]\n",
      "Step 539183  [5.307 sec/step, loss=0.07466, avg_loss=0.07341]\n",
      "Step 539184  [5.359 sec/step, loss=0.06607, avg_loss=0.07332]\n",
      "Step 539185  [5.362 sec/step, loss=0.07306, avg_loss=0.07331]\n",
      "Step 539186  [5.374 sec/step, loss=0.07192, avg_loss=0.07337]\n",
      "Step 539187  [5.337 sec/step, loss=0.06450, avg_loss=0.07327]\n",
      "Step 539188  [5.348 sec/step, loss=0.07457, avg_loss=0.07328]\n",
      "Step 539189  [5.360 sec/step, loss=0.07613, avg_loss=0.07329]\n",
      "Step 539190  [5.351 sec/step, loss=0.07268, avg_loss=0.07326]\n",
      "Step 539191  [5.338 sec/step, loss=0.07378, avg_loss=0.07325]\n",
      "Step 539192  [5.332 sec/step, loss=0.07365, avg_loss=0.07324]\n",
      "Step 539193  [5.347 sec/step, loss=0.07478, avg_loss=0.07328]\n",
      "Step 539194  [5.359 sec/step, loss=0.07206, avg_loss=0.07326]\n",
      "Step 539195  [5.350 sec/step, loss=0.07081, avg_loss=0.07324]\n",
      "Step 539196  [5.338 sec/step, loss=0.07451, avg_loss=0.07325]\n",
      "Step 539197  [5.328 sec/step, loss=0.07004, avg_loss=0.07323]\n",
      "Step 539198  [5.313 sec/step, loss=0.07532, avg_loss=0.07323]\n",
      "Step 539199  [5.311 sec/step, loss=0.07554, avg_loss=0.07325]\n",
      "Step 539200  [5.353 sec/step, loss=0.06548, avg_loss=0.07314]\n",
      "Writing summary at step: 539200\n",
      "Step 539201  [5.351 sec/step, loss=0.07285, avg_loss=0.07313]\n",
      "Step 539202  [5.364 sec/step, loss=0.07494, avg_loss=0.07313]\n",
      "Step 539203  [5.358 sec/step, loss=0.07151, avg_loss=0.07310]\n",
      "Step 539204  [5.363 sec/step, loss=0.07132, avg_loss=0.07307]\n",
      "Step 539205  [5.358 sec/step, loss=0.07543, avg_loss=0.07307]\n",
      "Step 539206  [5.380 sec/step, loss=0.07664, avg_loss=0.07310]\n",
      "Step 539207  [5.366 sec/step, loss=0.07483, avg_loss=0.07309]\n",
      "Step 539208  [5.369 sec/step, loss=0.07418, avg_loss=0.07309]\n",
      "Generated 32 batches of size 32 in 2.408 sec\n",
      "Step 539209  [5.369 sec/step, loss=0.07521, avg_loss=0.07310]\n",
      "Step 539210  [5.368 sec/step, loss=0.07398, avg_loss=0.07312]\n",
      "Step 539211  [5.359 sec/step, loss=0.07326, avg_loss=0.07310]\n",
      "Step 539212  [5.373 sec/step, loss=0.07444, avg_loss=0.07314]\n",
      "Step 539213  [5.336 sec/step, loss=0.07345, avg_loss=0.07322]\n",
      "Step 539214  [5.320 sec/step, loss=0.07433, avg_loss=0.07321]\n",
      "Step 539215  [5.315 sec/step, loss=0.07374, avg_loss=0.07320]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 539216  [5.325 sec/step, loss=0.07380, avg_loss=0.07320]\n",
      "Step 539217  [5.349 sec/step, loss=0.07457, avg_loss=0.07330]\n",
      "Step 539218  [5.352 sec/step, loss=0.07354, avg_loss=0.07333]\n",
      "Step 539219  [5.365 sec/step, loss=0.07169, avg_loss=0.07333]\n",
      "Step 539220  [5.396 sec/step, loss=0.07259, avg_loss=0.07333]\n",
      "Step 539221  [5.391 sec/step, loss=0.07404, avg_loss=0.07332]\n",
      "Step 539222  [5.385 sec/step, loss=0.07262, avg_loss=0.07331]\n",
      "Step 539223  [5.381 sec/step, loss=0.07268, avg_loss=0.07333]\n",
      "Step 539224  [5.373 sec/step, loss=0.07528, avg_loss=0.07332]\n",
      "Step 539225  [5.361 sec/step, loss=0.07356, avg_loss=0.07331]\n",
      "Step 539226  [5.358 sec/step, loss=0.07288, avg_loss=0.07330]\n",
      "Step 539227  [5.347 sec/step, loss=0.07453, avg_loss=0.07330]\n",
      "Step 539228  [5.350 sec/step, loss=0.07433, avg_loss=0.07329]\n",
      "Step 539229  [5.360 sec/step, loss=0.07565, avg_loss=0.07330]\n",
      "Step 539230  [5.311 sec/step, loss=0.07097, avg_loss=0.07335]\n",
      "Step 539231  [5.309 sec/step, loss=0.07215, avg_loss=0.07334]\n",
      "Step 539232  [5.315 sec/step, loss=0.07154, avg_loss=0.07332]\n",
      "Step 539233  [5.294 sec/step, loss=0.07066, avg_loss=0.07327]\n",
      "Step 539234  [5.262 sec/step, loss=0.07488, avg_loss=0.07329]\n",
      "Step 539235  [5.261 sec/step, loss=0.07459, avg_loss=0.07329]\n",
      "Step 539236  [5.271 sec/step, loss=0.07532, avg_loss=0.07333]\n",
      "Step 539237  [5.240 sec/step, loss=0.07398, avg_loss=0.07335]\n",
      "Step 539238  [5.244 sec/step, loss=0.07298, avg_loss=0.07333]\n",
      "Step 539239  [5.256 sec/step, loss=0.07573, avg_loss=0.07335]\n",
      "Step 539240  [5.324 sec/step, loss=0.06602, avg_loss=0.07335]\n",
      "Generated 32 batches of size 32 in 2.869 sec\n",
      "Step 539241  [5.317 sec/step, loss=0.06537, avg_loss=0.07328]\n",
      "Step 539242  [5.312 sec/step, loss=0.07086, avg_loss=0.07325]\n",
      "Step 539243  [5.336 sec/step, loss=0.07518, avg_loss=0.07329]\n",
      "Step 539244  [5.340 sec/step, loss=0.07529, avg_loss=0.07329]\n",
      "Step 539245  [5.346 sec/step, loss=0.07468, avg_loss=0.07329]\n",
      "Step 539246  [5.351 sec/step, loss=0.07405, avg_loss=0.07328]\n",
      "Step 539247  [5.348 sec/step, loss=0.07581, avg_loss=0.07328]\n",
      "Step 539248  [5.376 sec/step, loss=0.07490, avg_loss=0.07333]\n",
      "Step 539249  [5.382 sec/step, loss=0.07426, avg_loss=0.07334]\n",
      "Step 539250  [5.377 sec/step, loss=0.07432, avg_loss=0.07332]\n",
      "Step 539251  [5.382 sec/step, loss=0.07455, avg_loss=0.07334]\n",
      "Step 539252  [5.381 sec/step, loss=0.07447, avg_loss=0.07334]\n",
      "Step 539253  [5.382 sec/step, loss=0.07510, avg_loss=0.07336]\n",
      "Step 539254  [5.389 sec/step, loss=0.07284, avg_loss=0.07335]\n",
      "Step 539255  [5.393 sec/step, loss=0.07059, avg_loss=0.07334]\n",
      "Step 539256  [5.393 sec/step, loss=0.07581, avg_loss=0.07336]\n",
      "Step 539257  [5.389 sec/step, loss=0.07250, avg_loss=0.07335]\n",
      "Step 539258  [5.391 sec/step, loss=0.07579, avg_loss=0.07335]\n",
      "Step 539259  [5.377 sec/step, loss=0.07325, avg_loss=0.07334]\n",
      "Step 539260  [5.410 sec/step, loss=0.07508, avg_loss=0.07336]\n",
      "Step 539261  [5.414 sec/step, loss=0.07409, avg_loss=0.07336]\n",
      "Step 539262  [5.424 sec/step, loss=0.07315, avg_loss=0.07335]\n",
      "Step 539263  [5.390 sec/step, loss=0.07312, avg_loss=0.07336]\n",
      "Step 539264  [5.392 sec/step, loss=0.07213, avg_loss=0.07333]\n",
      "Step 539265  [5.401 sec/step, loss=0.07530, avg_loss=0.07333]\n",
      "Step 539266  [5.387 sec/step, loss=0.06559, avg_loss=0.07326]\n",
      "Step 539267  [5.385 sec/step, loss=0.07617, avg_loss=0.07327]\n",
      "Step 539268  [5.367 sec/step, loss=0.07343, avg_loss=0.07325]\n",
      "Step 539269  [5.378 sec/step, loss=0.07521, avg_loss=0.07325]\n",
      "Step 539270  [5.384 sec/step, loss=0.07558, avg_loss=0.07327]\n",
      "Step 539271  [5.391 sec/step, loss=0.07390, avg_loss=0.07327]\n",
      "Step 539272  [5.384 sec/step, loss=0.07327, avg_loss=0.07327]\n",
      "Generated 32 batches of size 32 in 2.629 sec\n",
      "Step 539273  [5.450 sec/step, loss=0.06568, avg_loss=0.07319]\n",
      "Step 539274  [5.476 sec/step, loss=0.07416, avg_loss=0.07327]\n",
      "Step 539275  [5.462 sec/step, loss=0.07457, avg_loss=0.07326]\n",
      "Step 539276  [5.460 sec/step, loss=0.07299, avg_loss=0.07325]\n",
      "Step 539277  [5.438 sec/step, loss=0.07024, avg_loss=0.07319]\n",
      "Step 539278  [5.425 sec/step, loss=0.07479, avg_loss=0.07319]\n",
      "Step 539279  [5.424 sec/step, loss=0.07097, avg_loss=0.07317]\n",
      "Step 539280  [5.417 sec/step, loss=0.07526, avg_loss=0.07316]\n",
      "Step 539281  [5.427 sec/step, loss=0.07394, avg_loss=0.07319]\n",
      "Step 539282  [5.426 sec/step, loss=0.07269, avg_loss=0.07321]\n",
      "Step 539283  [5.414 sec/step, loss=0.07211, avg_loss=0.07319]\n",
      "Step 539284  [5.358 sec/step, loss=0.07433, avg_loss=0.07327]\n",
      "Step 539285  [5.363 sec/step, loss=0.07528, avg_loss=0.07329]\n",
      "Step 539286  [5.380 sec/step, loss=0.07351, avg_loss=0.07331]\n",
      "Step 539287  [5.403 sec/step, loss=0.07392, avg_loss=0.07340]\n",
      "Step 539288  [5.406 sec/step, loss=0.07426, avg_loss=0.07340]\n",
      "Step 539289  [5.401 sec/step, loss=0.07422, avg_loss=0.07338]\n",
      "Step 539290  [5.401 sec/step, loss=0.07356, avg_loss=0.07339]\n",
      "Step 539291  [5.416 sec/step, loss=0.07427, avg_loss=0.07339]\n",
      "Step 539292  [5.476 sec/step, loss=0.06587, avg_loss=0.07332]\n",
      "Step 539293  [5.467 sec/step, loss=0.07317, avg_loss=0.07330]\n",
      "Step 539294  [5.440 sec/step, loss=0.07336, avg_loss=0.07331]\n",
      "Step 539295  [5.480 sec/step, loss=0.07283, avg_loss=0.07333]\n",
      "Step 539296  [5.500 sec/step, loss=0.07468, avg_loss=0.07333]\n",
      "Step 539297  [5.503 sec/step, loss=0.06965, avg_loss=0.07333]\n",
      "Step 539298  [5.503 sec/step, loss=0.07257, avg_loss=0.07330]\n",
      "Step 539299  [5.505 sec/step, loss=0.07541, avg_loss=0.07330]\n",
      "Step 539300  [5.450 sec/step, loss=0.07290, avg_loss=0.07338]\n",
      "Writing summary at step: 539300\n",
      "Step 539301  [5.457 sec/step, loss=0.07469, avg_loss=0.07339]\n",
      "Step 539302  [5.441 sec/step, loss=0.07496, avg_loss=0.07339]\n",
      "Step 539303  [5.462 sec/step, loss=0.07541, avg_loss=0.07343]\n",
      "Generated 32 batches of size 32 in 2.940 sec\n",
      "Step 539304  [5.453 sec/step, loss=0.06591, avg_loss=0.07338]\n",
      "Step 539305  [5.443 sec/step, loss=0.07126, avg_loss=0.07334]\n",
      "Step 539306  [5.437 sec/step, loss=0.07458, avg_loss=0.07332]\n",
      "Step 539307  [5.449 sec/step, loss=0.07577, avg_loss=0.07333]\n",
      "Step 539308  [5.448 sec/step, loss=0.07162, avg_loss=0.07330]\n",
      "Step 539309  [5.416 sec/step, loss=0.07214, avg_loss=0.07327]\n",
      "Step 539310  [5.410 sec/step, loss=0.07391, avg_loss=0.07327]\n",
      "Step 539311  [5.412 sec/step, loss=0.07519, avg_loss=0.07329]\n",
      "Step 539312  [5.420 sec/step, loss=0.07572, avg_loss=0.07330]\n",
      "Step 539313  [5.406 sec/step, loss=0.07120, avg_loss=0.07328]\n",
      "Step 539314  [5.399 sec/step, loss=0.07235, avg_loss=0.07326]\n",
      "Step 539315  [5.391 sec/step, loss=0.07350, avg_loss=0.07326]\n",
      "Step 539316  [5.380 sec/step, loss=0.07412, avg_loss=0.07326]\n",
      "Step 539317  [5.373 sec/step, loss=0.07531, avg_loss=0.07327]\n",
      "Step 539318  [5.383 sec/step, loss=0.07526, avg_loss=0.07328]\n",
      "Step 539319  [5.429 sec/step, loss=0.06524, avg_loss=0.07322]\n",
      "Step 539320  [5.427 sec/step, loss=0.07480, avg_loss=0.07324]\n",
      "Step 539321  [5.435 sec/step, loss=0.07470, avg_loss=0.07325]\n",
      "Step 539322  [5.428 sec/step, loss=0.07029, avg_loss=0.07322]\n",
      "Step 539323  [5.436 sec/step, loss=0.07500, avg_loss=0.07325]\n",
      "Step 539324  [5.443 sec/step, loss=0.07564, avg_loss=0.07325]\n",
      "Step 539325  [5.440 sec/step, loss=0.07007, avg_loss=0.07322]\n",
      "Step 539326  [5.441 sec/step, loss=0.07447, avg_loss=0.07323]\n",
      "Step 539327  [5.444 sec/step, loss=0.07390, avg_loss=0.07323]\n",
      "Step 539328  [5.444 sec/step, loss=0.07521, avg_loss=0.07324]\n",
      "Step 539329  [5.431 sec/step, loss=0.07287, avg_loss=0.07321]\n",
      "Step 539330  [5.430 sec/step, loss=0.07374, avg_loss=0.07324]\n",
      "Step 539331  [5.452 sec/step, loss=0.07582, avg_loss=0.07327]\n",
      "Step 539332  [5.444 sec/step, loss=0.07087, avg_loss=0.07327]\n",
      "Step 539333  [5.448 sec/step, loss=0.07366, avg_loss=0.07330]\n",
      "Step 539334  [5.460 sec/step, loss=0.07344, avg_loss=0.07328]\n",
      "Step 539335  [5.438 sec/step, loss=0.06418, avg_loss=0.07318]\n",
      "Generated 32 batches of size 32 in 2.357 sec\n",
      "Step 539336  [5.438 sec/step, loss=0.07194, avg_loss=0.07314]\n",
      "Step 539337  [5.439 sec/step, loss=0.07388, avg_loss=0.07314]\n",
      "Step 539338  [5.436 sec/step, loss=0.07505, avg_loss=0.07316]\n",
      "Step 539339  [5.438 sec/step, loss=0.07341, avg_loss=0.07314]\n",
      "Step 539340  [5.383 sec/step, loss=0.07371, avg_loss=0.07322]\n",
      "Step 539341  [5.395 sec/step, loss=0.07451, avg_loss=0.07331]\n",
      "Step 539342  [5.412 sec/step, loss=0.07531, avg_loss=0.07335]\n",
      "Step 539343  [5.420 sec/step, loss=0.07469, avg_loss=0.07335]\n",
      "Step 539344  [5.425 sec/step, loss=0.07540, avg_loss=0.07335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 539345  [5.413 sec/step, loss=0.07086, avg_loss=0.07331]\n",
      "Step 539346  [5.418 sec/step, loss=0.07522, avg_loss=0.07332]\n",
      "Step 539347  [5.414 sec/step, loss=0.07556, avg_loss=0.07332]\n",
      "Step 539348  [5.447 sec/step, loss=0.06618, avg_loss=0.07323]\n",
      "Step 539349  [5.437 sec/step, loss=0.07068, avg_loss=0.07320]\n",
      "Step 539350  [5.434 sec/step, loss=0.07436, avg_loss=0.07320]\n",
      "Step 539351  [5.442 sec/step, loss=0.07280, avg_loss=0.07318]\n",
      "Step 539352  [5.434 sec/step, loss=0.07462, avg_loss=0.07318]\n",
      "Step 539353  [5.419 sec/step, loss=0.07443, avg_loss=0.07317]\n",
      "Step 539354  [5.410 sec/step, loss=0.07338, avg_loss=0.07318]\n",
      "Step 539355  [5.428 sec/step, loss=0.07603, avg_loss=0.07323]\n",
      "Step 539356  [5.417 sec/step, loss=0.07258, avg_loss=0.07320]\n",
      "Step 539357  [5.438 sec/step, loss=0.07521, avg_loss=0.07323]\n",
      "Step 539358  [5.432 sec/step, loss=0.07512, avg_loss=0.07322]\n",
      "Step 539359  [5.442 sec/step, loss=0.07401, avg_loss=0.07323]\n",
      "Step 539360  [5.403 sec/step, loss=0.07161, avg_loss=0.07319]\n",
      "Step 539361  [5.428 sec/step, loss=0.07250, avg_loss=0.07318]\n",
      "Step 539362  [5.418 sec/step, loss=0.07134, avg_loss=0.07316]\n",
      "Step 539363  [5.433 sec/step, loss=0.07331, avg_loss=0.07316]\n",
      "Step 539364  [5.421 sec/step, loss=0.07403, avg_loss=0.07318]\n",
      "Step 539365  [5.400 sec/step, loss=0.07362, avg_loss=0.07316]\n",
      "Step 539366  [5.430 sec/step, loss=0.07365, avg_loss=0.07325]\n",
      "Step 539367  [5.409 sec/step, loss=0.06485, avg_loss=0.07313]\n",
      "Generated 32 batches of size 32 in 2.386 sec\n",
      "Step 539368  [5.422 sec/step, loss=0.07491, avg_loss=0.07315]\n",
      "Step 539369  [5.407 sec/step, loss=0.07164, avg_loss=0.07311]\n",
      "Step 539370  [5.411 sec/step, loss=0.07567, avg_loss=0.07311]\n",
      "Step 539371  [5.416 sec/step, loss=0.07393, avg_loss=0.07311]\n",
      "Step 539372  [5.436 sec/step, loss=0.07561, avg_loss=0.07314]\n",
      "Step 539373  [5.381 sec/step, loss=0.07409, avg_loss=0.07322]\n",
      "Step 539374  [5.364 sec/step, loss=0.07311, avg_loss=0.07321]\n",
      "Step 539375  [5.367 sec/step, loss=0.07498, avg_loss=0.07321]\n",
      "Step 539376  [5.367 sec/step, loss=0.07275, avg_loss=0.07321]\n",
      "Step 539377  [5.366 sec/step, loss=0.07176, avg_loss=0.07323]\n",
      "Step 539378  [5.357 sec/step, loss=0.07021, avg_loss=0.07318]\n",
      "Step 539379  [5.351 sec/step, loss=0.07326, avg_loss=0.07320]\n",
      "Step 539380  [5.343 sec/step, loss=0.07289, avg_loss=0.07318]\n",
      "Step 539381  [5.369 sec/step, loss=0.07259, avg_loss=0.07317]\n",
      "Step 539382  [5.371 sec/step, loss=0.07120, avg_loss=0.07315]\n",
      "Step 539383  [5.401 sec/step, loss=0.07197, avg_loss=0.07315]\n",
      "Step 539384  [5.402 sec/step, loss=0.07449, avg_loss=0.07315]\n",
      "Step 539385  [5.406 sec/step, loss=0.07467, avg_loss=0.07315]\n",
      "Step 539386  [5.396 sec/step, loss=0.07498, avg_loss=0.07316]\n",
      "Step 539387  [5.389 sec/step, loss=0.07255, avg_loss=0.07315]\n",
      "Step 539388  [5.384 sec/step, loss=0.07385, avg_loss=0.07314]\n",
      "Step 539389  [5.383 sec/step, loss=0.07201, avg_loss=0.07312]\n",
      "Step 539390  [5.392 sec/step, loss=0.07352, avg_loss=0.07312]\n",
      "Step 539391  [5.395 sec/step, loss=0.07524, avg_loss=0.07313]\n",
      "Step 539392  [5.351 sec/step, loss=0.07538, avg_loss=0.07322]\n",
      "Step 539393  [5.363 sec/step, loss=0.07400, avg_loss=0.07323]\n",
      "Step 539394  [5.348 sec/step, loss=0.06634, avg_loss=0.07316]\n",
      "Step 539395  [5.333 sec/step, loss=0.07507, avg_loss=0.07319]\n",
      "Step 539396  [5.307 sec/step, loss=0.07173, avg_loss=0.07316]\n",
      "Step 539397  [5.329 sec/step, loss=0.07549, avg_loss=0.07321]\n",
      "Step 539398  [5.318 sec/step, loss=0.07370, avg_loss=0.07323]\n",
      "Step 539399  [5.313 sec/step, loss=0.07266, avg_loss=0.07320]\n",
      "Generated 32 batches of size 32 in 2.370 sec\n",
      "Step 539400  [5.323 sec/step, loss=0.07396, avg_loss=0.07321]\n",
      "Writing summary at step: 539400\n",
      "Step 539401  [5.335 sec/step, loss=0.07559, avg_loss=0.07322]\n",
      "Step 539402  [5.330 sec/step, loss=0.07341, avg_loss=0.07320]\n",
      "Step 539403  [5.317 sec/step, loss=0.07461, avg_loss=0.07319]\n",
      "Step 539404  [5.337 sec/step, loss=0.07604, avg_loss=0.07330]\n",
      "Step 539405  [5.335 sec/step, loss=0.07304, avg_loss=0.07331]\n",
      "Step 539406  [5.338 sec/step, loss=0.07324, avg_loss=0.07330]\n",
      "Step 539407  [5.337 sec/step, loss=0.07489, avg_loss=0.07329]\n",
      "Step 539408  [5.341 sec/step, loss=0.07515, avg_loss=0.07333]\n",
      "Step 539409  [5.356 sec/step, loss=0.07277, avg_loss=0.07333]\n",
      "Step 539410  [5.367 sec/step, loss=0.07380, avg_loss=0.07333]\n",
      "Step 539411  [5.361 sec/step, loss=0.07302, avg_loss=0.07331]\n",
      "Step 539412  [5.362 sec/step, loss=0.07556, avg_loss=0.07331]\n",
      "Step 539413  [5.355 sec/step, loss=0.07313, avg_loss=0.07333]\n",
      "Step 539414  [5.355 sec/step, loss=0.07166, avg_loss=0.07332]\n",
      "Step 539415  [5.364 sec/step, loss=0.07473, avg_loss=0.07333]\n",
      "Step 539416  [5.360 sec/step, loss=0.07308, avg_loss=0.07332]\n",
      "Step 539417  [5.410 sec/step, loss=0.06699, avg_loss=0.07324]\n",
      "Step 539418  [5.408 sec/step, loss=0.07531, avg_loss=0.07324]\n",
      "Step 539419  [5.386 sec/step, loss=0.07232, avg_loss=0.07331]\n",
      "Step 539420  [5.352 sec/step, loss=0.06970, avg_loss=0.07326]\n",
      "Step 539421  [5.347 sec/step, loss=0.07200, avg_loss=0.07323]\n",
      "Step 539422  [5.375 sec/step, loss=0.07324, avg_loss=0.07326]\n",
      "Step 539423  [5.368 sec/step, loss=0.07427, avg_loss=0.07325]\n",
      "Step 539424  [5.353 sec/step, loss=0.07176, avg_loss=0.07322]\n",
      "Step 539425  [5.348 sec/step, loss=0.07013, avg_loss=0.07322]\n",
      "Step 539426  [5.334 sec/step, loss=0.06459, avg_loss=0.07312]\n",
      "Step 539427  [5.328 sec/step, loss=0.07400, avg_loss=0.07312]\n",
      "Step 539428  [5.320 sec/step, loss=0.07483, avg_loss=0.07312]\n",
      "Step 539429  [5.316 sec/step, loss=0.07385, avg_loss=0.07312]\n",
      "Step 539430  [5.322 sec/step, loss=0.07349, avg_loss=0.07312]\n",
      "Generated 32 batches of size 32 in 2.352 sec\n",
      "Step 539431  [5.331 sec/step, loss=0.07562, avg_loss=0.07312]\n",
      "Step 539432  [5.338 sec/step, loss=0.07344, avg_loss=0.07315]\n",
      "Step 539433  [5.365 sec/step, loss=0.07341, avg_loss=0.07314]\n",
      "Step 539434  [5.368 sec/step, loss=0.07355, avg_loss=0.07314]\n",
      "Step 539435  [5.388 sec/step, loss=0.07474, avg_loss=0.07325]\n",
      "Step 539436  [5.392 sec/step, loss=0.07609, avg_loss=0.07329]\n",
      "Step 539437  [5.392 sec/step, loss=0.07360, avg_loss=0.07329]\n",
      "Step 539438  [5.392 sec/step, loss=0.07395, avg_loss=0.07328]\n",
      "Step 539439  [5.380 sec/step, loss=0.07437, avg_loss=0.07329]\n",
      "Step 539440  [5.369 sec/step, loss=0.06760, avg_loss=0.07323]\n",
      "Step 539441  [5.376 sec/step, loss=0.07528, avg_loss=0.07323]\n",
      "Step 539442  [5.376 sec/step, loss=0.07515, avg_loss=0.07323]\n",
      "Step 539443  [5.357 sec/step, loss=0.07332, avg_loss=0.07322]\n",
      "Step 539444  [5.346 sec/step, loss=0.07453, avg_loss=0.07321]\n",
      "Step 539445  [5.401 sec/step, loss=0.06437, avg_loss=0.07315]\n",
      "Step 539446  [5.388 sec/step, loss=0.07469, avg_loss=0.07314]\n",
      "Step 539447  [5.375 sec/step, loss=0.07353, avg_loss=0.07312]\n",
      "Step 539448  [5.317 sec/step, loss=0.07317, avg_loss=0.07319]\n",
      "Step 539449  [5.318 sec/step, loss=0.07351, avg_loss=0.07322]\n",
      "Step 539450  [5.304 sec/step, loss=0.07140, avg_loss=0.07319]\n",
      "Step 539451  [5.307 sec/step, loss=0.07359, avg_loss=0.07320]\n",
      "Step 539452  [5.308 sec/step, loss=0.07049, avg_loss=0.07316]\n",
      "Step 539453  [5.322 sec/step, loss=0.07571, avg_loss=0.07317]\n",
      "Step 539454  [5.326 sec/step, loss=0.07353, avg_loss=0.07317]\n",
      "Step 539455  [5.335 sec/step, loss=0.07455, avg_loss=0.07315]\n",
      "Step 539456  [5.363 sec/step, loss=0.07339, avg_loss=0.07316]\n",
      "Step 539457  [5.356 sec/step, loss=0.07243, avg_loss=0.07314]\n",
      "Step 539458  [5.351 sec/step, loss=0.07522, avg_loss=0.07314]\n",
      "Step 539459  [5.357 sec/step, loss=0.07353, avg_loss=0.07313]\n",
      "Step 539460  [5.370 sec/step, loss=0.07402, avg_loss=0.07316]\n",
      "Step 539461  [5.340 sec/step, loss=0.07169, avg_loss=0.07315]\n",
      "Step 539462  [5.356 sec/step, loss=0.07470, avg_loss=0.07318]\n",
      "Generated 32 batches of size 32 in 2.400 sec\n",
      "Step 539463  [5.365 sec/step, loss=0.07520, avg_loss=0.07320]\n",
      "Step 539464  [5.354 sec/step, loss=0.07455, avg_loss=0.07320]\n",
      "Step 539465  [5.370 sec/step, loss=0.07463, avg_loss=0.07322]\n",
      "Step 539466  [5.366 sec/step, loss=0.07570, avg_loss=0.07324]\n",
      "Step 539467  [5.376 sec/step, loss=0.07302, avg_loss=0.07332]\n",
      "Step 539468  [5.355 sec/step, loss=0.07074, avg_loss=0.07328]\n",
      "Step 539469  [5.367 sec/step, loss=0.07394, avg_loss=0.07330]\n",
      "Step 539470  [5.354 sec/step, loss=0.07453, avg_loss=0.07329]\n",
      "Step 539471  [5.354 sec/step, loss=0.07366, avg_loss=0.07328]\n",
      "Step 539472  [5.355 sec/step, loss=0.07575, avg_loss=0.07329]\n",
      "Step 539473  [5.382 sec/step, loss=0.07282, avg_loss=0.07327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 539474  [5.386 sec/step, loss=0.07501, avg_loss=0.07329]\n",
      "Step 539475  [5.384 sec/step, loss=0.07356, avg_loss=0.07328]\n",
      "Step 539476  [5.385 sec/step, loss=0.07531, avg_loss=0.07330]\n",
      "Step 539477  [5.389 sec/step, loss=0.07036, avg_loss=0.07329]\n",
      "Step 539478  [5.393 sec/step, loss=0.07455, avg_loss=0.07333]\n",
      "Step 539479  [5.397 sec/step, loss=0.07212, avg_loss=0.07332]\n",
      "Step 539480  [5.413 sec/step, loss=0.07493, avg_loss=0.07334]\n",
      "Step 539481  [5.385 sec/step, loss=0.07305, avg_loss=0.07335]\n",
      "Step 539482  [5.373 sec/step, loss=0.07075, avg_loss=0.07334]\n",
      "Step 539483  [5.335 sec/step, loss=0.06329, avg_loss=0.07326]\n",
      "Step 539484  [5.388 sec/step, loss=0.06658, avg_loss=0.07318]\n",
      "Step 539485  [5.377 sec/step, loss=0.07416, avg_loss=0.07317]\n",
      "Step 539486  [5.398 sec/step, loss=0.07399, avg_loss=0.07316]\n",
      "Step 539487  [5.405 sec/step, loss=0.07474, avg_loss=0.07318]\n",
      "Step 539488  [5.419 sec/step, loss=0.07518, avg_loss=0.07320]\n",
      "Step 539489  [5.412 sec/step, loss=0.07252, avg_loss=0.07320]\n",
      "Step 539490  [5.409 sec/step, loss=0.07372, avg_loss=0.07320]\n",
      "Step 539491  [5.387 sec/step, loss=0.07353, avg_loss=0.07319]\n",
      "Step 539492  [5.375 sec/step, loss=0.07329, avg_loss=0.07317]\n",
      "Step 539493  [5.372 sec/step, loss=0.07449, avg_loss=0.07317]\n",
      "Step 539494  [5.401 sec/step, loss=0.07324, avg_loss=0.07324]\n",
      "Generated 32 batches of size 32 in 2.438 sec\n",
      "Step 539495  [5.405 sec/step, loss=0.07296, avg_loss=0.07322]\n",
      "Step 539496  [5.425 sec/step, loss=0.07601, avg_loss=0.07326]\n",
      "Step 539497  [5.418 sec/step, loss=0.07519, avg_loss=0.07326]\n",
      "Step 539498  [5.420 sec/step, loss=0.07471, avg_loss=0.07327]\n",
      "Step 539499  [5.418 sec/step, loss=0.07158, avg_loss=0.07326]\n",
      "Step 539500  [5.422 sec/step, loss=0.07526, avg_loss=0.07327]\n",
      "Writing summary at step: 539500\n",
      "Step 539501  [5.400 sec/step, loss=0.07372, avg_loss=0.07325]\n",
      "Step 539502  [5.417 sec/step, loss=0.07555, avg_loss=0.07327]\n",
      "Step 539503  [5.412 sec/step, loss=0.07378, avg_loss=0.07326]\n",
      "Step 539504  [5.408 sec/step, loss=0.07417, avg_loss=0.07325]\n",
      "Step 539505  [5.397 sec/step, loss=0.06563, avg_loss=0.07317]\n",
      "Step 539506  [5.383 sec/step, loss=0.07455, avg_loss=0.07318]\n",
      "Step 539507  [5.376 sec/step, loss=0.07517, avg_loss=0.07319]\n",
      "Step 539508  [5.359 sec/step, loss=0.07016, avg_loss=0.07314]\n",
      "Step 539509  [5.408 sec/step, loss=0.06581, avg_loss=0.07307]\n",
      "Step 539510  [5.403 sec/step, loss=0.07431, avg_loss=0.07307]\n",
      "Step 539511  [5.398 sec/step, loss=0.07328, avg_loss=0.07308]\n",
      "Step 539512  [5.373 sec/step, loss=0.07078, avg_loss=0.07303]\n",
      "Step 539513  [5.395 sec/step, loss=0.07473, avg_loss=0.07304]\n",
      "Step 539514  [5.428 sec/step, loss=0.07220, avg_loss=0.07305]\n",
      "Step 539515  [5.444 sec/step, loss=0.07514, avg_loss=0.07305]\n",
      "Step 539516  [5.448 sec/step, loss=0.07206, avg_loss=0.07304]\n",
      "Step 539517  [5.409 sec/step, loss=0.07549, avg_loss=0.07313]\n",
      "Step 539518  [5.411 sec/step, loss=0.07439, avg_loss=0.07312]\n",
      "Step 539519  [5.379 sec/step, loss=0.07279, avg_loss=0.07312]\n",
      "Step 539520  [5.381 sec/step, loss=0.07318, avg_loss=0.07316]\n",
      "Step 539521  [5.394 sec/step, loss=0.07524, avg_loss=0.07319]\n",
      "Step 539522  [5.377 sec/step, loss=0.07217, avg_loss=0.07318]\n",
      "Step 539523  [5.391 sec/step, loss=0.07531, avg_loss=0.07319]\n",
      "Step 539524  [5.398 sec/step, loss=0.07345, avg_loss=0.07321]\n",
      "Step 539525  [5.414 sec/step, loss=0.07428, avg_loss=0.07325]\n",
      "Generated 32 batches of size 32 in 2.585 sec\n",
      "Step 539526  [5.433 sec/step, loss=0.07480, avg_loss=0.07335]\n",
      "Step 539527  [5.443 sec/step, loss=0.07567, avg_loss=0.07337]\n",
      "Step 539528  [5.449 sec/step, loss=0.07553, avg_loss=0.07338]\n",
      "Step 539529  [5.452 sec/step, loss=0.07507, avg_loss=0.07339]\n",
      "Step 539530  [5.452 sec/step, loss=0.07439, avg_loss=0.07340]\n",
      "Step 539531  [5.443 sec/step, loss=0.07536, avg_loss=0.07339]\n",
      "Step 539532  [5.428 sec/step, loss=0.07333, avg_loss=0.07339]\n",
      "Step 539533  [5.406 sec/step, loss=0.06966, avg_loss=0.07336]\n",
      "Step 539534  [5.401 sec/step, loss=0.07477, avg_loss=0.07337]\n",
      "Step 539535  [5.410 sec/step, loss=0.07539, avg_loss=0.07337]\n",
      "Step 539536  [5.409 sec/step, loss=0.07338, avg_loss=0.07335]\n",
      "Step 539537  [5.421 sec/step, loss=0.07442, avg_loss=0.07335]\n",
      "Step 539538  [5.424 sec/step, loss=0.07523, avg_loss=0.07337]\n",
      "Step 539539  [5.430 sec/step, loss=0.07424, avg_loss=0.07337]\n",
      "Step 539540  [5.451 sec/step, loss=0.07468, avg_loss=0.07344]\n",
      "Step 539541  [5.435 sec/step, loss=0.07337, avg_loss=0.07342]\n",
      "Step 539542  [5.448 sec/step, loss=0.07302, avg_loss=0.07340]\n",
      "Step 539543  [5.439 sec/step, loss=0.07340, avg_loss=0.07340]\n",
      "Step 539544  [5.457 sec/step, loss=0.07329, avg_loss=0.07339]\n",
      "Step 539545  [5.404 sec/step, loss=0.07418, avg_loss=0.07348]\n",
      "Step 539546  [5.403 sec/step, loss=0.07310, avg_loss=0.07347]\n",
      "Step 539547  [5.403 sec/step, loss=0.07272, avg_loss=0.07346]\n",
      "Step 539548  [5.461 sec/step, loss=0.06597, avg_loss=0.07339]\n",
      "Step 539549  [5.477 sec/step, loss=0.07356, avg_loss=0.07339]\n",
      "Step 539550  [5.490 sec/step, loss=0.07380, avg_loss=0.07341]\n",
      "Step 539551  [5.490 sec/step, loss=0.07471, avg_loss=0.07342]\n",
      "Step 539552  [5.502 sec/step, loss=0.07477, avg_loss=0.07347]\n",
      "Step 539553  [5.493 sec/step, loss=0.07147, avg_loss=0.07342]\n",
      "Step 539554  [5.491 sec/step, loss=0.07233, avg_loss=0.07341]\n",
      "Step 539555  [5.479 sec/step, loss=0.07461, avg_loss=0.07341]\n",
      "Step 539556  [5.456 sec/step, loss=0.07439, avg_loss=0.07342]\n",
      "Step 539557  [5.461 sec/step, loss=0.07608, avg_loss=0.07346]\n",
      "Generated 32 batches of size 32 in 2.381 sec\n",
      "Step 539558  [5.467 sec/step, loss=0.07491, avg_loss=0.07346]\n",
      "Step 539559  [5.468 sec/step, loss=0.07550, avg_loss=0.07348]\n",
      "Step 539560  [5.455 sec/step, loss=0.07163, avg_loss=0.07345]\n",
      "Step 539561  [5.449 sec/step, loss=0.07150, avg_loss=0.07345]\n",
      "Step 539562  [5.432 sec/step, loss=0.07111, avg_loss=0.07341]\n",
      "Step 539563  [5.413 sec/step, loss=0.07470, avg_loss=0.07341]\n",
      "Step 539564  [5.426 sec/step, loss=0.07498, avg_loss=0.07341]\n",
      "Step 539565  [5.414 sec/step, loss=0.07334, avg_loss=0.07340]\n",
      "Step 539566  [5.389 sec/step, loss=0.06600, avg_loss=0.07330]\n",
      "Step 539567  [5.404 sec/step, loss=0.07525, avg_loss=0.07332]\n",
      "Step 539568  [5.419 sec/step, loss=0.07420, avg_loss=0.07336]\n",
      "Step 539569  [5.440 sec/step, loss=0.07287, avg_loss=0.07335]\n",
      "Step 539570  [5.449 sec/step, loss=0.07318, avg_loss=0.07334]\n",
      "Step 539571  [5.494 sec/step, loss=0.06593, avg_loss=0.07326]\n",
      "Step 539572  [5.485 sec/step, loss=0.07426, avg_loss=0.07324]\n",
      "Step 539573  [5.462 sec/step, loss=0.07471, avg_loss=0.07326]\n",
      "Step 539574  [5.467 sec/step, loss=0.07500, avg_loss=0.07326]\n",
      "Step 539575  [5.471 sec/step, loss=0.07442, avg_loss=0.07327]\n",
      "Step 539576  [5.482 sec/step, loss=0.07562, avg_loss=0.07327]\n",
      "Step 539577  [5.493 sec/step, loss=0.07352, avg_loss=0.07331]\n",
      "Step 539578  [5.507 sec/step, loss=0.07466, avg_loss=0.07331]\n",
      "Step 539579  [5.519 sec/step, loss=0.07540, avg_loss=0.07334]\n",
      "Step 539580  [5.505 sec/step, loss=0.07373, avg_loss=0.07333]\n",
      "Step 539581  [5.519 sec/step, loss=0.07589, avg_loss=0.07336]\n",
      "Step 539582  [5.551 sec/step, loss=0.07500, avg_loss=0.07340]\n",
      "Step 539583  [5.572 sec/step, loss=0.07461, avg_loss=0.07351]\n",
      "Step 539584  [5.518 sec/step, loss=0.07357, avg_loss=0.07358]\n",
      "Step 539585  [5.508 sec/step, loss=0.07314, avg_loss=0.07357]\n",
      "Step 539586  [5.482 sec/step, loss=0.07458, avg_loss=0.07358]\n",
      "Step 539587  [5.482 sec/step, loss=0.07153, avg_loss=0.07354]\n",
      "Step 539588  [5.483 sec/step, loss=0.07362, avg_loss=0.07353]\n",
      "Step 539589  [5.474 sec/step, loss=0.07114, avg_loss=0.07352]\n",
      "Generated 32 batches of size 32 in 2.535 sec\n",
      "Step 539590  [5.471 sec/step, loss=0.07014, avg_loss=0.07348]\n",
      "Step 539591  [5.480 sec/step, loss=0.07483, avg_loss=0.07349]\n",
      "Step 539592  [5.484 sec/step, loss=0.07285, avg_loss=0.07349]\n",
      "Step 539593  [5.485 sec/step, loss=0.07174, avg_loss=0.07346]\n",
      "Step 539594  [5.473 sec/step, loss=0.07456, avg_loss=0.07347]\n",
      "Step 539595  [5.448 sec/step, loss=0.07101, avg_loss=0.07345]\n",
      "Step 539596  [5.422 sec/step, loss=0.06614, avg_loss=0.07336]\n",
      "Step 539597  [5.411 sec/step, loss=0.07074, avg_loss=0.07331]\n",
      "Step 539598  [5.424 sec/step, loss=0.07643, avg_loss=0.07333]\n",
      "Step 539599  [5.403 sec/step, loss=0.06555, avg_loss=0.07327]\n",
      "Step 539600  [5.403 sec/step, loss=0.07425, avg_loss=0.07326]\n",
      "Writing summary at step: 539600\n",
      "Step 539601  [5.412 sec/step, loss=0.07477, avg_loss=0.07327]\n",
      "Step 539602  [5.400 sec/step, loss=0.07388, avg_loss=0.07325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 539603  [5.390 sec/step, loss=0.07013, avg_loss=0.07322]\n",
      "Step 539604  [5.390 sec/step, loss=0.07384, avg_loss=0.07321]\n",
      "Step 539605  [5.410 sec/step, loss=0.07428, avg_loss=0.07330]\n",
      "Step 539606  [5.419 sec/step, loss=0.07495, avg_loss=0.07330]\n",
      "Step 539607  [5.411 sec/step, loss=0.07277, avg_loss=0.07328]\n",
      "Step 539608  [5.416 sec/step, loss=0.07017, avg_loss=0.07328]\n",
      "Step 539609  [5.379 sec/step, loss=0.07560, avg_loss=0.07338]\n",
      "Step 539610  [5.361 sec/step, loss=0.07112, avg_loss=0.07334]\n",
      "Step 539611  [5.360 sec/step, loss=0.07378, avg_loss=0.07335]\n",
      "Step 539612  [5.385 sec/step, loss=0.07549, avg_loss=0.07340]\n",
      "Step 539613  [5.387 sec/step, loss=0.07535, avg_loss=0.07340]\n",
      "Step 539614  [5.374 sec/step, loss=0.07475, avg_loss=0.07343]\n",
      "Step 539615  [5.355 sec/step, loss=0.07439, avg_loss=0.07342]\n",
      "Step 539616  [5.351 sec/step, loss=0.07376, avg_loss=0.07344]\n",
      "Step 539617  [5.340 sec/step, loss=0.07471, avg_loss=0.07343]\n",
      "Step 539618  [5.328 sec/step, loss=0.07255, avg_loss=0.07341]\n",
      "Step 539619  [5.333 sec/step, loss=0.07418, avg_loss=0.07343]\n",
      "Step 539620  [5.348 sec/step, loss=0.07522, avg_loss=0.07345]\n",
      "Generated 32 batches of size 32 in 2.412 sec\n",
      "Step 539621  [5.347 sec/step, loss=0.07598, avg_loss=0.07345]\n",
      "Step 539622  [5.400 sec/step, loss=0.06464, avg_loss=0.07338]\n",
      "Step 539623  [5.400 sec/step, loss=0.07535, avg_loss=0.07338]\n",
      "Step 539624  [5.412 sec/step, loss=0.07314, avg_loss=0.07338]\n",
      "Step 539625  [5.409 sec/step, loss=0.07370, avg_loss=0.07337]\n",
      "Step 539626  [5.412 sec/step, loss=0.07489, avg_loss=0.07337]\n",
      "Step 539627  [5.428 sec/step, loss=0.07461, avg_loss=0.07336]\n",
      "Step 539628  [5.417 sec/step, loss=0.07486, avg_loss=0.07335]\n",
      "Step 539629  [5.408 sec/step, loss=0.07298, avg_loss=0.07333]\n",
      "Step 539630  [5.400 sec/step, loss=0.07222, avg_loss=0.07331]\n",
      "Step 539631  [5.402 sec/step, loss=0.07468, avg_loss=0.07330]\n",
      "Step 539632  [5.398 sec/step, loss=0.07248, avg_loss=0.07330]\n",
      "Step 539633  [5.398 sec/step, loss=0.07455, avg_loss=0.07334]\n",
      "Step 539634  [5.381 sec/step, loss=0.07095, avg_loss=0.07331]\n",
      "Step 539635  [5.372 sec/step, loss=0.07513, avg_loss=0.07330]\n",
      "Step 539636  [5.375 sec/step, loss=0.07575, avg_loss=0.07333]\n",
      "Step 539637  [5.380 sec/step, loss=0.07480, avg_loss=0.07333]\n",
      "Step 539638  [5.367 sec/step, loss=0.07128, avg_loss=0.07329]\n",
      "Step 539639  [5.365 sec/step, loss=0.07501, avg_loss=0.07330]\n",
      "Step 539640  [5.368 sec/step, loss=0.07562, avg_loss=0.07331]\n",
      "Step 539641  [5.379 sec/step, loss=0.07374, avg_loss=0.07331]\n",
      "Step 539642  [5.336 sec/step, loss=0.06634, avg_loss=0.07325]\n",
      "Step 539643  [5.355 sec/step, loss=0.07555, avg_loss=0.07327]\n",
      "Step 539644  [5.326 sec/step, loss=0.07395, avg_loss=0.07327]\n",
      "Step 539645  [5.355 sec/step, loss=0.07196, avg_loss=0.07325]\n",
      "Step 539646  [5.409 sec/step, loss=0.06601, avg_loss=0.07318]\n",
      "Step 539647  [5.407 sec/step, loss=0.07317, avg_loss=0.07319]\n",
      "Step 539648  [5.353 sec/step, loss=0.07169, avg_loss=0.07324]\n",
      "Step 539649  [5.359 sec/step, loss=0.07600, avg_loss=0.07327]\n",
      "Step 539650  [5.360 sec/step, loss=0.07508, avg_loss=0.07328]\n",
      "Step 539651  [5.350 sec/step, loss=0.07480, avg_loss=0.07328]\n",
      "Step 539652  [5.351 sec/step, loss=0.07401, avg_loss=0.07327]\n",
      "Generated 32 batches of size 32 in 2.399 sec\n",
      "Step 539653  [5.357 sec/step, loss=0.07383, avg_loss=0.07330]\n",
      "Step 539654  [5.365 sec/step, loss=0.07496, avg_loss=0.07332]\n",
      "Step 539655  [5.372 sec/step, loss=0.07557, avg_loss=0.07333]\n",
      "Step 539656  [5.371 sec/step, loss=0.07433, avg_loss=0.07333]\n",
      "Step 539657  [5.373 sec/step, loss=0.07569, avg_loss=0.07333]\n",
      "Step 539658  [5.360 sec/step, loss=0.07305, avg_loss=0.07331]\n",
      "Step 539659  [5.346 sec/step, loss=0.07376, avg_loss=0.07329]\n",
      "Step 539660  [5.360 sec/step, loss=0.07270, avg_loss=0.07330]\n",
      "Step 539661  [5.389 sec/step, loss=0.07545, avg_loss=0.07334]\n",
      "Step 539662  [5.391 sec/step, loss=0.07331, avg_loss=0.07336]\n",
      "Step 539663  [5.417 sec/step, loss=0.07375, avg_loss=0.07335]\n",
      "Step 539664  [5.399 sec/step, loss=0.07321, avg_loss=0.07334]\n",
      "Step 539665  [5.392 sec/step, loss=0.07087, avg_loss=0.07331]\n",
      "Step 539666  [5.419 sec/step, loss=0.07330, avg_loss=0.07339]\n",
      "Step 539667  [5.419 sec/step, loss=0.07577, avg_loss=0.07339]\n",
      "Step 539668  [5.416 sec/step, loss=0.07488, avg_loss=0.07340]\n",
      "Step 539669  [5.393 sec/step, loss=0.07357, avg_loss=0.07340]\n",
      "Step 539670  [5.409 sec/step, loss=0.07353, avg_loss=0.07341]\n",
      "Step 539671  [5.370 sec/step, loss=0.07558, avg_loss=0.07350]\n",
      "Step 539672  [5.372 sec/step, loss=0.07425, avg_loss=0.07350]\n",
      "Step 539673  [5.375 sec/step, loss=0.07540, avg_loss=0.07351]\n",
      "Step 539674  [5.363 sec/step, loss=0.07095, avg_loss=0.07347]\n",
      "Step 539675  [5.371 sec/step, loss=0.07522, avg_loss=0.07348]\n",
      "Step 539676  [5.357 sec/step, loss=0.07407, avg_loss=0.07346]\n",
      "Step 539677  [5.407 sec/step, loss=0.06587, avg_loss=0.07339]\n",
      "Step 539678  [5.395 sec/step, loss=0.07179, avg_loss=0.07336]\n",
      "Step 539679  [5.376 sec/step, loss=0.07366, avg_loss=0.07334]\n",
      "Step 539680  [5.375 sec/step, loss=0.07306, avg_loss=0.07333]\n",
      "Step 539681  [5.369 sec/step, loss=0.07373, avg_loss=0.07331]\n",
      "Step 539682  [5.348 sec/step, loss=0.07248, avg_loss=0.07329]\n",
      "Step 539683  [5.351 sec/step, loss=0.07580, avg_loss=0.07330]\n",
      "Step 539684  [5.353 sec/step, loss=0.07480, avg_loss=0.07331]\n",
      "Generated 32 batches of size 32 in 2.496 sec\n",
      "Step 539685  [5.366 sec/step, loss=0.07439, avg_loss=0.07332]\n",
      "Step 539686  [5.383 sec/step, loss=0.07559, avg_loss=0.07333]\n",
      "Step 539687  [5.377 sec/step, loss=0.07458, avg_loss=0.07336]\n",
      "Step 539688  [5.362 sec/step, loss=0.07006, avg_loss=0.07333]\n",
      "Step 539689  [5.390 sec/step, loss=0.07408, avg_loss=0.07336]\n",
      "Step 539690  [5.379 sec/step, loss=0.06513, avg_loss=0.07331]\n",
      "Step 539691  [5.385 sec/step, loss=0.07436, avg_loss=0.07330]\n",
      "Step 539692  [5.389 sec/step, loss=0.07381, avg_loss=0.07331]\n",
      "Step 539693  [5.380 sec/step, loss=0.07311, avg_loss=0.07333]\n",
      "Step 539694  [5.381 sec/step, loss=0.07426, avg_loss=0.07332]\n",
      "Step 539695  [5.441 sec/step, loss=0.06547, avg_loss=0.07327]\n",
      "Step 539696  [5.447 sec/step, loss=0.07329, avg_loss=0.07334]\n",
      "Step 539697  [5.453 sec/step, loss=0.07199, avg_loss=0.07335]\n",
      "Step 539698  [5.436 sec/step, loss=0.07439, avg_loss=0.07333]\n",
      "Step 539699  [5.452 sec/step, loss=0.07377, avg_loss=0.07341]\n",
      "Step 539700  [5.440 sec/step, loss=0.07236, avg_loss=0.07339]\n",
      "Writing summary at step: 539700\n",
      "Step 539701  [5.444 sec/step, loss=0.07442, avg_loss=0.07339]\n",
      "Step 539702  [5.456 sec/step, loss=0.07522, avg_loss=0.07340]\n",
      "Step 539703  [5.471 sec/step, loss=0.07407, avg_loss=0.07344]\n",
      "Step 539704  [5.484 sec/step, loss=0.07532, avg_loss=0.07346]\n",
      "Step 539705  [5.475 sec/step, loss=0.07296, avg_loss=0.07345]\n",
      "Step 539706  [5.474 sec/step, loss=0.07448, avg_loss=0.07344]\n",
      "Step 539707  [5.494 sec/step, loss=0.07526, avg_loss=0.07347]\n",
      "Step 539708  [5.485 sec/step, loss=0.07045, avg_loss=0.07347]\n",
      "Step 539709  [5.469 sec/step, loss=0.07459, avg_loss=0.07346]\n",
      "Step 539710  [5.467 sec/step, loss=0.06369, avg_loss=0.07338]\n",
      "Step 539711  [5.492 sec/step, loss=0.07502, avg_loss=0.07340]\n",
      "Step 539712  [5.470 sec/step, loss=0.07001, avg_loss=0.07334]\n",
      "Step 539713  [5.462 sec/step, loss=0.07259, avg_loss=0.07331]\n",
      "Step 539714  [5.458 sec/step, loss=0.07497, avg_loss=0.07332]\n",
      "Step 539715  [5.463 sec/step, loss=0.07521, avg_loss=0.07332]\n",
      "Generated 32 batches of size 32 in 2.403 sec\n",
      "Step 539716  [5.479 sec/step, loss=0.07572, avg_loss=0.07334]\n",
      "Step 539717  [5.482 sec/step, loss=0.07416, avg_loss=0.07334]\n",
      "Step 539718  [5.486 sec/step, loss=0.07501, avg_loss=0.07336]\n",
      "Step 539719  [5.483 sec/step, loss=0.07144, avg_loss=0.07334]\n",
      "Step 539720  [5.468 sec/step, loss=0.07221, avg_loss=0.07331]\n",
      "Step 539721  [5.469 sec/step, loss=0.07569, avg_loss=0.07330]\n",
      "Step 539722  [5.412 sec/step, loss=0.07352, avg_loss=0.07339]\n",
      "Step 539723  [5.427 sec/step, loss=0.07357, avg_loss=0.07337]\n",
      "Step 539724  [5.414 sec/step, loss=0.07403, avg_loss=0.07338]\n",
      "Step 539725  [5.401 sec/step, loss=0.06675, avg_loss=0.07331]\n",
      "Step 539726  [5.389 sec/step, loss=0.07365, avg_loss=0.07330]\n",
      "Step 539727  [5.413 sec/step, loss=0.06632, avg_loss=0.07322]\n",
      "Step 539728  [5.406 sec/step, loss=0.07345, avg_loss=0.07320]\n",
      "Step 539729  [5.413 sec/step, loss=0.07426, avg_loss=0.07322]\n",
      "Step 539730  [5.423 sec/step, loss=0.07498, avg_loss=0.07324]\n",
      "Step 539731  [5.423 sec/step, loss=0.07606, avg_loss=0.07326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 539732  [5.438 sec/step, loss=0.07476, avg_loss=0.07328]\n",
      "Step 539733  [5.455 sec/step, loss=0.07480, avg_loss=0.07328]\n",
      "Step 539734  [5.479 sec/step, loss=0.07526, avg_loss=0.07333]\n",
      "Step 539735  [5.479 sec/step, loss=0.07445, avg_loss=0.07332]\n",
      "Step 539736  [5.477 sec/step, loss=0.07520, avg_loss=0.07331]\n",
      "Step 539737  [5.467 sec/step, loss=0.07339, avg_loss=0.07330]\n",
      "Step 539738  [5.475 sec/step, loss=0.07630, avg_loss=0.07335]\n",
      "Step 539739  [5.471 sec/step, loss=0.07449, avg_loss=0.07334]\n",
      "Step 539740  [5.461 sec/step, loss=0.07418, avg_loss=0.07333]\n",
      "Step 539741  [5.467 sec/step, loss=0.07322, avg_loss=0.07332]\n",
      "Step 539742  [5.490 sec/step, loss=0.07533, avg_loss=0.07341]\n",
      "Step 539743  [5.480 sec/step, loss=0.07176, avg_loss=0.07338]\n",
      "Step 539744  [5.491 sec/step, loss=0.07469, avg_loss=0.07338]\n",
      "Step 539745  [5.451 sec/step, loss=0.07113, avg_loss=0.07338]\n",
      "Step 539746  [5.405 sec/step, loss=0.07348, avg_loss=0.07345]\n",
      "Step 539747  [5.406 sec/step, loss=0.07259, avg_loss=0.07344]\n",
      "Generated 32 batches of size 32 in 2.640 sec\n",
      "Step 539748  [5.403 sec/step, loss=0.07075, avg_loss=0.07344]\n",
      "Step 539749  [5.387 sec/step, loss=0.07220, avg_loss=0.07340]\n",
      "Step 539750  [5.412 sec/step, loss=0.07261, avg_loss=0.07337]\n",
      "Step 539751  [5.419 sec/step, loss=0.07367, avg_loss=0.07336]\n",
      "Step 539752  [5.408 sec/step, loss=0.07225, avg_loss=0.07334]\n",
      "Step 539753  [5.407 sec/step, loss=0.07316, avg_loss=0.07334]\n",
      "Step 539754  [5.420 sec/step, loss=0.07569, avg_loss=0.07334]\n",
      "Step 539755  [5.408 sec/step, loss=0.07413, avg_loss=0.07333]\n",
      "Step 539756  [5.411 sec/step, loss=0.07423, avg_loss=0.07333]\n",
      "Step 539757  [5.402 sec/step, loss=0.07589, avg_loss=0.07333]\n",
      "Step 539758  [5.413 sec/step, loss=0.07561, avg_loss=0.07336]\n",
      "Step 539759  [5.446 sec/step, loss=0.07321, avg_loss=0.07335]\n",
      "Step 539760  [5.432 sec/step, loss=0.07199, avg_loss=0.07334]\n",
      "Step 539761  [5.416 sec/step, loss=0.07524, avg_loss=0.07334]\n",
      "Step 539762  [5.423 sec/step, loss=0.07549, avg_loss=0.07336]\n",
      "Step 539763  [5.402 sec/step, loss=0.07445, avg_loss=0.07337]\n",
      "Step 539764  [5.408 sec/step, loss=0.07429, avg_loss=0.07338]\n",
      "Step 539765  [5.435 sec/step, loss=0.07376, avg_loss=0.07341]\n",
      "Step 539766  [5.418 sec/step, loss=0.07332, avg_loss=0.07341]\n",
      "Step 539767  [5.403 sec/step, loss=0.07327, avg_loss=0.07339]\n",
      "Step 539768  [5.392 sec/step, loss=0.07344, avg_loss=0.07337]\n",
      "Step 539769  [5.393 sec/step, loss=0.07445, avg_loss=0.07338]\n",
      "Step 539770  [5.362 sec/step, loss=0.07249, avg_loss=0.07337]\n",
      "Step 539771  [5.333 sec/step, loss=0.06520, avg_loss=0.07327]\n",
      "Step 539772  [5.323 sec/step, loss=0.07296, avg_loss=0.07325]\n",
      "Step 539773  [5.326 sec/step, loss=0.07337, avg_loss=0.07323]\n",
      "Step 539774  [5.334 sec/step, loss=0.07374, avg_loss=0.07326]\n",
      "Step 539775  [5.333 sec/step, loss=0.07544, avg_loss=0.07326]\n",
      "Step 539776  [5.342 sec/step, loss=0.07258, avg_loss=0.07325]\n",
      "Step 539777  [5.280 sec/step, loss=0.07102, avg_loss=0.07330]\n",
      "Step 539778  [5.278 sec/step, loss=0.07353, avg_loss=0.07332]\n",
      "Step 539779  [5.292 sec/step, loss=0.07439, avg_loss=0.07332]\n",
      "Generated 32 batches of size 32 in 2.278 sec\n",
      "Step 539780  [5.301 sec/step, loss=0.07178, avg_loss=0.07331]\n",
      "Step 539781  [5.308 sec/step, loss=0.07538, avg_loss=0.07333]\n",
      "Step 539782  [5.334 sec/step, loss=0.07301, avg_loss=0.07333]\n",
      "Step 539783  [5.336 sec/step, loss=0.07539, avg_loss=0.07333]\n",
      "Step 539784  [5.353 sec/step, loss=0.07605, avg_loss=0.07334]\n",
      "Step 539785  [5.398 sec/step, loss=0.06598, avg_loss=0.07326]\n",
      "Step 539786  [5.381 sec/step, loss=0.07209, avg_loss=0.07322]\n",
      "Step 539787  [5.381 sec/step, loss=0.07141, avg_loss=0.07319]\n",
      "Step 539788  [5.388 sec/step, loss=0.07297, avg_loss=0.07322]\n",
      "Step 539789  [5.371 sec/step, loss=0.07451, avg_loss=0.07322]\n",
      "Step 539790  [5.377 sec/step, loss=0.07033, avg_loss=0.07328]\n",
      "Step 539791  [5.371 sec/step, loss=0.07419, avg_loss=0.07327]\n",
      "Step 539792  [5.367 sec/step, loss=0.07055, avg_loss=0.07324]\n",
      "Step 539793  [5.375 sec/step, loss=0.07456, avg_loss=0.07326]\n",
      "Step 539794  [5.382 sec/step, loss=0.07411, avg_loss=0.07325]\n",
      "Step 539795  [5.340 sec/step, loss=0.07365, avg_loss=0.07334]\n",
      "Step 539796  [5.356 sec/step, loss=0.07544, avg_loss=0.07336]\n",
      "Step 539797  [5.347 sec/step, loss=0.07287, avg_loss=0.07337]\n",
      "Step 539798  [5.333 sec/step, loss=0.06544, avg_loss=0.07328]\n",
      "Step 539799  [5.347 sec/step, loss=0.07512, avg_loss=0.07329]\n",
      "Step 539800  [5.360 sec/step, loss=0.07517, avg_loss=0.07332]\n",
      "Writing summary at step: 539800\n",
      "Step 539801  [5.343 sec/step, loss=0.07367, avg_loss=0.07331]\n",
      "Step 539802  [5.330 sec/step, loss=0.07350, avg_loss=0.07329]\n",
      "Step 539803  [5.375 sec/step, loss=0.06415, avg_loss=0.07320]\n",
      "Step 539804  [5.356 sec/step, loss=0.07270, avg_loss=0.07317]\n",
      "Step 539805  [5.371 sec/step, loss=0.07552, avg_loss=0.07319]\n",
      "Step 539806  [5.363 sec/step, loss=0.07080, avg_loss=0.07316]\n",
      "Step 539807  [5.351 sec/step, loss=0.07432, avg_loss=0.07315]\n",
      "Step 539808  [5.369 sec/step, loss=0.07483, avg_loss=0.07319]\n",
      "Step 539809  [5.390 sec/step, loss=0.07523, avg_loss=0.07320]\n",
      "Step 539810  [5.402 sec/step, loss=0.07340, avg_loss=0.07330]\n",
      "Generated 32 batches of size 32 in 2.380 sec\n",
      "Step 539811  [5.395 sec/step, loss=0.07331, avg_loss=0.07328]\n",
      "Step 539812  [5.420 sec/step, loss=0.07462, avg_loss=0.07332]\n",
      "Step 539813  [5.413 sec/step, loss=0.07494, avg_loss=0.07335]\n",
      "Step 539814  [5.389 sec/step, loss=0.07084, avg_loss=0.07331]\n",
      "Step 539815  [5.384 sec/step, loss=0.07467, avg_loss=0.07330]\n",
      "Step 539816  [5.365 sec/step, loss=0.07285, avg_loss=0.07327]\n",
      "Step 539817  [5.364 sec/step, loss=0.07411, avg_loss=0.07327]\n",
      "Step 539818  [5.374 sec/step, loss=0.07565, avg_loss=0.07328]\n",
      "Step 539819  [5.401 sec/step, loss=0.07513, avg_loss=0.07332]\n",
      "Step 539820  [5.418 sec/step, loss=0.07560, avg_loss=0.07335]\n",
      "Step 539821  [5.432 sec/step, loss=0.07273, avg_loss=0.07332]\n",
      "Step 539822  [5.428 sec/step, loss=0.07054, avg_loss=0.07329]\n",
      "Step 539823  [5.414 sec/step, loss=0.07582, avg_loss=0.07331]\n",
      "Step 539824  [5.411 sec/step, loss=0.07268, avg_loss=0.07330]\n",
      "Step 539825  [5.426 sec/step, loss=0.07434, avg_loss=0.07338]\n",
      "Step 539826  [5.434 sec/step, loss=0.07183, avg_loss=0.07336]\n",
      "Step 539827  [5.380 sec/step, loss=0.07481, avg_loss=0.07344]\n",
      "Step 539828  [5.398 sec/step, loss=0.07568, avg_loss=0.07346]\n",
      "Step 539829  [5.392 sec/step, loss=0.07410, avg_loss=0.07346]\n",
      "Step 539830  [5.375 sec/step, loss=0.07067, avg_loss=0.07342]\n",
      "Step 539831  [5.376 sec/step, loss=0.07596, avg_loss=0.07342]\n",
      "Step 539832  [5.359 sec/step, loss=0.06600, avg_loss=0.07333]\n",
      "Step 539833  [5.393 sec/step, loss=0.06683, avg_loss=0.07325]\n",
      "Step 539834  [5.392 sec/step, loss=0.07393, avg_loss=0.07324]\n",
      "Step 539835  [5.388 sec/step, loss=0.07518, avg_loss=0.07325]\n",
      "Step 539836  [5.393 sec/step, loss=0.07519, avg_loss=0.07325]\n",
      "Step 539837  [5.392 sec/step, loss=0.07025, avg_loss=0.07321]\n",
      "Step 539838  [5.394 sec/step, loss=0.07408, avg_loss=0.07319]\n",
      "Step 539839  [5.400 sec/step, loss=0.07480, avg_loss=0.07319]\n",
      "Step 539840  [5.402 sec/step, loss=0.07478, avg_loss=0.07320]\n",
      "Step 539841  [5.394 sec/step, loss=0.07377, avg_loss=0.07321]\n",
      "Step 539842  [5.396 sec/step, loss=0.07481, avg_loss=0.07320]\n",
      "Generated 32 batches of size 32 in 2.388 sec\n",
      "Step 539843  [5.405 sec/step, loss=0.07464, avg_loss=0.07323]\n",
      "Step 539844  [5.412 sec/step, loss=0.07533, avg_loss=0.07324]\n",
      "Step 539845  [5.431 sec/step, loss=0.07415, avg_loss=0.07327]\n",
      "Step 539846  [5.423 sec/step, loss=0.07307, avg_loss=0.07326]\n",
      "Step 539847  [5.431 sec/step, loss=0.07529, avg_loss=0.07329]\n",
      "Step 539848  [5.453 sec/step, loss=0.07603, avg_loss=0.07334]\n",
      "Step 539849  [5.454 sec/step, loss=0.07338, avg_loss=0.07335]\n",
      "Step 539850  [5.414 sec/step, loss=0.07137, avg_loss=0.07334]\n",
      "Step 539851  [5.412 sec/step, loss=0.07479, avg_loss=0.07335]\n",
      "Step 539852  [5.405 sec/step, loss=0.07260, avg_loss=0.07336]\n",
      "Step 539853  [5.394 sec/step, loss=0.07467, avg_loss=0.07337]\n",
      "Step 539854  [5.367 sec/step, loss=0.07353, avg_loss=0.07335]\n",
      "Step 539855  [5.369 sec/step, loss=0.07444, avg_loss=0.07335]\n",
      "Step 539856  [5.383 sec/step, loss=0.07283, avg_loss=0.07334]\n",
      "Step 539857  [5.376 sec/step, loss=0.07516, avg_loss=0.07333]\n",
      "Step 539858  [5.380 sec/step, loss=0.07526, avg_loss=0.07333]\n",
      "Step 539859  [5.350 sec/step, loss=0.07297, avg_loss=0.07333]\n",
      "Step 539860  [5.373 sec/step, loss=0.07439, avg_loss=0.07335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 539861  [5.367 sec/step, loss=0.07438, avg_loss=0.07334]\n",
      "Step 539862  [5.367 sec/step, loss=0.07538, avg_loss=0.07334]\n",
      "Step 539863  [5.392 sec/step, loss=0.07259, avg_loss=0.07332]\n",
      "Step 539864  [5.401 sec/step, loss=0.07543, avg_loss=0.07333]\n",
      "Step 539865  [5.374 sec/step, loss=0.07102, avg_loss=0.07331]\n",
      "Step 539866  [5.380 sec/step, loss=0.07337, avg_loss=0.07331]\n",
      "Step 539867  [5.386 sec/step, loss=0.07384, avg_loss=0.07331]\n",
      "Step 539868  [5.380 sec/step, loss=0.06683, avg_loss=0.07325]\n",
      "Step 539869  [5.426 sec/step, loss=0.06637, avg_loss=0.07316]\n",
      "Step 539870  [5.434 sec/step, loss=0.07466, avg_loss=0.07319]\n",
      "Step 539871  [5.450 sec/step, loss=0.07118, avg_loss=0.07325]\n",
      "Step 539872  [5.453 sec/step, loss=0.07377, avg_loss=0.07325]\n",
      "Step 539873  [5.446 sec/step, loss=0.07461, avg_loss=0.07327]\n",
      "Step 539874  [5.452 sec/step, loss=0.07308, avg_loss=0.07326]\n",
      "Generated 32 batches of size 32 in 2.542 sec\n",
      "Step 539875  [5.438 sec/step, loss=0.07363, avg_loss=0.07324]\n",
      "Step 539876  [5.441 sec/step, loss=0.07356, avg_loss=0.07325]\n",
      "Step 539877  [5.465 sec/step, loss=0.07570, avg_loss=0.07330]\n",
      "Step 539878  [5.478 sec/step, loss=0.07536, avg_loss=0.07332]\n",
      "Step 539879  [5.477 sec/step, loss=0.07549, avg_loss=0.07333]\n",
      "Step 539880  [5.484 sec/step, loss=0.07382, avg_loss=0.07335]\n",
      "Step 539881  [5.461 sec/step, loss=0.07133, avg_loss=0.07331]\n",
      "Step 539882  [5.434 sec/step, loss=0.07383, avg_loss=0.07332]\n",
      "Step 539883  [5.440 sec/step, loss=0.07631, avg_loss=0.07332]\n",
      "Step 539884  [5.428 sec/step, loss=0.07527, avg_loss=0.07332]\n",
      "Step 539885  [5.369 sec/step, loss=0.07339, avg_loss=0.07339]\n",
      "Step 539886  [5.373 sec/step, loss=0.07453, avg_loss=0.07342]\n",
      "Step 539887  [5.379 sec/step, loss=0.07516, avg_loss=0.07345]\n",
      "Step 539888  [5.384 sec/step, loss=0.07296, avg_loss=0.07345]\n",
      "Step 539889  [5.399 sec/step, loss=0.07539, avg_loss=0.07346]\n",
      "Step 539890  [5.415 sec/step, loss=0.07614, avg_loss=0.07352]\n",
      "Step 539891  [5.420 sec/step, loss=0.07486, avg_loss=0.07353]\n",
      "Step 539892  [5.408 sec/step, loss=0.07140, avg_loss=0.07353]\n",
      "Step 539893  [5.417 sec/step, loss=0.07595, avg_loss=0.07355]\n",
      "Step 539894  [5.433 sec/step, loss=0.07479, avg_loss=0.07356]\n",
      "Step 539895  [5.414 sec/step, loss=0.07054, avg_loss=0.07352]\n",
      "Step 539896  [5.424 sec/step, loss=0.07388, avg_loss=0.07351]\n",
      "Step 539897  [5.437 sec/step, loss=0.07355, avg_loss=0.07352]\n",
      "Step 539898  [5.503 sec/step, loss=0.06486, avg_loss=0.07351]\n",
      "Step 539899  [5.509 sec/step, loss=0.07492, avg_loss=0.07351]\n",
      "Step 539900  [5.494 sec/step, loss=0.07339, avg_loss=0.07349]\n",
      "Writing summary at step: 539900\n",
      "Step 539901  [5.486 sec/step, loss=0.06453, avg_loss=0.07340]\n",
      "Step 539902  [5.478 sec/step, loss=0.07370, avg_loss=0.07340]\n",
      "Step 539903  [5.439 sec/step, loss=0.07400, avg_loss=0.07350]\n",
      "Step 539904  [5.440 sec/step, loss=0.07451, avg_loss=0.07352]\n",
      "Step 539905  [5.425 sec/step, loss=0.07191, avg_loss=0.07348]\n",
      "Generated 32 batches of size 32 in 2.520 sec\n",
      "Step 539906  [5.431 sec/step, loss=0.07490, avg_loss=0.07352]\n",
      "Step 539907  [5.428 sec/step, loss=0.07098, avg_loss=0.07349]\n",
      "Step 539908  [5.435 sec/step, loss=0.07563, avg_loss=0.07350]\n",
      "Step 539909  [5.420 sec/step, loss=0.07505, avg_loss=0.07350]\n",
      "Step 539910  [5.421 sec/step, loss=0.07433, avg_loss=0.07350]\n",
      "Step 539911  [5.413 sec/step, loss=0.07184, avg_loss=0.07349]\n",
      "Step 539912  [5.396 sec/step, loss=0.07351, avg_loss=0.07348]\n",
      "Step 539913  [5.399 sec/step, loss=0.07406, avg_loss=0.07347]\n",
      "Step 539914  [5.412 sec/step, loss=0.07341, avg_loss=0.07350]\n",
      "Step 539915  [5.416 sec/step, loss=0.07527, avg_loss=0.07350]\n",
      "Step 539916  [5.430 sec/step, loss=0.07457, avg_loss=0.07352]\n",
      "Step 539917  [5.476 sec/step, loss=0.06636, avg_loss=0.07344]\n",
      "Step 539918  [5.449 sec/step, loss=0.06573, avg_loss=0.07334]\n",
      "Step 539919  [5.429 sec/step, loss=0.07446, avg_loss=0.07334]\n",
      "Step 539920  [5.417 sec/step, loss=0.07476, avg_loss=0.07333]\n",
      "Step 539921  [5.399 sec/step, loss=0.07272, avg_loss=0.07333]\n",
      "Step 539922  [5.419 sec/step, loss=0.07602, avg_loss=0.07338]\n",
      "Step 539923  [5.410 sec/step, loss=0.07484, avg_loss=0.07337]\n",
      "Step 539924  [5.399 sec/step, loss=0.07183, avg_loss=0.07336]\n",
      "Step 539925  [5.409 sec/step, loss=0.07566, avg_loss=0.07338]\n",
      "Step 539926  [5.408 sec/step, loss=0.07200, avg_loss=0.07338]\n",
      "Step 539927  [5.426 sec/step, loss=0.07316, avg_loss=0.07336]\n",
      "Step 539928  [5.414 sec/step, loss=0.07232, avg_loss=0.07333]\n",
      "Step 539929  [5.435 sec/step, loss=0.07544, avg_loss=0.07334]\n",
      "Step 539930  [5.449 sec/step, loss=0.07443, avg_loss=0.07338]\n",
      "Step 539931  [5.437 sec/step, loss=0.07430, avg_loss=0.07336]\n",
      "Step 539932  [5.449 sec/step, loss=0.07456, avg_loss=0.07345]\n",
      "Step 539933  [5.401 sec/step, loss=0.07209, avg_loss=0.07350]\n",
      "Step 539934  [5.378 sec/step, loss=0.07091, avg_loss=0.07347]\n",
      "Step 539935  [5.371 sec/step, loss=0.07366, avg_loss=0.07346]\n",
      "Step 539936  [5.358 sec/step, loss=0.07413, avg_loss=0.07344]\n",
      "Step 539937  [5.357 sec/step, loss=0.07262, avg_loss=0.07347]\n",
      "Generated 32 batches of size 32 in 2.404 sec\n",
      "Step 539938  [5.357 sec/step, loss=0.07530, avg_loss=0.07348]\n",
      "Step 539939  [5.369 sec/step, loss=0.07586, avg_loss=0.07349]\n",
      "Step 539940  [5.361 sec/step, loss=0.07316, avg_loss=0.07347]\n",
      "Step 539941  [5.379 sec/step, loss=0.07328, avg_loss=0.07347]\n",
      "Step 539942  [5.382 sec/step, loss=0.07564, avg_loss=0.07348]\n",
      "Step 539943  [5.366 sec/step, loss=0.07339, avg_loss=0.07347]\n",
      "Step 539944  [5.385 sec/step, loss=0.07301, avg_loss=0.07344]\n",
      "Step 539945  [5.378 sec/step, loss=0.07147, avg_loss=0.07342]\n",
      "Step 539946  [5.382 sec/step, loss=0.07437, avg_loss=0.07343]\n",
      "Step 539947  [5.380 sec/step, loss=0.07348, avg_loss=0.07341]\n",
      "Step 539948  [5.373 sec/step, loss=0.07570, avg_loss=0.07341]\n",
      "Step 539949  [5.386 sec/step, loss=0.07453, avg_loss=0.07342]\n",
      "Step 539950  [5.423 sec/step, loss=0.07248, avg_loss=0.07343]\n",
      "Step 539951  [5.422 sec/step, loss=0.07506, avg_loss=0.07343]\n",
      "Step 539952  [5.418 sec/step, loss=0.06966, avg_loss=0.07340]\n",
      "Step 539953  [5.422 sec/step, loss=0.07434, avg_loss=0.07340]\n",
      "Step 539954  [5.430 sec/step, loss=0.07108, avg_loss=0.07338]\n",
      "Step 539955  [5.441 sec/step, loss=0.07577, avg_loss=0.07339]\n",
      "Step 539956  [5.424 sec/step, loss=0.07401, avg_loss=0.07340]\n",
      "Step 539957  [5.421 sec/step, loss=0.07502, avg_loss=0.07340]\n",
      "Step 539958  [5.415 sec/step, loss=0.07434, avg_loss=0.07339]\n",
      "Step 539959  [5.411 sec/step, loss=0.07280, avg_loss=0.07339]\n",
      "Step 539960  [5.414 sec/step, loss=0.07318, avg_loss=0.07338]\n",
      "Step 539961  [5.405 sec/step, loss=0.06995, avg_loss=0.07333]\n",
      "Step 539962  [5.397 sec/step, loss=0.07049, avg_loss=0.07328]\n",
      "Step 539963  [5.360 sec/step, loss=0.07342, avg_loss=0.07329]\n",
      "Step 539964  [5.361 sec/step, loss=0.07552, avg_loss=0.07329]\n",
      "Step 539965  [5.372 sec/step, loss=0.07240, avg_loss=0.07331]\n",
      "Step 539966  [5.386 sec/step, loss=0.07570, avg_loss=0.07333]\n",
      "Step 539967  [5.396 sec/step, loss=0.07388, avg_loss=0.07333]\n",
      "Step 539968  [5.417 sec/step, loss=0.07435, avg_loss=0.07341]\n",
      "Step 539969  [5.364 sec/step, loss=0.07432, avg_loss=0.07348]\n",
      "Generated 32 batches of size 32 in 2.334 sec\n",
      "Step 539970  [5.372 sec/step, loss=0.07475, avg_loss=0.07349]\n",
      "Step 539971  [5.357 sec/step, loss=0.06466, avg_loss=0.07342]\n",
      "Step 539972  [5.375 sec/step, loss=0.07474, avg_loss=0.07343]\n",
      "Step 539973  [5.367 sec/step, loss=0.07305, avg_loss=0.07341]\n",
      "Step 539974  [5.364 sec/step, loss=0.07478, avg_loss=0.07343]\n",
      "Step 539975  [5.419 sec/step, loss=0.06587, avg_loss=0.07335]\n",
      "Step 539976  [5.419 sec/step, loss=0.07532, avg_loss=0.07337]\n",
      "Step 539977  [5.399 sec/step, loss=0.07374, avg_loss=0.07335]\n",
      "Step 539978  [5.392 sec/step, loss=0.07476, avg_loss=0.07335]\n",
      "Step 539979  [5.380 sec/step, loss=0.07126, avg_loss=0.07330]\n",
      "Step 539980  [5.376 sec/step, loss=0.07563, avg_loss=0.07332]\n",
      "Step 539981  [5.399 sec/step, loss=0.07516, avg_loss=0.07336]\n",
      "Step 539982  [5.415 sec/step, loss=0.07532, avg_loss=0.07337]\n",
      "Step 539983  [5.405 sec/step, loss=0.07380, avg_loss=0.07335]\n",
      "Step 539984  [5.415 sec/step, loss=0.07498, avg_loss=0.07335]\n",
      "Step 539985  [5.472 sec/step, loss=0.06562, avg_loss=0.07327]\n",
      "Step 539986  [5.473 sec/step, loss=0.07464, avg_loss=0.07327]\n",
      "Step 539987  [5.486 sec/step, loss=0.07475, avg_loss=0.07327]\n",
      "Step 539988  [5.488 sec/step, loss=0.07540, avg_loss=0.07329]\n",
      "Step 539989  [5.480 sec/step, loss=0.07404, avg_loss=0.07328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 539990  [5.486 sec/step, loss=0.07278, avg_loss=0.07324]\n",
      "Step 539991  [5.490 sec/step, loss=0.07605, avg_loss=0.07326]\n",
      "Step 539992  [5.504 sec/step, loss=0.07372, avg_loss=0.07328]\n",
      "Step 539993  [5.481 sec/step, loss=0.07187, avg_loss=0.07324]\n",
      "Step 539994  [5.461 sec/step, loss=0.07454, avg_loss=0.07324]\n",
      "Step 539995  [5.468 sec/step, loss=0.07495, avg_loss=0.07328]\n",
      "Step 539996  [5.460 sec/step, loss=0.07491, avg_loss=0.07329]\n",
      "Step 539997  [5.452 sec/step, loss=0.07400, avg_loss=0.07329]\n",
      "Step 539998  [5.395 sec/step, loss=0.07341, avg_loss=0.07338]\n",
      "Step 539999  [5.376 sec/step, loss=0.07429, avg_loss=0.07337]\n",
      "Step 540000  [5.380 sec/step, loss=0.07351, avg_loss=0.07337]\n",
      "Writing summary at step: 540000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-540000\n",
      "Saving audio and alignment...\n",
      "Generated 32 batches of size 32 in 1.889 sec\n",
      "Input: shuuur kay ziir asar naygaytiv tdasuurutdaatd patdhaaloodzist kay tdadzziiay kayliay ayk gutdhtdhii hae~___________\n",
      "Step 540001  [5.391 sec/step, loss=0.07269, avg_loss=0.07346]\n",
      "Step 540002  [5.402 sec/step, loss=0.07441, avg_loss=0.07346]\n",
      "Step 540003  [5.374 sec/step, loss=0.06677, avg_loss=0.07339]\n",
      "Step 540004  [5.364 sec/step, loss=0.07345, avg_loss=0.07338]\n",
      "Step 540005  [5.365 sec/step, loss=0.07175, avg_loss=0.07338]\n",
      "Step 540006  [5.388 sec/step, loss=0.07270, avg_loss=0.07336]\n",
      "Step 540007  [5.398 sec/step, loss=0.07389, avg_loss=0.07339]\n",
      "Step 540008  [5.380 sec/step, loss=0.07136, avg_loss=0.07334]\n",
      "Step 540009  [5.365 sec/step, loss=0.06978, avg_loss=0.07329]\n",
      "Step 540010  [5.364 sec/step, loss=0.07046, avg_loss=0.07325]\n",
      "Step 540011  [5.368 sec/step, loss=0.07438, avg_loss=0.07328]\n",
      "Step 540012  [5.382 sec/step, loss=0.07397, avg_loss=0.07328]\n",
      "Step 540013  [5.370 sec/step, loss=0.07170, avg_loss=0.07326]\n",
      "Step 540014  [5.369 sec/step, loss=0.07346, avg_loss=0.07326]\n",
      "Step 540015  [5.379 sec/step, loss=0.07540, avg_loss=0.07326]\n",
      "Step 540016  [5.379 sec/step, loss=0.07214, avg_loss=0.07324]\n",
      "Step 540017  [5.339 sec/step, loss=0.07548, avg_loss=0.07333]\n",
      "Step 540018  [5.358 sec/step, loss=0.07387, avg_loss=0.07341]\n",
      "Step 540019  [5.342 sec/step, loss=0.06987, avg_loss=0.07336]\n",
      "Step 540020  [5.352 sec/step, loss=0.07490, avg_loss=0.07336]\n",
      "Step 540021  [5.341 sec/step, loss=0.07460, avg_loss=0.07338]\n",
      "Step 540022  [5.344 sec/step, loss=0.07413, avg_loss=0.07336]\n",
      "Step 540023  [5.338 sec/step, loss=0.07458, avg_loss=0.07336]\n",
      "Step 540024  [5.359 sec/step, loss=0.07544, avg_loss=0.07340]\n",
      "Step 540025  [5.401 sec/step, loss=0.06635, avg_loss=0.07330]\n",
      "Step 540026  [5.404 sec/step, loss=0.07483, avg_loss=0.07333]\n",
      "Step 540027  [5.411 sec/step, loss=0.07306, avg_loss=0.07333]\n",
      "Step 540028  [5.411 sec/step, loss=0.07267, avg_loss=0.07333]\n",
      "Step 540029  [5.398 sec/step, loss=0.07437, avg_loss=0.07332]\n",
      "Step 540030  [5.397 sec/step, loss=0.07182, avg_loss=0.07330]\n",
      "Step 540031  [5.404 sec/step, loss=0.07571, avg_loss=0.07331]\n",
      "Generated 32 batches of size 32 in 2.497 sec\n",
      "Step 540032  [5.413 sec/step, loss=0.07291, avg_loss=0.07330]\n",
      "Step 540033  [5.410 sec/step, loss=0.07438, avg_loss=0.07332]\n",
      "Step 540034  [5.436 sec/step, loss=0.07614, avg_loss=0.07337]\n",
      "Step 540035  [5.475 sec/step, loss=0.07316, avg_loss=0.07337]\n",
      "Step 540036  [5.461 sec/step, loss=0.07330, avg_loss=0.07336]\n",
      "Step 540037  [5.460 sec/step, loss=0.07346, avg_loss=0.07337]\n",
      "Step 540038  [5.435 sec/step, loss=0.06373, avg_loss=0.07325]\n",
      "Step 540039  [5.429 sec/step, loss=0.07583, avg_loss=0.07325]\n",
      "Step 540040  [5.429 sec/step, loss=0.07263, avg_loss=0.07324]\n",
      "Step 540041  [5.408 sec/step, loss=0.07457, avg_loss=0.07326]\n",
      "Step 540042  [5.405 sec/step, loss=0.07524, avg_loss=0.07325]\n",
      "Step 540043  [5.413 sec/step, loss=0.07426, avg_loss=0.07326]\n",
      "Step 540044  [5.382 sec/step, loss=0.07271, avg_loss=0.07326]\n",
      "Step 540045  [5.377 sec/step, loss=0.07318, avg_loss=0.07328]\n",
      "Step 540046  [5.393 sec/step, loss=0.07410, avg_loss=0.07327]\n",
      "Step 540047  [5.409 sec/step, loss=0.07433, avg_loss=0.07328]\n",
      "Step 540048  [5.408 sec/step, loss=0.07659, avg_loss=0.07329]\n",
      "Step 540049  [5.412 sec/step, loss=0.07530, avg_loss=0.07330]\n",
      "Step 540050  [5.393 sec/step, loss=0.07423, avg_loss=0.07332]\n",
      "Step 540051  [5.375 sec/step, loss=0.06604, avg_loss=0.07323]\n",
      "Step 540052  [5.383 sec/step, loss=0.07259, avg_loss=0.07326]\n",
      "Step 540053  [5.380 sec/step, loss=0.07145, avg_loss=0.07323]\n",
      "Step 540054  [5.387 sec/step, loss=0.07412, avg_loss=0.07326]\n",
      "Step 540055  [5.390 sec/step, loss=0.07507, avg_loss=0.07325]\n",
      "Step 540056  [5.394 sec/step, loss=0.07350, avg_loss=0.07324]\n",
      "Step 540057  [5.389 sec/step, loss=0.07316, avg_loss=0.07323]\n",
      "Step 540058  [5.412 sec/step, loss=0.07433, avg_loss=0.07323]\n",
      "Step 540059  [5.429 sec/step, loss=0.07250, avg_loss=0.07322]\n",
      "Step 540060  [5.416 sec/step, loss=0.07280, avg_loss=0.07322]\n",
      "Step 540061  [5.442 sec/step, loss=0.07531, avg_loss=0.07327]\n",
      "Step 540062  [5.431 sec/step, loss=0.07197, avg_loss=0.07329]\n",
      "Step 540063  [5.442 sec/step, loss=0.07467, avg_loss=0.07330]\n",
      "Generated 32 batches of size 32 in 2.464 sec\n",
      "Step 540064  [5.442 sec/step, loss=0.07447, avg_loss=0.07329]\n",
      "Step 540065  [5.434 sec/step, loss=0.07359, avg_loss=0.07330]\n",
      "Step 540066  [5.430 sec/step, loss=0.07625, avg_loss=0.07331]\n",
      "Step 540067  [5.424 sec/step, loss=0.07460, avg_loss=0.07331]\n",
      "Step 540068  [5.428 sec/step, loss=0.07262, avg_loss=0.07330]\n",
      "Step 540069  [5.429 sec/step, loss=0.07459, avg_loss=0.07330]\n",
      "Step 540070  [5.469 sec/step, loss=0.06486, avg_loss=0.07320]\n",
      "Step 540071  [5.488 sec/step, loss=0.07061, avg_loss=0.07326]\n",
      "Step 540072  [5.458 sec/step, loss=0.07172, avg_loss=0.07323]\n",
      "Step 540073  [5.491 sec/step, loss=0.07224, avg_loss=0.07322]\n",
      "Step 540074  [5.482 sec/step, loss=0.07310, avg_loss=0.07321]\n",
      "Step 540075  [5.437 sec/step, loss=0.07421, avg_loss=0.07329]\n",
      "Step 540076  [5.423 sec/step, loss=0.07309, avg_loss=0.07327]\n",
      "Step 540077  [5.426 sec/step, loss=0.07151, avg_loss=0.07324]\n",
      "Step 540078  [5.433 sec/step, loss=0.07457, avg_loss=0.07324]\n",
      "Step 540079  [5.441 sec/step, loss=0.07065, avg_loss=0.07324]\n",
      "Step 540080  [5.438 sec/step, loss=0.07406, avg_loss=0.07322]\n",
      "Step 540081  [5.427 sec/step, loss=0.07493, avg_loss=0.07322]\n",
      "Step 540082  [5.401 sec/step, loss=0.06491, avg_loss=0.07311]\n",
      "Step 540083  [5.384 sec/step, loss=0.07068, avg_loss=0.07308]\n",
      "Step 540084  [5.368 sec/step, loss=0.07496, avg_loss=0.07308]\n",
      "Step 540085  [5.327 sec/step, loss=0.07605, avg_loss=0.07319]\n",
      "Step 540086  [5.327 sec/step, loss=0.07423, avg_loss=0.07318]\n",
      "Step 540087  [5.311 sec/step, loss=0.07490, avg_loss=0.07318]\n",
      "Step 540088  [5.298 sec/step, loss=0.07384, avg_loss=0.07317]\n",
      "Step 540089  [5.305 sec/step, loss=0.07552, avg_loss=0.07318]\n",
      "Step 540090  [5.290 sec/step, loss=0.07447, avg_loss=0.07320]\n",
      "Step 540091  [5.299 sec/step, loss=0.07512, avg_loss=0.07319]\n",
      "Step 540092  [5.308 sec/step, loss=0.07515, avg_loss=0.07321]\n",
      "Step 540093  [5.328 sec/step, loss=0.07473, avg_loss=0.07323]\n",
      "Step 540094  [5.335 sec/step, loss=0.07514, avg_loss=0.07324]\n",
      "Step 540095  [5.353 sec/step, loss=0.07536, avg_loss=0.07324]\n",
      "Generated 32 batches of size 32 in 2.538 sec\n",
      "Step 540096  [5.343 sec/step, loss=0.07280, avg_loss=0.07322]\n",
      "Step 540097  [5.347 sec/step, loss=0.07102, avg_loss=0.07319]\n",
      "Step 540098  [5.344 sec/step, loss=0.07338, avg_loss=0.07319]\n",
      "Step 540099  [5.396 sec/step, loss=0.06551, avg_loss=0.07311]\n",
      "Step 540100  [5.398 sec/step, loss=0.07361, avg_loss=0.07311]\n",
      "Writing summary at step: 540100\n",
      "Step 540101  [5.419 sec/step, loss=0.07424, avg_loss=0.07312]\n",
      "Step 540102  [5.431 sec/step, loss=0.07264, avg_loss=0.07310]\n",
      "Step 540103  [5.453 sec/step, loss=0.07422, avg_loss=0.07318]\n",
      "Step 540104  [5.464 sec/step, loss=0.07410, avg_loss=0.07318]\n",
      "Step 540105  [5.449 sec/step, loss=0.06640, avg_loss=0.07313]\n",
      "Step 540106  [5.438 sec/step, loss=0.07614, avg_loss=0.07317]\n",
      "Step 540107  [5.423 sec/step, loss=0.07331, avg_loss=0.07316]\n",
      "Step 540108  [5.456 sec/step, loss=0.07306, avg_loss=0.07318]\n",
      "Step 540109  [5.477 sec/step, loss=0.07506, avg_loss=0.07323]\n",
      "Step 540110  [5.492 sec/step, loss=0.07560, avg_loss=0.07328]\n",
      "Step 540111  [5.484 sec/step, loss=0.07270, avg_loss=0.07326]\n",
      "Step 540112  [5.484 sec/step, loss=0.07547, avg_loss=0.07328]\n",
      "Step 540113  [5.489 sec/step, loss=0.07195, avg_loss=0.07328]\n",
      "Step 540114  [5.489 sec/step, loss=0.07312, avg_loss=0.07328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 540115  [5.478 sec/step, loss=0.07379, avg_loss=0.07326]\n",
      "Step 540116  [5.520 sec/step, loss=0.06608, avg_loss=0.07320]\n",
      "Step 540117  [5.512 sec/step, loss=0.07494, avg_loss=0.07320]\n",
      "Step 540118  [5.508 sec/step, loss=0.07424, avg_loss=0.07320]\n",
      "Step 540119  [5.533 sec/step, loss=0.07283, avg_loss=0.07323]\n",
      "Step 540120  [5.523 sec/step, loss=0.07453, avg_loss=0.07323]\n",
      "Step 540121  [5.529 sec/step, loss=0.07565, avg_loss=0.07324]\n",
      "Step 540122  [5.510 sec/step, loss=0.07000, avg_loss=0.07320]\n",
      "Step 540123  [5.515 sec/step, loss=0.07409, avg_loss=0.07319]\n",
      "Step 540124  [5.513 sec/step, loss=0.07389, avg_loss=0.07317]\n",
      "Step 540125  [5.473 sec/step, loss=0.07333, avg_loss=0.07324]\n",
      "Step 540126  [5.469 sec/step, loss=0.07404, avg_loss=0.07324]\n",
      "Generated 32 batches of size 32 in 2.400 sec\n",
      "Step 540127  [5.475 sec/step, loss=0.07287, avg_loss=0.07323]\n",
      "Step 540128  [5.490 sec/step, loss=0.07368, avg_loss=0.07324]\n",
      "Step 540129  [5.475 sec/step, loss=0.07193, avg_loss=0.07322]\n",
      "Step 540130  [5.476 sec/step, loss=0.07493, avg_loss=0.07325]\n",
      "Step 540131  [5.477 sec/step, loss=0.07363, avg_loss=0.07323]\n",
      "Step 540132  [5.469 sec/step, loss=0.07302, avg_loss=0.07323]\n",
      "Step 540133  [5.464 sec/step, loss=0.07354, avg_loss=0.07322]\n",
      "Step 540134  [5.454 sec/step, loss=0.07441, avg_loss=0.07321]\n",
      "Step 540135  [5.415 sec/step, loss=0.07136, avg_loss=0.07319]\n",
      "Step 540136  [5.429 sec/step, loss=0.07446, avg_loss=0.07320]\n",
      "Step 540137  [5.445 sec/step, loss=0.07525, avg_loss=0.07322]\n",
      "Step 540138  [5.442 sec/step, loss=0.06579, avg_loss=0.07324]\n",
      "Step 540139  [5.484 sec/step, loss=0.06514, avg_loss=0.07313]\n",
      "Step 540140  [5.486 sec/step, loss=0.07340, avg_loss=0.07314]\n",
      "Step 540141  [5.497 sec/step, loss=0.07438, avg_loss=0.07314]\n",
      "Step 540142  [5.488 sec/step, loss=0.07398, avg_loss=0.07312]\n",
      "Step 540143  [5.477 sec/step, loss=0.07304, avg_loss=0.07311]\n",
      "Step 540144  [5.492 sec/step, loss=0.07561, avg_loss=0.07314]\n",
      "Step 540145  [5.486 sec/step, loss=0.07083, avg_loss=0.07312]\n",
      "Step 540146  [5.484 sec/step, loss=0.07506, avg_loss=0.07313]\n",
      "Step 540147  [5.475 sec/step, loss=0.07543, avg_loss=0.07314]\n",
      "Step 540148  [5.462 sec/step, loss=0.07236, avg_loss=0.07310]\n",
      "Step 540149  [5.475 sec/step, loss=0.07495, avg_loss=0.07309]\n",
      "Step 540150  [5.481 sec/step, loss=0.07541, avg_loss=0.07310]\n",
      "Step 540151  [5.486 sec/step, loss=0.07023, avg_loss=0.07315]\n",
      "Step 540152  [5.487 sec/step, loss=0.07458, avg_loss=0.07317]\n",
      "Step 540153  [5.504 sec/step, loss=0.07531, avg_loss=0.07320]\n",
      "Step 540154  [5.496 sec/step, loss=0.07189, avg_loss=0.07318]\n",
      "Step 540155  [5.489 sec/step, loss=0.07528, avg_loss=0.07318]\n",
      "Step 540156  [5.496 sec/step, loss=0.07272, avg_loss=0.07318]\n",
      "Step 540157  [5.499 sec/step, loss=0.07331, avg_loss=0.07318]\n",
      "Step 540158  [5.491 sec/step, loss=0.07456, avg_loss=0.07318]\n",
      "Generated 32 batches of size 32 in 2.555 sec\n",
      "Step 540159  [5.486 sec/step, loss=0.07345, avg_loss=0.07319]\n",
      "Step 540160  [5.490 sec/step, loss=0.07492, avg_loss=0.07321]\n",
      "Step 540161  [5.483 sec/step, loss=0.07386, avg_loss=0.07320]\n",
      "Step 540162  [5.492 sec/step, loss=0.07454, avg_loss=0.07322]\n",
      "Step 540163  [5.497 sec/step, loss=0.07172, avg_loss=0.07319]\n",
      "Step 540164  [5.491 sec/step, loss=0.07484, avg_loss=0.07320]\n",
      "Step 540165  [5.505 sec/step, loss=0.07206, avg_loss=0.07318]\n",
      "Step 540166  [5.489 sec/step, loss=0.07308, avg_loss=0.07315]\n",
      "Step 540167  [5.492 sec/step, loss=0.07512, avg_loss=0.07315]\n",
      "Step 540168  [5.485 sec/step, loss=0.07152, avg_loss=0.07314]\n",
      "Step 540169  [5.496 sec/step, loss=0.07457, avg_loss=0.07314]\n",
      "Step 540170  [5.454 sec/step, loss=0.07558, avg_loss=0.07325]\n",
      "Step 540171  [5.460 sec/step, loss=0.07515, avg_loss=0.07330]\n",
      "Step 540172  [5.474 sec/step, loss=0.07396, avg_loss=0.07332]\n",
      "Step 540173  [5.436 sec/step, loss=0.07330, avg_loss=0.07333]\n",
      "Step 540174  [5.440 sec/step, loss=0.07464, avg_loss=0.07334]\n",
      "Step 540175  [5.432 sec/step, loss=0.07003, avg_loss=0.07330]\n",
      "Step 540176  [5.444 sec/step, loss=0.07289, avg_loss=0.07330]\n",
      "Step 540177  [5.441 sec/step, loss=0.07218, avg_loss=0.07331]\n",
      "Step 540178  [5.416 sec/step, loss=0.07096, avg_loss=0.07327]\n",
      "Step 540179  [5.432 sec/step, loss=0.07565, avg_loss=0.07332]\n",
      "Step 540180  [5.455 sec/step, loss=0.07325, avg_loss=0.07331]\n",
      "Step 540181  [5.435 sec/step, loss=0.06475, avg_loss=0.07321]\n",
      "Step 540182  [5.454 sec/step, loss=0.07484, avg_loss=0.07331]\n",
      "Step 540183  [5.468 sec/step, loss=0.07152, avg_loss=0.07332]\n",
      "Step 540184  [5.485 sec/step, loss=0.07528, avg_loss=0.07332]\n",
      "Step 540185  [5.479 sec/step, loss=0.07423, avg_loss=0.07330]\n",
      "Step 540186  [5.467 sec/step, loss=0.07143, avg_loss=0.07328]\n",
      "Step 540187  [5.477 sec/step, loss=0.07540, avg_loss=0.07328]\n",
      "Step 540188  [5.470 sec/step, loss=0.07291, avg_loss=0.07327]\n",
      "Step 540189  [5.471 sec/step, loss=0.07294, avg_loss=0.07325]\n",
      "Step 540190  [5.470 sec/step, loss=0.07490, avg_loss=0.07325]\n",
      "Generated 32 batches of size 32 in 2.421 sec\n",
      "Step 540191  [5.459 sec/step, loss=0.07499, avg_loss=0.07325]\n",
      "Step 540192  [5.447 sec/step, loss=0.07209, avg_loss=0.07322]\n",
      "Step 540193  [5.460 sec/step, loss=0.07239, avg_loss=0.07319]\n",
      "Step 540194  [5.452 sec/step, loss=0.07391, avg_loss=0.07318]\n",
      "Step 540195  [5.439 sec/step, loss=0.07444, avg_loss=0.07317]\n",
      "Step 540196  [5.447 sec/step, loss=0.07334, avg_loss=0.07318]\n",
      "Step 540197  [5.456 sec/step, loss=0.07329, avg_loss=0.07320]\n",
      "Step 540198  [5.517 sec/step, loss=0.06562, avg_loss=0.07312]\n",
      "Step 540199  [5.463 sec/step, loss=0.07258, avg_loss=0.07319]\n",
      "Step 540200  [5.459 sec/step, loss=0.07330, avg_loss=0.07319]\n",
      "Writing summary at step: 540200\n",
      "Step 540201  [5.455 sec/step, loss=0.07562, avg_loss=0.07321]\n",
      "Step 540202  [5.444 sec/step, loss=0.07514, avg_loss=0.07323]\n",
      "Step 540203  [5.449 sec/step, loss=0.07586, avg_loss=0.07325]\n",
      "Step 540204  [5.457 sec/step, loss=0.07516, avg_loss=0.07326]\n",
      "Step 540205  [5.475 sec/step, loss=0.07423, avg_loss=0.07334]\n",
      "Step 540206  [5.462 sec/step, loss=0.07460, avg_loss=0.07332]\n",
      "Step 540207  [5.474 sec/step, loss=0.07390, avg_loss=0.07333]\n",
      "Step 540208  [5.462 sec/step, loss=0.07432, avg_loss=0.07334]\n",
      "Step 540209  [5.447 sec/step, loss=0.07327, avg_loss=0.07332]\n",
      "Step 540210  [5.426 sec/step, loss=0.07006, avg_loss=0.07327]\n",
      "Step 540211  [5.442 sec/step, loss=0.07544, avg_loss=0.07329]\n",
      "Step 540212  [5.431 sec/step, loss=0.07368, avg_loss=0.07327]\n",
      "Step 540213  [5.432 sec/step, loss=0.07300, avg_loss=0.07329]\n",
      "Step 540214  [5.447 sec/step, loss=0.07532, avg_loss=0.07331]\n",
      "Step 540215  [5.434 sec/step, loss=0.07131, avg_loss=0.07328]\n",
      "Step 540216  [5.381 sec/step, loss=0.07506, avg_loss=0.07337]\n",
      "Step 540217  [5.391 sec/step, loss=0.07533, avg_loss=0.07338]\n",
      "Step 540218  [5.397 sec/step, loss=0.07418, avg_loss=0.07338]\n",
      "Step 540219  [5.380 sec/step, loss=0.07307, avg_loss=0.07338]\n",
      "Step 540220  [5.381 sec/step, loss=0.07267, avg_loss=0.07336]\n",
      "Step 540221  [5.383 sec/step, loss=0.07445, avg_loss=0.07335]\n",
      "Generated 32 batches of size 32 in 2.424 sec\n",
      "Step 540222  [5.422 sec/step, loss=0.07369, avg_loss=0.07338]\n",
      "Step 540223  [5.418 sec/step, loss=0.07462, avg_loss=0.07339]\n",
      "Step 540224  [5.465 sec/step, loss=0.06504, avg_loss=0.07330]\n",
      "Step 540225  [5.472 sec/step, loss=0.07445, avg_loss=0.07331]\n",
      "Step 540226  [5.467 sec/step, loss=0.07363, avg_loss=0.07331]\n",
      "Step 540227  [5.425 sec/step, loss=0.06635, avg_loss=0.07324]\n",
      "Step 540228  [5.417 sec/step, loss=0.07393, avg_loss=0.07325]\n",
      "Step 540229  [5.436 sec/step, loss=0.07434, avg_loss=0.07327]\n",
      "Step 540230  [5.436 sec/step, loss=0.07436, avg_loss=0.07326]\n",
      "Step 540231  [5.425 sec/step, loss=0.07288, avg_loss=0.07326]\n",
      "Step 540232  [5.423 sec/step, loss=0.07089, avg_loss=0.07324]\n",
      "Step 540233  [5.438 sec/step, loss=0.07500, avg_loss=0.07325]\n",
      "Step 540234  [5.417 sec/step, loss=0.06550, avg_loss=0.07316]\n",
      "Step 540235  [5.441 sec/step, loss=0.07324, avg_loss=0.07318]\n",
      "Step 540236  [5.437 sec/step, loss=0.07492, avg_loss=0.07318]\n",
      "Step 540237  [5.424 sec/step, loss=0.07282, avg_loss=0.07316]\n",
      "Step 540238  [5.435 sec/step, loss=0.07329, avg_loss=0.07323]\n",
      "Step 540239  [5.388 sec/step, loss=0.07483, avg_loss=0.07333]\n",
      "Step 540240  [5.407 sec/step, loss=0.07539, avg_loss=0.07335]\n",
      "Step 540241  [5.425 sec/step, loss=0.07262, avg_loss=0.07333]\n",
      "Step 540242  [5.414 sec/step, loss=0.07329, avg_loss=0.07333]\n",
      "Step 540243  [5.435 sec/step, loss=0.07323, avg_loss=0.07333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 540244  [5.423 sec/step, loss=0.07337, avg_loss=0.07331]\n",
      "Step 540245  [5.487 sec/step, loss=0.06649, avg_loss=0.07326]\n",
      "Step 540246  [5.467 sec/step, loss=0.07449, avg_loss=0.07326]\n",
      "Step 540247  [5.468 sec/step, loss=0.07541, avg_loss=0.07326]\n",
      "Step 540248  [5.472 sec/step, loss=0.07398, avg_loss=0.07327]\n",
      "Step 540249  [5.452 sec/step, loss=0.07437, avg_loss=0.07327]\n",
      "Step 540250  [5.443 sec/step, loss=0.07522, avg_loss=0.07327]\n",
      "Step 540251  [5.469 sec/step, loss=0.07551, avg_loss=0.07332]\n",
      "Step 540252  [5.471 sec/step, loss=0.07376, avg_loss=0.07331]\n",
      "Step 540253  [5.465 sec/step, loss=0.07440, avg_loss=0.07330]\n",
      "Generated 32 batches of size 32 in 2.882 sec\n",
      "Step 540254  [5.459 sec/step, loss=0.07083, avg_loss=0.07329]\n",
      "Step 540255  [5.453 sec/step, loss=0.07437, avg_loss=0.07328]\n",
      "Step 540256  [5.431 sec/step, loss=0.06989, avg_loss=0.07325]\n",
      "Step 540257  [5.458 sec/step, loss=0.07338, avg_loss=0.07325]\n",
      "Step 540258  [5.438 sec/step, loss=0.07430, avg_loss=0.07325]\n",
      "Step 540259  [5.443 sec/step, loss=0.07544, avg_loss=0.07327]\n",
      "Step 540260  [5.449 sec/step, loss=0.07490, avg_loss=0.07327]\n",
      "Step 540261  [5.437 sec/step, loss=0.07170, avg_loss=0.07325]\n",
      "Step 540262  [5.442 sec/step, loss=0.07284, avg_loss=0.07323]\n",
      "Step 540263  [5.450 sec/step, loss=0.07263, avg_loss=0.07324]\n",
      "Step 540264  [5.450 sec/step, loss=0.07463, avg_loss=0.07324]\n",
      "Step 540265  [5.460 sec/step, loss=0.07545, avg_loss=0.07327]\n",
      "Step 540266  [5.468 sec/step, loss=0.07500, avg_loss=0.07329]\n",
      "Step 540267  [5.445 sec/step, loss=0.06529, avg_loss=0.07319]\n",
      "Step 540268  [5.432 sec/step, loss=0.07109, avg_loss=0.07319]\n",
      "Step 540269  [5.414 sec/step, loss=0.07308, avg_loss=0.07318]\n",
      "Step 540270  [5.455 sec/step, loss=0.06681, avg_loss=0.07309]\n",
      "Step 540271  [5.455 sec/step, loss=0.07468, avg_loss=0.07308]\n",
      "Step 540272  [5.463 sec/step, loss=0.07528, avg_loss=0.07310]\n",
      "Step 540273  [5.500 sec/step, loss=0.07263, avg_loss=0.07309]\n",
      "Step 540274  [5.508 sec/step, loss=0.07454, avg_loss=0.07309]\n",
      "Step 540275  [5.503 sec/step, loss=0.07001, avg_loss=0.07309]\n",
      "Step 540276  [5.490 sec/step, loss=0.07490, avg_loss=0.07311]\n",
      "Step 540277  [5.497 sec/step, loss=0.07399, avg_loss=0.07313]\n",
      "Step 540278  [5.515 sec/step, loss=0.07437, avg_loss=0.07316]\n",
      "Step 540279  [5.499 sec/step, loss=0.07367, avg_loss=0.07314]\n",
      "Step 540280  [5.492 sec/step, loss=0.07243, avg_loss=0.07313]\n",
      "Step 540281  [5.521 sec/step, loss=0.07359, avg_loss=0.07322]\n",
      "Step 540282  [5.513 sec/step, loss=0.07316, avg_loss=0.07320]\n",
      "Step 540283  [5.509 sec/step, loss=0.07276, avg_loss=0.07322]\n",
      "Step 540284  [5.502 sec/step, loss=0.07571, avg_loss=0.07322]\n",
      "Step 540285  [5.497 sec/step, loss=0.07168, avg_loss=0.07320]\n",
      "Generated 32 batches of size 32 in 2.607 sec\n",
      "Step 540286  [5.511 sec/step, loss=0.07395, avg_loss=0.07322]\n",
      "Step 540287  [5.504 sec/step, loss=0.07391, avg_loss=0.07321]\n",
      "Step 540288  [5.523 sec/step, loss=0.07507, avg_loss=0.07323]\n",
      "Step 540289  [5.497 sec/step, loss=0.07135, avg_loss=0.07321]\n",
      "Step 540290  [5.510 sec/step, loss=0.07503, avg_loss=0.07321]\n",
      "Step 540291  [5.497 sec/step, loss=0.07371, avg_loss=0.07320]\n",
      "Step 540292  [5.503 sec/step, loss=0.07475, avg_loss=0.07323]\n",
      "Step 540293  [5.485 sec/step, loss=0.07117, avg_loss=0.07321]\n",
      "Step 540294  [5.498 sec/step, loss=0.07509, avg_loss=0.07323]\n",
      "Step 540295  [5.493 sec/step, loss=0.07291, avg_loss=0.07321]\n",
      "Step 540296  [5.478 sec/step, loss=0.07118, avg_loss=0.07319]\n",
      "Step 540297  [5.511 sec/step, loss=0.06826, avg_loss=0.07314]\n",
      "Step 540298  [5.478 sec/step, loss=0.07439, avg_loss=0.07323]\n",
      "Step 540299  [5.481 sec/step, loss=0.07434, avg_loss=0.07324]\n",
      "Step 540300  [5.471 sec/step, loss=0.07125, avg_loss=0.07322]\n",
      "Writing summary at step: 540300\n",
      "Step 540301  [5.467 sec/step, loss=0.07537, avg_loss=0.07322]\n",
      "Step 540302  [5.466 sec/step, loss=0.07449, avg_loss=0.07321]\n",
      "Step 540303  [5.447 sec/step, loss=0.07355, avg_loss=0.07319]\n",
      "Step 540304  [5.454 sec/step, loss=0.07424, avg_loss=0.07318]\n",
      "Step 540305  [5.458 sec/step, loss=0.07269, avg_loss=0.07317]\n",
      "Step 540306  [5.469 sec/step, loss=0.07322, avg_loss=0.07315]\n",
      "Step 540307  [5.449 sec/step, loss=0.06564, avg_loss=0.07307]\n",
      "Step 540308  [5.426 sec/step, loss=0.07341, avg_loss=0.07306]\n",
      "Step 540309  [5.460 sec/step, loss=0.07173, avg_loss=0.07305]\n",
      "Step 540310  [5.475 sec/step, loss=0.07654, avg_loss=0.07311]\n",
      "Step 540311  [5.464 sec/step, loss=0.07357, avg_loss=0.07309]\n",
      "Step 540312  [5.475 sec/step, loss=0.07395, avg_loss=0.07309]\n",
      "Step 540313  [5.477 sec/step, loss=0.07716, avg_loss=0.07314]\n",
      "Step 540314  [5.463 sec/step, loss=0.07584, avg_loss=0.07314]\n",
      "Step 540315  [5.473 sec/step, loss=0.07759, avg_loss=0.07320]\n",
      "Step 540316  [5.491 sec/step, loss=0.07895, avg_loss=0.07324]\n",
      "Generated 32 batches of size 32 in 2.383 sec\n",
      "Step 540317  [5.489 sec/step, loss=0.07759, avg_loss=0.07327]\n",
      "Step 540318  [5.479 sec/step, loss=0.07291, avg_loss=0.07325]\n",
      "Step 540319  [5.493 sec/step, loss=0.07943, avg_loss=0.07332]\n",
      "Step 540320  [5.493 sec/step, loss=0.07753, avg_loss=0.07337]\n",
      "Step 540321  [5.487 sec/step, loss=0.07777, avg_loss=0.07340]\n",
      "Step 540322  [5.466 sec/step, loss=0.07839, avg_loss=0.07345]\n",
      "Step 540323  [5.479 sec/step, loss=0.07691, avg_loss=0.07347]\n",
      "Step 540324  [5.423 sec/step, loss=0.07473, avg_loss=0.07357]\n",
      "Step 540325  [5.415 sec/step, loss=0.07869, avg_loss=0.07361]\n",
      "Step 540326  [5.409 sec/step, loss=0.07332, avg_loss=0.07360]\n",
      "Step 540327  [5.433 sec/step, loss=0.07800, avg_loss=0.07372]\n",
      "Step 540328  [5.426 sec/step, loss=0.07404, avg_loss=0.07372]\n",
      "Step 540329  [5.425 sec/step, loss=0.07626, avg_loss=0.07374]\n",
      "Step 540330  [5.424 sec/step, loss=0.07697, avg_loss=0.07377]\n",
      "Step 540331  [5.438 sec/step, loss=0.07537, avg_loss=0.07379]\n",
      "Step 540332  [5.427 sec/step, loss=0.06742, avg_loss=0.07376]\n",
      "Step 540333  [5.471 sec/step, loss=0.06783, avg_loss=0.07369]\n",
      "Step 540334  [5.499 sec/step, loss=0.07705, avg_loss=0.07380]\n",
      "Step 540335  [5.501 sec/step, loss=0.07794, avg_loss=0.07385]\n",
      "Step 540336  [5.497 sec/step, loss=0.07496, avg_loss=0.07385]\n",
      "Step 540337  [5.502 sec/step, loss=0.07707, avg_loss=0.07389]\n",
      "Step 540338  [5.510 sec/step, loss=0.07633, avg_loss=0.07392]\n",
      "Step 540339  [5.504 sec/step, loss=0.07657, avg_loss=0.07394]\n",
      "Step 540340  [5.501 sec/step, loss=0.07532, avg_loss=0.07394]\n",
      "Step 540341  [5.464 sec/step, loss=0.07141, avg_loss=0.07393]\n",
      "Step 540342  [5.502 sec/step, loss=0.07437, avg_loss=0.07394]\n",
      "Step 540343  [5.483 sec/step, loss=0.07469, avg_loss=0.07395]\n",
      "Step 540344  [5.477 sec/step, loss=0.07484, avg_loss=0.07397]\n",
      "Step 540345  [5.432 sec/step, loss=0.07542, avg_loss=0.07406]\n",
      "Step 540346  [5.436 sec/step, loss=0.07600, avg_loss=0.07407]\n",
      "Step 540347  [5.436 sec/step, loss=0.07692, avg_loss=0.07409]\n",
      "Step 540348  [5.443 sec/step, loss=0.07692, avg_loss=0.07412]\n",
      "Generated 32 batches of size 32 in 2.505 sec\n",
      "Step 540349  [5.443 sec/step, loss=0.07542, avg_loss=0.07413]\n",
      "Step 540350  [5.430 sec/step, loss=0.07471, avg_loss=0.07412]\n",
      "Step 540351  [5.418 sec/step, loss=0.07249, avg_loss=0.07409]\n",
      "Step 540352  [5.419 sec/step, loss=0.07595, avg_loss=0.07411]\n",
      "Step 540353  [5.424 sec/step, loss=0.07674, avg_loss=0.07414]\n",
      "Step 540354  [5.458 sec/step, loss=0.07426, avg_loss=0.07417]\n",
      "Step 540355  [5.454 sec/step, loss=0.07579, avg_loss=0.07418]\n",
      "Step 540356  [5.463 sec/step, loss=0.07282, avg_loss=0.07421]\n",
      "Step 540357  [5.449 sec/step, loss=0.07667, avg_loss=0.07425]\n",
      "Step 540358  [5.462 sec/step, loss=0.07653, avg_loss=0.07427]\n",
      "Step 540359  [5.456 sec/step, loss=0.07535, avg_loss=0.07427]\n",
      "Step 540360  [5.444 sec/step, loss=0.07580, avg_loss=0.07428]\n",
      "Step 540361  [5.456 sec/step, loss=0.07528, avg_loss=0.07431]\n",
      "Step 540362  [5.453 sec/step, loss=0.07546, avg_loss=0.07434]\n",
      "Step 540363  [5.426 sec/step, loss=0.07169, avg_loss=0.07433]\n",
      "Step 540364  [5.420 sec/step, loss=0.07450, avg_loss=0.07433]\n",
      "Step 540365  [5.408 sec/step, loss=0.07198, avg_loss=0.07429]\n",
      "Step 540366  [5.412 sec/step, loss=0.07464, avg_loss=0.07429]\n",
      "Step 540367  [5.420 sec/step, loss=0.07061, avg_loss=0.07434]\n",
      "Step 540368  [5.438 sec/step, loss=0.07607, avg_loss=0.07439]\n",
      "Step 540369  [5.451 sec/step, loss=0.07385, avg_loss=0.07440]\n",
      "Step 540370  [5.412 sec/step, loss=0.07343, avg_loss=0.07447]\n",
      "Step 540371  [5.399 sec/step, loss=0.07364, avg_loss=0.07446]\n",
      "Step 540372  [5.385 sec/step, loss=0.07041, avg_loss=0.07441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 540373  [5.371 sec/step, loss=0.07380, avg_loss=0.07442]\n",
      "Step 540374  [5.379 sec/step, loss=0.07574, avg_loss=0.07443]\n",
      "Step 540375  [5.438 sec/step, loss=0.06652, avg_loss=0.07440]\n",
      "Step 540376  [5.459 sec/step, loss=0.07570, avg_loss=0.07440]\n",
      "Step 540377  [5.487 sec/step, loss=0.07349, avg_loss=0.07440]\n",
      "Step 540378  [5.472 sec/step, loss=0.07204, avg_loss=0.07438]\n",
      "Step 540379  [5.455 sec/step, loss=0.06625, avg_loss=0.07430]\n",
      "Step 540380  [5.435 sec/step, loss=0.07346, avg_loss=0.07431]\n",
      "Generated 32 batches of size 32 in 2.467 sec\n",
      "Step 540381  [5.432 sec/step, loss=0.07515, avg_loss=0.07433]\n",
      "Step 540382  [5.434 sec/step, loss=0.07266, avg_loss=0.07432]\n",
      "Step 540383  [5.446 sec/step, loss=0.07697, avg_loss=0.07437]\n",
      "Step 540384  [5.454 sec/step, loss=0.07621, avg_loss=0.07437]\n",
      "Step 540385  [5.453 sec/step, loss=0.07521, avg_loss=0.07441]\n",
      "Step 540386  [5.458 sec/step, loss=0.07582, avg_loss=0.07442]\n",
      "Step 540387  [5.455 sec/step, loss=0.07540, avg_loss=0.07444]\n",
      "Step 540388  [5.443 sec/step, loss=0.07422, avg_loss=0.07443]\n",
      "Step 540389  [5.449 sec/step, loss=0.07362, avg_loss=0.07445]\n",
      "Step 540390  [5.437 sec/step, loss=0.07471, avg_loss=0.07445]\n",
      "Step 540391  [5.457 sec/step, loss=0.07612, avg_loss=0.07447]\n",
      "Step 540392  [5.481 sec/step, loss=0.07275, avg_loss=0.07445]\n",
      "Step 540393  [5.482 sec/step, loss=0.07183, avg_loss=0.07446]\n",
      "Step 540394  [5.485 sec/step, loss=0.07286, avg_loss=0.07444]\n",
      "Step 540395  [5.497 sec/step, loss=0.07675, avg_loss=0.07448]\n",
      "Step 540396  [5.509 sec/step, loss=0.07578, avg_loss=0.07452]\n",
      "Step 540397  [5.449 sec/step, loss=0.06559, avg_loss=0.07450]\n",
      "Step 540398  [5.442 sec/step, loss=0.07573, avg_loss=0.07451]\n",
      "Step 540399  [5.451 sec/step, loss=0.07475, avg_loss=0.07451]\n",
      "Step 540400  [5.465 sec/step, loss=0.07422, avg_loss=0.07454]\n",
      "Writing summary at step: 540400\n",
      "Step 540401  [5.456 sec/step, loss=0.07322, avg_loss=0.07452]\n",
      "Step 540402  [5.463 sec/step, loss=0.07680, avg_loss=0.07454]\n",
      "Step 540403  [5.467 sec/step, loss=0.07323, avg_loss=0.07454]\n",
      "Step 540404  [5.456 sec/step, loss=0.07515, avg_loss=0.07455]\n",
      "Step 540405  [5.465 sec/step, loss=0.07369, avg_loss=0.07456]\n",
      "Step 540406  [5.445 sec/step, loss=0.07363, avg_loss=0.07456]\n",
      "Step 540407  [5.467 sec/step, loss=0.07405, avg_loss=0.07465]\n",
      "Step 540408  [5.484 sec/step, loss=0.07563, avg_loss=0.07467]\n",
      "Step 540409  [5.469 sec/step, loss=0.07303, avg_loss=0.07468]\n",
      "Step 540410  [5.448 sec/step, loss=0.07162, avg_loss=0.07463]\n",
      "Step 540411  [5.444 sec/step, loss=0.07511, avg_loss=0.07465]\n",
      "Generated 32 batches of size 32 in 2.537 sec\n",
      "Step 540412  [5.427 sec/step, loss=0.07028, avg_loss=0.07461]\n",
      "Step 540413  [5.431 sec/step, loss=0.07442, avg_loss=0.07459]\n",
      "Step 540414  [5.428 sec/step, loss=0.07180, avg_loss=0.07455]\n",
      "Step 540415  [5.421 sec/step, loss=0.07414, avg_loss=0.07451]\n",
      "Step 540416  [5.409 sec/step, loss=0.07517, avg_loss=0.07447]\n",
      "Step 540417  [5.400 sec/step, loss=0.07157, avg_loss=0.07441]\n",
      "Step 540418  [5.401 sec/step, loss=0.07089, avg_loss=0.07439]\n",
      "Step 540419  [5.391 sec/step, loss=0.07356, avg_loss=0.07433]\n",
      "Step 540420  [5.407 sec/step, loss=0.07592, avg_loss=0.07432]\n",
      "Step 540421  [5.405 sec/step, loss=0.07447, avg_loss=0.07429]\n",
      "Step 540422  [5.393 sec/step, loss=0.07189, avg_loss=0.07422]\n",
      "Step 540423  [5.397 sec/step, loss=0.07509, avg_loss=0.07420]\n",
      "Step 540424  [5.405 sec/step, loss=0.07465, avg_loss=0.07420]\n",
      "Step 540425  [5.402 sec/step, loss=0.07321, avg_loss=0.07415]\n",
      "Step 540426  [5.426 sec/step, loss=0.07518, avg_loss=0.07417]\n",
      "Step 540427  [5.415 sec/step, loss=0.07316, avg_loss=0.07412]\n",
      "Step 540428  [5.421 sec/step, loss=0.07563, avg_loss=0.07413]\n",
      "Step 540429  [5.430 sec/step, loss=0.07513, avg_loss=0.07412]\n",
      "Step 540430  [5.415 sec/step, loss=0.07398, avg_loss=0.07409]\n",
      "Step 540431  [5.400 sec/step, loss=0.07502, avg_loss=0.07409]\n",
      "Step 540432  [5.416 sec/step, loss=0.07172, avg_loss=0.07413]\n",
      "Step 540433  [5.368 sec/step, loss=0.07494, avg_loss=0.07420]\n",
      "Step 540434  [5.368 sec/step, loss=0.07580, avg_loss=0.07419]\n",
      "Step 540435  [5.351 sec/step, loss=0.07423, avg_loss=0.07415]\n",
      "Step 540436  [5.351 sec/step, loss=0.07300, avg_loss=0.07413]\n",
      "Step 540437  [5.342 sec/step, loss=0.07303, avg_loss=0.07409]\n",
      "Step 540438  [5.350 sec/step, loss=0.07577, avg_loss=0.07409]\n",
      "Step 540439  [5.336 sec/step, loss=0.06444, avg_loss=0.07397]\n",
      "Step 540440  [5.311 sec/step, loss=0.07104, avg_loss=0.07392]\n",
      "Step 540441  [5.372 sec/step, loss=0.06601, avg_loss=0.07387]\n",
      "Step 540442  [5.347 sec/step, loss=0.07375, avg_loss=0.07386]\n",
      "Step 540443  [5.373 sec/step, loss=0.07517, avg_loss=0.07387]\n",
      "Generated 32 batches of size 32 in 2.421 sec\n",
      "Step 540444  [5.413 sec/step, loss=0.07327, avg_loss=0.07385]\n",
      "Step 540445  [5.397 sec/step, loss=0.07055, avg_loss=0.07380]\n",
      "Step 540446  [5.406 sec/step, loss=0.07409, avg_loss=0.07378]\n",
      "Step 540447  [5.405 sec/step, loss=0.07472, avg_loss=0.07376]\n",
      "Step 540448  [5.396 sec/step, loss=0.07482, avg_loss=0.07374]\n",
      "Step 540449  [5.384 sec/step, loss=0.07373, avg_loss=0.07372]\n",
      "Step 540450  [5.404 sec/step, loss=0.07537, avg_loss=0.07373]\n",
      "Step 540451  [5.412 sec/step, loss=0.07640, avg_loss=0.07377]\n",
      "Step 540452  [5.415 sec/step, loss=0.07358, avg_loss=0.07375]\n",
      "Step 540453  [5.414 sec/step, loss=0.07364, avg_loss=0.07372]\n",
      "Step 540454  [5.388 sec/step, loss=0.07481, avg_loss=0.07372]\n",
      "Step 540455  [5.392 sec/step, loss=0.07468, avg_loss=0.07371]\n",
      "Step 540456  [5.378 sec/step, loss=0.06595, avg_loss=0.07364]\n",
      "Step 540457  [5.359 sec/step, loss=0.07305, avg_loss=0.07360]\n",
      "Step 540458  [5.348 sec/step, loss=0.07412, avg_loss=0.07358]\n",
      "Step 540459  [5.338 sec/step, loss=0.07286, avg_loss=0.07356]\n",
      "Step 540460  [5.327 sec/step, loss=0.07148, avg_loss=0.07351]\n",
      "Step 540461  [5.323 sec/step, loss=0.07492, avg_loss=0.07351]\n",
      "Step 540462  [5.317 sec/step, loss=0.07311, avg_loss=0.07349]\n",
      "Step 540463  [5.327 sec/step, loss=0.07453, avg_loss=0.07351]\n",
      "Step 540464  [5.382 sec/step, loss=0.06572, avg_loss=0.07343]\n",
      "Step 540465  [5.371 sec/step, loss=0.07040, avg_loss=0.07341]\n",
      "Step 540466  [5.375 sec/step, loss=0.07590, avg_loss=0.07342]\n",
      "Step 540467  [5.381 sec/step, loss=0.07297, avg_loss=0.07345]\n",
      "Step 540468  [5.388 sec/step, loss=0.07570, avg_loss=0.07344]\n",
      "Step 540469  [5.394 sec/step, loss=0.07634, avg_loss=0.07347]\n",
      "Step 540470  [5.394 sec/step, loss=0.07358, avg_loss=0.07347]\n",
      "Step 540471  [5.403 sec/step, loss=0.07460, avg_loss=0.07348]\n",
      "Step 540472  [5.401 sec/step, loss=0.07324, avg_loss=0.07351]\n",
      "Step 540473  [5.397 sec/step, loss=0.07607, avg_loss=0.07353]\n",
      "Step 540474  [5.395 sec/step, loss=0.07612, avg_loss=0.07353]\n",
      "Step 540475  [5.349 sec/step, loss=0.07515, avg_loss=0.07362]\n",
      "Generated 32 batches of size 32 in 2.409 sec\n",
      "Step 540476  [5.343 sec/step, loss=0.07600, avg_loss=0.07362]\n",
      "Step 540477  [5.309 sec/step, loss=0.07282, avg_loss=0.07362]\n",
      "Step 540478  [5.323 sec/step, loss=0.07436, avg_loss=0.07364]\n",
      "Step 540479  [5.353 sec/step, loss=0.07568, avg_loss=0.07373]\n",
      "Step 540480  [5.353 sec/step, loss=0.07156, avg_loss=0.07371]\n",
      "Step 540481  [5.345 sec/step, loss=0.07396, avg_loss=0.07370]\n",
      "Step 540482  [5.372 sec/step, loss=0.07273, avg_loss=0.07370]\n",
      "Step 540483  [5.364 sec/step, loss=0.07469, avg_loss=0.07368]\n",
      "Step 540484  [5.370 sec/step, loss=0.07270, avg_loss=0.07365]\n",
      "Step 540485  [5.377 sec/step, loss=0.07465, avg_loss=0.07364]\n",
      "Step 540486  [5.417 sec/step, loss=0.06484, avg_loss=0.07353]\n",
      "Step 540487  [5.435 sec/step, loss=0.07268, avg_loss=0.07350]\n",
      "Step 540488  [5.445 sec/step, loss=0.07616, avg_loss=0.07352]\n",
      "Step 540489  [5.454 sec/step, loss=0.07534, avg_loss=0.07354]\n",
      "Step 540490  [5.453 sec/step, loss=0.07463, avg_loss=0.07354]\n",
      "Step 540491  [5.450 sec/step, loss=0.07629, avg_loss=0.07354]\n",
      "Step 540492  [5.437 sec/step, loss=0.07574, avg_loss=0.07357]\n",
      "Step 540493  [5.434 sec/step, loss=0.07406, avg_loss=0.07359]\n",
      "Step 540494  [5.427 sec/step, loss=0.07558, avg_loss=0.07362]\n",
      "Step 540495  [5.429 sec/step, loss=0.07413, avg_loss=0.07359]\n",
      "Step 540496  [5.439 sec/step, loss=0.07435, avg_loss=0.07358]\n",
      "Step 540497  [5.458 sec/step, loss=0.07189, avg_loss=0.07364]\n",
      "Step 540498  [5.438 sec/step, loss=0.06995, avg_loss=0.07358]\n",
      "Step 540499  [5.423 sec/step, loss=0.07391, avg_loss=0.07358]\n",
      "Step 540500  [5.421 sec/step, loss=0.07226, avg_loss=0.07356]\n",
      "Writing summary at step: 540500\n",
      "Step 540501  [5.421 sec/step, loss=0.07339, avg_loss=0.07356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 540502  [5.416 sec/step, loss=0.07411, avg_loss=0.07353]\n",
      "Step 540503  [5.419 sec/step, loss=0.07324, avg_loss=0.07353]\n",
      "Step 540504  [5.416 sec/step, loss=0.07271, avg_loss=0.07351]\n",
      "Step 540505  [5.417 sec/step, loss=0.07622, avg_loss=0.07353]\n",
      "Step 540506  [5.451 sec/step, loss=0.07281, avg_loss=0.07352]\n",
      "Generated 32 batches of size 32 in 2.421 sec\n",
      "Step 540507  [5.456 sec/step, loss=0.07432, avg_loss=0.07353]\n",
      "Step 540508  [5.432 sec/step, loss=0.07118, avg_loss=0.07348]\n",
      "Step 540509  [5.422 sec/step, loss=0.07345, avg_loss=0.07349]\n",
      "Step 540510  [5.429 sec/step, loss=0.07312, avg_loss=0.07350]\n",
      "Step 540511  [5.441 sec/step, loss=0.07540, avg_loss=0.07350]\n",
      "Step 540512  [5.432 sec/step, loss=0.06614, avg_loss=0.07346]\n",
      "Step 540513  [5.441 sec/step, loss=0.07300, avg_loss=0.07345]\n",
      "Step 540514  [5.433 sec/step, loss=0.07119, avg_loss=0.07344]\n",
      "Step 540515  [5.445 sec/step, loss=0.07440, avg_loss=0.07345]\n",
      "Step 540516  [5.437 sec/step, loss=0.07371, avg_loss=0.07343]\n",
      "Step 540517  [5.422 sec/step, loss=0.07203, avg_loss=0.07344]\n",
      "Step 540518  [5.434 sec/step, loss=0.07456, avg_loss=0.07347]\n",
      "Step 540519  [5.436 sec/step, loss=0.07449, avg_loss=0.07348]\n",
      "Step 540520  [5.411 sec/step, loss=0.07004, avg_loss=0.07342]\n",
      "Step 540521  [5.408 sec/step, loss=0.07481, avg_loss=0.07343]\n",
      "Step 540522  [5.410 sec/step, loss=0.07361, avg_loss=0.07344]\n",
      "Step 540523  [5.407 sec/step, loss=0.07567, avg_loss=0.07345]\n",
      "Step 540524  [5.452 sec/step, loss=0.06604, avg_loss=0.07336]\n",
      "Step 540525  [5.444 sec/step, loss=0.07163, avg_loss=0.07335]\n",
      "Step 540526  [5.442 sec/step, loss=0.07582, avg_loss=0.07335]\n",
      "Step 540527  [5.463 sec/step, loss=0.07445, avg_loss=0.07337]\n",
      "Step 540528  [5.456 sec/step, loss=0.07006, avg_loss=0.07331]\n",
      "Step 540529  [5.447 sec/step, loss=0.07282, avg_loss=0.07329]\n",
      "Step 540530  [5.463 sec/step, loss=0.07471, avg_loss=0.07330]\n",
      "Step 540531  [5.462 sec/step, loss=0.07379, avg_loss=0.07328]\n",
      "Step 540532  [5.457 sec/step, loss=0.07321, avg_loss=0.07330]\n",
      "Step 540533  [5.482 sec/step, loss=0.07233, avg_loss=0.07327]\n",
      "Step 540534  [5.454 sec/step, loss=0.06475, avg_loss=0.07316]\n",
      "Step 540535  [5.460 sec/step, loss=0.07414, avg_loss=0.07316]\n",
      "Step 540536  [5.462 sec/step, loss=0.07416, avg_loss=0.07317]\n",
      "Step 540537  [5.482 sec/step, loss=0.07455, avg_loss=0.07319]\n",
      "Step 540538  [5.476 sec/step, loss=0.07330, avg_loss=0.07316]\n",
      "Generated 32 batches of size 32 in 2.381 sec\n",
      "Step 540539  [5.503 sec/step, loss=0.07365, avg_loss=0.07325]\n",
      "Step 540540  [5.529 sec/step, loss=0.07398, avg_loss=0.07328]\n",
      "Step 540541  [5.487 sec/step, loss=0.07567, avg_loss=0.07338]\n",
      "Step 540542  [5.487 sec/step, loss=0.07498, avg_loss=0.07339]\n",
      "Step 540543  [5.461 sec/step, loss=0.07201, avg_loss=0.07336]\n",
      "Step 540544  [5.418 sec/step, loss=0.07374, avg_loss=0.07337]\n",
      "Step 540545  [5.444 sec/step, loss=0.07406, avg_loss=0.07340]\n",
      "Step 540546  [5.442 sec/step, loss=0.07610, avg_loss=0.07342]\n",
      "Step 540547  [5.434 sec/step, loss=0.07315, avg_loss=0.07341]\n",
      "Step 540548  [5.428 sec/step, loss=0.07320, avg_loss=0.07339]\n",
      "Step 540549  [5.439 sec/step, loss=0.07421, avg_loss=0.07339]\n",
      "Step 540550  [5.436 sec/step, loss=0.07414, avg_loss=0.07338]\n",
      "Step 540551  [5.438 sec/step, loss=0.07391, avg_loss=0.07336]\n",
      "Step 540552  [5.444 sec/step, loss=0.07544, avg_loss=0.07338]\n",
      "Step 540553  [5.446 sec/step, loss=0.07503, avg_loss=0.07339]\n",
      "Step 540554  [5.446 sec/step, loss=0.07431, avg_loss=0.07338]\n",
      "Step 540555  [5.445 sec/step, loss=0.07320, avg_loss=0.07337]\n",
      "Step 540556  [5.456 sec/step, loss=0.07258, avg_loss=0.07344]\n",
      "Step 540557  [5.456 sec/step, loss=0.07053, avg_loss=0.07341]\n",
      "Step 540558  [5.480 sec/step, loss=0.07427, avg_loss=0.07341]\n",
      "Step 540559  [5.486 sec/step, loss=0.07482, avg_loss=0.07343]\n",
      "Step 540560  [5.491 sec/step, loss=0.06972, avg_loss=0.07341]\n",
      "Step 540561  [5.500 sec/step, loss=0.07620, avg_loss=0.07343]\n",
      "Step 540562  [5.502 sec/step, loss=0.07191, avg_loss=0.07341]\n",
      "Step 540563  [5.520 sec/step, loss=0.07553, avg_loss=0.07342]\n",
      "Step 540564  [5.487 sec/step, loss=0.07468, avg_loss=0.07351]\n",
      "Step 540565  [5.503 sec/step, loss=0.07320, avg_loss=0.07354]\n",
      "Step 540566  [5.481 sec/step, loss=0.07055, avg_loss=0.07349]\n",
      "Step 540567  [5.480 sec/step, loss=0.07507, avg_loss=0.07351]\n",
      "Step 540568  [5.467 sec/step, loss=0.07092, avg_loss=0.07346]\n",
      "Step 540569  [5.442 sec/step, loss=0.06573, avg_loss=0.07336]\n",
      "Step 540570  [5.423 sec/step, loss=0.07354, avg_loss=0.07336]\n",
      "Generated 32 batches of size 32 in 2.394 sec\n",
      "Step 540571  [5.431 sec/step, loss=0.07513, avg_loss=0.07336]\n",
      "Step 540572  [5.436 sec/step, loss=0.07505, avg_loss=0.07338]\n",
      "Step 540573  [5.424 sec/step, loss=0.07320, avg_loss=0.07335]\n",
      "Step 540574  [5.413 sec/step, loss=0.07514, avg_loss=0.07334]\n",
      "Step 540575  [5.414 sec/step, loss=0.07540, avg_loss=0.07334]\n",
      "Step 540576  [5.409 sec/step, loss=0.07545, avg_loss=0.07334]\n",
      "Step 540577  [5.419 sec/step, loss=0.07492, avg_loss=0.07336]\n",
      "Step 540578  [5.465 sec/step, loss=0.06567, avg_loss=0.07327]\n",
      "Step 540579  [5.463 sec/step, loss=0.07641, avg_loss=0.07328]\n",
      "Step 540580  [5.450 sec/step, loss=0.07162, avg_loss=0.07328]\n",
      "Step 540581  [5.454 sec/step, loss=0.07523, avg_loss=0.07329]\n",
      "Step 540582  [5.427 sec/step, loss=0.07511, avg_loss=0.07332]\n",
      "Step 540583  [5.440 sec/step, loss=0.07597, avg_loss=0.07333]\n",
      "Step 540584  [5.428 sec/step, loss=0.07623, avg_loss=0.07336]\n",
      "Step 540585  [5.451 sec/step, loss=0.07361, avg_loss=0.07335]\n",
      "Step 540586  [5.401 sec/step, loss=0.07345, avg_loss=0.07344]\n",
      "Step 540587  [5.383 sec/step, loss=0.07133, avg_loss=0.07343]\n",
      "Step 540588  [5.371 sec/step, loss=0.07504, avg_loss=0.07342]\n",
      "Step 540589  [5.357 sec/step, loss=0.07102, avg_loss=0.07337]\n",
      "Step 540590  [5.344 sec/step, loss=0.06635, avg_loss=0.07329]\n",
      "Step 540591  [5.328 sec/step, loss=0.07322, avg_loss=0.07326]\n",
      "Step 540592  [5.323 sec/step, loss=0.07620, avg_loss=0.07326]\n",
      "Step 540593  [5.373 sec/step, loss=0.06646, avg_loss=0.07319]\n",
      "Step 540594  [5.371 sec/step, loss=0.07444, avg_loss=0.07318]\n",
      "Step 540595  [5.353 sec/step, loss=0.07382, avg_loss=0.07317]\n",
      "Step 540596  [5.335 sec/step, loss=0.07356, avg_loss=0.07316]\n",
      "Step 540597  [5.347 sec/step, loss=0.07574, avg_loss=0.07320]\n",
      "Step 540598  [5.366 sec/step, loss=0.07590, avg_loss=0.07326]\n",
      "Step 540599  [5.377 sec/step, loss=0.07492, avg_loss=0.07327]\n",
      "Step 540600  [5.388 sec/step, loss=0.07503, avg_loss=0.07330]\n",
      "Writing summary at step: 540600\n",
      "Step 540601  [5.393 sec/step, loss=0.07526, avg_loss=0.07332]\n",
      "Generated 32 batches of size 32 in 2.410 sec\n",
      "Step 540602  [5.405 sec/step, loss=0.07355, avg_loss=0.07331]\n",
      "Step 540603  [5.406 sec/step, loss=0.07338, avg_loss=0.07332]\n",
      "Step 540604  [5.394 sec/step, loss=0.07389, avg_loss=0.07333]\n",
      "Step 540605  [5.382 sec/step, loss=0.07584, avg_loss=0.07332]\n",
      "Step 540606  [5.355 sec/step, loss=0.07452, avg_loss=0.07334]\n",
      "Step 540607  [5.350 sec/step, loss=0.07534, avg_loss=0.07335]\n",
      "Step 540608  [5.367 sec/step, loss=0.07574, avg_loss=0.07340]\n",
      "Step 540609  [5.365 sec/step, loss=0.07256, avg_loss=0.07339]\n",
      "Step 540610  [5.392 sec/step, loss=0.07320, avg_loss=0.07339]\n",
      "Step 540611  [5.396 sec/step, loss=0.07590, avg_loss=0.07339]\n",
      "Step 540612  [5.431 sec/step, loss=0.07555, avg_loss=0.07349]\n",
      "Step 540613  [5.412 sec/step, loss=0.07510, avg_loss=0.07351]\n",
      "Step 540614  [5.412 sec/step, loss=0.07154, avg_loss=0.07351]\n",
      "Step 540615  [5.400 sec/step, loss=0.07391, avg_loss=0.07351]\n",
      "Step 540616  [5.416 sec/step, loss=0.07639, avg_loss=0.07353]\n",
      "Step 540617  [5.434 sec/step, loss=0.07488, avg_loss=0.07356]\n",
      "Step 540618  [5.437 sec/step, loss=0.07645, avg_loss=0.07358]\n",
      "Step 540619  [5.433 sec/step, loss=0.07053, avg_loss=0.07354]\n",
      "Step 540620  [5.452 sec/step, loss=0.07581, avg_loss=0.07360]\n",
      "Step 540621  [5.452 sec/step, loss=0.07495, avg_loss=0.07360]\n",
      "Step 540622  [5.480 sec/step, loss=0.07292, avg_loss=0.07359]\n",
      "Step 540623  [5.453 sec/step, loss=0.06507, avg_loss=0.07349]\n",
      "Step 540624  [5.411 sec/step, loss=0.07502, avg_loss=0.07358]\n",
      "Step 540625  [5.421 sec/step, loss=0.07566, avg_loss=0.07362]\n",
      "Step 540626  [5.464 sec/step, loss=0.06622, avg_loss=0.07352]\n",
      "Step 540627  [5.448 sec/step, loss=0.07497, avg_loss=0.07353]\n",
      "Step 540628  [5.465 sec/step, loss=0.07359, avg_loss=0.07356]\n",
      "Step 540629  [5.474 sec/step, loss=0.07388, avg_loss=0.07357]\n",
      "Step 540630  [5.470 sec/step, loss=0.07386, avg_loss=0.07356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 540631  [5.459 sec/step, loss=0.07164, avg_loss=0.07354]\n",
      "Step 540632  [5.457 sec/step, loss=0.07394, avg_loss=0.07355]\n",
      "Step 540633  [5.425 sec/step, loss=0.07268, avg_loss=0.07355]\n",
      "Generated 32 batches of size 32 in 2.493 sec\n",
      "Step 540634  [5.447 sec/step, loss=0.07420, avg_loss=0.07365]\n",
      "Step 540635  [5.447 sec/step, loss=0.07445, avg_loss=0.07365]\n",
      "Step 540636  [5.451 sec/step, loss=0.07473, avg_loss=0.07366]\n",
      "Step 540637  [5.443 sec/step, loss=0.07478, avg_loss=0.07366]\n",
      "Step 540638  [5.434 sec/step, loss=0.07365, avg_loss=0.07366]\n",
      "Step 540639  [5.424 sec/step, loss=0.07428, avg_loss=0.07367]\n",
      "Step 540640  [5.427 sec/step, loss=0.07609, avg_loss=0.07369]\n",
      "Step 540641  [5.420 sec/step, loss=0.07524, avg_loss=0.07369]\n",
      "Step 540642  [5.415 sec/step, loss=0.07358, avg_loss=0.07367]\n",
      "Step 540643  [5.410 sec/step, loss=0.07078, avg_loss=0.07366]\n",
      "Step 540644  [5.424 sec/step, loss=0.07185, avg_loss=0.07364]\n",
      "Step 540645  [5.420 sec/step, loss=0.07386, avg_loss=0.07364]\n",
      "Step 540646  [5.418 sec/step, loss=0.07644, avg_loss=0.07364]\n",
      "Step 540647  [5.416 sec/step, loss=0.07342, avg_loss=0.07364]\n",
      "Step 540648  [5.437 sec/step, loss=0.07403, avg_loss=0.07365]\n",
      "Step 540649  [5.424 sec/step, loss=0.07009, avg_loss=0.07361]\n",
      "Step 540650  [5.419 sec/step, loss=0.07516, avg_loss=0.07362]\n",
      "Step 540651  [5.400 sec/step, loss=0.07289, avg_loss=0.07361]\n",
      "Step 540652  [5.406 sec/step, loss=0.07579, avg_loss=0.07361]\n",
      "Step 540653  [5.402 sec/step, loss=0.07536, avg_loss=0.07362]\n",
      "Step 540654  [5.402 sec/step, loss=0.07420, avg_loss=0.07362]\n",
      "Step 540655  [5.399 sec/step, loss=0.07473, avg_loss=0.07363]\n",
      "Step 540656  [5.399 sec/step, loss=0.07277, avg_loss=0.07363]\n",
      "Step 540657  [5.403 sec/step, loss=0.07402, avg_loss=0.07367]\n",
      "Step 540658  [5.374 sec/step, loss=0.07489, avg_loss=0.07368]\n",
      "Step 540659  [5.381 sec/step, loss=0.07453, avg_loss=0.07367]\n",
      "Step 540660  [5.394 sec/step, loss=0.07386, avg_loss=0.07371]\n",
      "Step 540661  [5.396 sec/step, loss=0.07517, avg_loss=0.07370]\n",
      "Step 540662  [5.449 sec/step, loss=0.06544, avg_loss=0.07364]\n",
      "Step 540663  [5.450 sec/step, loss=0.07544, avg_loss=0.07364]\n",
      "Step 540664  [5.459 sec/step, loss=0.07292, avg_loss=0.07362]\n",
      "Step 540665  [5.453 sec/step, loss=0.07355, avg_loss=0.07362]\n",
      "Generated 32 batches of size 32 in 2.361 sec\n",
      "Step 540666  [5.477 sec/step, loss=0.07411, avg_loss=0.07366]\n",
      "Step 540667  [5.464 sec/step, loss=0.06575, avg_loss=0.07357]\n",
      "Step 540668  [5.471 sec/step, loss=0.07591, avg_loss=0.07362]\n",
      "Step 540669  [5.488 sec/step, loss=0.07241, avg_loss=0.07368]\n",
      "Step 540670  [5.485 sec/step, loss=0.07351, avg_loss=0.07368]\n",
      "Step 540671  [5.472 sec/step, loss=0.07206, avg_loss=0.07365]\n",
      "Step 540672  [5.476 sec/step, loss=0.07469, avg_loss=0.07365]\n",
      "Step 540673  [5.484 sec/step, loss=0.07490, avg_loss=0.07367]\n",
      "Step 540674  [5.494 sec/step, loss=0.07550, avg_loss=0.07367]\n",
      "Step 540675  [5.472 sec/step, loss=0.06493, avg_loss=0.07356]\n",
      "Step 540676  [5.473 sec/step, loss=0.07572, avg_loss=0.07357]\n",
      "Step 540677  [5.490 sec/step, loss=0.07300, avg_loss=0.07355]\n",
      "Step 540678  [5.466 sec/step, loss=0.07298, avg_loss=0.07362]\n",
      "Step 540679  [5.442 sec/step, loss=0.07157, avg_loss=0.07357]\n",
      "Step 540680  [5.453 sec/step, loss=0.07154, avg_loss=0.07357]\n",
      "Step 540681  [5.445 sec/step, loss=0.07500, avg_loss=0.07357]\n",
      "Step 540682  [5.461 sec/step, loss=0.07478, avg_loss=0.07357]\n",
      "Step 540683  [5.449 sec/step, loss=0.07482, avg_loss=0.07355]\n",
      "Step 540684  [5.492 sec/step, loss=0.06662, avg_loss=0.07346]\n",
      "Step 540685  [5.466 sec/step, loss=0.07341, avg_loss=0.07346]\n",
      "Step 540686  [5.457 sec/step, loss=0.07113, avg_loss=0.07343]\n",
      "Step 540687  [5.467 sec/step, loss=0.07570, avg_loss=0.07348]\n",
      "Step 540688  [5.473 sec/step, loss=0.07382, avg_loss=0.07346]\n",
      "Step 540689  [5.497 sec/step, loss=0.07520, avg_loss=0.07351]\n",
      "Step 540690  [5.527 sec/step, loss=0.07594, avg_loss=0.07360]\n",
      "Step 540691  [5.530 sec/step, loss=0.07297, avg_loss=0.07360]\n",
      "Step 540692  [5.530 sec/step, loss=0.07414, avg_loss=0.07358]\n",
      "Step 540693  [5.485 sec/step, loss=0.07492, avg_loss=0.07366]\n",
      "Step 540694  [5.479 sec/step, loss=0.07352, avg_loss=0.07365]\n",
      "Step 540695  [5.498 sec/step, loss=0.07284, avg_loss=0.07364]\n",
      "Step 540696  [5.508 sec/step, loss=0.07488, avg_loss=0.07366]\n",
      "Step 540697  [5.491 sec/step, loss=0.07453, avg_loss=0.07365]\n",
      "Generated 32 batches of size 32 in 2.552 sec\n",
      "Step 540698  [5.480 sec/step, loss=0.07201, avg_loss=0.07361]\n",
      "Step 540699  [5.484 sec/step, loss=0.07563, avg_loss=0.07361]\n",
      "Step 540700  [5.478 sec/step, loss=0.07530, avg_loss=0.07362]\n",
      "Writing summary at step: 540700\n",
      "Step 540701  [5.470 sec/step, loss=0.07270, avg_loss=0.07359]\n",
      "Step 540702  [5.460 sec/step, loss=0.07412, avg_loss=0.07360]\n",
      "Step 540703  [5.464 sec/step, loss=0.07118, avg_loss=0.07357]\n",
      "Step 540704  [5.465 sec/step, loss=0.07333, avg_loss=0.07357]\n",
      "Step 540705  [5.461 sec/step, loss=0.07340, avg_loss=0.07354]\n",
      "Step 540706  [5.465 sec/step, loss=0.07479, avg_loss=0.07355]\n",
      "Step 540707  [5.468 sec/step, loss=0.07513, avg_loss=0.07355]\n",
      "Step 540708  [5.466 sec/step, loss=0.07323, avg_loss=0.07352]\n",
      "Step 540709  [5.478 sec/step, loss=0.07530, avg_loss=0.07355]\n",
      "Step 540710  [5.452 sec/step, loss=0.07227, avg_loss=0.07354]\n",
      "Step 540711  [5.433 sec/step, loss=0.07390, avg_loss=0.07352]\n",
      "Step 540712  [5.418 sec/step, loss=0.07375, avg_loss=0.07350]\n",
      "Step 540713  [5.430 sec/step, loss=0.07381, avg_loss=0.07349]\n",
      "Step 540714  [5.459 sec/step, loss=0.07494, avg_loss=0.07352]\n",
      "Step 540715  [5.522 sec/step, loss=0.06588, avg_loss=0.07344]\n",
      "Step 540716  [5.511 sec/step, loss=0.07534, avg_loss=0.07343]\n",
      "Step 540717  [5.517 sec/step, loss=0.07624, avg_loss=0.07344]\n",
      "Step 540718  [5.504 sec/step, loss=0.07421, avg_loss=0.07342]\n",
      "Step 540719  [5.512 sec/step, loss=0.07440, avg_loss=0.07346]\n",
      "Step 540720  [5.506 sec/step, loss=0.07495, avg_loss=0.07345]\n",
      "Step 540721  [5.499 sec/step, loss=0.07239, avg_loss=0.07343]\n",
      "Step 540722  [5.479 sec/step, loss=0.07337, avg_loss=0.07343]\n",
      "Step 540723  [5.507 sec/step, loss=0.07555, avg_loss=0.07354]\n",
      "Step 540724  [5.517 sec/step, loss=0.07512, avg_loss=0.07354]\n",
      "Step 540725  [5.511 sec/step, loss=0.07558, avg_loss=0.07354]\n",
      "Step 540726  [5.475 sec/step, loss=0.07527, avg_loss=0.07363]\n",
      "Step 540727  [5.499 sec/step, loss=0.07434, avg_loss=0.07362]\n",
      "Step 540728  [5.489 sec/step, loss=0.07403, avg_loss=0.07362]\n",
      "Generated 32 batches of size 32 in 2.747 sec\n",
      "Step 540729  [5.466 sec/step, loss=0.07147, avg_loss=0.07360]\n",
      "Step 540730  [5.475 sec/step, loss=0.07620, avg_loss=0.07362]\n",
      "Step 540731  [5.473 sec/step, loss=0.06580, avg_loss=0.07357]\n",
      "Step 540732  [5.482 sec/step, loss=0.07130, avg_loss=0.07354]\n",
      "Step 540733  [5.476 sec/step, loss=0.07334, avg_loss=0.07355]\n",
      "Step 540734  [5.467 sec/step, loss=0.07327, avg_loss=0.07354]\n",
      "Step 540735  [5.460 sec/step, loss=0.07055, avg_loss=0.07350]\n",
      "Step 540736  [5.455 sec/step, loss=0.07484, avg_loss=0.07350]\n",
      "Step 540737  [5.442 sec/step, loss=0.07339, avg_loss=0.07348]\n",
      "Step 540738  [5.454 sec/step, loss=0.07377, avg_loss=0.07349]\n",
      "Step 540739  [5.466 sec/step, loss=0.07588, avg_loss=0.07350]\n",
      "Step 540740  [5.460 sec/step, loss=0.07603, avg_loss=0.07350]\n",
      "Step 540741  [5.462 sec/step, loss=0.07345, avg_loss=0.07348]\n",
      "Step 540742  [5.467 sec/step, loss=0.07498, avg_loss=0.07350]\n",
      "Step 540743  [5.485 sec/step, loss=0.07482, avg_loss=0.07354]\n",
      "Step 540744  [5.510 sec/step, loss=0.07339, avg_loss=0.07355]\n",
      "Step 540745  [5.509 sec/step, loss=0.07579, avg_loss=0.07357]\n",
      "Step 540746  [5.494 sec/step, loss=0.07364, avg_loss=0.07354]\n",
      "Step 540747  [5.496 sec/step, loss=0.07474, avg_loss=0.07356]\n",
      "Step 540748  [5.486 sec/step, loss=0.07445, avg_loss=0.07356]\n",
      "Step 540749  [5.520 sec/step, loss=0.07364, avg_loss=0.07360]\n",
      "Step 540750  [5.516 sec/step, loss=0.07067, avg_loss=0.07355]\n",
      "Step 540751  [5.507 sec/step, loss=0.06596, avg_loss=0.07348]\n",
      "Step 540752  [5.493 sec/step, loss=0.07439, avg_loss=0.07347]\n",
      "Step 540753  [5.479 sec/step, loss=0.07200, avg_loss=0.07344]\n",
      "Step 540754  [5.489 sec/step, loss=0.07525, avg_loss=0.07345]\n",
      "Step 540755  [5.505 sec/step, loss=0.07356, avg_loss=0.07343]\n",
      "Step 540756  [5.499 sec/step, loss=0.06952, avg_loss=0.07340]\n",
      "Step 540757  [5.507 sec/step, loss=0.07388, avg_loss=0.07340]\n",
      "Step 540758  [5.512 sec/step, loss=0.07269, avg_loss=0.07338]\n",
      "Step 540759  [5.505 sec/step, loss=0.07412, avg_loss=0.07337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 540760  [5.499 sec/step, loss=0.07428, avg_loss=0.07338]\n",
      "Generated 32 batches of size 32 in 2.529 sec\n",
      "Step 540761  [5.484 sec/step, loss=0.07283, avg_loss=0.07335]\n",
      "Step 540762  [5.427 sec/step, loss=0.07325, avg_loss=0.07343]\n",
      "Step 540763  [5.427 sec/step, loss=0.07514, avg_loss=0.07343]\n",
      "Step 540764  [5.411 sec/step, loss=0.07380, avg_loss=0.07344]\n",
      "Step 540765  [5.430 sec/step, loss=0.07359, avg_loss=0.07344]\n",
      "Step 540766  [5.417 sec/step, loss=0.07317, avg_loss=0.07343]\n",
      "Step 540767  [5.442 sec/step, loss=0.07611, avg_loss=0.07353]\n",
      "Step 540768  [5.420 sec/step, loss=0.07063, avg_loss=0.07348]\n",
      "Step 540769  [5.469 sec/step, loss=0.06572, avg_loss=0.07341]\n",
      "Step 540770  [5.481 sec/step, loss=0.07400, avg_loss=0.07342]\n",
      "Step 540771  [5.487 sec/step, loss=0.07395, avg_loss=0.07344]\n",
      "Step 540772  [5.487 sec/step, loss=0.07488, avg_loss=0.07344]\n",
      "Step 540773  [5.497 sec/step, loss=0.07543, avg_loss=0.07344]\n",
      "Step 540774  [5.495 sec/step, loss=0.07570, avg_loss=0.07345]\n",
      "Step 540775  [5.499 sec/step, loss=0.07005, avg_loss=0.07350]\n",
      "Step 540776  [5.515 sec/step, loss=0.07247, avg_loss=0.07347]\n",
      "Step 540777  [5.501 sec/step, loss=0.07438, avg_loss=0.07348]\n",
      "Step 540778  [5.474 sec/step, loss=0.07344, avg_loss=0.07348]\n",
      "Step 540779  [5.487 sec/step, loss=0.07017, avg_loss=0.07347]\n",
      "Step 540780  [5.493 sec/step, loss=0.07479, avg_loss=0.07350]\n",
      "Step 540781  [5.501 sec/step, loss=0.07428, avg_loss=0.07349]\n",
      "Step 540782  [5.500 sec/step, loss=0.07562, avg_loss=0.07350]\n",
      "Step 540783  [5.498 sec/step, loss=0.07179, avg_loss=0.07347]\n",
      "Step 540784  [5.440 sec/step, loss=0.07282, avg_loss=0.07354]\n",
      "Step 540785  [5.447 sec/step, loss=0.07529, avg_loss=0.07355]\n",
      "Step 540786  [5.468 sec/step, loss=0.07347, avg_loss=0.07358]\n",
      "Step 540787  [5.482 sec/step, loss=0.07294, avg_loss=0.07355]\n",
      "Step 540788  [5.472 sec/step, loss=0.07304, avg_loss=0.07354]\n",
      "Step 540789  [5.450 sec/step, loss=0.07361, avg_loss=0.07353]\n",
      "Step 540790  [5.432 sec/step, loss=0.07297, avg_loss=0.07350]\n",
      "Step 540791  [5.426 sec/step, loss=0.06996, avg_loss=0.07347]\n",
      "Step 540792  [5.468 sec/step, loss=0.06670, avg_loss=0.07339]\n",
      "Generated 32 batches of size 32 in 2.549 sec\n",
      "Step 540793  [5.471 sec/step, loss=0.07478, avg_loss=0.07339]\n",
      "Step 540794  [5.480 sec/step, loss=0.07560, avg_loss=0.07341]\n",
      "Step 540795  [5.478 sec/step, loss=0.07569, avg_loss=0.07344]\n",
      "Step 540796  [5.474 sec/step, loss=0.07432, avg_loss=0.07343]\n",
      "Step 540797  [5.472 sec/step, loss=0.07172, avg_loss=0.07341]\n",
      "Step 540798  [5.469 sec/step, loss=0.07460, avg_loss=0.07343]\n",
      "Step 540799  [5.472 sec/step, loss=0.07327, avg_loss=0.07341]\n",
      "Step 540800  [5.484 sec/step, loss=0.07388, avg_loss=0.07339]\n",
      "Writing summary at step: 540800\n",
      "Step 540801  [5.487 sec/step, loss=0.07243, avg_loss=0.07339]\n",
      "Step 540802  [5.469 sec/step, loss=0.07297, avg_loss=0.07338]\n",
      "Step 540803  [5.477 sec/step, loss=0.07474, avg_loss=0.07342]\n",
      "Step 540804  [5.500 sec/step, loss=0.07341, avg_loss=0.07342]\n",
      "Step 540805  [5.507 sec/step, loss=0.07531, avg_loss=0.07344]\n",
      "Step 540806  [5.511 sec/step, loss=0.07541, avg_loss=0.07344]\n",
      "Step 540807  [5.499 sec/step, loss=0.07325, avg_loss=0.07342]\n",
      "Step 540808  [5.510 sec/step, loss=0.07290, avg_loss=0.07342]\n",
      "Step 540809  [5.549 sec/step, loss=0.06515, avg_loss=0.07332]\n",
      "Step 540810  [5.559 sec/step, loss=0.07500, avg_loss=0.07334]\n",
      "Step 540811  [5.557 sec/step, loss=0.07088, avg_loss=0.07331]\n",
      "Step 540812  [5.567 sec/step, loss=0.07560, avg_loss=0.07333]\n",
      "Step 540813  [5.584 sec/step, loss=0.07488, avg_loss=0.07334]\n",
      "Step 540814  [5.564 sec/step, loss=0.07284, avg_loss=0.07332]\n",
      "Step 540815  [5.521 sec/step, loss=0.07541, avg_loss=0.07342]\n",
      "Step 540816  [5.538 sec/step, loss=0.07585, avg_loss=0.07342]\n",
      "Step 540817  [5.534 sec/step, loss=0.07458, avg_loss=0.07341]\n",
      "Step 540818  [5.545 sec/step, loss=0.07558, avg_loss=0.07342]\n",
      "Step 540819  [5.542 sec/step, loss=0.07332, avg_loss=0.07341]\n",
      "Step 540820  [5.543 sec/step, loss=0.07486, avg_loss=0.07341]\n",
      "Step 540821  [5.552 sec/step, loss=0.07475, avg_loss=0.07343]\n",
      "Step 540822  [5.538 sec/step, loss=0.07397, avg_loss=0.07344]\n",
      "Step 540823  [5.529 sec/step, loss=0.07216, avg_loss=0.07340]\n",
      "Generated 32 batches of size 32 in 2.434 sec\n",
      "Step 540824  [5.519 sec/step, loss=0.07457, avg_loss=0.07340]\n",
      "Step 540825  [5.517 sec/step, loss=0.07389, avg_loss=0.07338]\n",
      "Step 540826  [5.509 sec/step, loss=0.07397, avg_loss=0.07337]\n",
      "Step 540827  [5.468 sec/step, loss=0.07180, avg_loss=0.07334]\n",
      "Step 540828  [5.459 sec/step, loss=0.07311, avg_loss=0.07333]\n",
      "Step 540829  [5.474 sec/step, loss=0.07263, avg_loss=0.07335]\n",
      "Step 540830  [5.484 sec/step, loss=0.07502, avg_loss=0.07333]\n",
      "Step 540831  [5.484 sec/step, loss=0.06577, avg_loss=0.07333]\n",
      "Step 540832  [5.483 sec/step, loss=0.07128, avg_loss=0.07333]\n",
      "Step 540833  [5.477 sec/step, loss=0.06550, avg_loss=0.07326]\n",
      "Step 540834  [5.492 sec/step, loss=0.07546, avg_loss=0.07328]\n",
      "Step 540835  [5.487 sec/step, loss=0.07340, avg_loss=0.07331]\n",
      "Step 540836  [5.480 sec/step, loss=0.07324, avg_loss=0.07329]\n",
      "Step 540837  [5.483 sec/step, loss=0.07357, avg_loss=0.07329]\n",
      "Step 540838  [5.475 sec/step, loss=0.07420, avg_loss=0.07330]\n",
      "Step 540839  [5.463 sec/step, loss=0.07019, avg_loss=0.07324]\n",
      "Step 540840  [5.447 sec/step, loss=0.07297, avg_loss=0.07321]\n",
      "Step 540841  [5.445 sec/step, loss=0.07519, avg_loss=0.07323]\n",
      "Step 540842  [5.460 sec/step, loss=0.07488, avg_loss=0.07322]\n",
      "Step 540843  [5.458 sec/step, loss=0.07497, avg_loss=0.07323]\n",
      "Step 540844  [5.439 sec/step, loss=0.07492, avg_loss=0.07324]\n",
      "Step 540845  [5.434 sec/step, loss=0.07289, avg_loss=0.07321]\n",
      "Step 540846  [5.451 sec/step, loss=0.07533, avg_loss=0.07323]\n",
      "Step 540847  [5.453 sec/step, loss=0.07450, avg_loss=0.07323]\n",
      "Step 540848  [5.446 sec/step, loss=0.07197, avg_loss=0.07320]\n",
      "Step 540849  [5.450 sec/step, loss=0.07272, avg_loss=0.07319]\n",
      "Step 540850  [5.450 sec/step, loss=0.07469, avg_loss=0.07323]\n",
      "Step 540851  [5.480 sec/step, loss=0.07505, avg_loss=0.07332]\n",
      "Step 540852  [5.528 sec/step, loss=0.06482, avg_loss=0.07323]\n",
      "Step 540853  [5.540 sec/step, loss=0.07587, avg_loss=0.07327]\n",
      "Step 540854  [5.518 sec/step, loss=0.07115, avg_loss=0.07323]\n",
      "Step 540855  [5.510 sec/step, loss=0.07419, avg_loss=0.07323]\n",
      "Generated 32 batches of size 32 in 2.477 sec\n",
      "Step 540856  [5.543 sec/step, loss=0.07495, avg_loss=0.07329]\n",
      "Step 540857  [5.546 sec/step, loss=0.07312, avg_loss=0.07328]\n",
      "Step 540858  [5.546 sec/step, loss=0.07332, avg_loss=0.07329]\n",
      "Step 540859  [5.551 sec/step, loss=0.07439, avg_loss=0.07329]\n",
      "Step 540860  [5.555 sec/step, loss=0.07285, avg_loss=0.07327]\n",
      "Step 540861  [5.567 sec/step, loss=0.07420, avg_loss=0.07329]\n",
      "Step 540862  [5.584 sec/step, loss=0.07505, avg_loss=0.07331]\n",
      "Step 540863  [5.582 sec/step, loss=0.07532, avg_loss=0.07331]\n",
      "Step 540864  [5.560 sec/step, loss=0.07126, avg_loss=0.07328]\n",
      "Step 540865  [5.557 sec/step, loss=0.07590, avg_loss=0.07330]\n",
      "Step 540866  [5.557 sec/step, loss=0.07468, avg_loss=0.07332]\n",
      "Step 540867  [5.550 sec/step, loss=0.07366, avg_loss=0.07330]\n",
      "Step 540868  [5.570 sec/step, loss=0.07372, avg_loss=0.07333]\n",
      "Step 540869  [5.512 sec/step, loss=0.07124, avg_loss=0.07338]\n",
      "Step 540870  [5.521 sec/step, loss=0.07290, avg_loss=0.07337]\n",
      "Step 540871  [5.528 sec/step, loss=0.07531, avg_loss=0.07338]\n",
      "Step 540872  [5.523 sec/step, loss=0.07521, avg_loss=0.07339]\n",
      "Step 540873  [5.514 sec/step, loss=0.07451, avg_loss=0.07338]\n",
      "Step 540874  [5.533 sec/step, loss=0.07229, avg_loss=0.07334]\n",
      "Step 540875  [5.540 sec/step, loss=0.07336, avg_loss=0.07338]\n",
      "Step 540876  [5.521 sec/step, loss=0.07435, avg_loss=0.07340]\n",
      "Step 540877  [5.534 sec/step, loss=0.07413, avg_loss=0.07339]\n",
      "Step 540878  [5.545 sec/step, loss=0.07579, avg_loss=0.07342]\n",
      "Step 540879  [5.548 sec/step, loss=0.07332, avg_loss=0.07345]\n",
      "Step 540880  [5.547 sec/step, loss=0.07401, avg_loss=0.07344]\n",
      "Step 540881  [5.528 sec/step, loss=0.07073, avg_loss=0.07341]\n",
      "Step 540882  [5.501 sec/step, loss=0.06524, avg_loss=0.07330]\n",
      "Step 540883  [5.500 sec/step, loss=0.06986, avg_loss=0.07328]\n",
      "Step 540884  [5.560 sec/step, loss=0.06664, avg_loss=0.07322]\n",
      "Step 540885  [5.543 sec/step, loss=0.06910, avg_loss=0.07316]\n",
      "Step 540886  [5.543 sec/step, loss=0.07356, avg_loss=0.07316]\n",
      "Step 540887  [5.514 sec/step, loss=0.07286, avg_loss=0.07316]\n",
      "Generated 32 batches of size 32 in 2.464 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 540888  [5.514 sec/step, loss=0.07354, avg_loss=0.07316]\n",
      "Step 540889  [5.526 sec/step, loss=0.07351, avg_loss=0.07316]\n",
      "Step 540890  [5.540 sec/step, loss=0.07557, avg_loss=0.07319]\n",
      "Step 540891  [5.550 sec/step, loss=0.07139, avg_loss=0.07320]\n",
      "Step 540892  [5.503 sec/step, loss=0.07457, avg_loss=0.07328]\n",
      "Step 540893  [5.501 sec/step, loss=0.07345, avg_loss=0.07327]\n",
      "Step 540894  [5.496 sec/step, loss=0.07606, avg_loss=0.07327]\n",
      "Step 540895  [5.482 sec/step, loss=0.07046, avg_loss=0.07322]\n",
      "Step 540896  [5.498 sec/step, loss=0.07512, avg_loss=0.07323]\n",
      "Step 540897  [5.494 sec/step, loss=0.06969, avg_loss=0.07321]\n",
      "Step 540898  [5.498 sec/step, loss=0.07373, avg_loss=0.07320]\n",
      "Step 540899  [5.492 sec/step, loss=0.07459, avg_loss=0.07321]\n",
      "Step 540900  [5.485 sec/step, loss=0.07326, avg_loss=0.07321]\n",
      "Writing summary at step: 540900\n",
      "Step 540901  [5.492 sec/step, loss=0.07501, avg_loss=0.07323]\n",
      "Step 540902  [5.499 sec/step, loss=0.07338, avg_loss=0.07324]\n",
      "Step 540903  [5.483 sec/step, loss=0.07349, avg_loss=0.07322]\n",
      "Step 540904  [5.474 sec/step, loss=0.07498, avg_loss=0.07324]\n",
      "Step 540905  [5.465 sec/step, loss=0.07475, avg_loss=0.07323]\n",
      "Step 540906  [5.462 sec/step, loss=0.07213, avg_loss=0.07320]\n",
      "Step 540907  [5.457 sec/step, loss=0.07176, avg_loss=0.07319]\n",
      "Step 540908  [5.443 sec/step, loss=0.07305, avg_loss=0.07319]\n",
      "Step 540909  [5.401 sec/step, loss=0.07534, avg_loss=0.07329]\n",
      "Step 540910  [5.405 sec/step, loss=0.07576, avg_loss=0.07330]\n",
      "Step 540911  [5.399 sec/step, loss=0.06534, avg_loss=0.07324]\n",
      "Step 540912  [5.389 sec/step, loss=0.07111, avg_loss=0.07320]\n",
      "Step 540913  [5.359 sec/step, loss=0.06963, avg_loss=0.07315]\n",
      "Step 540914  [5.378 sec/step, loss=0.07577, avg_loss=0.07317]\n",
      "Step 540915  [5.379 sec/step, loss=0.07610, avg_loss=0.07318]\n",
      "Step 540916  [5.378 sec/step, loss=0.07562, avg_loss=0.07318]\n",
      "Step 540917  [5.372 sec/step, loss=0.07097, avg_loss=0.07314]\n",
      "Step 540918  [5.371 sec/step, loss=0.07266, avg_loss=0.07311]\n",
      "Generated 32 batches of size 32 in 2.394 sec\n",
      "Step 540919  [5.402 sec/step, loss=0.07297, avg_loss=0.07311]\n",
      "Step 540920  [5.385 sec/step, loss=0.07144, avg_loss=0.07308]\n",
      "Step 540921  [5.398 sec/step, loss=0.07342, avg_loss=0.07306]\n",
      "Step 540922  [5.407 sec/step, loss=0.07272, avg_loss=0.07305]\n",
      "Step 540923  [5.405 sec/step, loss=0.07428, avg_loss=0.07307]\n",
      "Step 540924  [5.409 sec/step, loss=0.07510, avg_loss=0.07308]\n",
      "Step 540925  [5.423 sec/step, loss=0.07520, avg_loss=0.07309]\n",
      "Step 540926  [5.427 sec/step, loss=0.07321, avg_loss=0.07308]\n",
      "Step 540927  [5.492 sec/step, loss=0.06510, avg_loss=0.07302]\n",
      "Step 540928  [5.493 sec/step, loss=0.07291, avg_loss=0.07301]\n",
      "Step 540929  [5.476 sec/step, loss=0.07371, avg_loss=0.07302]\n",
      "Step 540930  [5.481 sec/step, loss=0.07230, avg_loss=0.07300]\n",
      "Step 540931  [5.483 sec/step, loss=0.07122, avg_loss=0.07305]\n",
      "Step 540932  [5.467 sec/step, loss=0.06495, avg_loss=0.07299]\n",
      "Step 540933  [5.488 sec/step, loss=0.07291, avg_loss=0.07306]\n",
      "Step 540934  [5.503 sec/step, loss=0.07237, avg_loss=0.07303]\n",
      "Step 540935  [5.521 sec/step, loss=0.07571, avg_loss=0.07305]\n",
      "Step 540936  [5.582 sec/step, loss=0.06532, avg_loss=0.07298]\n",
      "Step 540937  [5.595 sec/step, loss=0.07417, avg_loss=0.07298]\n",
      "Step 540938  [5.597 sec/step, loss=0.07036, avg_loss=0.07294]\n",
      "Step 540939  [5.600 sec/step, loss=0.07301, avg_loss=0.07297]\n",
      "Step 540940  [5.624 sec/step, loss=0.07566, avg_loss=0.07300]\n",
      "Step 540941  [5.640 sec/step, loss=0.07278, avg_loss=0.07297]\n",
      "Step 540942  [5.637 sec/step, loss=0.07379, avg_loss=0.07296]\n",
      "Step 540943  [5.645 sec/step, loss=0.07597, avg_loss=0.07297]\n",
      "Step 540944  [5.634 sec/step, loss=0.07438, avg_loss=0.07297]\n",
      "Step 540945  [5.635 sec/step, loss=0.07446, avg_loss=0.07298]\n",
      "Step 540946  [5.631 sec/step, loss=0.07512, avg_loss=0.07298]\n",
      "Step 540947  [5.634 sec/step, loss=0.07414, avg_loss=0.07298]\n",
      "Step 540948  [5.638 sec/step, loss=0.07387, avg_loss=0.07300]\n",
      "Step 540949  [5.610 sec/step, loss=0.07366, avg_loss=0.07301]\n",
      "Step 540950  [5.605 sec/step, loss=0.07306, avg_loss=0.07299]\n",
      "Generated 32 batches of size 32 in 2.412 sec\n",
      "Step 540951  [5.607 sec/step, loss=0.07504, avg_loss=0.07299]\n",
      "Step 540952  [5.563 sec/step, loss=0.07276, avg_loss=0.07307]\n",
      "Step 540953  [5.543 sec/step, loss=0.06999, avg_loss=0.07301]\n",
      "Step 540954  [5.556 sec/step, loss=0.07407, avg_loss=0.07304]\n",
      "Step 540955  [5.553 sec/step, loss=0.07478, avg_loss=0.07305]\n",
      "Step 540956  [5.539 sec/step, loss=0.07549, avg_loss=0.07305]\n",
      "Step 540957  [5.531 sec/step, loss=0.07173, avg_loss=0.07304]\n",
      "Step 540958  [5.526 sec/step, loss=0.07330, avg_loss=0.07304]\n",
      "Step 540959  [5.535 sec/step, loss=0.07494, avg_loss=0.07304]\n",
      "Step 540960  [5.515 sec/step, loss=0.06558, avg_loss=0.07297]\n",
      "Step 540961  [5.515 sec/step, loss=0.07557, avg_loss=0.07298]\n",
      "Step 540962  [5.498 sec/step, loss=0.07324, avg_loss=0.07297]\n",
      "Step 540963  [5.486 sec/step, loss=0.07499, avg_loss=0.07296]\n",
      "Step 540964  [5.501 sec/step, loss=0.07413, avg_loss=0.07299]\n",
      "Step 540965  [5.486 sec/step, loss=0.07121, avg_loss=0.07294]\n",
      "Step 540966  [5.504 sec/step, loss=0.07421, avg_loss=0.07294]\n",
      "Step 540967  [5.513 sec/step, loss=0.07576, avg_loss=0.07296]\n",
      "Step 540968  [5.525 sec/step, loss=0.07532, avg_loss=0.07298]\n",
      "Step 540969  [5.529 sec/step, loss=0.07189, avg_loss=0.07298]\n",
      "Step 540970  [5.544 sec/step, loss=0.07196, avg_loss=0.07297]\n",
      "Step 540971  [5.544 sec/step, loss=0.07395, avg_loss=0.07296]\n",
      "Step 540972  [5.535 sec/step, loss=0.07325, avg_loss=0.07294]\n",
      "Step 540973  [5.539 sec/step, loss=0.07514, avg_loss=0.07295]\n",
      "Step 540974  [5.522 sec/step, loss=0.07529, avg_loss=0.07298]\n",
      "Step 540975  [5.535 sec/step, loss=0.07170, avg_loss=0.07296]\n",
      "Step 540976  [5.526 sec/step, loss=0.07295, avg_loss=0.07295]\n",
      "Step 540977  [5.550 sec/step, loss=0.06866, avg_loss=0.07289]\n",
      "Step 540978  [5.541 sec/step, loss=0.07422, avg_loss=0.07287]\n",
      "Step 540979  [5.540 sec/step, loss=0.07369, avg_loss=0.07288]\n",
      "Step 540980  [5.536 sec/step, loss=0.07441, avg_loss=0.07288]\n",
      "Step 540981  [5.540 sec/step, loss=0.07048, avg_loss=0.07288]\n",
      "Step 540982  [5.559 sec/step, loss=0.07043, avg_loss=0.07293]\n",
      "Generated 32 batches of size 32 in 2.395 sec\n",
      "Step 540983  [5.572 sec/step, loss=0.07502, avg_loss=0.07298]\n",
      "Step 540984  [5.506 sec/step, loss=0.07117, avg_loss=0.07303]\n",
      "Step 540985  [5.514 sec/step, loss=0.07461, avg_loss=0.07308]\n",
      "Step 540986  [5.504 sec/step, loss=0.07385, avg_loss=0.07309]\n",
      "Step 540987  [5.505 sec/step, loss=0.07233, avg_loss=0.07308]\n",
      "Step 540988  [5.520 sec/step, loss=0.07511, avg_loss=0.07310]\n",
      "Step 540989  [5.510 sec/step, loss=0.07340, avg_loss=0.07310]\n",
      "Step 540990  [5.506 sec/step, loss=0.07441, avg_loss=0.07308]\n",
      "Step 540991  [5.516 sec/step, loss=0.07592, avg_loss=0.07313]\n",
      "Step 540992  [5.530 sec/step, loss=0.07479, avg_loss=0.07313]\n",
      "Step 540993  [5.525 sec/step, loss=0.07449, avg_loss=0.07314]\n",
      "Step 540994  [5.528 sec/step, loss=0.07535, avg_loss=0.07314]\n",
      "Step 540995  [5.537 sec/step, loss=0.07349, avg_loss=0.07317]\n",
      "Step 540996  [5.507 sec/step, loss=0.06490, avg_loss=0.07306]\n",
      "Step 540997  [5.564 sec/step, loss=0.06502, avg_loss=0.07302]\n",
      "Step 540998  [5.573 sec/step, loss=0.07520, avg_loss=0.07303]\n",
      "Step 540999  [5.593 sec/step, loss=0.07449, avg_loss=0.07303]\n",
      "Step 541000  [5.585 sec/step, loss=0.07446, avg_loss=0.07304]\n",
      "Writing summary at step: 541000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-541000\n",
      "Saving audio and alignment...\n",
      "Input: baquul shaxsay dzamhuuriijatd kii haesijatd hoz kay saatdh xuddro ghaas dzitdnii hae~___________________________________\n",
      "Step 541001  [5.590 sec/step, loss=0.07562, avg_loss=0.07305]\n",
      "Step 541002  [5.581 sec/step, loss=0.07075, avg_loss=0.07302]\n",
      "Step 541003  [5.587 sec/step, loss=0.07360, avg_loss=0.07302]\n",
      "Step 541004  [5.594 sec/step, loss=0.07540, avg_loss=0.07303]\n",
      "Step 541005  [5.590 sec/step, loss=0.07258, avg_loss=0.07301]\n",
      "Step 541006  [5.579 sec/step, loss=0.07285, avg_loss=0.07301]\n",
      "Step 541007  [5.582 sec/step, loss=0.07352, avg_loss=0.07303]\n",
      "Step 541008  [5.587 sec/step, loss=0.07470, avg_loss=0.07305]\n",
      "Step 541009  [5.584 sec/step, loss=0.07396, avg_loss=0.07303]\n",
      "Step 541010  [5.570 sec/step, loss=0.07322, avg_loss=0.07301]\n",
      "Step 541011  [5.585 sec/step, loss=0.07273, avg_loss=0.07308]\n",
      "Step 541012  [5.575 sec/step, loss=0.07110, avg_loss=0.07308]\n",
      "Generated 32 batches of size 32 in 2.771 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 541013  [5.574 sec/step, loss=0.07017, avg_loss=0.07309]\n",
      "Step 541014  [5.567 sec/step, loss=0.07306, avg_loss=0.07306]\n",
      "Step 541015  [5.562 sec/step, loss=0.07482, avg_loss=0.07305]\n",
      "Step 541016  [5.547 sec/step, loss=0.07368, avg_loss=0.07303]\n",
      "Step 541017  [5.561 sec/step, loss=0.07540, avg_loss=0.07307]\n",
      "Step 541018  [5.553 sec/step, loss=0.07424, avg_loss=0.07309]\n",
      "Step 541019  [5.532 sec/step, loss=0.07483, avg_loss=0.07311]\n",
      "Step 541020  [5.536 sec/step, loss=0.07387, avg_loss=0.07313]\n",
      "Step 541021  [5.532 sec/step, loss=0.07410, avg_loss=0.07314]\n",
      "Step 541022  [5.530 sec/step, loss=0.07210, avg_loss=0.07313]\n",
      "Step 541023  [5.524 sec/step, loss=0.07272, avg_loss=0.07312]\n",
      "Step 541024  [5.507 sec/step, loss=0.07425, avg_loss=0.07311]\n",
      "Step 541025  [5.498 sec/step, loss=0.07537, avg_loss=0.07311]\n",
      "Step 541026  [5.478 sec/step, loss=0.07025, avg_loss=0.07308]\n",
      "Step 541027  [5.416 sec/step, loss=0.07277, avg_loss=0.07316]\n",
      "Step 541028  [5.433 sec/step, loss=0.07543, avg_loss=0.07318]\n",
      "Step 541029  [5.456 sec/step, loss=0.07470, avg_loss=0.07319]\n",
      "Step 541030  [5.445 sec/step, loss=0.07523, avg_loss=0.07322]\n",
      "Step 541031  [5.462 sec/step, loss=0.07448, avg_loss=0.07325]\n",
      "Step 541032  [5.529 sec/step, loss=0.06605, avg_loss=0.07326]\n",
      "Step 541033  [5.524 sec/step, loss=0.07402, avg_loss=0.07328]\n",
      "Step 541034  [5.483 sec/step, loss=0.07126, avg_loss=0.07326]\n",
      "Step 541035  [5.471 sec/step, loss=0.07344, avg_loss=0.07324]\n",
      "Step 541036  [5.432 sec/step, loss=0.07516, avg_loss=0.07334]\n",
      "Step 541037  [5.421 sec/step, loss=0.07217, avg_loss=0.07332]\n",
      "Step 541038  [5.429 sec/step, loss=0.07546, avg_loss=0.07337]\n",
      "Step 541039  [5.430 sec/step, loss=0.07391, avg_loss=0.07338]\n",
      "Step 541040  [5.411 sec/step, loss=0.07476, avg_loss=0.07337]\n",
      "Step 541041  [5.394 sec/step, loss=0.07517, avg_loss=0.07339]\n",
      "Step 541042  [5.373 sec/step, loss=0.07303, avg_loss=0.07339]\n",
      "Step 541043  [5.365 sec/step, loss=0.07391, avg_loss=0.07337]\n",
      "Step 541044  [5.363 sec/step, loss=0.07220, avg_loss=0.07334]\n",
      "Generated 32 batches of size 32 in 2.409 sec\n",
      "Step 541045  [5.372 sec/step, loss=0.07491, avg_loss=0.07335]\n",
      "Step 541046  [5.351 sec/step, loss=0.06544, avg_loss=0.07325]\n",
      "Step 541047  [5.349 sec/step, loss=0.07460, avg_loss=0.07326]\n",
      "Step 541048  [5.360 sec/step, loss=0.07335, avg_loss=0.07325]\n",
      "Step 541049  [5.352 sec/step, loss=0.07360, avg_loss=0.07325]\n",
      "Step 541050  [5.382 sec/step, loss=0.07291, avg_loss=0.07325]\n",
      "Step 541051  [5.369 sec/step, loss=0.07500, avg_loss=0.07325]\n",
      "Step 541052  [5.368 sec/step, loss=0.07483, avg_loss=0.07327]\n",
      "Step 541053  [5.407 sec/step, loss=0.07312, avg_loss=0.07330]\n",
      "Step 541054  [5.425 sec/step, loss=0.07514, avg_loss=0.07331]\n",
      "Step 541055  [5.416 sec/step, loss=0.07247, avg_loss=0.07329]\n",
      "Step 541056  [5.418 sec/step, loss=0.07530, avg_loss=0.07329]\n",
      "Step 541057  [5.411 sec/step, loss=0.07142, avg_loss=0.07328]\n",
      "Step 541058  [5.411 sec/step, loss=0.07154, avg_loss=0.07327]\n",
      "Step 541059  [5.405 sec/step, loss=0.07389, avg_loss=0.07326]\n",
      "Step 541060  [5.422 sec/step, loss=0.07016, avg_loss=0.07330]\n",
      "Step 541061  [5.404 sec/step, loss=0.07314, avg_loss=0.07328]\n",
      "Step 541062  [5.423 sec/step, loss=0.07245, avg_loss=0.07327]\n",
      "Step 541063  [5.436 sec/step, loss=0.07632, avg_loss=0.07328]\n",
      "Step 541064  [5.434 sec/step, loss=0.07399, avg_loss=0.07328]\n",
      "Step 541065  [5.434 sec/step, loss=0.07506, avg_loss=0.07332]\n",
      "Step 541066  [5.414 sec/step, loss=0.07294, avg_loss=0.07331]\n",
      "Step 541067  [5.408 sec/step, loss=0.07586, avg_loss=0.07331]\n",
      "Step 541068  [5.391 sec/step, loss=0.07381, avg_loss=0.07329]\n",
      "Step 541069  [5.395 sec/step, loss=0.07481, avg_loss=0.07332]\n",
      "Step 541070  [5.394 sec/step, loss=0.07132, avg_loss=0.07332]\n",
      "Step 541071  [5.380 sec/step, loss=0.07458, avg_loss=0.07332]\n",
      "Step 541072  [5.394 sec/step, loss=0.07418, avg_loss=0.07333]\n",
      "Step 541073  [5.385 sec/step, loss=0.07444, avg_loss=0.07332]\n",
      "Step 541074  [5.423 sec/step, loss=0.06605, avg_loss=0.07323]\n",
      "Step 541075  [5.431 sec/step, loss=0.07571, avg_loss=0.07327]\n",
      "Step 541076  [5.425 sec/step, loss=0.07356, avg_loss=0.07328]\n",
      "Generated 32 batches of size 32 in 2.394 sec\n",
      "Step 541077  [5.395 sec/step, loss=0.07519, avg_loss=0.07334]\n",
      "Step 541078  [5.399 sec/step, loss=0.07441, avg_loss=0.07335]\n",
      "Step 541079  [5.406 sec/step, loss=0.07588, avg_loss=0.07337]\n",
      "Step 541080  [5.393 sec/step, loss=0.06554, avg_loss=0.07328]\n",
      "Step 541081  [5.401 sec/step, loss=0.07438, avg_loss=0.07332]\n",
      "Step 541082  [5.397 sec/step, loss=0.07292, avg_loss=0.07334]\n",
      "Step 541083  [5.386 sec/step, loss=0.07395, avg_loss=0.07333]\n",
      "Step 541084  [5.386 sec/step, loss=0.07176, avg_loss=0.07334]\n",
      "Step 541085  [5.398 sec/step, loss=0.07584, avg_loss=0.07335]\n",
      "Step 541086  [5.401 sec/step, loss=0.07296, avg_loss=0.07334]\n",
      "Step 541087  [5.405 sec/step, loss=0.07427, avg_loss=0.07336]\n",
      "Step 541088  [5.397 sec/step, loss=0.07503, avg_loss=0.07336]\n",
      "Step 541089  [5.409 sec/step, loss=0.07372, avg_loss=0.07336]\n",
      "Step 541090  [5.405 sec/step, loss=0.07473, avg_loss=0.07337]\n",
      "Step 541091  [5.395 sec/step, loss=0.07265, avg_loss=0.07333]\n",
      "Step 541092  [5.376 sec/step, loss=0.07402, avg_loss=0.07333]\n",
      "Step 541093  [5.372 sec/step, loss=0.07474, avg_loss=0.07333]\n",
      "Step 541094  [5.378 sec/step, loss=0.07323, avg_loss=0.07331]\n",
      "Step 541095  [5.373 sec/step, loss=0.07373, avg_loss=0.07331]\n",
      "Step 541096  [5.383 sec/step, loss=0.07315, avg_loss=0.07339]\n",
      "Step 541097  [5.343 sec/step, loss=0.07525, avg_loss=0.07349]\n",
      "Step 541098  [5.329 sec/step, loss=0.07275, avg_loss=0.07347]\n",
      "Step 541099  [5.311 sec/step, loss=0.07561, avg_loss=0.07348]\n",
      "Step 541100  [5.320 sec/step, loss=0.07558, avg_loss=0.07349]\n",
      "Writing summary at step: 541100\n",
      "Step 541101  [5.301 sec/step, loss=0.06999, avg_loss=0.07344]\n",
      "Step 541102  [5.311 sec/step, loss=0.07306, avg_loss=0.07346]\n",
      "Step 541103  [5.337 sec/step, loss=0.07290, avg_loss=0.07345]\n",
      "Step 541104  [5.322 sec/step, loss=0.07157, avg_loss=0.07341]\n",
      "Step 541105  [5.332 sec/step, loss=0.07401, avg_loss=0.07343]\n",
      "Step 541106  [5.346 sec/step, loss=0.07455, avg_loss=0.07344]\n",
      "Step 541107  [5.339 sec/step, loss=0.07058, avg_loss=0.07342]\n",
      "Generated 32 batches of size 32 in 2.426 sec\n",
      "Step 541108  [5.354 sec/step, loss=0.07473, avg_loss=0.07342]\n",
      "Step 541109  [5.347 sec/step, loss=0.07409, avg_loss=0.07342]\n",
      "Step 541110  [5.395 sec/step, loss=0.06897, avg_loss=0.07337]\n",
      "Step 541111  [5.389 sec/step, loss=0.07327, avg_loss=0.07338]\n",
      "Step 541112  [5.407 sec/step, loss=0.07513, avg_loss=0.07342]\n",
      "Step 541113  [5.424 sec/step, loss=0.07568, avg_loss=0.07348]\n",
      "Step 541114  [5.420 sec/step, loss=0.07379, avg_loss=0.07348]\n",
      "Step 541115  [5.423 sec/step, loss=0.07592, avg_loss=0.07349]\n",
      "Step 541116  [5.414 sec/step, loss=0.07336, avg_loss=0.07349]\n",
      "Step 541117  [5.392 sec/step, loss=0.06956, avg_loss=0.07343]\n",
      "Step 541118  [5.386 sec/step, loss=0.07293, avg_loss=0.07342]\n",
      "Step 541119  [5.364 sec/step, loss=0.07093, avg_loss=0.07338]\n",
      "Step 541120  [5.391 sec/step, loss=0.07499, avg_loss=0.07339]\n",
      "Step 541121  [5.393 sec/step, loss=0.07579, avg_loss=0.07341]\n",
      "Step 541122  [5.398 sec/step, loss=0.07451, avg_loss=0.07343]\n",
      "Step 541123  [5.429 sec/step, loss=0.07187, avg_loss=0.07342]\n",
      "Step 541124  [5.430 sec/step, loss=0.07418, avg_loss=0.07342]\n",
      "Step 541125  [5.419 sec/step, loss=0.07263, avg_loss=0.07340]\n",
      "Step 541126  [5.479 sec/step, loss=0.06450, avg_loss=0.07334]\n",
      "Step 541127  [5.492 sec/step, loss=0.07423, avg_loss=0.07335]\n",
      "Step 541128  [5.478 sec/step, loss=0.07154, avg_loss=0.07331]\n",
      "Step 541129  [5.462 sec/step, loss=0.07234, avg_loss=0.07329]\n",
      "Step 541130  [5.442 sec/step, loss=0.07351, avg_loss=0.07327]\n",
      "Step 541131  [5.446 sec/step, loss=0.07610, avg_loss=0.07329]\n",
      "Step 541132  [5.404 sec/step, loss=0.07338, avg_loss=0.07336]\n",
      "Step 541133  [5.390 sec/step, loss=0.07139, avg_loss=0.07334]\n",
      "Step 541134  [5.408 sec/step, loss=0.07384, avg_loss=0.07336]\n",
      "Step 541135  [5.410 sec/step, loss=0.07132, avg_loss=0.07334]\n",
      "Step 541136  [5.411 sec/step, loss=0.07671, avg_loss=0.07336]\n",
      "Step 541137  [5.425 sec/step, loss=0.07394, avg_loss=0.07337]\n",
      "Step 541138  [5.415 sec/step, loss=0.07477, avg_loss=0.07337]\n",
      "Step 541139  [5.424 sec/step, loss=0.07298, avg_loss=0.07336]\n",
      "Generated 32 batches of size 32 in 2.423 sec\n",
      "Step 541140  [5.439 sec/step, loss=0.07593, avg_loss=0.07337]\n",
      "Step 541141  [5.444 sec/step, loss=0.07524, avg_loss=0.07337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 541142  [5.445 sec/step, loss=0.07330, avg_loss=0.07337]\n",
      "Step 541143  [5.441 sec/step, loss=0.07377, avg_loss=0.07337]\n",
      "Step 541144  [5.446 sec/step, loss=0.07515, avg_loss=0.07340]\n",
      "Step 541145  [5.445 sec/step, loss=0.07563, avg_loss=0.07341]\n",
      "Step 541146  [5.465 sec/step, loss=0.07485, avg_loss=0.07350]\n",
      "Step 541147  [5.469 sec/step, loss=0.07428, avg_loss=0.07350]\n",
      "Step 541148  [5.441 sec/step, loss=0.06498, avg_loss=0.07342]\n",
      "Step 541149  [5.452 sec/step, loss=0.07433, avg_loss=0.07342]\n",
      "Step 541150  [5.411 sec/step, loss=0.06550, avg_loss=0.07335]\n",
      "Step 541151  [5.420 sec/step, loss=0.07286, avg_loss=0.07333]\n",
      "Step 541152  [5.399 sec/step, loss=0.07131, avg_loss=0.07329]\n",
      "Step 541153  [5.393 sec/step, loss=0.07382, avg_loss=0.07330]\n",
      "Step 541154  [5.372 sec/step, loss=0.07139, avg_loss=0.07326]\n",
      "Step 541155  [5.390 sec/step, loss=0.07502, avg_loss=0.07329]\n",
      "Step 541156  [5.398 sec/step, loss=0.07288, avg_loss=0.07326]\n",
      "Step 541157  [5.409 sec/step, loss=0.07476, avg_loss=0.07330]\n",
      "Step 541158  [5.426 sec/step, loss=0.07652, avg_loss=0.07335]\n",
      "Step 541159  [5.420 sec/step, loss=0.07395, avg_loss=0.07335]\n",
      "Step 541160  [5.411 sec/step, loss=0.07143, avg_loss=0.07336]\n",
      "Step 541161  [5.414 sec/step, loss=0.07328, avg_loss=0.07336]\n",
      "Step 541162  [5.453 sec/step, loss=0.06718, avg_loss=0.07331]\n",
      "Step 541163  [5.448 sec/step, loss=0.07529, avg_loss=0.07330]\n",
      "Step 541164  [5.446 sec/step, loss=0.07507, avg_loss=0.07331]\n",
      "Step 541165  [5.441 sec/step, loss=0.07319, avg_loss=0.07329]\n",
      "Step 541166  [5.440 sec/step, loss=0.07010, avg_loss=0.07326]\n",
      "Step 541167  [5.443 sec/step, loss=0.07477, avg_loss=0.07325]\n",
      "Step 541168  [5.438 sec/step, loss=0.07210, avg_loss=0.07323]\n",
      "Step 541169  [5.440 sec/step, loss=0.07242, avg_loss=0.07321]\n",
      "Step 541170  [5.409 sec/step, loss=0.06938, avg_loss=0.07319]\n",
      "Step 541171  [5.421 sec/step, loss=0.07647, avg_loss=0.07321]\n",
      "Generated 32 batches of size 32 in 2.377 sec\n",
      "Step 541172  [5.425 sec/step, loss=0.07465, avg_loss=0.07321]\n",
      "Step 541173  [5.438 sec/step, loss=0.07357, avg_loss=0.07321]\n",
      "Step 541174  [5.381 sec/step, loss=0.07330, avg_loss=0.07328]\n",
      "Step 541175  [5.368 sec/step, loss=0.07297, avg_loss=0.07325]\n",
      "Step 541176  [5.378 sec/step, loss=0.07338, avg_loss=0.07325]\n",
      "Step 541177  [5.362 sec/step, loss=0.07460, avg_loss=0.07324]\n",
      "Step 541178  [5.357 sec/step, loss=0.07381, avg_loss=0.07324]\n",
      "Step 541179  [5.354 sec/step, loss=0.07578, avg_loss=0.07324]\n",
      "Step 541180  [5.381 sec/step, loss=0.07560, avg_loss=0.07334]\n",
      "Step 541181  [5.392 sec/step, loss=0.07512, avg_loss=0.07334]\n",
      "Step 541182  [5.388 sec/step, loss=0.07439, avg_loss=0.07336]\n",
      "Step 541183  [5.388 sec/step, loss=0.07487, avg_loss=0.07337]\n",
      "Step 541184  [5.416 sec/step, loss=0.07467, avg_loss=0.07340]\n",
      "Step 541185  [5.391 sec/step, loss=0.07033, avg_loss=0.07334]\n",
      "Step 541186  [5.397 sec/step, loss=0.07349, avg_loss=0.07335]\n",
      "Step 541187  [5.401 sec/step, loss=0.07310, avg_loss=0.07334]\n",
      "Step 541188  [5.404 sec/step, loss=0.07440, avg_loss=0.07333]\n",
      "Step 541189  [5.413 sec/step, loss=0.07265, avg_loss=0.07332]\n",
      "Step 541190  [5.423 sec/step, loss=0.07570, avg_loss=0.07333]\n",
      "Step 541191  [5.424 sec/step, loss=0.07375, avg_loss=0.07334]\n",
      "Step 541192  [5.414 sec/step, loss=0.07338, avg_loss=0.07333]\n",
      "Step 541193  [5.410 sec/step, loss=0.07249, avg_loss=0.07331]\n",
      "Step 541194  [5.415 sec/step, loss=0.07409, avg_loss=0.07332]\n",
      "Step 541195  [5.417 sec/step, loss=0.07320, avg_loss=0.07331]\n",
      "Step 541196  [5.422 sec/step, loss=0.07398, avg_loss=0.07332]\n",
      "Step 541197  [5.421 sec/step, loss=0.07449, avg_loss=0.07331]\n",
      "Step 541198  [5.422 sec/step, loss=0.07441, avg_loss=0.07333]\n",
      "Step 541199  [5.410 sec/step, loss=0.07289, avg_loss=0.07330]\n",
      "Step 541200  [5.396 sec/step, loss=0.07301, avg_loss=0.07328]\n",
      "Writing summary at step: 541200\n",
      "Step 541201  [5.402 sec/step, loss=0.07206, avg_loss=0.07330]\n",
      "Step 541202  [5.456 sec/step, loss=0.06508, avg_loss=0.07322]\n",
      "Generated 32 batches of size 32 in 2.330 sec\n",
      "Step 541203  [5.443 sec/step, loss=0.07460, avg_loss=0.07324]\n",
      "Step 541204  [5.449 sec/step, loss=0.07414, avg_loss=0.07326]\n",
      "Step 541205  [5.429 sec/step, loss=0.06538, avg_loss=0.07317]\n",
      "Step 541206  [5.437 sec/step, loss=0.07548, avg_loss=0.07318]\n",
      "Step 541207  [5.442 sec/step, loss=0.06988, avg_loss=0.07318]\n",
      "Step 541208  [5.433 sec/step, loss=0.07521, avg_loss=0.07318]\n",
      "Step 541209  [5.450 sec/step, loss=0.07553, avg_loss=0.07320]\n",
      "Step 541210  [5.404 sec/step, loss=0.07145, avg_loss=0.07322]\n",
      "Step 541211  [5.415 sec/step, loss=0.07392, avg_loss=0.07323]\n",
      "Step 541212  [5.419 sec/step, loss=0.07470, avg_loss=0.07322]\n",
      "Step 541213  [5.399 sec/step, loss=0.06946, avg_loss=0.07316]\n",
      "Step 541214  [5.403 sec/step, loss=0.07412, avg_loss=0.07316]\n",
      "Step 541215  [5.399 sec/step, loss=0.07494, avg_loss=0.07315]\n",
      "Step 541216  [5.405 sec/step, loss=0.07357, avg_loss=0.07316]\n",
      "Step 541217  [5.412 sec/step, loss=0.07438, avg_loss=0.07320]\n",
      "Step 541218  [5.415 sec/step, loss=0.07496, avg_loss=0.07323]\n",
      "Step 541219  [5.423 sec/step, loss=0.07363, avg_loss=0.07325]\n",
      "Step 541220  [5.419 sec/step, loss=0.07391, avg_loss=0.07324]\n",
      "Step 541221  [5.407 sec/step, loss=0.07151, avg_loss=0.07320]\n",
      "Step 541222  [5.411 sec/step, loss=0.07468, avg_loss=0.07320]\n",
      "Step 541223  [5.391 sec/step, loss=0.07514, avg_loss=0.07323]\n",
      "Step 541224  [5.395 sec/step, loss=0.07432, avg_loss=0.07323]\n",
      "Step 541225  [5.394 sec/step, loss=0.07294, avg_loss=0.07324]\n",
      "Step 541226  [5.344 sec/step, loss=0.07400, avg_loss=0.07333]\n",
      "Step 541227  [5.332 sec/step, loss=0.07362, avg_loss=0.07333]\n",
      "Step 541228  [5.339 sec/step, loss=0.07379, avg_loss=0.07335]\n",
      "Step 541229  [5.339 sec/step, loss=0.07117, avg_loss=0.07334]\n",
      "Step 541230  [5.346 sec/step, loss=0.07320, avg_loss=0.07333]\n",
      "Step 541231  [5.349 sec/step, loss=0.07530, avg_loss=0.07333]\n",
      "Step 541232  [5.349 sec/step, loss=0.07596, avg_loss=0.07335]\n",
      "Step 541233  [5.377 sec/step, loss=0.07419, avg_loss=0.07338]\n",
      "Step 541234  [5.382 sec/step, loss=0.07230, avg_loss=0.07336]\n",
      "Generated 32 batches of size 32 in 2.860 sec\n",
      "Step 541235  [5.369 sec/step, loss=0.06532, avg_loss=0.07330]\n",
      "Step 541236  [5.349 sec/step, loss=0.07207, avg_loss=0.07326]\n",
      "Step 541237  [5.342 sec/step, loss=0.07649, avg_loss=0.07328]\n",
      "Step 541238  [5.338 sec/step, loss=0.07337, avg_loss=0.07327]\n",
      "Step 541239  [5.320 sec/step, loss=0.07513, avg_loss=0.07329]\n",
      "Step 541240  [5.357 sec/step, loss=0.06618, avg_loss=0.07319]\n",
      "Step 541241  [5.336 sec/step, loss=0.07101, avg_loss=0.07315]\n",
      "Step 541242  [5.346 sec/step, loss=0.07302, avg_loss=0.07315]\n",
      "Step 541243  [5.372 sec/step, loss=0.07187, avg_loss=0.07313]\n",
      "Step 541244  [5.383 sec/step, loss=0.07357, avg_loss=0.07311]\n",
      "Step 541245  [5.365 sec/step, loss=0.07227, avg_loss=0.07308]\n",
      "Step 541246  [5.389 sec/step, loss=0.07222, avg_loss=0.07305]\n",
      "Step 541247  [5.394 sec/step, loss=0.07639, avg_loss=0.07307]\n",
      "Step 541248  [5.397 sec/step, loss=0.07137, avg_loss=0.07314]\n",
      "Step 541249  [5.395 sec/step, loss=0.07538, avg_loss=0.07315]\n",
      "Step 541250  [5.423 sec/step, loss=0.07507, avg_loss=0.07324]\n",
      "Step 541251  [5.413 sec/step, loss=0.07119, avg_loss=0.07323]\n",
      "Step 541252  [5.431 sec/step, loss=0.07409, avg_loss=0.07326]\n",
      "Step 541253  [5.421 sec/step, loss=0.07527, avg_loss=0.07327]\n",
      "Step 541254  [5.414 sec/step, loss=0.07336, avg_loss=0.07329]\n",
      "Step 541255  [5.399 sec/step, loss=0.07467, avg_loss=0.07329]\n",
      "Step 541256  [5.382 sec/step, loss=0.07433, avg_loss=0.07330]\n",
      "Step 541257  [5.376 sec/step, loss=0.07412, avg_loss=0.07329]\n",
      "Step 541258  [5.366 sec/step, loss=0.07178, avg_loss=0.07325]\n",
      "Step 541259  [5.357 sec/step, loss=0.07275, avg_loss=0.07324]\n",
      "Step 541260  [5.418 sec/step, loss=0.06507, avg_loss=0.07317]\n",
      "Step 541261  [5.433 sec/step, loss=0.07373, avg_loss=0.07318]\n",
      "Step 541262  [5.390 sec/step, loss=0.07612, avg_loss=0.07327]\n",
      "Step 541263  [5.388 sec/step, loss=0.07267, avg_loss=0.07324]\n",
      "Step 541264  [5.407 sec/step, loss=0.07305, avg_loss=0.07322]\n",
      "Step 541265  [5.401 sec/step, loss=0.07127, avg_loss=0.07320]\n",
      "Step 541266  [5.410 sec/step, loss=0.07420, avg_loss=0.07324]\n",
      "Generated 32 batches of size 32 in 2.561 sec\n",
      "Step 541267  [5.399 sec/step, loss=0.07319, avg_loss=0.07323]\n",
      "Step 541268  [5.415 sec/step, loss=0.07551, avg_loss=0.07326]\n",
      "Step 541269  [5.405 sec/step, loss=0.07350, avg_loss=0.07327]\n",
      "Step 541270  [5.426 sec/step, loss=0.07549, avg_loss=0.07333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 541271  [5.419 sec/step, loss=0.07377, avg_loss=0.07330]\n",
      "Step 541272  [5.406 sec/step, loss=0.07258, avg_loss=0.07328]\n",
      "Step 541273  [5.403 sec/step, loss=0.07494, avg_loss=0.07330]\n",
      "Step 541274  [5.395 sec/step, loss=0.06583, avg_loss=0.07322]\n",
      "Step 541275  [5.398 sec/step, loss=0.07418, avg_loss=0.07323]\n",
      "Step 541276  [5.450 sec/step, loss=0.06692, avg_loss=0.07317]\n",
      "Step 541277  [5.442 sec/step, loss=0.07323, avg_loss=0.07316]\n",
      "Step 541278  [5.445 sec/step, loss=0.07508, avg_loss=0.07317]\n",
      "Step 541279  [5.442 sec/step, loss=0.07516, avg_loss=0.07316]\n",
      "Step 541280  [5.433 sec/step, loss=0.07418, avg_loss=0.07315]\n",
      "Step 541281  [5.424 sec/step, loss=0.07326, avg_loss=0.07313]\n",
      "Step 541282  [5.447 sec/step, loss=0.07314, avg_loss=0.07312]\n",
      "Step 541283  [5.443 sec/step, loss=0.07303, avg_loss=0.07310]\n",
      "Step 541284  [5.419 sec/step, loss=0.07056, avg_loss=0.07306]\n",
      "Step 541285  [5.445 sec/step, loss=0.07556, avg_loss=0.07311]\n",
      "Step 541286  [5.464 sec/step, loss=0.07295, avg_loss=0.07311]\n",
      "Step 541287  [5.452 sec/step, loss=0.07336, avg_loss=0.07311]\n",
      "Step 541288  [5.444 sec/step, loss=0.07441, avg_loss=0.07311]\n",
      "Step 541289  [5.440 sec/step, loss=0.07486, avg_loss=0.07313]\n",
      "Step 541290  [5.436 sec/step, loss=0.07490, avg_loss=0.07312]\n",
      "Step 541291  [5.450 sec/step, loss=0.07413, avg_loss=0.07313]\n",
      "Step 541292  [5.446 sec/step, loss=0.07088, avg_loss=0.07310]\n",
      "Step 541293  [5.468 sec/step, loss=0.07299, avg_loss=0.07311]\n",
      "Step 541294  [5.441 sec/step, loss=0.07127, avg_loss=0.07308]\n",
      "Step 541295  [5.445 sec/step, loss=0.07471, avg_loss=0.07309]\n",
      "Step 541296  [5.431 sec/step, loss=0.06550, avg_loss=0.07301]\n",
      "Step 541297  [5.431 sec/step, loss=0.07555, avg_loss=0.07302]\n",
      "Step 541298  [5.441 sec/step, loss=0.07403, avg_loss=0.07301]\n",
      "Generated 32 batches of size 32 in 2.402 sec\n",
      "Step 541299  [5.461 sec/step, loss=0.07419, avg_loss=0.07303]\n",
      "Step 541300  [5.467 sec/step, loss=0.07445, avg_loss=0.07304]\n",
      "Writing summary at step: 541300\n",
      "Step 541301  [5.490 sec/step, loss=0.07388, avg_loss=0.07306]\n",
      "Step 541302  [5.438 sec/step, loss=0.07228, avg_loss=0.07313]\n",
      "Step 541303  [5.428 sec/step, loss=0.07156, avg_loss=0.07310]\n",
      "Step 541304  [5.431 sec/step, loss=0.07418, avg_loss=0.07310]\n",
      "Step 541305  [5.440 sec/step, loss=0.07294, avg_loss=0.07318]\n",
      "Step 541306  [5.431 sec/step, loss=0.07518, avg_loss=0.07318]\n",
      "Step 541307  [5.444 sec/step, loss=0.07459, avg_loss=0.07322]\n",
      "Step 541308  [5.419 sec/step, loss=0.06517, avg_loss=0.07312]\n",
      "Step 541309  [5.455 sec/step, loss=0.06423, avg_loss=0.07301]\n",
      "Step 541310  [5.470 sec/step, loss=0.07532, avg_loss=0.07305]\n",
      "Step 541311  [5.468 sec/step, loss=0.07442, avg_loss=0.07305]\n",
      "Step 541312  [5.452 sec/step, loss=0.07110, avg_loss=0.07302]\n",
      "Step 541313  [5.475 sec/step, loss=0.07465, avg_loss=0.07307]\n",
      "Step 541314  [5.474 sec/step, loss=0.07545, avg_loss=0.07308]\n",
      "Step 541315  [5.468 sec/step, loss=0.07435, avg_loss=0.07308]\n",
      "Step 541316  [5.470 sec/step, loss=0.07432, avg_loss=0.07308]\n",
      "Step 541317  [5.466 sec/step, loss=0.07329, avg_loss=0.07307]\n",
      "Step 541318  [5.455 sec/step, loss=0.07026, avg_loss=0.07303]\n",
      "Step 541319  [5.463 sec/step, loss=0.07345, avg_loss=0.07302]\n",
      "Step 541320  [5.464 sec/step, loss=0.07484, avg_loss=0.07303]\n",
      "Step 541321  [5.482 sec/step, loss=0.07394, avg_loss=0.07306]\n",
      "Step 541322  [5.483 sec/step, loss=0.07282, avg_loss=0.07304]\n",
      "Step 541323  [5.479 sec/step, loss=0.07380, avg_loss=0.07303]\n",
      "Step 541324  [5.484 sec/step, loss=0.07405, avg_loss=0.07302]\n",
      "Step 541325  [5.516 sec/step, loss=0.07412, avg_loss=0.07303]\n",
      "Step 541326  [5.514 sec/step, loss=0.07175, avg_loss=0.07301]\n",
      "Step 541327  [5.522 sec/step, loss=0.07238, avg_loss=0.07300]\n",
      "Step 541328  [5.519 sec/step, loss=0.07388, avg_loss=0.07300]\n",
      "Step 541329  [5.509 sec/step, loss=0.07105, avg_loss=0.07300]\n",
      "Generated 32 batches of size 32 in 2.421 sec\n",
      "Step 541330  [5.523 sec/step, loss=0.07545, avg_loss=0.07302]\n",
      "Step 541331  [5.515 sec/step, loss=0.07462, avg_loss=0.07301]\n",
      "Step 541332  [5.501 sec/step, loss=0.07318, avg_loss=0.07299]\n",
      "Step 541333  [5.493 sec/step, loss=0.07415, avg_loss=0.07299]\n",
      "Step 541334  [5.479 sec/step, loss=0.07266, avg_loss=0.07299]\n",
      "Step 541335  [5.479 sec/step, loss=0.07330, avg_loss=0.07307]\n",
      "Step 541336  [5.506 sec/step, loss=0.07464, avg_loss=0.07310]\n",
      "Step 541337  [5.511 sec/step, loss=0.07583, avg_loss=0.07309]\n",
      "Step 541338  [5.529 sec/step, loss=0.07566, avg_loss=0.07311]\n",
      "Step 541339  [5.541 sec/step, loss=0.07302, avg_loss=0.07309]\n",
      "Step 541340  [5.487 sec/step, loss=0.07260, avg_loss=0.07316]\n",
      "Step 541341  [5.509 sec/step, loss=0.07564, avg_loss=0.07320]\n",
      "Step 541342  [5.504 sec/step, loss=0.07023, avg_loss=0.07317]\n",
      "Step 541343  [5.472 sec/step, loss=0.07314, avg_loss=0.07319]\n",
      "Step 541344  [5.469 sec/step, loss=0.07346, avg_loss=0.07319]\n",
      "Step 541345  [5.473 sec/step, loss=0.07402, avg_loss=0.07320]\n",
      "Step 541346  [5.437 sec/step, loss=0.07300, avg_loss=0.07321]\n",
      "Step 541347  [5.421 sec/step, loss=0.07298, avg_loss=0.07318]\n",
      "Step 541348  [5.436 sec/step, loss=0.07394, avg_loss=0.07320]\n",
      "Step 541349  [5.443 sec/step, loss=0.07560, avg_loss=0.07320]\n",
      "Step 541350  [5.438 sec/step, loss=0.07410, avg_loss=0.07319]\n",
      "Step 541351  [5.421 sec/step, loss=0.06552, avg_loss=0.07314]\n",
      "Step 541352  [5.419 sec/step, loss=0.07462, avg_loss=0.07314]\n",
      "Step 541353  [5.419 sec/step, loss=0.07534, avg_loss=0.07314]\n",
      "Step 541354  [5.480 sec/step, loss=0.06568, avg_loss=0.07307]\n",
      "Step 541355  [5.481 sec/step, loss=0.07264, avg_loss=0.07305]\n",
      "Step 541356  [5.481 sec/step, loss=0.07298, avg_loss=0.07303]\n",
      "Step 541357  [5.514 sec/step, loss=0.07212, avg_loss=0.07301]\n",
      "Step 541358  [5.519 sec/step, loss=0.07483, avg_loss=0.07304]\n",
      "Step 541359  [5.514 sec/step, loss=0.07058, avg_loss=0.07302]\n",
      "Step 541360  [5.460 sec/step, loss=0.07450, avg_loss=0.07312]\n",
      "Step 541361  [5.437 sec/step, loss=0.07171, avg_loss=0.07310]\n",
      "Generated 32 batches of size 32 in 2.430 sec\n",
      "Step 541362  [5.448 sec/step, loss=0.07480, avg_loss=0.07308]\n",
      "Step 541363  [5.454 sec/step, loss=0.07570, avg_loss=0.07311]\n",
      "Step 541364  [5.443 sec/step, loss=0.07335, avg_loss=0.07312]\n",
      "Step 541365  [5.458 sec/step, loss=0.07508, avg_loss=0.07315]\n",
      "Step 541366  [5.465 sec/step, loss=0.07478, avg_loss=0.07316]\n",
      "Step 541367  [5.468 sec/step, loss=0.07372, avg_loss=0.07317]\n",
      "Step 541368  [5.476 sec/step, loss=0.07484, avg_loss=0.07316]\n",
      "Step 541369  [5.482 sec/step, loss=0.07456, avg_loss=0.07317]\n",
      "Step 541370  [5.481 sec/step, loss=0.07472, avg_loss=0.07316]\n",
      "Step 541371  [5.493 sec/step, loss=0.07560, avg_loss=0.07318]\n",
      "Step 541372  [5.500 sec/step, loss=0.07445, avg_loss=0.07320]\n",
      "Step 541373  [5.500 sec/step, loss=0.07471, avg_loss=0.07320]\n",
      "Step 541374  [5.517 sec/step, loss=0.07379, avg_loss=0.07328]\n",
      "Step 541375  [5.523 sec/step, loss=0.07521, avg_loss=0.07329]\n",
      "Step 541376  [5.457 sec/step, loss=0.06482, avg_loss=0.07327]\n",
      "Step 541377  [5.477 sec/step, loss=0.07612, avg_loss=0.07329]\n",
      "Step 541378  [5.464 sec/step, loss=0.06990, avg_loss=0.07324]\n",
      "Step 541379  [5.451 sec/step, loss=0.07344, avg_loss=0.07322]\n",
      "Step 541380  [5.443 sec/step, loss=0.07319, avg_loss=0.07321]\n",
      "Step 541381  [5.455 sec/step, loss=0.07527, avg_loss=0.07323]\n",
      "Step 541382  [5.436 sec/step, loss=0.07367, avg_loss=0.07324]\n",
      "Step 541383  [5.443 sec/step, loss=0.07397, avg_loss=0.07325]\n",
      "Step 541384  [5.454 sec/step, loss=0.07385, avg_loss=0.07328]\n",
      "Step 541385  [5.462 sec/step, loss=0.07248, avg_loss=0.07325]\n",
      "Step 541386  [5.433 sec/step, loss=0.07168, avg_loss=0.07324]\n",
      "Step 541387  [5.451 sec/step, loss=0.07318, avg_loss=0.07324]\n",
      "Step 541388  [5.461 sec/step, loss=0.07402, avg_loss=0.07323]\n",
      "Step 541389  [5.439 sec/step, loss=0.07007, avg_loss=0.07319]\n",
      "Step 541390  [5.437 sec/step, loss=0.07427, avg_loss=0.07318]\n",
      "Step 541391  [5.438 sec/step, loss=0.07514, avg_loss=0.07319]\n",
      "Step 541392  [5.445 sec/step, loss=0.07207, avg_loss=0.07320]\n",
      "Step 541393  [5.429 sec/step, loss=0.07398, avg_loss=0.07321]\n",
      "Generated 32 batches of size 32 in 2.620 sec\n",
      "Step 541394  [5.434 sec/step, loss=0.07445, avg_loss=0.07324]\n",
      "Step 541395  [5.424 sec/step, loss=0.07343, avg_loss=0.07323]\n",
      "Step 541396  [5.444 sec/step, loss=0.07314, avg_loss=0.07331]\n",
      "Step 541397  [5.430 sec/step, loss=0.07025, avg_loss=0.07325]\n",
      "Step 541398  [5.475 sec/step, loss=0.06560, avg_loss=0.07317]\n",
      "Step 541399  [5.486 sec/step, loss=0.07259, avg_loss=0.07315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 541400  [5.482 sec/step, loss=0.07276, avg_loss=0.07314]\n",
      "Writing summary at step: 541400\n",
      "Step 541401  [5.473 sec/step, loss=0.07511, avg_loss=0.07315]\n",
      "Step 541402  [5.527 sec/step, loss=0.06511, avg_loss=0.07308]\n",
      "Step 541403  [5.523 sec/step, loss=0.07440, avg_loss=0.07311]\n",
      "Step 541404  [5.527 sec/step, loss=0.07544, avg_loss=0.07312]\n",
      "Step 541405  [5.542 sec/step, loss=0.07516, avg_loss=0.07314]\n",
      "Step 541406  [5.545 sec/step, loss=0.07324, avg_loss=0.07312]\n",
      "Step 541407  [5.540 sec/step, loss=0.07307, avg_loss=0.07311]\n",
      "Step 541408  [5.559 sec/step, loss=0.07521, avg_loss=0.07321]\n",
      "Step 541409  [5.501 sec/step, loss=0.07356, avg_loss=0.07330]\n",
      "Step 541410  [5.491 sec/step, loss=0.07409, avg_loss=0.07329]\n",
      "Step 541411  [5.486 sec/step, loss=0.07211, avg_loss=0.07326]\n",
      "Step 541412  [5.480 sec/step, loss=0.06973, avg_loss=0.07325]\n",
      "Step 541413  [5.458 sec/step, loss=0.07182, avg_loss=0.07322]\n",
      "Step 541414  [5.453 sec/step, loss=0.07470, avg_loss=0.07321]\n",
      "Step 541415  [5.455 sec/step, loss=0.07406, avg_loss=0.07321]\n",
      "Step 541416  [5.450 sec/step, loss=0.07009, avg_loss=0.07317]\n",
      "Step 541417  [5.468 sec/step, loss=0.07539, avg_loss=0.07319]\n",
      "Step 541418  [5.495 sec/step, loss=0.07479, avg_loss=0.07324]\n",
      "Step 541419  [5.500 sec/step, loss=0.07494, avg_loss=0.07325]\n",
      "Step 541420  [5.500 sec/step, loss=0.07515, avg_loss=0.07325]\n",
      "Step 541421  [5.468 sec/step, loss=0.06667, avg_loss=0.07318]\n",
      "Step 541422  [5.460 sec/step, loss=0.07124, avg_loss=0.07316]\n",
      "Step 541423  [5.469 sec/step, loss=0.07518, avg_loss=0.07318]\n",
      "Step 541424  [5.449 sec/step, loss=0.07131, avg_loss=0.07315]\n",
      "Generated 32 batches of size 32 in 2.593 sec\n",
      "Step 541425  [5.422 sec/step, loss=0.07294, avg_loss=0.07314]\n",
      "Step 541426  [5.453 sec/step, loss=0.07177, avg_loss=0.07314]\n",
      "Step 541427  [5.459 sec/step, loss=0.07449, avg_loss=0.07316]\n",
      "Step 541428  [5.462 sec/step, loss=0.07098, avg_loss=0.07313]\n",
      "Step 541429  [5.477 sec/step, loss=0.07257, avg_loss=0.07315]\n",
      "Step 541430  [5.464 sec/step, loss=0.07344, avg_loss=0.07313]\n",
      "Step 541431  [5.473 sec/step, loss=0.07310, avg_loss=0.07311]\n",
      "Step 541432  [5.485 sec/step, loss=0.07632, avg_loss=0.07314]\n",
      "Step 541433  [5.488 sec/step, loss=0.07203, avg_loss=0.07312]\n",
      "Step 541434  [5.491 sec/step, loss=0.07445, avg_loss=0.07314]\n",
      "Step 541435  [5.494 sec/step, loss=0.07340, avg_loss=0.07314]\n",
      "Step 541436  [5.488 sec/step, loss=0.07282, avg_loss=0.07312]\n",
      "Step 541437  [5.472 sec/step, loss=0.07286, avg_loss=0.07309]\n",
      "Step 541438  [5.465 sec/step, loss=0.07413, avg_loss=0.07308]\n",
      "Step 541439  [5.459 sec/step, loss=0.07429, avg_loss=0.07309]\n",
      "Step 541440  [5.479 sec/step, loss=0.07554, avg_loss=0.07312]\n",
      "Step 541441  [5.476 sec/step, loss=0.07398, avg_loss=0.07310]\n",
      "Step 541442  [5.474 sec/step, loss=0.07295, avg_loss=0.07313]\n",
      "Step 541443  [5.480 sec/step, loss=0.07348, avg_loss=0.07313]\n",
      "Step 541444  [5.483 sec/step, loss=0.07577, avg_loss=0.07316]\n",
      "Step 541445  [5.503 sec/step, loss=0.07234, avg_loss=0.07314]\n",
      "Step 541446  [5.518 sec/step, loss=0.07320, avg_loss=0.07314]\n",
      "Step 541447  [5.524 sec/step, loss=0.07084, avg_loss=0.07312]\n",
      "Step 541448  [5.521 sec/step, loss=0.07112, avg_loss=0.07309]\n",
      "Step 541449  [5.524 sec/step, loss=0.07342, avg_loss=0.07307]\n",
      "Step 541450  [5.521 sec/step, loss=0.07478, avg_loss=0.07308]\n",
      "Step 541451  [5.550 sec/step, loss=0.07500, avg_loss=0.07317]\n",
      "Step 541452  [5.534 sec/step, loss=0.07081, avg_loss=0.07313]\n",
      "Step 541453  [5.514 sec/step, loss=0.07002, avg_loss=0.07308]\n",
      "Step 541454  [5.465 sec/step, loss=0.07492, avg_loss=0.07317]\n",
      "Step 541455  [5.518 sec/step, loss=0.06542, avg_loss=0.07310]\n",
      "Step 541456  [5.527 sec/step, loss=0.07542, avg_loss=0.07313]\n",
      "Generated 32 batches of size 32 in 2.422 sec\n",
      "Step 541457  [5.513 sec/step, loss=0.07526, avg_loss=0.07316]\n",
      "Step 541458  [5.516 sec/step, loss=0.07372, avg_loss=0.07315]\n",
      "Step 541459  [5.555 sec/step, loss=0.07273, avg_loss=0.07317]\n",
      "Step 541460  [5.548 sec/step, loss=0.07150, avg_loss=0.07314]\n",
      "Step 541461  [5.559 sec/step, loss=0.07475, avg_loss=0.07317]\n",
      "Step 541462  [5.548 sec/step, loss=0.07228, avg_loss=0.07314]\n",
      "Step 541463  [5.540 sec/step, loss=0.07478, avg_loss=0.07313]\n",
      "Step 541464  [5.514 sec/step, loss=0.06611, avg_loss=0.07306]\n",
      "Step 541465  [5.506 sec/step, loss=0.07328, avg_loss=0.07304]\n",
      "Step 541466  [5.512 sec/step, loss=0.07443, avg_loss=0.07304]\n",
      "Step 541467  [5.516 sec/step, loss=0.07372, avg_loss=0.07304]\n",
      "Step 541468  [5.497 sec/step, loss=0.07147, avg_loss=0.07301]\n",
      "Step 541469  [5.509 sec/step, loss=0.07297, avg_loss=0.07299]\n",
      "Step 541470  [5.485 sec/step, loss=0.07317, avg_loss=0.07297]\n",
      "Step 541471  [5.468 sec/step, loss=0.07083, avg_loss=0.07293]\n",
      "Step 541472  [5.459 sec/step, loss=0.07302, avg_loss=0.07291]\n",
      "Step 541473  [5.450 sec/step, loss=0.07350, avg_loss=0.07290]\n",
      "Step 541474  [5.467 sec/step, loss=0.07539, avg_loss=0.07292]\n",
      "Step 541475  [5.505 sec/step, loss=0.06674, avg_loss=0.07283]\n",
      "Step 541476  [5.527 sec/step, loss=0.07157, avg_loss=0.07290]\n",
      "Step 541477  [5.527 sec/step, loss=0.07472, avg_loss=0.07288]\n",
      "Step 541478  [5.533 sec/step, loss=0.07406, avg_loss=0.07293]\n",
      "Step 541479  [5.527 sec/step, loss=0.06487, avg_loss=0.07284]\n",
      "Step 541480  [5.536 sec/step, loss=0.07412, avg_loss=0.07285]\n",
      "Step 541481  [5.537 sec/step, loss=0.07292, avg_loss=0.07283]\n",
      "Step 541482  [5.539 sec/step, loss=0.07459, avg_loss=0.07284]\n",
      "Step 541483  [5.544 sec/step, loss=0.07558, avg_loss=0.07285]\n",
      "Step 541484  [5.542 sec/step, loss=0.07058, avg_loss=0.07282]\n",
      "Step 541485  [5.508 sec/step, loss=0.07084, avg_loss=0.07280]\n",
      "Step 541486  [5.516 sec/step, loss=0.07608, avg_loss=0.07285]\n",
      "Step 541487  [5.497 sec/step, loss=0.07343, avg_loss=0.07285]\n",
      "Step 541488  [5.483 sec/step, loss=0.07319, avg_loss=0.07284]\n",
      "Generated 32 batches of size 32 in 2.603 sec\n",
      "Step 541489  [5.499 sec/step, loss=0.07472, avg_loss=0.07289]\n",
      "Step 541490  [5.522 sec/step, loss=0.07269, avg_loss=0.07287]\n",
      "Step 541491  [5.510 sec/step, loss=0.07473, avg_loss=0.07287]\n",
      "Step 541492  [5.528 sec/step, loss=0.07592, avg_loss=0.07291]\n",
      "Step 541493  [5.540 sec/step, loss=0.07609, avg_loss=0.07293]\n",
      "Step 541494  [5.551 sec/step, loss=0.07600, avg_loss=0.07294]\n",
      "Step 541495  [5.544 sec/step, loss=0.06971, avg_loss=0.07291]\n",
      "Step 541496  [5.541 sec/step, loss=0.07548, avg_loss=0.07293]\n",
      "Step 541497  [5.551 sec/step, loss=0.07500, avg_loss=0.07298]\n",
      "Step 541498  [5.501 sec/step, loss=0.07384, avg_loss=0.07306]\n",
      "Step 541499  [5.471 sec/step, loss=0.07071, avg_loss=0.07304]\n",
      "Step 541500  [5.502 sec/step, loss=0.07325, avg_loss=0.07304]\n",
      "Writing summary at step: 541500\n",
      "Step 541501  [5.543 sec/step, loss=0.06611, avg_loss=0.07295]\n",
      "Step 541502  [5.490 sec/step, loss=0.07208, avg_loss=0.07302]\n",
      "Step 541503  [5.499 sec/step, loss=0.07457, avg_loss=0.07303]\n",
      "Step 541504  [5.496 sec/step, loss=0.07575, avg_loss=0.07303]\n",
      "Step 541505  [5.484 sec/step, loss=0.07239, avg_loss=0.07300]\n",
      "Step 541506  [5.482 sec/step, loss=0.07181, avg_loss=0.07299]\n",
      "Step 541507  [5.492 sec/step, loss=0.07539, avg_loss=0.07301]\n",
      "Step 541508  [5.491 sec/step, loss=0.07487, avg_loss=0.07301]\n",
      "Step 541509  [5.504 sec/step, loss=0.07418, avg_loss=0.07301]\n",
      "Step 541510  [5.504 sec/step, loss=0.07425, avg_loss=0.07302]\n",
      "Step 541511  [5.519 sec/step, loss=0.07586, avg_loss=0.07305]\n",
      "Step 541512  [5.521 sec/step, loss=0.07429, avg_loss=0.07310]\n",
      "Step 541513  [5.521 sec/step, loss=0.07085, avg_loss=0.07309]\n",
      "Step 541514  [5.532 sec/step, loss=0.07262, avg_loss=0.07307]\n",
      "Step 541515  [5.517 sec/step, loss=0.07129, avg_loss=0.07304]\n",
      "Step 541516  [5.515 sec/step, loss=0.07298, avg_loss=0.07307]\n",
      "Step 541517  [5.515 sec/step, loss=0.07363, avg_loss=0.07305]\n",
      "Step 541518  [5.500 sec/step, loss=0.07163, avg_loss=0.07302]\n",
      "Step 541519  [5.484 sec/step, loss=0.07340, avg_loss=0.07300]\n",
      "Generated 32 batches of size 32 in 2.379 sec\n",
      "Step 541520  [5.482 sec/step, loss=0.07567, avg_loss=0.07301]\n",
      "Step 541521  [5.495 sec/step, loss=0.07251, avg_loss=0.07307]\n",
      "Step 541522  [5.477 sec/step, loss=0.06440, avg_loss=0.07300]\n",
      "Step 541523  [5.459 sec/step, loss=0.07299, avg_loss=0.07298]\n",
      "Step 541524  [5.474 sec/step, loss=0.07383, avg_loss=0.07300]\n",
      "Step 541525  [5.477 sec/step, loss=0.07152, avg_loss=0.07299]\n",
      "Step 541526  [5.453 sec/step, loss=0.07521, avg_loss=0.07302]\n",
      "Step 541527  [5.463 sec/step, loss=0.07618, avg_loss=0.07304]\n",
      "Step 541528  [5.471 sec/step, loss=0.07446, avg_loss=0.07307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 541529  [5.459 sec/step, loss=0.06966, avg_loss=0.07305]\n",
      "Step 541530  [5.458 sec/step, loss=0.07215, avg_loss=0.07303]\n",
      "Step 541531  [5.456 sec/step, loss=0.07428, avg_loss=0.07304]\n",
      "Step 541532  [5.475 sec/step, loss=0.07370, avg_loss=0.07302]\n",
      "Step 541533  [5.467 sec/step, loss=0.07366, avg_loss=0.07303]\n",
      "Step 541534  [5.522 sec/step, loss=0.06461, avg_loss=0.07294]\n",
      "Step 541535  [5.547 sec/step, loss=0.07487, avg_loss=0.07295]\n",
      "Step 541536  [5.538 sec/step, loss=0.07308, avg_loss=0.07295]\n",
      "Step 541537  [5.561 sec/step, loss=0.07378, avg_loss=0.07296]\n",
      "Step 541538  [5.556 sec/step, loss=0.07424, avg_loss=0.07296]\n",
      "Step 541539  [5.557 sec/step, loss=0.07341, avg_loss=0.07296]\n",
      "Step 541540  [5.543 sec/step, loss=0.07442, avg_loss=0.07294]\n",
      "Step 541541  [5.541 sec/step, loss=0.07469, avg_loss=0.07295]\n",
      "Step 541542  [5.537 sec/step, loss=0.07322, avg_loss=0.07295]\n",
      "Step 541543  [5.542 sec/step, loss=0.07464, avg_loss=0.07297]\n",
      "Step 541544  [5.523 sec/step, loss=0.07343, avg_loss=0.07294]\n",
      "Step 541545  [5.492 sec/step, loss=0.07375, avg_loss=0.07296]\n",
      "Step 541546  [5.497 sec/step, loss=0.07220, avg_loss=0.07295]\n",
      "Step 541547  [5.511 sec/step, loss=0.07526, avg_loss=0.07299]\n",
      "Step 541548  [5.511 sec/step, loss=0.07104, avg_loss=0.07299]\n",
      "Step 541549  [5.483 sec/step, loss=0.06612, avg_loss=0.07292]\n",
      "Step 541550  [5.478 sec/step, loss=0.07332, avg_loss=0.07290]\n",
      "Step 541551  [5.460 sec/step, loss=0.07242, avg_loss=0.07288]\n",
      "Generated 32 batches of size 32 in 2.599 sec\n",
      "Step 541552  [5.476 sec/step, loss=0.07481, avg_loss=0.07292]\n",
      "Step 541553  [5.471 sec/step, loss=0.07070, avg_loss=0.07292]\n",
      "Step 541554  [5.475 sec/step, loss=0.07506, avg_loss=0.07292]\n",
      "Step 541555  [5.434 sec/step, loss=0.07524, avg_loss=0.07302]\n",
      "Step 541556  [5.437 sec/step, loss=0.07529, avg_loss=0.07302]\n",
      "Step 541557  [5.434 sec/step, loss=0.07638, avg_loss=0.07303]\n",
      "Step 541558  [5.431 sec/step, loss=0.07526, avg_loss=0.07305]\n",
      "Step 541559  [5.411 sec/step, loss=0.07299, avg_loss=0.07305]\n",
      "Step 541560  [5.423 sec/step, loss=0.07526, avg_loss=0.07309]\n",
      "Step 541561  [5.431 sec/step, loss=0.07432, avg_loss=0.07308]\n",
      "Step 541562  [5.424 sec/step, loss=0.07370, avg_loss=0.07310]\n",
      "Step 541563  [5.403 sec/step, loss=0.06529, avg_loss=0.07300]\n",
      "Step 541564  [5.421 sec/step, loss=0.07451, avg_loss=0.07309]\n",
      "Step 541565  [5.429 sec/step, loss=0.07284, avg_loss=0.07308]\n",
      "Step 541566  [5.414 sec/step, loss=0.07240, avg_loss=0.07306]\n",
      "Step 541567  [5.460 sec/step, loss=0.06549, avg_loss=0.07298]\n",
      "Step 541568  [5.464 sec/step, loss=0.07405, avg_loss=0.07301]\n",
      "Step 541569  [5.461 sec/step, loss=0.07542, avg_loss=0.07303]\n",
      "Step 541570  [5.470 sec/step, loss=0.06991, avg_loss=0.07300]\n",
      "Step 541571  [5.475 sec/step, loss=0.07373, avg_loss=0.07303]\n",
      "Step 541572  [5.496 sec/step, loss=0.07489, avg_loss=0.07305]\n",
      "Step 541573  [5.524 sec/step, loss=0.07257, avg_loss=0.07304]\n",
      "Step 541574  [5.515 sec/step, loss=0.07546, avg_loss=0.07304]\n",
      "Step 541575  [5.468 sec/step, loss=0.07335, avg_loss=0.07310]\n",
      "Step 541576  [5.451 sec/step, loss=0.07331, avg_loss=0.07312]\n",
      "Step 541577  [5.455 sec/step, loss=0.07534, avg_loss=0.07313]\n",
      "Step 541578  [5.451 sec/step, loss=0.07315, avg_loss=0.07312]\n",
      "Step 541579  [5.463 sec/step, loss=0.07242, avg_loss=0.07319]\n",
      "Step 541580  [5.459 sec/step, loss=0.07457, avg_loss=0.07320]\n",
      "Step 541581  [5.438 sec/step, loss=0.07265, avg_loss=0.07319]\n",
      "Step 541582  [5.454 sec/step, loss=0.07443, avg_loss=0.07319]\n",
      "Step 541583  [5.433 sec/step, loss=0.07109, avg_loss=0.07315]\n",
      "Generated 32 batches of size 32 in 2.428 sec\n",
      "Step 541584  [5.442 sec/step, loss=0.07496, avg_loss=0.07319]\n",
      "Step 541585  [5.467 sec/step, loss=0.07571, avg_loss=0.07324]\n",
      "Step 541586  [5.457 sec/step, loss=0.07490, avg_loss=0.07323]\n",
      "Step 541587  [5.477 sec/step, loss=0.07232, avg_loss=0.07322]\n",
      "Step 541588  [5.481 sec/step, loss=0.07305, avg_loss=0.07322]\n",
      "Step 541589  [5.493 sec/step, loss=0.07515, avg_loss=0.07322]\n",
      "Step 541590  [5.454 sec/step, loss=0.06966, avg_loss=0.07319]\n",
      "Step 541591  [5.455 sec/step, loss=0.07482, avg_loss=0.07319]\n",
      "Step 541592  [5.456 sec/step, loss=0.07377, avg_loss=0.07317]\n",
      "Step 541593  [5.446 sec/step, loss=0.07140, avg_loss=0.07312]\n",
      "Step 541594  [5.443 sec/step, loss=0.07544, avg_loss=0.07312]\n",
      "Step 541595  [5.467 sec/step, loss=0.07302, avg_loss=0.07315]\n",
      "Step 541596  [5.475 sec/step, loss=0.07543, avg_loss=0.07315]\n",
      "Step 541597  [5.466 sec/step, loss=0.07406, avg_loss=0.07314]\n",
      "Step 541598  [5.466 sec/step, loss=0.07383, avg_loss=0.07314]\n",
      "Step 541599  [5.465 sec/step, loss=0.07258, avg_loss=0.07316]\n",
      "Step 541600  [5.440 sec/step, loss=0.07399, avg_loss=0.07317]\n",
      "Writing summary at step: 541600\n",
      "Step 541601  [5.401 sec/step, loss=0.07350, avg_loss=0.07324]\n",
      "Step 541602  [5.416 sec/step, loss=0.07530, avg_loss=0.07327]\n",
      "Step 541603  [5.417 sec/step, loss=0.07538, avg_loss=0.07328]\n",
      "Step 541604  [5.407 sec/step, loss=0.07299, avg_loss=0.07325]\n",
      "Step 541605  [5.416 sec/step, loss=0.07367, avg_loss=0.07327]\n",
      "Step 541606  [5.409 sec/step, loss=0.07477, avg_loss=0.07330]\n",
      "Step 541607  [5.406 sec/step, loss=0.07459, avg_loss=0.07329]\n",
      "Step 541608  [5.386 sec/step, loss=0.06525, avg_loss=0.07319]\n",
      "Step 541609  [5.401 sec/step, loss=0.07259, avg_loss=0.07318]\n",
      "Step 541610  [5.402 sec/step, loss=0.07395, avg_loss=0.07317]\n",
      "Step 541611  [5.441 sec/step, loss=0.06605, avg_loss=0.07307]\n",
      "Step 541612  [5.434 sec/step, loss=0.07338, avg_loss=0.07307]\n",
      "Step 541613  [5.438 sec/step, loss=0.07311, avg_loss=0.07309]\n",
      "Step 541614  [5.427 sec/step, loss=0.07264, avg_loss=0.07309]\n",
      "Generated 32 batches of size 32 in 2.362 sec\n",
      "Step 541615  [5.448 sec/step, loss=0.07292, avg_loss=0.07310]\n",
      "Step 541616  [5.444 sec/step, loss=0.07102, avg_loss=0.07308]\n",
      "Step 541617  [5.446 sec/step, loss=0.07594, avg_loss=0.07311]\n",
      "Step 541618  [5.436 sec/step, loss=0.07290, avg_loss=0.07312]\n",
      "Step 541619  [5.432 sec/step, loss=0.07150, avg_loss=0.07310]\n",
      "Step 541620  [5.427 sec/step, loss=0.07515, avg_loss=0.07310]\n",
      "Step 541621  [5.458 sec/step, loss=0.07244, avg_loss=0.07310]\n",
      "Step 541622  [5.477 sec/step, loss=0.07366, avg_loss=0.07319]\n",
      "Step 541623  [5.481 sec/step, loss=0.07333, avg_loss=0.07319]\n",
      "Step 541624  [5.490 sec/step, loss=0.07506, avg_loss=0.07320]\n",
      "Step 541625  [5.487 sec/step, loss=0.07387, avg_loss=0.07323]\n",
      "Step 541626  [5.533 sec/step, loss=0.06602, avg_loss=0.07314]\n",
      "Step 541627  [5.532 sec/step, loss=0.07461, avg_loss=0.07312]\n",
      "Step 541628  [5.524 sec/step, loss=0.07441, avg_loss=0.07312]\n",
      "Step 541629  [5.551 sec/step, loss=0.07373, avg_loss=0.07316]\n",
      "Step 541630  [5.548 sec/step, loss=0.07294, avg_loss=0.07317]\n",
      "Step 541631  [5.555 sec/step, loss=0.07412, avg_loss=0.07317]\n",
      "Step 541632  [5.539 sec/step, loss=0.07508, avg_loss=0.07318]\n",
      "Step 541633  [5.522 sec/step, loss=0.06609, avg_loss=0.07310]\n",
      "Step 541634  [5.475 sec/step, loss=0.07429, avg_loss=0.07320]\n",
      "Step 541635  [5.461 sec/step, loss=0.07151, avg_loss=0.07317]\n",
      "Step 541636  [5.446 sec/step, loss=0.07366, avg_loss=0.07317]\n",
      "Step 541637  [5.432 sec/step, loss=0.07471, avg_loss=0.07318]\n",
      "Step 541638  [5.432 sec/step, loss=0.07442, avg_loss=0.07318]\n",
      "Step 541639  [5.458 sec/step, loss=0.07199, avg_loss=0.07317]\n",
      "Step 541640  [5.468 sec/step, loss=0.07435, avg_loss=0.07317]\n",
      "Step 541641  [5.461 sec/step, loss=0.07161, avg_loss=0.07314]\n",
      "Step 541642  [5.479 sec/step, loss=0.07545, avg_loss=0.07316]\n",
      "Step 541643  [5.471 sec/step, loss=0.07300, avg_loss=0.07314]\n",
      "Step 541644  [5.476 sec/step, loss=0.07399, avg_loss=0.07315]\n",
      "Step 541645  [5.497 sec/step, loss=0.07579, avg_loss=0.07317]\n",
      "Step 541646  [5.490 sec/step, loss=0.07298, avg_loss=0.07318]\n",
      "Generated 32 batches of size 32 in 2.776 sec\n",
      "Step 541647  [5.470 sec/step, loss=0.07088, avg_loss=0.07313]\n",
      "Step 541648  [5.473 sec/step, loss=0.07348, avg_loss=0.07316]\n",
      "Step 541649  [5.482 sec/step, loss=0.07245, avg_loss=0.07322]\n",
      "Step 541650  [5.494 sec/step, loss=0.07166, avg_loss=0.07321]\n",
      "Step 541651  [5.485 sec/step, loss=0.07185, avg_loss=0.07320]\n",
      "Step 541652  [5.475 sec/step, loss=0.07325, avg_loss=0.07318]\n",
      "Step 541653  [5.497 sec/step, loss=0.07542, avg_loss=0.07323]\n",
      "Step 541654  [5.491 sec/step, loss=0.07054, avg_loss=0.07319]\n",
      "Step 541655  [5.487 sec/step, loss=0.07442, avg_loss=0.07318]\n",
      "Step 541656  [5.464 sec/step, loss=0.07331, avg_loss=0.07316]\n",
      "Step 541657  [5.446 sec/step, loss=0.07304, avg_loss=0.07313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 541658  [5.448 sec/step, loss=0.07295, avg_loss=0.07310]\n",
      "Step 541659  [5.446 sec/step, loss=0.07329, avg_loss=0.07311]\n",
      "Step 541660  [5.446 sec/step, loss=0.07471, avg_loss=0.07310]\n",
      "Step 541661  [5.443 sec/step, loss=0.07261, avg_loss=0.07308]\n",
      "Step 541662  [5.440 sec/step, loss=0.07122, avg_loss=0.07306]\n",
      "Step 541663  [5.456 sec/step, loss=0.07065, avg_loss=0.07311]\n",
      "Step 541664  [5.466 sec/step, loss=0.07471, avg_loss=0.07311]\n",
      "Step 541665  [5.467 sec/step, loss=0.07235, avg_loss=0.07311]\n",
      "Step 541666  [5.460 sec/step, loss=0.07277, avg_loss=0.07311]\n",
      "Step 541667  [5.424 sec/step, loss=0.07305, avg_loss=0.07319]\n",
      "Step 541668  [5.429 sec/step, loss=0.07514, avg_loss=0.07320]\n",
      "Step 541669  [5.417 sec/step, loss=0.07155, avg_loss=0.07316]\n",
      "Step 541670  [5.402 sec/step, loss=0.06489, avg_loss=0.07311]\n",
      "Step 541671  [5.408 sec/step, loss=0.07580, avg_loss=0.07313]\n",
      "Step 541672  [5.442 sec/step, loss=0.06560, avg_loss=0.07304]\n",
      "Step 541673  [5.427 sec/step, loss=0.07538, avg_loss=0.07307]\n",
      "Step 541674  [5.417 sec/step, loss=0.07414, avg_loss=0.07305]\n",
      "Step 541675  [5.413 sec/step, loss=0.07509, avg_loss=0.07307]\n",
      "Step 541676  [5.450 sec/step, loss=0.07287, avg_loss=0.07307]\n",
      "Step 541677  [5.445 sec/step, loss=0.07529, avg_loss=0.07306]\n",
      "Step 541678  [5.454 sec/step, loss=0.07458, avg_loss=0.07308]\n",
      "Generated 32 batches of size 32 in 2.374 sec\n",
      "Step 541679  [5.468 sec/step, loss=0.07431, avg_loss=0.07310]\n",
      "Step 541680  [5.470 sec/step, loss=0.07339, avg_loss=0.07309]\n",
      "Step 541681  [5.482 sec/step, loss=0.07456, avg_loss=0.07311]\n",
      "Step 541682  [5.484 sec/step, loss=0.07327, avg_loss=0.07309]\n",
      "Step 541683  [5.494 sec/step, loss=0.07328, avg_loss=0.07312]\n",
      "Step 541684  [5.476 sec/step, loss=0.07008, avg_loss=0.07307]\n",
      "Step 541685  [5.451 sec/step, loss=0.07038, avg_loss=0.07301]\n",
      "Step 541686  [5.471 sec/step, loss=0.07457, avg_loss=0.07301]\n",
      "Step 541687  [5.474 sec/step, loss=0.07388, avg_loss=0.07303]\n",
      "Step 541688  [5.504 sec/step, loss=0.07200, avg_loss=0.07302]\n",
      "Step 541689  [5.498 sec/step, loss=0.07600, avg_loss=0.07302]\n",
      "Step 541690  [5.517 sec/step, loss=0.07523, avg_loss=0.07308]\n",
      "Step 541691  [5.518 sec/step, loss=0.07433, avg_loss=0.07307]\n",
      "Step 541692  [5.501 sec/step, loss=0.07353, avg_loss=0.07307]\n",
      "Step 541693  [5.551 sec/step, loss=0.06604, avg_loss=0.07302]\n",
      "Step 541694  [5.545 sec/step, loss=0.07289, avg_loss=0.07299]\n",
      "Step 541695  [5.545 sec/step, loss=0.07301, avg_loss=0.07299]\n",
      "Step 541696  [5.526 sec/step, loss=0.07050, avg_loss=0.07294]\n",
      "Step 541697  [5.528 sec/step, loss=0.07448, avg_loss=0.07295]\n",
      "Step 541698  [5.540 sec/step, loss=0.07572, avg_loss=0.07297]\n",
      "Step 541699  [5.545 sec/step, loss=0.07388, avg_loss=0.07298]\n",
      "Step 541700  [5.547 sec/step, loss=0.07421, avg_loss=0.07298]\n",
      "Writing summary at step: 541700\n",
      "Step 541701  [5.525 sec/step, loss=0.07342, avg_loss=0.07298]\n",
      "Step 541702  [5.514 sec/step, loss=0.07431, avg_loss=0.07297]\n",
      "Step 541703  [5.487 sec/step, loss=0.06607, avg_loss=0.07288]\n",
      "Step 541704  [5.485 sec/step, loss=0.07333, avg_loss=0.07288]\n",
      "Step 541705  [5.483 sec/step, loss=0.07526, avg_loss=0.07290]\n",
      "Step 541706  [5.477 sec/step, loss=0.07384, avg_loss=0.07289]\n",
      "Step 541707  [5.466 sec/step, loss=0.07350, avg_loss=0.07288]\n",
      "Step 541708  [5.489 sec/step, loss=0.07407, avg_loss=0.07297]\n",
      "Step 541709  [5.488 sec/step, loss=0.07509, avg_loss=0.07299]\n",
      "Generated 32 batches of size 32 in 2.488 sec\n",
      "Step 541710  [5.489 sec/step, loss=0.07331, avg_loss=0.07298]\n",
      "Step 541711  [5.437 sec/step, loss=0.07337, avg_loss=0.07306]\n",
      "Step 541712  [5.443 sec/step, loss=0.07265, avg_loss=0.07305]\n",
      "Step 541713  [5.457 sec/step, loss=0.07363, avg_loss=0.07306]\n",
      "Step 541714  [5.463 sec/step, loss=0.07440, avg_loss=0.07307]\n",
      "Step 541715  [5.455 sec/step, loss=0.07171, avg_loss=0.07306]\n",
      "Step 541716  [5.479 sec/step, loss=0.07591, avg_loss=0.07311]\n",
      "Step 541717  [5.450 sec/step, loss=0.07134, avg_loss=0.07306]\n",
      "Step 541718  [5.471 sec/step, loss=0.07553, avg_loss=0.07309]\n",
      "Step 541719  [5.492 sec/step, loss=0.07565, avg_loss=0.07313]\n",
      "Step 541720  [5.491 sec/step, loss=0.07423, avg_loss=0.07312]\n",
      "Step 541721  [5.466 sec/step, loss=0.07448, avg_loss=0.07314]\n",
      "Step 541722  [5.472 sec/step, loss=0.07367, avg_loss=0.07314]\n",
      "Step 541723  [5.527 sec/step, loss=0.06466, avg_loss=0.07306]\n",
      "Step 541724  [5.529 sec/step, loss=0.07525, avg_loss=0.07306]\n",
      "Step 541725  [5.528 sec/step, loss=0.07417, avg_loss=0.07306]\n",
      "Step 541726  [5.490 sec/step, loss=0.07315, avg_loss=0.07313]\n",
      "Step 541727  [5.474 sec/step, loss=0.07085, avg_loss=0.07309]\n",
      "Step 541728  [5.457 sec/step, loss=0.07117, avg_loss=0.07306]\n",
      "Step 541729  [5.436 sec/step, loss=0.07246, avg_loss=0.07305]\n",
      "Step 541730  [5.442 sec/step, loss=0.07466, avg_loss=0.07307]\n",
      "Step 541731  [5.416 sec/step, loss=0.07297, avg_loss=0.07306]\n",
      "Step 541732  [5.412 sec/step, loss=0.07472, avg_loss=0.07305]\n",
      "Step 541733  [5.442 sec/step, loss=0.07598, avg_loss=0.07315]\n",
      "Step 541734  [5.455 sec/step, loss=0.07449, avg_loss=0.07315]\n",
      "Step 541735  [5.474 sec/step, loss=0.07484, avg_loss=0.07319]\n",
      "Step 541736  [5.486 sec/step, loss=0.07268, avg_loss=0.07318]\n",
      "Step 541737  [5.484 sec/step, loss=0.07310, avg_loss=0.07316]\n",
      "Step 541738  [5.482 sec/step, loss=0.07184, avg_loss=0.07313]\n",
      "Step 541739  [5.452 sec/step, loss=0.07484, avg_loss=0.07316]\n",
      "Step 541740  [5.430 sec/step, loss=0.07088, avg_loss=0.07313]\n",
      "Step 541741  [5.442 sec/step, loss=0.07538, avg_loss=0.07317]\n",
      "Generated 32 batches of size 32 in 2.454 sec\n",
      "Step 541742  [5.450 sec/step, loss=0.07529, avg_loss=0.07316]\n",
      "Step 541743  [5.444 sec/step, loss=0.07334, avg_loss=0.07317]\n",
      "Step 541744  [5.455 sec/step, loss=0.07519, avg_loss=0.07318]\n",
      "Step 541745  [5.448 sec/step, loss=0.07491, avg_loss=0.07317]\n",
      "Step 541746  [5.447 sec/step, loss=0.07403, avg_loss=0.07318]\n",
      "Step 541747  [5.459 sec/step, loss=0.07464, avg_loss=0.07322]\n",
      "Step 541748  [5.464 sec/step, loss=0.07429, avg_loss=0.07323]\n",
      "Step 541749  [5.454 sec/step, loss=0.06532, avg_loss=0.07316]\n",
      "Step 541750  [5.440 sec/step, loss=0.07301, avg_loss=0.07317]\n",
      "Step 541751  [5.466 sec/step, loss=0.07433, avg_loss=0.07319]\n",
      "Step 541752  [5.467 sec/step, loss=0.07325, avg_loss=0.07319]\n",
      "Step 541753  [5.467 sec/step, loss=0.07431, avg_loss=0.07318]\n",
      "Step 541754  [5.493 sec/step, loss=0.07320, avg_loss=0.07321]\n",
      "Step 541755  [5.472 sec/step, loss=0.06515, avg_loss=0.07312]\n",
      "Step 541756  [5.496 sec/step, loss=0.07554, avg_loss=0.07314]\n",
      "Step 541757  [5.511 sec/step, loss=0.07193, avg_loss=0.07313]\n",
      "Step 541758  [5.498 sec/step, loss=0.07274, avg_loss=0.07313]\n",
      "Step 541759  [5.503 sec/step, loss=0.07523, avg_loss=0.07315]\n",
      "Step 541760  [5.490 sec/step, loss=0.07094, avg_loss=0.07311]\n",
      "Step 541761  [5.497 sec/step, loss=0.07596, avg_loss=0.07314]\n",
      "Step 541762  [5.485 sec/step, loss=0.07094, avg_loss=0.07314]\n",
      "Step 541763  [5.481 sec/step, loss=0.06894, avg_loss=0.07312]\n",
      "Step 541764  [5.479 sec/step, loss=0.07611, avg_loss=0.07313]\n",
      "Step 541765  [5.480 sec/step, loss=0.07442, avg_loss=0.07316]\n",
      "Step 541766  [5.503 sec/step, loss=0.07582, avg_loss=0.07319]\n",
      "Step 541767  [5.494 sec/step, loss=0.07510, avg_loss=0.07321]\n",
      "Step 541768  [5.487 sec/step, loss=0.07132, avg_loss=0.07317]\n",
      "Step 541769  [5.491 sec/step, loss=0.07379, avg_loss=0.07319]\n",
      "Step 541770  [5.504 sec/step, loss=0.07307, avg_loss=0.07327]\n",
      "Step 541771  [5.511 sec/step, loss=0.07352, avg_loss=0.07325]\n",
      "Step 541772  [5.457 sec/step, loss=0.07463, avg_loss=0.07334]\n",
      "Step 541773  [5.448 sec/step, loss=0.07415, avg_loss=0.07333]\n",
      "Generated 32 batches of size 32 in 2.374 sec\n",
      "Step 541774  [5.455 sec/step, loss=0.07553, avg_loss=0.07334]\n",
      "Step 541775  [5.461 sec/step, loss=0.07208, avg_loss=0.07331]\n",
      "Step 541776  [5.485 sec/step, loss=0.06627, avg_loss=0.07325]\n",
      "Step 541777  [5.482 sec/step, loss=0.07442, avg_loss=0.07324]\n",
      "Step 541778  [5.477 sec/step, loss=0.07363, avg_loss=0.07323]\n",
      "Step 541779  [5.457 sec/step, loss=0.07339, avg_loss=0.07322]\n",
      "Step 541780  [5.484 sec/step, loss=0.07340, avg_loss=0.07322]\n",
      "Step 541781  [5.477 sec/step, loss=0.07459, avg_loss=0.07322]\n",
      "Step 541782  [5.458 sec/step, loss=0.07396, avg_loss=0.07323]\n",
      "Step 541783  [5.459 sec/step, loss=0.06997, avg_loss=0.07319]\n",
      "Step 541784  [5.491 sec/step, loss=0.07492, avg_loss=0.07324]\n",
      "Step 541785  [5.505 sec/step, loss=0.07283, avg_loss=0.07327]\n",
      "Step 541786  [5.501 sec/step, loss=0.07419, avg_loss=0.07326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 541787  [5.496 sec/step, loss=0.07623, avg_loss=0.07329]\n",
      "Step 541788  [5.474 sec/step, loss=0.07343, avg_loss=0.07330]\n",
      "Step 541789  [5.474 sec/step, loss=0.07618, avg_loss=0.07330]\n",
      "Step 541790  [5.471 sec/step, loss=0.07396, avg_loss=0.07329]\n",
      "Step 541791  [5.455 sec/step, loss=0.07048, avg_loss=0.07325]\n",
      "Step 541792  [5.472 sec/step, loss=0.07329, avg_loss=0.07325]\n",
      "Step 541793  [5.474 sec/step, loss=0.06501, avg_loss=0.07324]\n",
      "Step 541794  [5.468 sec/step, loss=0.07429, avg_loss=0.07325]\n",
      "Step 541795  [5.444 sec/step, loss=0.07358, avg_loss=0.07326]\n",
      "Step 541796  [5.448 sec/step, loss=0.07312, avg_loss=0.07328]\n",
      "Step 541797  [5.458 sec/step, loss=0.07342, avg_loss=0.07327]\n",
      "Step 541798  [5.449 sec/step, loss=0.07493, avg_loss=0.07326]\n",
      "Step 541799  [5.458 sec/step, loss=0.07466, avg_loss=0.07327]\n",
      "Step 541800  [5.441 sec/step, loss=0.07051, avg_loss=0.07324]\n",
      "Writing summary at step: 541800\n",
      "Step 541801  [5.454 sec/step, loss=0.07399, avg_loss=0.07324]\n",
      "Step 541802  [5.446 sec/step, loss=0.07330, avg_loss=0.07323]\n",
      "Step 541803  [5.456 sec/step, loss=0.07298, avg_loss=0.07330]\n",
      "Step 541804  [5.471 sec/step, loss=0.07562, avg_loss=0.07332]\n",
      "Generated 32 batches of size 32 in 2.471 sec\n",
      "Step 541805  [5.489 sec/step, loss=0.07397, avg_loss=0.07331]\n",
      "Step 541806  [5.510 sec/step, loss=0.07438, avg_loss=0.07332]\n",
      "Step 541807  [5.519 sec/step, loss=0.07433, avg_loss=0.07332]\n",
      "Step 541808  [5.512 sec/step, loss=0.07491, avg_loss=0.07333]\n",
      "Step 541809  [5.500 sec/step, loss=0.07386, avg_loss=0.07332]\n",
      "Step 541810  [5.476 sec/step, loss=0.06556, avg_loss=0.07324]\n",
      "Step 541811  [5.473 sec/step, loss=0.07185, avg_loss=0.07323]\n",
      "Step 541812  [5.480 sec/step, loss=0.07435, avg_loss=0.07324]\n",
      "Step 541813  [5.472 sec/step, loss=0.07371, avg_loss=0.07325]\n",
      "Step 541814  [5.480 sec/step, loss=0.07466, avg_loss=0.07325]\n",
      "Step 541815  [5.475 sec/step, loss=0.07236, avg_loss=0.07325]\n",
      "Step 541816  [5.469 sec/step, loss=0.07452, avg_loss=0.07324]\n",
      "Step 541817  [5.473 sec/step, loss=0.07356, avg_loss=0.07326]\n",
      "Step 541818  [5.475 sec/step, loss=0.07620, avg_loss=0.07327]\n",
      "Step 541819  [5.464 sec/step, loss=0.07416, avg_loss=0.07325]\n",
      "Step 541820  [5.457 sec/step, loss=0.07246, avg_loss=0.07324]\n",
      "Step 541821  [5.457 sec/step, loss=0.07449, avg_loss=0.07324]\n",
      "Step 541822  [5.461 sec/step, loss=0.07328, avg_loss=0.07323]\n",
      "Step 541823  [5.407 sec/step, loss=0.07035, avg_loss=0.07329]\n",
      "Step 541824  [5.400 sec/step, loss=0.07413, avg_loss=0.07328]\n",
      "Step 541825  [5.394 sec/step, loss=0.07365, avg_loss=0.07327]\n",
      "Step 541826  [5.389 sec/step, loss=0.07446, avg_loss=0.07329]\n",
      "Step 541827  [5.389 sec/step, loss=0.07477, avg_loss=0.07333]\n",
      "Step 541828  [5.414 sec/step, loss=0.07582, avg_loss=0.07337]\n",
      "Step 541829  [5.414 sec/step, loss=0.07389, avg_loss=0.07339]\n",
      "Step 541830  [5.425 sec/step, loss=0.07632, avg_loss=0.07340]\n",
      "Step 541831  [5.425 sec/step, loss=0.07139, avg_loss=0.07339]\n",
      "Step 541832  [5.428 sec/step, loss=0.07578, avg_loss=0.07340]\n",
      "Step 541833  [5.398 sec/step, loss=0.06622, avg_loss=0.07330]\n",
      "Step 541834  [5.379 sec/step, loss=0.07407, avg_loss=0.07330]\n",
      "Step 541835  [5.406 sec/step, loss=0.06656, avg_loss=0.07321]\n",
      "Step 541836  [5.418 sec/step, loss=0.07386, avg_loss=0.07323]\n",
      "Generated 32 batches of size 32 in 2.436 sec\n",
      "Step 541837  [5.432 sec/step, loss=0.07622, avg_loss=0.07326]\n",
      "Step 541838  [5.439 sec/step, loss=0.07521, avg_loss=0.07329]\n",
      "Step 541839  [5.470 sec/step, loss=0.07283, avg_loss=0.07327]\n",
      "Step 541840  [5.484 sec/step, loss=0.07483, avg_loss=0.07331]\n",
      "Step 541841  [5.473 sec/step, loss=0.07454, avg_loss=0.07330]\n",
      "Step 541842  [5.460 sec/step, loss=0.07438, avg_loss=0.07329]\n",
      "Step 541843  [5.459 sec/step, loss=0.07035, avg_loss=0.07326]\n",
      "Step 541844  [5.458 sec/step, loss=0.07374, avg_loss=0.07325]\n",
      "Step 541845  [5.441 sec/step, loss=0.07139, avg_loss=0.07321]\n",
      "Step 541846  [5.436 sec/step, loss=0.07301, avg_loss=0.07320]\n",
      "Step 541847  [5.432 sec/step, loss=0.07523, avg_loss=0.07321]\n",
      "Step 541848  [5.435 sec/step, loss=0.07526, avg_loss=0.07322]\n",
      "Step 541849  [5.441 sec/step, loss=0.07078, avg_loss=0.07327]\n",
      "Step 541850  [5.456 sec/step, loss=0.07545, avg_loss=0.07330]\n",
      "Step 541851  [5.454 sec/step, loss=0.07483, avg_loss=0.07330]\n",
      "Step 541852  [5.487 sec/step, loss=0.07243, avg_loss=0.07329]\n",
      "Step 541853  [5.478 sec/step, loss=0.07079, avg_loss=0.07326]\n",
      "Step 541854  [5.467 sec/step, loss=0.07504, avg_loss=0.07328]\n",
      "Step 541855  [5.484 sec/step, loss=0.07356, avg_loss=0.07336]\n",
      "Step 541856  [5.475 sec/step, loss=0.07219, avg_loss=0.07333]\n",
      "Step 541857  [5.458 sec/step, loss=0.07333, avg_loss=0.07334]\n",
      "Step 541858  [5.456 sec/step, loss=0.06981, avg_loss=0.07331]\n",
      "Step 541859  [5.457 sec/step, loss=0.07355, avg_loss=0.07330]\n",
      "Step 541860  [5.485 sec/step, loss=0.07516, avg_loss=0.07334]\n",
      "Step 541861  [5.467 sec/step, loss=0.07343, avg_loss=0.07331]\n",
      "Step 541862  [5.479 sec/step, loss=0.07329, avg_loss=0.07334]\n",
      "Step 541863  [5.470 sec/step, loss=0.07111, avg_loss=0.07336]\n",
      "Step 541864  [5.465 sec/step, loss=0.07585, avg_loss=0.07336]\n",
      "Step 541865  [5.459 sec/step, loss=0.07433, avg_loss=0.07335]\n",
      "Step 541866  [5.494 sec/step, loss=0.06599, avg_loss=0.07326]\n",
      "Step 541867  [5.499 sec/step, loss=0.07344, avg_loss=0.07324]\n",
      "Step 541868  [5.502 sec/step, loss=0.07401, avg_loss=0.07327]\n",
      "Generated 32 batches of size 32 in 2.402 sec\n",
      "Step 541869  [5.509 sec/step, loss=0.07246, avg_loss=0.07325]\n",
      "Step 541870  [5.515 sec/step, loss=0.07405, avg_loss=0.07326]\n",
      "Step 541871  [5.514 sec/step, loss=0.07550, avg_loss=0.07328]\n",
      "Step 541872  [5.524 sec/step, loss=0.07509, avg_loss=0.07329]\n",
      "Step 541873  [5.537 sec/step, loss=0.07588, avg_loss=0.07330]\n",
      "Step 541874  [5.526 sec/step, loss=0.07342, avg_loss=0.07328]\n",
      "Step 541875  [5.505 sec/step, loss=0.06588, avg_loss=0.07322]\n",
      "Step 541876  [5.452 sec/step, loss=0.07473, avg_loss=0.07331]\n",
      "Step 541877  [5.453 sec/step, loss=0.07421, avg_loss=0.07330]\n",
      "Step 541878  [5.442 sec/step, loss=0.07053, avg_loss=0.07327]\n",
      "Step 541879  [5.451 sec/step, loss=0.07428, avg_loss=0.07328]\n",
      "Step 541880  [5.408 sec/step, loss=0.06506, avg_loss=0.07320]\n",
      "Step 541881  [5.411 sec/step, loss=0.07520, avg_loss=0.07320]\n",
      "Step 541882  [5.413 sec/step, loss=0.07426, avg_loss=0.07321]\n",
      "Step 541883  [5.403 sec/step, loss=0.07062, avg_loss=0.07321]\n",
      "Step 541884  [5.374 sec/step, loss=0.07369, avg_loss=0.07320]\n",
      "Step 541885  [5.382 sec/step, loss=0.07533, avg_loss=0.07323]\n",
      "Step 541886  [5.396 sec/step, loss=0.07307, avg_loss=0.07322]\n",
      "Step 541887  [5.403 sec/step, loss=0.07470, avg_loss=0.07320]\n",
      "Step 541888  [5.399 sec/step, loss=0.07390, avg_loss=0.07320]\n",
      "Step 541889  [5.403 sec/step, loss=0.07556, avg_loss=0.07320]\n",
      "Step 541890  [5.408 sec/step, loss=0.07310, avg_loss=0.07319]\n",
      "Step 541891  [5.414 sec/step, loss=0.07163, avg_loss=0.07320]\n",
      "Step 541892  [5.402 sec/step, loss=0.07393, avg_loss=0.07321]\n",
      "Step 541893  [5.348 sec/step, loss=0.07019, avg_loss=0.07326]\n",
      "Step 541894  [5.350 sec/step, loss=0.07402, avg_loss=0.07326]\n",
      "Step 541895  [5.368 sec/step, loss=0.07573, avg_loss=0.07328]\n",
      "Step 541896  [5.388 sec/step, loss=0.07549, avg_loss=0.07330]\n",
      "Step 541897  [5.384 sec/step, loss=0.07474, avg_loss=0.07332]\n",
      "Step 541898  [5.378 sec/step, loss=0.07352, avg_loss=0.07330]\n",
      "Step 541899  [5.356 sec/step, loss=0.07329, avg_loss=0.07329]\n",
      "Step 541900  [5.368 sec/step, loss=0.07233, avg_loss=0.07331]\n",
      "Writing summary at step: 541900\n",
      "Generated 32 batches of size 32 in 2.396 sec\n",
      "Step 541901  [5.370 sec/step, loss=0.07353, avg_loss=0.07330]\n",
      "Step 541902  [5.372 sec/step, loss=0.07472, avg_loss=0.07332]\n",
      "Step 541903  [5.388 sec/step, loss=0.07394, avg_loss=0.07332]\n",
      "Step 541904  [5.373 sec/step, loss=0.07319, avg_loss=0.07330]\n",
      "Step 541905  [5.396 sec/step, loss=0.06800, avg_loss=0.07324]\n",
      "Step 541906  [5.382 sec/step, loss=0.07372, avg_loss=0.07323]\n",
      "Step 541907  [5.385 sec/step, loss=0.07474, avg_loss=0.07324]\n",
      "Step 541908  [5.402 sec/step, loss=0.07374, avg_loss=0.07323]\n",
      "Step 541909  [5.408 sec/step, loss=0.07544, avg_loss=0.07324]\n",
      "Step 541910  [5.415 sec/step, loss=0.07048, avg_loss=0.07329]\n",
      "Step 541911  [5.469 sec/step, loss=0.06608, avg_loss=0.07323]\n",
      "Step 541912  [5.474 sec/step, loss=0.07500, avg_loss=0.07324]\n",
      "Step 541913  [5.478 sec/step, loss=0.07364, avg_loss=0.07324]\n",
      "Step 541914  [5.463 sec/step, loss=0.07313, avg_loss=0.07322]\n",
      "Step 541915  [5.463 sec/step, loss=0.07196, avg_loss=0.07322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 541916  [5.460 sec/step, loss=0.07445, avg_loss=0.07322]\n",
      "Step 541917  [5.454 sec/step, loss=0.06506, avg_loss=0.07313]\n",
      "Step 541918  [5.454 sec/step, loss=0.07378, avg_loss=0.07311]\n",
      "Step 541919  [5.439 sec/step, loss=0.07040, avg_loss=0.07307]\n",
      "Step 541920  [5.440 sec/step, loss=0.07490, avg_loss=0.07310]\n",
      "Step 541921  [5.445 sec/step, loss=0.07579, avg_loss=0.07311]\n",
      "Step 541922  [5.433 sec/step, loss=0.07233, avg_loss=0.07310]\n",
      "Step 541923  [5.427 sec/step, loss=0.07349, avg_loss=0.07313]\n",
      "Step 541924  [5.419 sec/step, loss=0.07481, avg_loss=0.07314]\n",
      "Step 541925  [5.443 sec/step, loss=0.07581, avg_loss=0.07316]\n",
      "Step 541926  [5.437 sec/step, loss=0.07416, avg_loss=0.07316]\n",
      "Step 541927  [5.443 sec/step, loss=0.07498, avg_loss=0.07316]\n",
      "Step 541928  [5.440 sec/step, loss=0.07398, avg_loss=0.07314]\n",
      "Step 541929  [5.455 sec/step, loss=0.07526, avg_loss=0.07315]\n",
      "Step 541930  [5.439 sec/step, loss=0.07340, avg_loss=0.07313]\n",
      "Step 541931  [5.442 sec/step, loss=0.07359, avg_loss=0.07315]\n",
      "Generated 32 batches of size 32 in 2.541 sec\n",
      "Step 541932  [5.437 sec/step, loss=0.07434, avg_loss=0.07313]\n",
      "Step 541933  [5.458 sec/step, loss=0.07424, avg_loss=0.07321]\n",
      "Step 541934  [5.469 sec/step, loss=0.07531, avg_loss=0.07323]\n",
      "Step 541935  [5.414 sec/step, loss=0.07053, avg_loss=0.07327]\n",
      "Step 541936  [5.415 sec/step, loss=0.07540, avg_loss=0.07328]\n",
      "Step 541937  [5.400 sec/step, loss=0.07412, avg_loss=0.07326]\n",
      "Step 541938  [5.408 sec/step, loss=0.07348, avg_loss=0.07324]\n",
      "Step 541939  [5.369 sec/step, loss=0.07342, avg_loss=0.07325]\n",
      "Step 541940  [5.391 sec/step, loss=0.07277, avg_loss=0.07323]\n",
      "Step 541941  [5.397 sec/step, loss=0.07208, avg_loss=0.07320]\n",
      "Step 541942  [5.384 sec/step, loss=0.07319, avg_loss=0.07319]\n",
      "Step 541943  [5.404 sec/step, loss=0.07405, avg_loss=0.07323]\n",
      "Step 541944  [5.446 sec/step, loss=0.06507, avg_loss=0.07314]\n",
      "Step 541945  [5.470 sec/step, loss=0.07436, avg_loss=0.07317]\n",
      "Step 541946  [5.471 sec/step, loss=0.07459, avg_loss=0.07319]\n",
      "Step 541947  [5.478 sec/step, loss=0.07553, avg_loss=0.07319]\n",
      "Step 541948  [5.467 sec/step, loss=0.07189, avg_loss=0.07316]\n",
      "Step 541949  [5.478 sec/step, loss=0.07452, avg_loss=0.07319]\n",
      "Step 541950  [5.472 sec/step, loss=0.07375, avg_loss=0.07318]\n",
      "Step 541951  [5.466 sec/step, loss=0.07419, avg_loss=0.07317]\n",
      "Step 541952  [5.429 sec/step, loss=0.07334, avg_loss=0.07318]\n",
      "Step 541953  [5.441 sec/step, loss=0.07569, avg_loss=0.07323]\n",
      "Step 541954  [5.408 sec/step, loss=0.06491, avg_loss=0.07313]\n",
      "Step 541955  [5.427 sec/step, loss=0.07472, avg_loss=0.07314]\n",
      "Step 541956  [5.427 sec/step, loss=0.07352, avg_loss=0.07315]\n",
      "Step 541957  [5.427 sec/step, loss=0.07111, avg_loss=0.07313]\n",
      "Step 541958  [5.437 sec/step, loss=0.07457, avg_loss=0.07318]\n",
      "Step 541959  [5.420 sec/step, loss=0.07260, avg_loss=0.07317]\n",
      "Step 541960  [5.400 sec/step, loss=0.07438, avg_loss=0.07316]\n",
      "Step 541961  [5.420 sec/step, loss=0.07529, avg_loss=0.07318]\n",
      "Step 541962  [5.422 sec/step, loss=0.07182, avg_loss=0.07316]\n",
      "Step 541963  [5.422 sec/step, loss=0.07197, avg_loss=0.07317]\n",
      "Generated 32 batches of size 32 in 2.425 sec\n",
      "Step 541964  [5.447 sec/step, loss=0.07480, avg_loss=0.07316]\n",
      "Step 541965  [5.441 sec/step, loss=0.07382, avg_loss=0.07316]\n",
      "Step 541966  [5.387 sec/step, loss=0.07360, avg_loss=0.07323]\n",
      "Step 541967  [5.377 sec/step, loss=0.07459, avg_loss=0.07324]\n",
      "Step 541968  [5.382 sec/step, loss=0.07610, avg_loss=0.07327]\n",
      "Step 541969  [5.382 sec/step, loss=0.07564, avg_loss=0.07330]\n",
      "Step 541970  [5.392 sec/step, loss=0.07555, avg_loss=0.07331]\n",
      "Step 541971  [5.382 sec/step, loss=0.07449, avg_loss=0.07330]\n",
      "Step 541972  [5.391 sec/step, loss=0.07593, avg_loss=0.07331]\n",
      "Step 541973  [5.376 sec/step, loss=0.07414, avg_loss=0.07329]\n",
      "Step 541974  [5.385 sec/step, loss=0.07409, avg_loss=0.07330]\n",
      "Step 541975  [5.405 sec/step, loss=0.07508, avg_loss=0.07339]\n",
      "Step 541976  [5.425 sec/step, loss=0.07394, avg_loss=0.07338]\n",
      "Step 541977  [5.416 sec/step, loss=0.07442, avg_loss=0.07339]\n",
      "Step 541978  [5.440 sec/step, loss=0.07564, avg_loss=0.07344]\n",
      "Step 541979  [5.438 sec/step, loss=0.07328, avg_loss=0.07343]\n",
      "Step 541980  [5.458 sec/step, loss=0.07440, avg_loss=0.07352]\n",
      "Step 541981  [5.483 sec/step, loss=0.07285, avg_loss=0.07350]\n",
      "Step 541982  [5.492 sec/step, loss=0.07573, avg_loss=0.07351]\n",
      "Step 541983  [5.554 sec/step, loss=0.06626, avg_loss=0.07347]\n",
      "Step 541984  [5.578 sec/step, loss=0.07483, avg_loss=0.07348]\n",
      "Step 541985  [5.574 sec/step, loss=0.07129, avg_loss=0.07344]\n",
      "Step 541986  [5.545 sec/step, loss=0.07480, avg_loss=0.07346]\n",
      "Step 541987  [5.531 sec/step, loss=0.07153, avg_loss=0.07342]\n",
      "Step 541988  [5.525 sec/step, loss=0.07219, avg_loss=0.07341]\n",
      "Step 541989  [5.517 sec/step, loss=0.07474, avg_loss=0.07340]\n",
      "Step 541990  [5.495 sec/step, loss=0.07361, avg_loss=0.07340]\n",
      "Step 541991  [5.483 sec/step, loss=0.06625, avg_loss=0.07335]\n",
      "Step 541992  [5.481 sec/step, loss=0.07186, avg_loss=0.07333]\n",
      "Step 541993  [5.476 sec/step, loss=0.07350, avg_loss=0.07336]\n",
      "Step 541994  [5.485 sec/step, loss=0.07562, avg_loss=0.07338]\n",
      "Step 541995  [5.488 sec/step, loss=0.07458, avg_loss=0.07337]\n",
      "Generated 32 batches of size 32 in 2.432 sec\n",
      "Step 541996  [5.487 sec/step, loss=0.07437, avg_loss=0.07336]\n",
      "Step 541997  [5.469 sec/step, loss=0.07199, avg_loss=0.07333]\n",
      "Step 541998  [5.482 sec/step, loss=0.07500, avg_loss=0.07334]\n",
      "Step 541999  [5.485 sec/step, loss=0.07322, avg_loss=0.07334]\n",
      "Step 542000  [5.489 sec/step, loss=0.07350, avg_loss=0.07335]\n",
      "Writing summary at step: 542000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-542000\n",
      "Saving audio and alignment...\n",
      "Input: sozuukii kay muxtdalif haaddisay barray parhdzuum sarrak kay muqaam pir huuay~_____________________\n",
      "Step 542001  [5.485 sec/step, loss=0.07326, avg_loss=0.07335]\n",
      "Step 542002  [5.496 sec/step, loss=0.07402, avg_loss=0.07335]\n",
      "Step 542003  [5.502 sec/step, loss=0.07627, avg_loss=0.07337]\n",
      "Step 542004  [5.505 sec/step, loss=0.07290, avg_loss=0.07337]\n",
      "Step 542005  [5.464 sec/step, loss=0.07548, avg_loss=0.07344]\n",
      "Step 542006  [5.449 sec/step, loss=0.07108, avg_loss=0.07341]\n",
      "Step 542007  [5.444 sec/step, loss=0.07235, avg_loss=0.07339]\n",
      "Step 542008  [5.421 sec/step, loss=0.07336, avg_loss=0.07339]\n",
      "Step 542009  [5.405 sec/step, loss=0.07114, avg_loss=0.07334]\n",
      "Step 542010  [5.425 sec/step, loss=0.07288, avg_loss=0.07337]\n",
      "Step 542011  [5.381 sec/step, loss=0.07529, avg_loss=0.07346]\n",
      "Step 542012  [5.369 sec/step, loss=0.07492, avg_loss=0.07346]\n",
      "Step 542013  [5.376 sec/step, loss=0.07499, avg_loss=0.07347]\n",
      "Step 542014  [5.366 sec/step, loss=0.07293, avg_loss=0.07347]\n",
      "Step 542015  [5.382 sec/step, loss=0.07411, avg_loss=0.07349]\n",
      "Step 542016  [5.393 sec/step, loss=0.07507, avg_loss=0.07350]\n",
      "Step 542017  [5.430 sec/step, loss=0.07318, avg_loss=0.07358]\n",
      "Step 542018  [5.416 sec/step, loss=0.07371, avg_loss=0.07358]\n",
      "Step 542019  [5.433 sec/step, loss=0.07184, avg_loss=0.07359]\n",
      "Step 542020  [5.433 sec/step, loss=0.07475, avg_loss=0.07359]\n",
      "Step 542021  [5.437 sec/step, loss=0.07546, avg_loss=0.07359]\n",
      "Step 542022  [5.420 sec/step, loss=0.06595, avg_loss=0.07352]\n",
      "Step 542023  [5.424 sec/step, loss=0.07244, avg_loss=0.07351]\n",
      "Step 542024  [5.413 sec/step, loss=0.07119, avg_loss=0.07348]\n",
      "Step 542025  [5.399 sec/step, loss=0.07365, avg_loss=0.07346]\n",
      "Generated 32 batches of size 32 in 2.415 sec\n",
      "Step 542026  [5.429 sec/step, loss=0.07285, avg_loss=0.07344]\n",
      "Step 542027  [5.424 sec/step, loss=0.07366, avg_loss=0.07343]\n",
      "Step 542028  [5.405 sec/step, loss=0.07342, avg_loss=0.07342]\n",
      "Step 542029  [5.397 sec/step, loss=0.07427, avg_loss=0.07341]\n",
      "Step 542030  [5.407 sec/step, loss=0.07594, avg_loss=0.07344]\n",
      "Step 542031  [5.423 sec/step, loss=0.07553, avg_loss=0.07346]\n",
      "Step 542032  [5.434 sec/step, loss=0.07449, avg_loss=0.07346]\n",
      "Step 542033  [5.429 sec/step, loss=0.07115, avg_loss=0.07343]\n",
      "Step 542034  [5.470 sec/step, loss=0.06665, avg_loss=0.07334]\n",
      "Step 542035  [5.471 sec/step, loss=0.07300, avg_loss=0.07337]\n",
      "Step 542036  [5.449 sec/step, loss=0.07301, avg_loss=0.07334]\n",
      "Step 542037  [5.454 sec/step, loss=0.07270, avg_loss=0.07333]\n",
      "Step 542038  [5.440 sec/step, loss=0.07288, avg_loss=0.07332]\n",
      "Step 542039  [5.456 sec/step, loss=0.07460, avg_loss=0.07333]\n",
      "Step 542040  [5.421 sec/step, loss=0.07334, avg_loss=0.07334]\n",
      "Step 542041  [5.414 sec/step, loss=0.07481, avg_loss=0.07337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 542042  [5.433 sec/step, loss=0.07392, avg_loss=0.07338]\n",
      "Step 542043  [5.418 sec/step, loss=0.07220, avg_loss=0.07336]\n",
      "Step 542044  [5.367 sec/step, loss=0.07375, avg_loss=0.07344]\n",
      "Step 542045  [5.358 sec/step, loss=0.07443, avg_loss=0.07344]\n",
      "Step 542046  [5.359 sec/step, loss=0.07123, avg_loss=0.07341]\n",
      "Step 542047  [5.362 sec/step, loss=0.07531, avg_loss=0.07341]\n",
      "Step 542048  [5.349 sec/step, loss=0.07009, avg_loss=0.07339]\n",
      "Step 542049  [5.375 sec/step, loss=0.07198, avg_loss=0.07337]\n",
      "Step 542050  [5.355 sec/step, loss=0.06469, avg_loss=0.07327]\n",
      "Step 542051  [5.364 sec/step, loss=0.07534, avg_loss=0.07329]\n",
      "Step 542052  [5.371 sec/step, loss=0.07396, avg_loss=0.07329]\n",
      "Step 542053  [5.355 sec/step, loss=0.07192, avg_loss=0.07325]\n",
      "Step 542054  [5.363 sec/step, loss=0.06934, avg_loss=0.07330]\n",
      "Step 542055  [5.394 sec/step, loss=0.06511, avg_loss=0.07320]\n",
      "Step 542056  [5.393 sec/step, loss=0.07414, avg_loss=0.07321]\n",
      "Step 542057  [5.414 sec/step, loss=0.07294, avg_loss=0.07323]\n",
      "Generated 32 batches of size 32 in 2.359 sec\n",
      "Step 542058  [5.419 sec/step, loss=0.07369, avg_loss=0.07322]\n",
      "Step 542059  [5.424 sec/step, loss=0.07500, avg_loss=0.07324]\n",
      "Step 542060  [5.432 sec/step, loss=0.07556, avg_loss=0.07325]\n",
      "Step 542061  [5.431 sec/step, loss=0.07210, avg_loss=0.07322]\n",
      "Step 542062  [5.431 sec/step, loss=0.07511, avg_loss=0.07326]\n",
      "Step 542063  [5.463 sec/step, loss=0.07555, avg_loss=0.07329]\n",
      "Step 542064  [5.442 sec/step, loss=0.07602, avg_loss=0.07330]\n",
      "Step 542065  [5.465 sec/step, loss=0.07537, avg_loss=0.07332]\n",
      "Step 542066  [5.469 sec/step, loss=0.07363, avg_loss=0.07332]\n",
      "Step 542067  [5.469 sec/step, loss=0.07417, avg_loss=0.07331]\n",
      "Step 542068  [5.468 sec/step, loss=0.07576, avg_loss=0.07331]\n",
      "Step 542069  [5.442 sec/step, loss=0.06596, avg_loss=0.07321]\n",
      "Step 542070  [5.445 sec/step, loss=0.07477, avg_loss=0.07321]\n",
      "Step 542071  [5.432 sec/step, loss=0.07331, avg_loss=0.07320]\n",
      "Step 542072  [5.426 sec/step, loss=0.07496, avg_loss=0.07319]\n",
      "Step 542073  [5.423 sec/step, loss=0.07250, avg_loss=0.07317]\n",
      "Step 542074  [5.424 sec/step, loss=0.07362, avg_loss=0.07316]\n",
      "Step 542075  [5.411 sec/step, loss=0.07357, avg_loss=0.07315]\n",
      "Step 542076  [5.403 sec/step, loss=0.07529, avg_loss=0.07316]\n",
      "Step 542077  [5.416 sec/step, loss=0.07544, avg_loss=0.07317]\n",
      "Step 542078  [5.420 sec/step, loss=0.07450, avg_loss=0.07316]\n",
      "Step 542079  [5.417 sec/step, loss=0.07323, avg_loss=0.07316]\n",
      "Step 542080  [5.411 sec/step, loss=0.07496, avg_loss=0.07317]\n",
      "Step 542081  [5.389 sec/step, loss=0.07437, avg_loss=0.07318]\n",
      "Step 542082  [5.378 sec/step, loss=0.07479, avg_loss=0.07317]\n",
      "Step 542083  [5.329 sec/step, loss=0.07448, avg_loss=0.07325]\n",
      "Step 542084  [5.313 sec/step, loss=0.07062, avg_loss=0.07321]\n",
      "Step 542085  [5.320 sec/step, loss=0.07567, avg_loss=0.07326]\n",
      "Step 542086  [5.320 sec/step, loss=0.07425, avg_loss=0.07325]\n",
      "Step 542087  [5.322 sec/step, loss=0.07478, avg_loss=0.07328]\n",
      "Step 542088  [5.312 sec/step, loss=0.07070, avg_loss=0.07327]\n",
      "Step 542089  [5.298 sec/step, loss=0.07110, avg_loss=0.07323]\n",
      "Generated 32 batches of size 32 in 2.357 sec\n",
      "Step 542090  [5.324 sec/step, loss=0.07460, avg_loss=0.07324]\n",
      "Step 542091  [5.351 sec/step, loss=0.07373, avg_loss=0.07332]\n",
      "Step 542092  [5.357 sec/step, loss=0.07323, avg_loss=0.07333]\n",
      "Step 542093  [5.363 sec/step, loss=0.07356, avg_loss=0.07333]\n",
      "Step 542094  [5.349 sec/step, loss=0.07262, avg_loss=0.07330]\n",
      "Step 542095  [5.337 sec/step, loss=0.07381, avg_loss=0.07329]\n",
      "Step 542096  [5.374 sec/step, loss=0.06536, avg_loss=0.07320]\n",
      "Step 542097  [5.415 sec/step, loss=0.07162, avg_loss=0.07320]\n",
      "Step 542098  [5.403 sec/step, loss=0.07227, avg_loss=0.07317]\n",
      "Step 542099  [5.421 sec/step, loss=0.07523, avg_loss=0.07319]\n",
      "Step 542100  [5.428 sec/step, loss=0.07427, avg_loss=0.07320]\n",
      "Writing summary at step: 542100\n",
      "Step 542101  [5.428 sec/step, loss=0.07468, avg_loss=0.07321]\n",
      "Step 542102  [5.424 sec/step, loss=0.07440, avg_loss=0.07322]\n",
      "Step 542103  [5.406 sec/step, loss=0.07332, avg_loss=0.07319]\n",
      "Step 542104  [5.399 sec/step, loss=0.07001, avg_loss=0.07316]\n",
      "Step 542105  [5.423 sec/step, loss=0.07265, avg_loss=0.07313]\n",
      "Step 542106  [5.438 sec/step, loss=0.07392, avg_loss=0.07316]\n",
      "Step 542107  [5.448 sec/step, loss=0.07264, avg_loss=0.07316]\n",
      "Step 542108  [5.440 sec/step, loss=0.06571, avg_loss=0.07309]\n",
      "Step 542109  [5.445 sec/step, loss=0.07480, avg_loss=0.07312]\n",
      "Step 542110  [5.435 sec/step, loss=0.07215, avg_loss=0.07312]\n",
      "Step 542111  [5.427 sec/step, loss=0.07363, avg_loss=0.07310]\n",
      "Step 542112  [5.441 sec/step, loss=0.07508, avg_loss=0.07310]\n",
      "Step 542113  [5.443 sec/step, loss=0.07483, avg_loss=0.07310]\n",
      "Step 542114  [5.464 sec/step, loss=0.07542, avg_loss=0.07312]\n",
      "Step 542115  [5.449 sec/step, loss=0.07317, avg_loss=0.07311]\n",
      "Step 542116  [5.442 sec/step, loss=0.07463, avg_loss=0.07311]\n",
      "Step 542117  [5.423 sec/step, loss=0.07455, avg_loss=0.07312]\n",
      "Step 542118  [5.419 sec/step, loss=0.07440, avg_loss=0.07313]\n",
      "Step 542119  [5.433 sec/step, loss=0.07575, avg_loss=0.07317]\n",
      "Step 542120  [5.431 sec/step, loss=0.07409, avg_loss=0.07316]\n",
      "Generated 32 batches of size 32 in 2.394 sec\n",
      "Step 542121  [5.432 sec/step, loss=0.07591, avg_loss=0.07317]\n",
      "Step 542122  [5.439 sec/step, loss=0.07303, avg_loss=0.07324]\n",
      "Step 542123  [5.449 sec/step, loss=0.07424, avg_loss=0.07326]\n",
      "Step 542124  [5.474 sec/step, loss=0.07550, avg_loss=0.07330]\n",
      "Step 542125  [5.466 sec/step, loss=0.07177, avg_loss=0.07328]\n",
      "Step 542126  [5.422 sec/step, loss=0.07135, avg_loss=0.07327]\n",
      "Step 542127  [5.474 sec/step, loss=0.06555, avg_loss=0.07318]\n",
      "Step 542128  [5.477 sec/step, loss=0.07304, avg_loss=0.07318]\n",
      "Step 542129  [5.465 sec/step, loss=0.07336, avg_loss=0.07317]\n",
      "Step 542130  [5.467 sec/step, loss=0.07598, avg_loss=0.07317]\n",
      "Step 542131  [5.460 sec/step, loss=0.07499, avg_loss=0.07317]\n",
      "Step 542132  [5.437 sec/step, loss=0.07343, avg_loss=0.07316]\n",
      "Step 542133  [5.450 sec/step, loss=0.07535, avg_loss=0.07320]\n",
      "Step 542134  [5.400 sec/step, loss=0.07491, avg_loss=0.07328]\n",
      "Step 542135  [5.409 sec/step, loss=0.07492, avg_loss=0.07330]\n",
      "Step 542136  [5.441 sec/step, loss=0.07256, avg_loss=0.07330]\n",
      "Step 542137  [5.443 sec/step, loss=0.07225, avg_loss=0.07329]\n",
      "Step 542138  [5.430 sec/step, loss=0.06676, avg_loss=0.07323]\n",
      "Step 542139  [5.429 sec/step, loss=0.07362, avg_loss=0.07322]\n",
      "Step 542140  [5.437 sec/step, loss=0.07365, avg_loss=0.07322]\n",
      "Step 542141  [5.452 sec/step, loss=0.07288, avg_loss=0.07320]\n",
      "Step 542142  [5.436 sec/step, loss=0.07243, avg_loss=0.07319]\n",
      "Step 542143  [5.452 sec/step, loss=0.07576, avg_loss=0.07322]\n",
      "Step 542144  [5.448 sec/step, loss=0.07463, avg_loss=0.07323]\n",
      "Step 542145  [5.442 sec/step, loss=0.07257, avg_loss=0.07321]\n",
      "Step 542146  [5.454 sec/step, loss=0.07502, avg_loss=0.07325]\n",
      "Step 542147  [5.429 sec/step, loss=0.07153, avg_loss=0.07321]\n",
      "Step 542148  [5.452 sec/step, loss=0.07478, avg_loss=0.07326]\n",
      "Step 542149  [5.476 sec/step, loss=0.06657, avg_loss=0.07321]\n",
      "Step 542150  [5.496 sec/step, loss=0.07124, avg_loss=0.07327]\n",
      "Step 542151  [5.482 sec/step, loss=0.07353, avg_loss=0.07325]\n",
      "Step 542152  [5.475 sec/step, loss=0.07094, avg_loss=0.07322]\n",
      "Generated 32 batches of size 32 in 2.483 sec\n",
      "Step 542153  [5.474 sec/step, loss=0.07389, avg_loss=0.07324]\n",
      "Step 542154  [5.486 sec/step, loss=0.07416, avg_loss=0.07329]\n",
      "Step 542155  [5.435 sec/step, loss=0.07126, avg_loss=0.07335]\n",
      "Step 542156  [5.446 sec/step, loss=0.07523, avg_loss=0.07336]\n",
      "Step 542157  [5.454 sec/step, loss=0.07258, avg_loss=0.07336]\n",
      "Step 542158  [5.455 sec/step, loss=0.07376, avg_loss=0.07336]\n",
      "Step 542159  [5.451 sec/step, loss=0.07429, avg_loss=0.07335]\n",
      "Step 542160  [5.439 sec/step, loss=0.07276, avg_loss=0.07333]\n",
      "Step 542161  [5.425 sec/step, loss=0.07283, avg_loss=0.07333]\n",
      "Step 542162  [5.421 sec/step, loss=0.07462, avg_loss=0.07333]\n",
      "Step 542163  [5.392 sec/step, loss=0.07329, avg_loss=0.07331]\n",
      "Step 542164  [5.380 sec/step, loss=0.07179, avg_loss=0.07326]\n",
      "Step 542165  [5.413 sec/step, loss=0.06529, avg_loss=0.07316]\n",
      "Step 542166  [5.424 sec/step, loss=0.07542, avg_loss=0.07318]\n",
      "Step 542167  [5.433 sec/step, loss=0.07252, avg_loss=0.07316]\n",
      "Step 542168  [5.413 sec/step, loss=0.06986, avg_loss=0.07311]\n",
      "Step 542169  [5.445 sec/step, loss=0.07560, avg_loss=0.07320]\n",
      "Step 542170  [5.427 sec/step, loss=0.07377, avg_loss=0.07319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 542171  [5.432 sec/step, loss=0.07426, avg_loss=0.07320]\n",
      "Step 542172  [5.422 sec/step, loss=0.07374, avg_loss=0.07319]\n",
      "Step 542173  [5.450 sec/step, loss=0.07240, avg_loss=0.07319]\n",
      "Step 542174  [5.438 sec/step, loss=0.07269, avg_loss=0.07318]\n",
      "Step 542175  [5.454 sec/step, loss=0.07396, avg_loss=0.07318]\n",
      "Step 542176  [5.453 sec/step, loss=0.07552, avg_loss=0.07319]\n",
      "Step 542177  [5.452 sec/step, loss=0.07532, avg_loss=0.07318]\n",
      "Step 542178  [5.441 sec/step, loss=0.07365, avg_loss=0.07318]\n",
      "Step 542179  [5.453 sec/step, loss=0.07438, avg_loss=0.07319]\n",
      "Step 542180  [5.469 sec/step, loss=0.07287, avg_loss=0.07317]\n",
      "Step 542181  [5.464 sec/step, loss=0.07364, avg_loss=0.07316]\n",
      "Step 542182  [5.481 sec/step, loss=0.07465, avg_loss=0.07316]\n",
      "Step 542183  [5.482 sec/step, loss=0.07453, avg_loss=0.07316]\n",
      "Step 542184  [5.465 sec/step, loss=0.06431, avg_loss=0.07310]\n",
      "Generated 32 batches of size 32 in 2.411 sec\n",
      "Step 542185  [5.461 sec/step, loss=0.07424, avg_loss=0.07308]\n",
      "Step 542186  [5.461 sec/step, loss=0.07111, avg_loss=0.07305]\n",
      "Step 542187  [5.450 sec/step, loss=0.07326, avg_loss=0.07303]\n",
      "Step 542188  [5.471 sec/step, loss=0.07293, avg_loss=0.07306]\n",
      "Step 542189  [5.477 sec/step, loss=0.07278, avg_loss=0.07307]\n",
      "Step 542190  [5.476 sec/step, loss=0.07555, avg_loss=0.07308]\n",
      "Step 542191  [5.451 sec/step, loss=0.07087, avg_loss=0.07305]\n",
      "Step 542192  [5.449 sec/step, loss=0.07446, avg_loss=0.07307]\n",
      "Step 542193  [5.454 sec/step, loss=0.07412, avg_loss=0.07307]\n",
      "Step 542194  [5.455 sec/step, loss=0.07267, avg_loss=0.07307]\n",
      "Step 542195  [5.451 sec/step, loss=0.07250, avg_loss=0.07306]\n",
      "Step 542196  [5.401 sec/step, loss=0.07416, avg_loss=0.07315]\n",
      "Step 542197  [5.367 sec/step, loss=0.07209, avg_loss=0.07315]\n",
      "Step 542198  [5.373 sec/step, loss=0.07321, avg_loss=0.07316]\n",
      "Step 542199  [5.361 sec/step, loss=0.07350, avg_loss=0.07314]\n",
      "Step 542200  [5.363 sec/step, loss=0.07580, avg_loss=0.07316]\n",
      "Writing summary at step: 542200\n",
      "Step 542201  [5.367 sec/step, loss=0.07432, avg_loss=0.07316]\n",
      "Step 542202  [5.363 sec/step, loss=0.07458, avg_loss=0.07316]\n",
      "Step 542203  [5.357 sec/step, loss=0.07340, avg_loss=0.07316]\n",
      "Step 542204  [5.349 sec/step, loss=0.06414, avg_loss=0.07310]\n",
      "Step 542205  [5.374 sec/step, loss=0.06505, avg_loss=0.07302]\n",
      "Step 542206  [5.387 sec/step, loss=0.07442, avg_loss=0.07303]\n",
      "Step 542207  [5.370 sec/step, loss=0.07328, avg_loss=0.07304]\n",
      "Step 542208  [5.383 sec/step, loss=0.07138, avg_loss=0.07309]\n",
      "Step 542209  [5.396 sec/step, loss=0.07473, avg_loss=0.07309]\n",
      "Step 542210  [5.408 sec/step, loss=0.07532, avg_loss=0.07312]\n",
      "Step 542211  [5.405 sec/step, loss=0.07318, avg_loss=0.07312]\n",
      "Step 542212  [5.422 sec/step, loss=0.07235, avg_loss=0.07309]\n",
      "Step 542213  [5.413 sec/step, loss=0.07418, avg_loss=0.07308]\n",
      "Step 542214  [5.415 sec/step, loss=0.07462, avg_loss=0.07308]\n",
      "Step 542215  [5.423 sec/step, loss=0.07425, avg_loss=0.07309]\n",
      "Generated 32 batches of size 32 in 2.432 sec\n",
      "Step 542216  [5.428 sec/step, loss=0.07467, avg_loss=0.07309]\n",
      "Step 542217  [5.435 sec/step, loss=0.07271, avg_loss=0.07307]\n",
      "Step 542218  [5.442 sec/step, loss=0.07495, avg_loss=0.07308]\n",
      "Step 542219  [5.433 sec/step, loss=0.07541, avg_loss=0.07307]\n",
      "Step 542220  [5.423 sec/step, loss=0.07015, avg_loss=0.07303]\n",
      "Step 542221  [5.409 sec/step, loss=0.07115, avg_loss=0.07298]\n",
      "Step 542222  [5.429 sec/step, loss=0.07542, avg_loss=0.07301]\n",
      "Step 542223  [5.411 sec/step, loss=0.07031, avg_loss=0.07297]\n",
      "Step 542224  [5.408 sec/step, loss=0.07560, avg_loss=0.07297]\n",
      "Step 542225  [5.411 sec/step, loss=0.07413, avg_loss=0.07299]\n",
      "Step 542226  [5.428 sec/step, loss=0.07202, avg_loss=0.07300]\n",
      "Step 542227  [5.377 sec/step, loss=0.07349, avg_loss=0.07308]\n",
      "Step 542228  [5.410 sec/step, loss=0.07269, avg_loss=0.07308]\n",
      "Step 542229  [5.426 sec/step, loss=0.07509, avg_loss=0.07309]\n",
      "Step 542230  [5.423 sec/step, loss=0.07360, avg_loss=0.07307]\n",
      "Step 542231  [5.419 sec/step, loss=0.07412, avg_loss=0.07306]\n",
      "Step 542232  [5.478 sec/step, loss=0.06542, avg_loss=0.07298]\n",
      "Step 542233  [5.467 sec/step, loss=0.07211, avg_loss=0.07295]\n",
      "Step 542234  [5.477 sec/step, loss=0.07368, avg_loss=0.07294]\n",
      "Step 542235  [5.480 sec/step, loss=0.07514, avg_loss=0.07294]\n",
      "Step 542236  [5.469 sec/step, loss=0.07571, avg_loss=0.07297]\n",
      "Step 542237  [5.462 sec/step, loss=0.07427, avg_loss=0.07299]\n",
      "Step 542238  [5.467 sec/step, loss=0.07089, avg_loss=0.07303]\n",
      "Step 542239  [5.476 sec/step, loss=0.07547, avg_loss=0.07305]\n",
      "Step 542240  [5.468 sec/step, loss=0.07005, avg_loss=0.07301]\n",
      "Step 542241  [5.462 sec/step, loss=0.07466, avg_loss=0.07303]\n",
      "Step 542242  [5.460 sec/step, loss=0.07377, avg_loss=0.07305]\n",
      "Step 542243  [5.448 sec/step, loss=0.07276, avg_loss=0.07302]\n",
      "Step 542244  [5.460 sec/step, loss=0.07571, avg_loss=0.07303]\n",
      "Step 542245  [5.480 sec/step, loss=0.07577, avg_loss=0.07306]\n",
      "Step 542246  [5.471 sec/step, loss=0.07434, avg_loss=0.07305]\n",
      "Step 542247  [5.490 sec/step, loss=0.07417, avg_loss=0.07308]\n",
      "Generated 32 batches of size 32 in 2.453 sec\n",
      "Step 542248  [5.498 sec/step, loss=0.07287, avg_loss=0.07306]\n",
      "Step 542249  [5.470 sec/step, loss=0.07243, avg_loss=0.07312]\n",
      "Step 542250  [5.454 sec/step, loss=0.07152, avg_loss=0.07312]\n",
      "Step 542251  [5.436 sec/step, loss=0.06618, avg_loss=0.07305]\n",
      "Step 542252  [5.448 sec/step, loss=0.07112, avg_loss=0.07305]\n",
      "Step 542253  [5.451 sec/step, loss=0.07291, avg_loss=0.07304]\n",
      "Step 542254  [5.446 sec/step, loss=0.07282, avg_loss=0.07303]\n",
      "Step 542255  [5.439 sec/step, loss=0.07294, avg_loss=0.07304]\n",
      "Step 542256  [5.434 sec/step, loss=0.07544, avg_loss=0.07304]\n",
      "Step 542257  [5.424 sec/step, loss=0.07399, avg_loss=0.07306]\n",
      "Step 542258  [5.405 sec/step, loss=0.07338, avg_loss=0.07305]\n",
      "Step 542259  [5.394 sec/step, loss=0.07084, avg_loss=0.07302]\n",
      "Step 542260  [5.400 sec/step, loss=0.07059, avg_loss=0.07300]\n",
      "Step 542261  [5.405 sec/step, loss=0.07461, avg_loss=0.07302]\n",
      "Step 542262  [5.403 sec/step, loss=0.07238, avg_loss=0.07299]\n",
      "Step 542263  [5.425 sec/step, loss=0.07548, avg_loss=0.07302]\n",
      "Step 542264  [5.430 sec/step, loss=0.07395, avg_loss=0.07304]\n",
      "Step 542265  [5.377 sec/step, loss=0.07462, avg_loss=0.07313]\n",
      "Step 542266  [5.377 sec/step, loss=0.07308, avg_loss=0.07311]\n",
      "Step 542267  [5.367 sec/step, loss=0.07397, avg_loss=0.07312]\n",
      "Step 542268  [5.397 sec/step, loss=0.07269, avg_loss=0.07315]\n",
      "Step 542269  [5.388 sec/step, loss=0.07248, avg_loss=0.07312]\n",
      "Step 542270  [5.405 sec/step, loss=0.07561, avg_loss=0.07314]\n",
      "Step 542271  [5.423 sec/step, loss=0.07567, avg_loss=0.07315]\n",
      "Step 542272  [5.431 sec/step, loss=0.07382, avg_loss=0.07315]\n",
      "Step 542273  [5.397 sec/step, loss=0.07275, avg_loss=0.07316]\n",
      "Step 542274  [5.403 sec/step, loss=0.07402, avg_loss=0.07317]\n",
      "Step 542275  [5.401 sec/step, loss=0.07568, avg_loss=0.07319]\n",
      "Step 542276  [5.397 sec/step, loss=0.07451, avg_loss=0.07318]\n",
      "Step 542277  [5.378 sec/step, loss=0.07338, avg_loss=0.07316]\n",
      "Step 542278  [5.401 sec/step, loss=0.07267, avg_loss=0.07315]\n",
      "Step 542279  [5.393 sec/step, loss=0.07442, avg_loss=0.07315]\n",
      "Generated 32 batches of size 32 in 2.638 sec\n",
      "Step 542280  [5.435 sec/step, loss=0.06633, avg_loss=0.07308]\n",
      "Step 542281  [5.433 sec/step, loss=0.07249, avg_loss=0.07307]\n",
      "Step 542282  [5.424 sec/step, loss=0.07532, avg_loss=0.07308]\n",
      "Step 542283  [5.435 sec/step, loss=0.07323, avg_loss=0.07306]\n",
      "Step 542284  [5.441 sec/step, loss=0.07090, avg_loss=0.07313]\n",
      "Step 542285  [5.417 sec/step, loss=0.06609, avg_loss=0.07305]\n",
      "Step 542286  [5.421 sec/step, loss=0.07467, avg_loss=0.07308]\n",
      "Step 542287  [5.428 sec/step, loss=0.07196, avg_loss=0.07307]\n",
      "Step 542288  [5.426 sec/step, loss=0.07388, avg_loss=0.07308]\n",
      "Step 542289  [5.432 sec/step, loss=0.07414, avg_loss=0.07309]\n",
      "Step 542290  [5.419 sec/step, loss=0.07366, avg_loss=0.07307]\n",
      "Step 542291  [5.445 sec/step, loss=0.07543, avg_loss=0.07312]\n",
      "Step 542292  [5.444 sec/step, loss=0.07127, avg_loss=0.07309]\n",
      "Step 542293  [5.449 sec/step, loss=0.07455, avg_loss=0.07309]\n",
      "Step 542294  [5.480 sec/step, loss=0.07274, avg_loss=0.07309]\n",
      "Step 542295  [5.469 sec/step, loss=0.06493, avg_loss=0.07302]\n",
      "Step 542296  [5.467 sec/step, loss=0.07445, avg_loss=0.07302]\n",
      "Step 542297  [5.479 sec/step, loss=0.07370, avg_loss=0.07304]\n",
      "Step 542298  [5.478 sec/step, loss=0.07162, avg_loss=0.07302]\n",
      "Step 542299  [5.466 sec/step, loss=0.07117, avg_loss=0.07300]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 542300  [5.457 sec/step, loss=0.07470, avg_loss=0.07299]\n",
      "Writing summary at step: 542300\n",
      "Step 542301  [5.473 sec/step, loss=0.07182, avg_loss=0.07296]\n",
      "Step 542302  [5.480 sec/step, loss=0.07465, avg_loss=0.07296]\n",
      "Step 542303  [5.499 sec/step, loss=0.07514, avg_loss=0.07298]\n",
      "Step 542304  [5.517 sec/step, loss=0.07137, avg_loss=0.07305]\n",
      "Step 542305  [5.470 sec/step, loss=0.07458, avg_loss=0.07315]\n",
      "Step 542306  [5.445 sec/step, loss=0.07345, avg_loss=0.07314]\n",
      "Step 542307  [5.463 sec/step, loss=0.07522, avg_loss=0.07316]\n",
      "Step 542308  [5.465 sec/step, loss=0.07337, avg_loss=0.07318]\n",
      "Step 542309  [5.464 sec/step, loss=0.07477, avg_loss=0.07318]\n",
      "Step 542310  [5.462 sec/step, loss=0.07561, avg_loss=0.07318]\n",
      "Generated 32 batches of size 32 in 2.580 sec\n",
      "Step 542311  [5.466 sec/step, loss=0.07242, avg_loss=0.07317]\n",
      "Step 542312  [5.490 sec/step, loss=0.06578, avg_loss=0.07311]\n",
      "Step 542313  [5.478 sec/step, loss=0.07236, avg_loss=0.07309]\n",
      "Step 542314  [5.468 sec/step, loss=0.07418, avg_loss=0.07308]\n",
      "Step 542315  [5.481 sec/step, loss=0.07578, avg_loss=0.07310]\n",
      "Step 542316  [5.460 sec/step, loss=0.07119, avg_loss=0.07307]\n",
      "Step 542317  [5.462 sec/step, loss=0.07318, avg_loss=0.07307]\n",
      "Step 542318  [5.450 sec/step, loss=0.07356, avg_loss=0.07306]\n",
      "Step 542319  [5.439 sec/step, loss=0.07156, avg_loss=0.07302]\n",
      "Step 542320  [5.452 sec/step, loss=0.07410, avg_loss=0.07306]\n",
      "Step 542321  [5.480 sec/step, loss=0.07401, avg_loss=0.07309]\n",
      "Step 542322  [5.474 sec/step, loss=0.07327, avg_loss=0.07306]\n",
      "Step 542323  [5.496 sec/step, loss=0.07541, avg_loss=0.07312]\n",
      "Step 542324  [5.499 sec/step, loss=0.07512, avg_loss=0.07311]\n",
      "Step 542325  [5.503 sec/step, loss=0.07427, avg_loss=0.07311]\n",
      "Step 542326  [5.493 sec/step, loss=0.07131, avg_loss=0.07310]\n",
      "Step 542327  [5.483 sec/step, loss=0.07339, avg_loss=0.07310]\n",
      "Step 542328  [5.468 sec/step, loss=0.07496, avg_loss=0.07313]\n",
      "Step 542329  [5.449 sec/step, loss=0.06894, avg_loss=0.07306]\n",
      "Step 542330  [5.428 sec/step, loss=0.06498, avg_loss=0.07298]\n",
      "Step 542331  [5.428 sec/step, loss=0.07483, avg_loss=0.07299]\n",
      "Step 542332  [5.393 sec/step, loss=0.07439, avg_loss=0.07308]\n",
      "Step 542333  [5.396 sec/step, loss=0.07384, avg_loss=0.07309]\n",
      "Step 542334  [5.399 sec/step, loss=0.07530, avg_loss=0.07311]\n",
      "Step 542335  [5.398 sec/step, loss=0.07272, avg_loss=0.07308]\n",
      "Step 542336  [5.392 sec/step, loss=0.07496, avg_loss=0.07308]\n",
      "Step 542337  [5.393 sec/step, loss=0.07241, avg_loss=0.07306]\n",
      "Step 542338  [5.404 sec/step, loss=0.07404, avg_loss=0.07309]\n",
      "Step 542339  [5.393 sec/step, loss=0.07361, avg_loss=0.07307]\n",
      "Step 542340  [5.410 sec/step, loss=0.07540, avg_loss=0.07312]\n",
      "Step 542341  [5.424 sec/step, loss=0.07465, avg_loss=0.07312]\n",
      "Step 542342  [5.429 sec/step, loss=0.07433, avg_loss=0.07313]\n",
      "Generated 32 batches of size 32 in 2.580 sec\n",
      "Step 542343  [5.432 sec/step, loss=0.07093, avg_loss=0.07311]\n",
      "Step 542344  [5.421 sec/step, loss=0.07088, avg_loss=0.07306]\n",
      "Step 542345  [5.403 sec/step, loss=0.07306, avg_loss=0.07304]\n",
      "Step 542346  [5.409 sec/step, loss=0.07399, avg_loss=0.07303]\n",
      "Step 542347  [5.397 sec/step, loss=0.07354, avg_loss=0.07303]\n",
      "Step 542348  [5.377 sec/step, loss=0.07482, avg_loss=0.07305]\n",
      "Step 542349  [5.361 sec/step, loss=0.07432, avg_loss=0.07307]\n",
      "Step 542350  [5.360 sec/step, loss=0.07021, avg_loss=0.07305]\n",
      "Step 542351  [5.430 sec/step, loss=0.06480, avg_loss=0.07304]\n",
      "Step 542352  [5.435 sec/step, loss=0.07506, avg_loss=0.07308]\n",
      "Step 542353  [5.440 sec/step, loss=0.07390, avg_loss=0.07309]\n",
      "Step 542354  [5.450 sec/step, loss=0.07290, avg_loss=0.07309]\n",
      "Step 542355  [5.453 sec/step, loss=0.07288, avg_loss=0.07309]\n",
      "Step 542356  [5.430 sec/step, loss=0.07068, avg_loss=0.07304]\n",
      "Step 542357  [5.420 sec/step, loss=0.07426, avg_loss=0.07304]\n",
      "Step 542358  [5.421 sec/step, loss=0.07174, avg_loss=0.07303]\n",
      "Step 542359  [5.443 sec/step, loss=0.07472, avg_loss=0.07307]\n",
      "Step 542360  [5.446 sec/step, loss=0.07488, avg_loss=0.07311]\n",
      "Step 542361  [5.442 sec/step, loss=0.07389, avg_loss=0.07310]\n",
      "Step 542362  [5.473 sec/step, loss=0.07299, avg_loss=0.07311]\n",
      "Step 542363  [5.475 sec/step, loss=0.07366, avg_loss=0.07309]\n",
      "Step 542364  [5.493 sec/step, loss=0.07303, avg_loss=0.07308]\n",
      "Step 542365  [5.487 sec/step, loss=0.07357, avg_loss=0.07307]\n",
      "Step 542366  [5.473 sec/step, loss=0.07436, avg_loss=0.07308]\n",
      "Step 542367  [5.487 sec/step, loss=0.07582, avg_loss=0.07310]\n",
      "Step 542368  [5.472 sec/step, loss=0.07390, avg_loss=0.07311]\n",
      "Step 542369  [5.460 sec/step, loss=0.07318, avg_loss=0.07312]\n",
      "Step 542370  [5.496 sec/step, loss=0.06570, avg_loss=0.07302]\n",
      "Step 542371  [5.494 sec/step, loss=0.07536, avg_loss=0.07302]\n",
      "Step 542372  [5.498 sec/step, loss=0.07408, avg_loss=0.07302]\n",
      "Step 542373  [5.503 sec/step, loss=0.07104, avg_loss=0.07300]\n",
      "Step 542374  [5.510 sec/step, loss=0.07422, avg_loss=0.07301]\n",
      "Generated 32 batches of size 32 in 2.535 sec\n",
      "Step 542375  [5.510 sec/step, loss=0.07256, avg_loss=0.07297]\n",
      "Step 542376  [5.504 sec/step, loss=0.07425, avg_loss=0.07297]\n",
      "Step 542377  [5.524 sec/step, loss=0.07561, avg_loss=0.07299]\n",
      "Step 542378  [5.499 sec/step, loss=0.07466, avg_loss=0.07301]\n",
      "Step 542379  [5.516 sec/step, loss=0.07525, avg_loss=0.07302]\n",
      "Step 542380  [5.451 sec/step, loss=0.07149, avg_loss=0.07307]\n",
      "Step 542381  [5.445 sec/step, loss=0.06985, avg_loss=0.07305]\n",
      "Step 542382  [5.420 sec/step, loss=0.06425, avg_loss=0.07294]\n",
      "Step 542383  [5.417 sec/step, loss=0.07610, avg_loss=0.07297]\n",
      "Step 542384  [5.411 sec/step, loss=0.06705, avg_loss=0.07293]\n",
      "Step 542385  [5.429 sec/step, loss=0.07494, avg_loss=0.07302]\n",
      "Step 542386  [5.414 sec/step, loss=0.07206, avg_loss=0.07299]\n",
      "Step 542387  [5.425 sec/step, loss=0.07527, avg_loss=0.07302]\n",
      "Step 542388  [5.426 sec/step, loss=0.07419, avg_loss=0.07303]\n",
      "Step 542389  [5.417 sec/step, loss=0.07352, avg_loss=0.07302]\n",
      "Step 542390  [5.418 sec/step, loss=0.07470, avg_loss=0.07303]\n",
      "Step 542391  [5.414 sec/step, loss=0.07536, avg_loss=0.07303]\n",
      "Step 542392  [5.412 sec/step, loss=0.07109, avg_loss=0.07303]\n",
      "Step 542393  [5.404 sec/step, loss=0.07399, avg_loss=0.07302]\n",
      "Step 542394  [5.377 sec/step, loss=0.07349, avg_loss=0.07303]\n",
      "Step 542395  [5.409 sec/step, loss=0.07489, avg_loss=0.07313]\n",
      "Step 542396  [5.416 sec/step, loss=0.07408, avg_loss=0.07312]\n",
      "Step 542397  [5.412 sec/step, loss=0.07327, avg_loss=0.07312]\n",
      "Step 542398  [5.414 sec/step, loss=0.07436, avg_loss=0.07315]\n",
      "Step 542399  [5.446 sec/step, loss=0.07461, avg_loss=0.07318]\n",
      "Step 542400  [5.448 sec/step, loss=0.07384, avg_loss=0.07317]\n",
      "Writing summary at step: 542400\n",
      "Step 542401  [5.425 sec/step, loss=0.07294, avg_loss=0.07318]\n",
      "Step 542402  [5.407 sec/step, loss=0.07167, avg_loss=0.07316]\n",
      "Step 542403  [5.406 sec/step, loss=0.07379, avg_loss=0.07314]\n",
      "Step 542404  [5.402 sec/step, loss=0.07299, avg_loss=0.07316]\n",
      "Step 542405  [5.411 sec/step, loss=0.07508, avg_loss=0.07316]\n",
      "Generated 32 batches of size 32 in 2.628 sec\n",
      "Step 542406  [5.474 sec/step, loss=0.06524, avg_loss=0.07308]\n",
      "Step 542407  [5.461 sec/step, loss=0.07441, avg_loss=0.07307]\n",
      "Step 542408  [5.489 sec/step, loss=0.07194, avg_loss=0.07306]\n",
      "Step 542409  [5.489 sec/step, loss=0.07514, avg_loss=0.07306]\n",
      "Step 542410  [5.472 sec/step, loss=0.06953, avg_loss=0.07300]\n",
      "Step 542411  [5.482 sec/step, loss=0.07525, avg_loss=0.07303]\n",
      "Step 542412  [5.432 sec/step, loss=0.07396, avg_loss=0.07311]\n",
      "Step 542413  [5.432 sec/step, loss=0.07324, avg_loss=0.07312]\n",
      "Step 542414  [5.436 sec/step, loss=0.07546, avg_loss=0.07313]\n",
      "Step 542415  [5.427 sec/step, loss=0.07180, avg_loss=0.07309]\n",
      "Step 542416  [5.424 sec/step, loss=0.07134, avg_loss=0.07309]\n",
      "Step 542417  [5.425 sec/step, loss=0.07347, avg_loss=0.07310]\n",
      "Step 542418  [5.439 sec/step, loss=0.07422, avg_loss=0.07310]\n",
      "Step 542419  [5.437 sec/step, loss=0.07246, avg_loss=0.07311]\n",
      "Step 542420  [5.428 sec/step, loss=0.07011, avg_loss=0.07307]\n",
      "Step 542421  [5.406 sec/step, loss=0.07161, avg_loss=0.07305]\n",
      "Step 542422  [5.400 sec/step, loss=0.07077, avg_loss=0.07302]\n",
      "Step 542423  [5.410 sec/step, loss=0.07473, avg_loss=0.07302]\n",
      "Step 542424  [5.381 sec/step, loss=0.06492, avg_loss=0.07292]\n",
      "Step 542425  [5.368 sec/step, loss=0.07087, avg_loss=0.07288]\n",
      "Step 542426  [5.388 sec/step, loss=0.07533, avg_loss=0.07292]\n",
      "Step 542427  [5.391 sec/step, loss=0.07300, avg_loss=0.07292]\n",
      "Step 542428  [5.376 sec/step, loss=0.07261, avg_loss=0.07289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 542429  [5.401 sec/step, loss=0.07448, avg_loss=0.07295]\n",
      "Step 542430  [5.427 sec/step, loss=0.07491, avg_loss=0.07305]\n",
      "Step 542431  [5.435 sec/step, loss=0.07612, avg_loss=0.07306]\n",
      "Step 542432  [5.470 sec/step, loss=0.06590, avg_loss=0.07298]\n",
      "Step 542433  [5.464 sec/step, loss=0.07409, avg_loss=0.07298]\n",
      "Step 542434  [5.459 sec/step, loss=0.07534, avg_loss=0.07298]\n",
      "Step 542435  [5.479 sec/step, loss=0.07252, avg_loss=0.07298]\n",
      "Step 542436  [5.480 sec/step, loss=0.07574, avg_loss=0.07299]\n",
      "Step 542437  [5.489 sec/step, loss=0.07472, avg_loss=0.07301]\n",
      "Generated 32 batches of size 32 in 2.547 sec\n",
      "Step 542438  [5.488 sec/step, loss=0.07333, avg_loss=0.07300]\n",
      "Step 542439  [5.489 sec/step, loss=0.07206, avg_loss=0.07299]\n",
      "Step 542440  [5.475 sec/step, loss=0.06972, avg_loss=0.07293]\n",
      "Step 542441  [5.458 sec/step, loss=0.07464, avg_loss=0.07293]\n",
      "Step 542442  [5.462 sec/step, loss=0.07460, avg_loss=0.07293]\n",
      "Step 542443  [5.472 sec/step, loss=0.07384, avg_loss=0.07296]\n",
      "Step 542444  [5.478 sec/step, loss=0.07415, avg_loss=0.07299]\n",
      "Step 542445  [5.482 sec/step, loss=0.07340, avg_loss=0.07300]\n",
      "Step 542446  [5.470 sec/step, loss=0.07447, avg_loss=0.07300]\n",
      "Step 542447  [5.470 sec/step, loss=0.07171, avg_loss=0.07298]\n",
      "Step 542448  [5.483 sec/step, loss=0.07571, avg_loss=0.07299]\n",
      "Step 542449  [5.466 sec/step, loss=0.06967, avg_loss=0.07295]\n",
      "Step 542450  [5.485 sec/step, loss=0.07411, avg_loss=0.07298]\n",
      "Step 542451  [5.426 sec/step, loss=0.07355, avg_loss=0.07307]\n",
      "Step 542452  [5.447 sec/step, loss=0.07229, avg_loss=0.07304]\n",
      "Step 542453  [5.495 sec/step, loss=0.06574, avg_loss=0.07296]\n",
      "Step 542454  [5.473 sec/step, loss=0.06600, avg_loss=0.07289]\n",
      "Step 542455  [5.478 sec/step, loss=0.07472, avg_loss=0.07291]\n",
      "Step 542456  [5.489 sec/step, loss=0.07376, avg_loss=0.07294]\n",
      "Step 542457  [5.504 sec/step, loss=0.07594, avg_loss=0.07296]\n",
      "Step 542458  [5.506 sec/step, loss=0.07315, avg_loss=0.07297]\n",
      "Step 542459  [5.500 sec/step, loss=0.07475, avg_loss=0.07297]\n",
      "Step 542460  [5.494 sec/step, loss=0.07116, avg_loss=0.07294]\n",
      "Step 542461  [5.508 sec/step, loss=0.07365, avg_loss=0.07293]\n",
      "Step 542462  [5.488 sec/step, loss=0.07547, avg_loss=0.07296]\n",
      "Step 542463  [5.499 sec/step, loss=0.07318, avg_loss=0.07295]\n",
      "Step 542464  [5.488 sec/step, loss=0.07588, avg_loss=0.07298]\n",
      "Step 542465  [5.502 sec/step, loss=0.07295, avg_loss=0.07298]\n",
      "Step 542466  [5.502 sec/step, loss=0.07293, avg_loss=0.07296]\n",
      "Step 542467  [5.473 sec/step, loss=0.07041, avg_loss=0.07291]\n",
      "Step 542468  [5.466 sec/step, loss=0.07447, avg_loss=0.07291]\n",
      "Step 542469  [5.473 sec/step, loss=0.07396, avg_loss=0.07292]\n",
      "Generated 32 batches of size 32 in 2.322 sec\n",
      "Step 542470  [5.429 sec/step, loss=0.07402, avg_loss=0.07301]\n",
      "Step 542471  [5.429 sec/step, loss=0.07527, avg_loss=0.07300]\n",
      "Step 542472  [5.416 sec/step, loss=0.07165, avg_loss=0.07298]\n",
      "Step 542473  [5.419 sec/step, loss=0.07375, avg_loss=0.07301]\n",
      "Step 542474  [5.418 sec/step, loss=0.07409, avg_loss=0.07301]\n",
      "Step 542475  [5.426 sec/step, loss=0.07589, avg_loss=0.07304]\n",
      "Step 542476  [5.418 sec/step, loss=0.07340, avg_loss=0.07303]\n",
      "Step 542477  [5.414 sec/step, loss=0.07431, avg_loss=0.07302]\n",
      "Step 542478  [5.423 sec/step, loss=0.07579, avg_loss=0.07303]\n",
      "Step 542479  [5.420 sec/step, loss=0.07298, avg_loss=0.07301]\n",
      "Step 542480  [5.435 sec/step, loss=0.07455, avg_loss=0.07304]\n",
      "Step 542481  [5.457 sec/step, loss=0.07574, avg_loss=0.07310]\n",
      "Step 542482  [5.463 sec/step, loss=0.07301, avg_loss=0.07318]\n",
      "Step 542483  [5.452 sec/step, loss=0.07378, avg_loss=0.07316]\n",
      "Step 542484  [5.470 sec/step, loss=0.07535, avg_loss=0.07324]\n",
      "Step 542485  [5.465 sec/step, loss=0.07086, avg_loss=0.07320]\n",
      "Step 542486  [5.487 sec/step, loss=0.07520, avg_loss=0.07323]\n",
      "Step 542487  [5.466 sec/step, loss=0.07332, avg_loss=0.07321]\n",
      "Step 542488  [5.472 sec/step, loss=0.07558, avg_loss=0.07323]\n",
      "Step 542489  [5.529 sec/step, loss=0.06533, avg_loss=0.07315]\n",
      "Step 542490  [5.523 sec/step, loss=0.07201, avg_loss=0.07312]\n",
      "Step 542491  [5.529 sec/step, loss=0.07524, avg_loss=0.07312]\n",
      "Step 542492  [5.520 sec/step, loss=0.07015, avg_loss=0.07311]\n",
      "Step 542493  [5.502 sec/step, loss=0.06451, avg_loss=0.07301]\n",
      "Step 542494  [5.498 sec/step, loss=0.07470, avg_loss=0.07303]\n",
      "Step 542495  [5.481 sec/step, loss=0.07340, avg_loss=0.07301]\n",
      "Step 542496  [5.480 sec/step, loss=0.07359, avg_loss=0.07301]\n",
      "Step 542497  [5.504 sec/step, loss=0.07235, avg_loss=0.07300]\n",
      "Step 542498  [5.501 sec/step, loss=0.07434, avg_loss=0.07300]\n",
      "Step 542499  [5.493 sec/step, loss=0.07578, avg_loss=0.07301]\n",
      "Step 542500  [5.491 sec/step, loss=0.07482, avg_loss=0.07302]\n",
      "Writing summary at step: 542500\n",
      "Generated 32 batches of size 32 in 2.387 sec\n",
      "Step 542501  [5.503 sec/step, loss=0.07376, avg_loss=0.07303]\n",
      "Step 542502  [5.524 sec/step, loss=0.07346, avg_loss=0.07304]\n",
      "Step 542503  [5.523 sec/step, loss=0.07457, avg_loss=0.07305]\n",
      "Step 542504  [5.512 sec/step, loss=0.07169, avg_loss=0.07304]\n",
      "Step 542505  [5.501 sec/step, loss=0.07332, avg_loss=0.07302]\n",
      "Step 542506  [5.455 sec/step, loss=0.07440, avg_loss=0.07311]\n",
      "Step 542507  [5.450 sec/step, loss=0.07252, avg_loss=0.07309]\n",
      "Step 542508  [5.446 sec/step, loss=0.07290, avg_loss=0.07310]\n",
      "Step 542509  [5.430 sec/step, loss=0.07460, avg_loss=0.07310]\n",
      "Step 542510  [5.439 sec/step, loss=0.07489, avg_loss=0.07315]\n",
      "Step 542511  [5.412 sec/step, loss=0.06713, avg_loss=0.07307]\n",
      "Step 542512  [5.420 sec/step, loss=0.07469, avg_loss=0.07308]\n",
      "Step 542513  [5.437 sec/step, loss=0.07284, avg_loss=0.07307]\n",
      "Step 542514  [5.432 sec/step, loss=0.07434, avg_loss=0.07306]\n",
      "Step 542515  [5.425 sec/step, loss=0.07316, avg_loss=0.07308]\n",
      "Step 542516  [5.432 sec/step, loss=0.07324, avg_loss=0.07310]\n",
      "Step 542517  [5.469 sec/step, loss=0.06536, avg_loss=0.07301]\n",
      "Step 542518  [5.462 sec/step, loss=0.07305, avg_loss=0.07300]\n",
      "Step 542519  [5.473 sec/step, loss=0.07499, avg_loss=0.07303]\n",
      "Step 542520  [5.470 sec/step, loss=0.07152, avg_loss=0.07304]\n",
      "Step 542521  [5.493 sec/step, loss=0.07215, avg_loss=0.07305]\n",
      "Step 542522  [5.492 sec/step, loss=0.07110, avg_loss=0.07305]\n",
      "Step 542523  [5.484 sec/step, loss=0.07585, avg_loss=0.07306]\n",
      "Step 542524  [5.512 sec/step, loss=0.07549, avg_loss=0.07317]\n",
      "Step 542525  [5.521 sec/step, loss=0.07439, avg_loss=0.07320]\n",
      "Step 542526  [5.527 sec/step, loss=0.07395, avg_loss=0.07319]\n",
      "Step 542527  [5.535 sec/step, loss=0.07137, avg_loss=0.07317]\n",
      "Step 542528  [5.532 sec/step, loss=0.07299, avg_loss=0.07318]\n",
      "Step 542529  [5.518 sec/step, loss=0.07383, avg_loss=0.07317]\n",
      "Step 542530  [5.494 sec/step, loss=0.07094, avg_loss=0.07313]\n",
      "Step 542531  [5.501 sec/step, loss=0.07538, avg_loss=0.07312]\n",
      "Step 542532  [5.456 sec/step, loss=0.07386, avg_loss=0.07320]\n",
      "Generated 32 batches of size 32 in 2.378 sec\n",
      "Step 542533  [5.467 sec/step, loss=0.07434, avg_loss=0.07320]\n",
      "Step 542534  [5.456 sec/step, loss=0.07465, avg_loss=0.07320]\n",
      "Step 542535  [5.419 sec/step, loss=0.07320, avg_loss=0.07320]\n",
      "Step 542536  [5.420 sec/step, loss=0.07434, avg_loss=0.07319]\n",
      "Step 542537  [5.414 sec/step, loss=0.07389, avg_loss=0.07318]\n",
      "Step 542538  [5.416 sec/step, loss=0.07449, avg_loss=0.07319]\n",
      "Step 542539  [5.410 sec/step, loss=0.07260, avg_loss=0.07320]\n",
      "Step 542540  [5.429 sec/step, loss=0.07502, avg_loss=0.07325]\n",
      "Step 542541  [5.439 sec/step, loss=0.07546, avg_loss=0.07326]\n",
      "Step 542542  [5.465 sec/step, loss=0.07253, avg_loss=0.07324]\n",
      "Step 542543  [5.445 sec/step, loss=0.07151, avg_loss=0.07322]\n",
      "Step 542544  [5.434 sec/step, loss=0.07279, avg_loss=0.07320]\n",
      "Step 542545  [5.428 sec/step, loss=0.07247, avg_loss=0.07319]\n",
      "Step 542546  [5.433 sec/step, loss=0.07405, avg_loss=0.07319]\n",
      "Step 542547  [5.424 sec/step, loss=0.06593, avg_loss=0.07313]\n",
      "Step 542548  [5.417 sec/step, loss=0.07540, avg_loss=0.07313]\n",
      "Step 542549  [5.438 sec/step, loss=0.07433, avg_loss=0.07318]\n",
      "Step 542550  [5.432 sec/step, loss=0.07354, avg_loss=0.07317]\n",
      "Step 542551  [5.451 sec/step, loss=0.07556, avg_loss=0.07319]\n",
      "Step 542552  [5.420 sec/step, loss=0.07326, avg_loss=0.07320]\n",
      "Step 542553  [5.361 sec/step, loss=0.07028, avg_loss=0.07324]\n",
      "Step 542554  [5.376 sec/step, loss=0.07330, avg_loss=0.07332]\n",
      "Step 542555  [5.386 sec/step, loss=0.07312, avg_loss=0.07330]\n",
      "Step 542556  [5.393 sec/step, loss=0.07352, avg_loss=0.07330]\n",
      "Step 542557  [5.396 sec/step, loss=0.07480, avg_loss=0.07329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 542558  [5.414 sec/step, loss=0.07531, avg_loss=0.07331]\n",
      "Step 542559  [5.421 sec/step, loss=0.07538, avg_loss=0.07332]\n",
      "Step 542560  [5.429 sec/step, loss=0.07475, avg_loss=0.07335]\n",
      "Step 542561  [5.404 sec/step, loss=0.07118, avg_loss=0.07333]\n",
      "Step 542562  [5.399 sec/step, loss=0.07150, avg_loss=0.07329]\n",
      "Step 542563  [5.389 sec/step, loss=0.07267, avg_loss=0.07328]\n",
      "Step 542564  [5.377 sec/step, loss=0.07146, avg_loss=0.07324]\n",
      "Generated 32 batches of size 32 in 2.555 sec\n",
      "Step 542565  [5.374 sec/step, loss=0.07451, avg_loss=0.07325]\n",
      "Step 542566  [5.429 sec/step, loss=0.06550, avg_loss=0.07318]\n",
      "Step 542567  [5.450 sec/step, loss=0.07434, avg_loss=0.07322]\n",
      "Step 542568  [5.451 sec/step, loss=0.07445, avg_loss=0.07322]\n",
      "Step 542569  [5.456 sec/step, loss=0.07425, avg_loss=0.07322]\n",
      "Step 542570  [5.442 sec/step, loss=0.07335, avg_loss=0.07321]\n",
      "Step 542571  [5.433 sec/step, loss=0.07508, avg_loss=0.07321]\n",
      "Step 542572  [5.443 sec/step, loss=0.07517, avg_loss=0.07325]\n",
      "Step 542573  [5.458 sec/step, loss=0.07535, avg_loss=0.07326]\n",
      "Step 542574  [5.454 sec/step, loss=0.07402, avg_loss=0.07326]\n",
      "Step 542575  [5.437 sec/step, loss=0.06993, avg_loss=0.07320]\n",
      "Step 542576  [5.444 sec/step, loss=0.07239, avg_loss=0.07319]\n",
      "Step 542577  [5.433 sec/step, loss=0.07330, avg_loss=0.07318]\n",
      "Step 542578  [5.409 sec/step, loss=0.06862, avg_loss=0.07311]\n",
      "Step 542579  [5.400 sec/step, loss=0.07458, avg_loss=0.07313]\n",
      "Step 542580  [5.420 sec/step, loss=0.07251, avg_loss=0.07311]\n",
      "Step 542581  [5.392 sec/step, loss=0.07057, avg_loss=0.07306]\n",
      "Step 542582  [5.417 sec/step, loss=0.07543, avg_loss=0.07308]\n",
      "Step 542583  [5.435 sec/step, loss=0.07470, avg_loss=0.07309]\n",
      "Step 542584  [5.442 sec/step, loss=0.07402, avg_loss=0.07308]\n",
      "Step 542585  [5.447 sec/step, loss=0.07414, avg_loss=0.07311]\n",
      "Step 542586  [5.439 sec/step, loss=0.07392, avg_loss=0.07310]\n",
      "Step 542587  [5.444 sec/step, loss=0.07263, avg_loss=0.07309]\n",
      "Step 542588  [5.446 sec/step, loss=0.07497, avg_loss=0.07308]\n",
      "Step 542589  [5.405 sec/step, loss=0.07551, avg_loss=0.07318]\n",
      "Step 542590  [5.414 sec/step, loss=0.07358, avg_loss=0.07320]\n",
      "Step 542591  [5.450 sec/step, loss=0.06625, avg_loss=0.07311]\n",
      "Step 542592  [5.450 sec/step, loss=0.07364, avg_loss=0.07315]\n",
      "Step 542593  [5.452 sec/step, loss=0.06442, avg_loss=0.07314]\n",
      "Step 542594  [5.460 sec/step, loss=0.07418, avg_loss=0.07314]\n",
      "Step 542595  [5.469 sec/step, loss=0.07524, avg_loss=0.07316]\n",
      "Step 542596  [5.471 sec/step, loss=0.07484, avg_loss=0.07317]\n",
      "Generated 32 batches of size 32 in 2.604 sec\n",
      "Step 542597  [5.448 sec/step, loss=0.07436, avg_loss=0.07319]\n",
      "Step 542598  [5.439 sec/step, loss=0.07204, avg_loss=0.07317]\n",
      "Step 542599  [5.428 sec/step, loss=0.07272, avg_loss=0.07314]\n",
      "Step 542600  [5.426 sec/step, loss=0.07385, avg_loss=0.07313]\n",
      "Writing summary at step: 542600\n",
      "Step 542601  [5.429 sec/step, loss=0.07517, avg_loss=0.07314]\n",
      "Step 542602  [5.426 sec/step, loss=0.07546, avg_loss=0.07316]\n",
      "Step 542603  [5.421 sec/step, loss=0.07441, avg_loss=0.07316]\n",
      "Step 542604  [5.446 sec/step, loss=0.07540, avg_loss=0.07320]\n",
      "Step 542605  [5.440 sec/step, loss=0.07159, avg_loss=0.07318]\n",
      "Step 542606  [5.447 sec/step, loss=0.07466, avg_loss=0.07318]\n",
      "Step 542607  [5.506 sec/step, loss=0.06472, avg_loss=0.07310]\n",
      "Step 542608  [5.497 sec/step, loss=0.07451, avg_loss=0.07312]\n",
      "Step 542609  [5.496 sec/step, loss=0.07030, avg_loss=0.07308]\n",
      "Step 542610  [5.497 sec/step, loss=0.07472, avg_loss=0.07308]\n",
      "Step 542611  [5.504 sec/step, loss=0.07332, avg_loss=0.07314]\n",
      "Step 542612  [5.506 sec/step, loss=0.07368, avg_loss=0.07313]\n",
      "Step 542613  [5.511 sec/step, loss=0.07474, avg_loss=0.07315]\n",
      "Step 542614  [5.505 sec/step, loss=0.07410, avg_loss=0.07314]\n",
      "Step 542615  [5.502 sec/step, loss=0.07314, avg_loss=0.07314]\n",
      "Step 542616  [5.514 sec/step, loss=0.07450, avg_loss=0.07316]\n",
      "Step 542617  [5.468 sec/step, loss=0.07329, avg_loss=0.07324]\n",
      "Step 542618  [5.468 sec/step, loss=0.07452, avg_loss=0.07325]\n",
      "Step 542619  [5.465 sec/step, loss=0.07339, avg_loss=0.07323]\n",
      "Step 542620  [5.478 sec/step, loss=0.07438, avg_loss=0.07326]\n",
      "Step 542621  [5.441 sec/step, loss=0.07134, avg_loss=0.07325]\n",
      "Step 542622  [5.453 sec/step, loss=0.07569, avg_loss=0.07330]\n",
      "Step 542623  [5.446 sec/step, loss=0.07423, avg_loss=0.07328]\n",
      "Step 542624  [5.441 sec/step, loss=0.07361, avg_loss=0.07327]\n",
      "Step 542625  [5.450 sec/step, loss=0.07578, avg_loss=0.07328]\n",
      "Step 542626  [5.439 sec/step, loss=0.07547, avg_loss=0.07329]\n",
      "Step 542627  [5.439 sec/step, loss=0.07315, avg_loss=0.07331]\n",
      "Generated 32 batches of size 32 in 2.558 sec\n",
      "Step 542628  [5.443 sec/step, loss=0.07279, avg_loss=0.07331]\n",
      "Step 542629  [5.429 sec/step, loss=0.07170, avg_loss=0.07329]\n",
      "Step 542630  [5.469 sec/step, loss=0.07201, avg_loss=0.07330]\n",
      "Step 542631  [5.435 sec/step, loss=0.06497, avg_loss=0.07320]\n",
      "Step 542632  [5.442 sec/step, loss=0.07534, avg_loss=0.07321]\n",
      "Step 542633  [5.431 sec/step, loss=0.07283, avg_loss=0.07320]\n",
      "Step 542634  [5.426 sec/step, loss=0.07287, avg_loss=0.07318]\n",
      "Step 542635  [5.448 sec/step, loss=0.07519, avg_loss=0.07320]\n",
      "Step 542636  [5.439 sec/step, loss=0.07442, avg_loss=0.07320]\n",
      "Step 542637  [5.433 sec/step, loss=0.07309, avg_loss=0.07319]\n",
      "Step 542638  [5.438 sec/step, loss=0.07534, avg_loss=0.07320]\n",
      "Step 542639  [5.450 sec/step, loss=0.07481, avg_loss=0.07322]\n",
      "Step 542640  [5.435 sec/step, loss=0.07376, avg_loss=0.07321]\n",
      "Step 542641  [5.438 sec/step, loss=0.07535, avg_loss=0.07321]\n",
      "Step 542642  [5.410 sec/step, loss=0.07356, avg_loss=0.07322]\n",
      "Step 542643  [5.431 sec/step, loss=0.07327, avg_loss=0.07324]\n",
      "Step 542644  [5.435 sec/step, loss=0.07428, avg_loss=0.07325]\n",
      "Step 542645  [5.448 sec/step, loss=0.07545, avg_loss=0.07328]\n",
      "Step 542646  [5.455 sec/step, loss=0.07284, avg_loss=0.07327]\n",
      "Step 542647  [5.473 sec/step, loss=0.07489, avg_loss=0.07336]\n",
      "Step 542648  [5.475 sec/step, loss=0.07381, avg_loss=0.07334]\n",
      "Step 542649  [5.462 sec/step, loss=0.07182, avg_loss=0.07332]\n",
      "Step 542650  [5.456 sec/step, loss=0.06992, avg_loss=0.07328]\n",
      "Step 542651  [5.429 sec/step, loss=0.06528, avg_loss=0.07318]\n",
      "Step 542652  [5.443 sec/step, loss=0.07484, avg_loss=0.07319]\n",
      "Step 542653  [5.474 sec/step, loss=0.07271, avg_loss=0.07322]\n",
      "Step 542654  [5.525 sec/step, loss=0.06560, avg_loss=0.07314]\n",
      "Step 542655  [5.504 sec/step, loss=0.07097, avg_loss=0.07312]\n",
      "Step 542656  [5.513 sec/step, loss=0.07600, avg_loss=0.07314]\n",
      "Step 542657  [5.483 sec/step, loss=0.07138, avg_loss=0.07311]\n",
      "Step 542658  [5.473 sec/step, loss=0.07304, avg_loss=0.07309]\n",
      "Step 542659  [5.457 sec/step, loss=0.07328, avg_loss=0.07307]\n",
      "Generated 32 batches of size 32 in 2.395 sec\n",
      "Step 542660  [5.467 sec/step, loss=0.07480, avg_loss=0.07307]\n",
      "Step 542661  [5.481 sec/step, loss=0.07110, avg_loss=0.07307]\n",
      "Step 542662  [5.507 sec/step, loss=0.07247, avg_loss=0.07308]\n",
      "Step 542663  [5.490 sec/step, loss=0.07313, avg_loss=0.07308]\n",
      "Step 542664  [5.486 sec/step, loss=0.07323, avg_loss=0.07310]\n",
      "Step 542665  [5.487 sec/step, loss=0.07477, avg_loss=0.07310]\n",
      "Step 542666  [5.449 sec/step, loss=0.07387, avg_loss=0.07318]\n",
      "Step 542667  [5.448 sec/step, loss=0.07400, avg_loss=0.07318]\n",
      "Step 542668  [5.451 sec/step, loss=0.07346, avg_loss=0.07317]\n",
      "Step 542669  [5.457 sec/step, loss=0.07526, avg_loss=0.07318]\n",
      "Step 542670  [5.461 sec/step, loss=0.07180, avg_loss=0.07317]\n",
      "Step 542671  [5.451 sec/step, loss=0.07320, avg_loss=0.07315]\n",
      "Step 542672  [5.435 sec/step, loss=0.06976, avg_loss=0.07309]\n",
      "Step 542673  [5.423 sec/step, loss=0.07302, avg_loss=0.07307]\n",
      "Step 542674  [5.448 sec/step, loss=0.07219, avg_loss=0.07305]\n",
      "Step 542675  [5.452 sec/step, loss=0.07492, avg_loss=0.07310]\n",
      "Step 542676  [5.505 sec/step, loss=0.06558, avg_loss=0.07303]\n",
      "Step 542677  [5.510 sec/step, loss=0.07385, avg_loss=0.07304]\n",
      "Step 542678  [5.533 sec/step, loss=0.07520, avg_loss=0.07310]\n",
      "Step 542679  [5.527 sec/step, loss=0.07279, avg_loss=0.07309]\n",
      "Step 542680  [5.510 sec/step, loss=0.07519, avg_loss=0.07311]\n",
      "Step 542681  [5.531 sec/step, loss=0.07362, avg_loss=0.07314]\n",
      "Step 542682  [5.531 sec/step, loss=0.07544, avg_loss=0.07314]\n",
      "Step 542683  [5.519 sec/step, loss=0.07482, avg_loss=0.07314]\n",
      "Step 542684  [5.493 sec/step, loss=0.06501, avg_loss=0.07305]\n",
      "Step 542685  [5.499 sec/step, loss=0.07576, avg_loss=0.07307]\n",
      "Step 542686  [5.503 sec/step, loss=0.07222, avg_loss=0.07305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 542687  [5.496 sec/step, loss=0.07335, avg_loss=0.07306]\n",
      "Step 542688  [5.482 sec/step, loss=0.07158, avg_loss=0.07303]\n",
      "Step 542689  [5.471 sec/step, loss=0.07201, avg_loss=0.07299]\n",
      "Step 542690  [5.474 sec/step, loss=0.07224, avg_loss=0.07298]\n",
      "Step 542691  [5.431 sec/step, loss=0.07527, avg_loss=0.07307]\n",
      "Generated 32 batches of size 32 in 2.437 sec\n",
      "Step 542692  [5.464 sec/step, loss=0.07497, avg_loss=0.07308]\n",
      "Step 542693  [5.473 sec/step, loss=0.07270, avg_loss=0.07316]\n",
      "Step 542694  [5.465 sec/step, loss=0.07441, avg_loss=0.07317]\n",
      "Step 542695  [5.442 sec/step, loss=0.07002, avg_loss=0.07311]\n",
      "Step 542696  [5.451 sec/step, loss=0.07461, avg_loss=0.07311]\n",
      "Step 542697  [5.452 sec/step, loss=0.07486, avg_loss=0.07312]\n",
      "Step 542698  [5.463 sec/step, loss=0.07406, avg_loss=0.07314]\n",
      "Step 542699  [5.462 sec/step, loss=0.07460, avg_loss=0.07316]\n",
      "Step 542700  [5.461 sec/step, loss=0.07355, avg_loss=0.07315]\n",
      "Writing summary at step: 542700\n",
      "Step 542701  [5.500 sec/step, loss=0.06570, avg_loss=0.07306]\n",
      "Step 542702  [5.477 sec/step, loss=0.07061, avg_loss=0.07301]\n",
      "Step 542703  [5.490 sec/step, loss=0.07523, avg_loss=0.07302]\n",
      "Step 542704  [5.473 sec/step, loss=0.07307, avg_loss=0.07300]\n",
      "Step 542705  [5.483 sec/step, loss=0.07165, avg_loss=0.07300]\n",
      "Step 542706  [5.494 sec/step, loss=0.07189, avg_loss=0.07297]\n",
      "Step 542707  [5.425 sec/step, loss=0.06406, avg_loss=0.07296]\n",
      "Step 542708  [5.402 sec/step, loss=0.06975, avg_loss=0.07291]\n",
      "Step 542709  [5.409 sec/step, loss=0.07403, avg_loss=0.07295]\n",
      "Step 542710  [5.413 sec/step, loss=0.07457, avg_loss=0.07295]\n",
      "Step 542711  [5.426 sec/step, loss=0.07461, avg_loss=0.07296]\n",
      "Step 542712  [5.414 sec/step, loss=0.07105, avg_loss=0.07294]\n",
      "Step 542713  [5.399 sec/step, loss=0.07306, avg_loss=0.07292]\n",
      "Step 542714  [5.394 sec/step, loss=0.07284, avg_loss=0.07291]\n",
      "Step 542715  [5.395 sec/step, loss=0.07506, avg_loss=0.07293]\n",
      "Step 542716  [5.403 sec/step, loss=0.07357, avg_loss=0.07292]\n",
      "Step 542717  [5.400 sec/step, loss=0.07488, avg_loss=0.07293]\n",
      "Step 542718  [5.396 sec/step, loss=0.07380, avg_loss=0.07293]\n",
      "Step 542719  [5.397 sec/step, loss=0.07552, avg_loss=0.07295]\n",
      "Step 542720  [5.403 sec/step, loss=0.07518, avg_loss=0.07295]\n",
      "Step 542721  [5.422 sec/step, loss=0.07546, avg_loss=0.07300]\n",
      "Step 542722  [5.424 sec/step, loss=0.07497, avg_loss=0.07299]\n",
      "Generated 32 batches of size 32 in 2.443 sec\n",
      "Step 542723  [5.428 sec/step, loss=0.07133, avg_loss=0.07296]\n",
      "Step 542724  [5.428 sec/step, loss=0.07195, avg_loss=0.07294]\n",
      "Step 542725  [5.437 sec/step, loss=0.07558, avg_loss=0.07294]\n",
      "Step 542726  [5.426 sec/step, loss=0.07263, avg_loss=0.07291]\n",
      "Step 542727  [5.415 sec/step, loss=0.07060, avg_loss=0.07289]\n",
      "Step 542728  [5.427 sec/step, loss=0.07486, avg_loss=0.07291]\n",
      "Step 542729  [5.438 sec/step, loss=0.07262, avg_loss=0.07292]\n",
      "Step 542730  [5.430 sec/step, loss=0.07547, avg_loss=0.07295]\n",
      "Step 542731  [5.459 sec/step, loss=0.07396, avg_loss=0.07304]\n",
      "Step 542732  [5.458 sec/step, loss=0.07264, avg_loss=0.07301]\n",
      "Step 542733  [5.467 sec/step, loss=0.07536, avg_loss=0.07304]\n",
      "Step 542734  [5.464 sec/step, loss=0.07328, avg_loss=0.07304]\n",
      "Step 542735  [5.465 sec/step, loss=0.07510, avg_loss=0.07304]\n",
      "Step 542736  [5.454 sec/step, loss=0.07069, avg_loss=0.07301]\n",
      "Step 542737  [5.463 sec/step, loss=0.07468, avg_loss=0.07302]\n",
      "Step 542738  [5.458 sec/step, loss=0.07300, avg_loss=0.07300]\n",
      "Step 542739  [5.436 sec/step, loss=0.07143, avg_loss=0.07296]\n",
      "Step 542740  [5.429 sec/step, loss=0.07238, avg_loss=0.07295]\n",
      "Step 542741  [5.397 sec/step, loss=0.06670, avg_loss=0.07286]\n",
      "Step 542742  [5.393 sec/step, loss=0.07220, avg_loss=0.07285]\n",
      "Step 542743  [5.395 sec/step, loss=0.07207, avg_loss=0.07284]\n",
      "Step 542744  [5.395 sec/step, loss=0.07138, avg_loss=0.07281]\n",
      "Step 542745  [5.392 sec/step, loss=0.07248, avg_loss=0.07278]\n",
      "Step 542746  [5.387 sec/step, loss=0.07359, avg_loss=0.07279]\n",
      "Step 542747  [5.393 sec/step, loss=0.07328, avg_loss=0.07277]\n",
      "Step 542748  [5.390 sec/step, loss=0.07512, avg_loss=0.07278]\n",
      "Step 542749  [5.394 sec/step, loss=0.07457, avg_loss=0.07281]\n",
      "Step 542750  [5.399 sec/step, loss=0.07445, avg_loss=0.07286]\n",
      "Step 542751  [5.428 sec/step, loss=0.07506, avg_loss=0.07296]\n",
      "Step 542752  [5.428 sec/step, loss=0.07553, avg_loss=0.07296]\n",
      "Step 542753  [5.404 sec/step, loss=0.07361, avg_loss=0.07297]\n",
      "Step 542754  [5.350 sec/step, loss=0.07398, avg_loss=0.07306]\n",
      "Generated 32 batches of size 32 in 2.410 sec\n",
      "Step 542755  [5.392 sec/step, loss=0.07254, avg_loss=0.07307]\n",
      "Step 542756  [5.379 sec/step, loss=0.07390, avg_loss=0.07305]\n",
      "Step 542757  [5.405 sec/step, loss=0.07566, avg_loss=0.07309]\n",
      "Step 542758  [5.451 sec/step, loss=0.06588, avg_loss=0.07302]\n",
      "Step 542759  [5.454 sec/step, loss=0.07358, avg_loss=0.07302]\n",
      "Step 542760  [5.439 sec/step, loss=0.07176, avg_loss=0.07299]\n",
      "Step 542761  [5.431 sec/step, loss=0.07317, avg_loss=0.07301]\n",
      "Step 542762  [5.425 sec/step, loss=0.07140, avg_loss=0.07300]\n",
      "Step 542763  [5.439 sec/step, loss=0.07567, avg_loss=0.07303]\n",
      "Step 542764  [5.451 sec/step, loss=0.07381, avg_loss=0.07304]\n",
      "Step 542765  [5.458 sec/step, loss=0.07582, avg_loss=0.07305]\n",
      "Step 542766  [5.431 sec/step, loss=0.07009, avg_loss=0.07301]\n",
      "Step 542767  [5.423 sec/step, loss=0.07458, avg_loss=0.07301]\n",
      "Step 542768  [5.418 sec/step, loss=0.06980, avg_loss=0.07298]\n",
      "Step 542769  [5.413 sec/step, loss=0.07403, avg_loss=0.07296]\n",
      "Step 542770  [5.413 sec/step, loss=0.07292, avg_loss=0.07298]\n",
      "Step 542771  [5.420 sec/step, loss=0.07389, avg_loss=0.07298]\n",
      "Step 542772  [5.419 sec/step, loss=0.07249, avg_loss=0.07301]\n",
      "Step 542773  [5.412 sec/step, loss=0.07132, avg_loss=0.07299]\n",
      "Step 542774  [5.410 sec/step, loss=0.07447, avg_loss=0.07302]\n",
      "Step 542775  [5.421 sec/step, loss=0.07513, avg_loss=0.07302]\n",
      "Step 542776  [5.370 sec/step, loss=0.07398, avg_loss=0.07310]\n",
      "Step 542777  [5.375 sec/step, loss=0.07404, avg_loss=0.07310]\n",
      "Step 542778  [5.415 sec/step, loss=0.06522, avg_loss=0.07300]\n",
      "Step 542779  [5.430 sec/step, loss=0.07528, avg_loss=0.07303]\n",
      "Step 542780  [5.425 sec/step, loss=0.07483, avg_loss=0.07303]\n",
      "Step 542781  [5.407 sec/step, loss=0.07345, avg_loss=0.07302]\n",
      "Step 542782  [5.410 sec/step, loss=0.07419, avg_loss=0.07301]\n",
      "Step 542783  [5.398 sec/step, loss=0.07329, avg_loss=0.07300]\n",
      "Step 542784  [5.424 sec/step, loss=0.07549, avg_loss=0.07310]\n",
      "Step 542785  [5.426 sec/step, loss=0.07244, avg_loss=0.07307]\n",
      "Step 542786  [5.423 sec/step, loss=0.07422, avg_loss=0.07309]\n",
      "Generated 32 batches of size 32 in 2.410 sec\n",
      "Step 542787  [5.444 sec/step, loss=0.07411, avg_loss=0.07309]\n",
      "Step 542788  [5.433 sec/step, loss=0.07008, avg_loss=0.07308]\n",
      "Step 542789  [5.447 sec/step, loss=0.07547, avg_loss=0.07311]\n",
      "Step 542790  [5.437 sec/step, loss=0.07424, avg_loss=0.07313]\n",
      "Step 542791  [5.446 sec/step, loss=0.07442, avg_loss=0.07313]\n",
      "Step 542792  [5.426 sec/step, loss=0.07503, avg_loss=0.07313]\n",
      "Step 542793  [5.433 sec/step, loss=0.07250, avg_loss=0.07312]\n",
      "Step 542794  [5.420 sec/step, loss=0.06507, avg_loss=0.07303]\n",
      "Step 542795  [5.443 sec/step, loss=0.07409, avg_loss=0.07307]\n",
      "Step 542796  [5.428 sec/step, loss=0.07438, avg_loss=0.07307]\n",
      "Step 542797  [5.425 sec/step, loss=0.07088, avg_loss=0.07303]\n",
      "Step 542798  [5.431 sec/step, loss=0.07416, avg_loss=0.07303]\n",
      "Step 542799  [5.425 sec/step, loss=0.06945, avg_loss=0.07298]\n",
      "Step 542800  [5.451 sec/step, loss=0.07281, avg_loss=0.07297]\n",
      "Writing summary at step: 542800\n",
      "Step 542801  [5.404 sec/step, loss=0.07531, avg_loss=0.07307]\n",
      "Step 542802  [5.428 sec/step, loss=0.07250, avg_loss=0.07309]\n",
      "Step 542803  [5.422 sec/step, loss=0.07616, avg_loss=0.07310]\n",
      "Step 542804  [5.440 sec/step, loss=0.07476, avg_loss=0.07311]\n",
      "Step 542805  [5.438 sec/step, loss=0.07413, avg_loss=0.07314]\n",
      "Step 542806  [5.400 sec/step, loss=0.07128, avg_loss=0.07313]\n",
      "Step 542807  [5.414 sec/step, loss=0.07277, avg_loss=0.07322]\n",
      "Step 542808  [5.404 sec/step, loss=0.06595, avg_loss=0.07318]\n",
      "Step 542809  [5.393 sec/step, loss=0.07302, avg_loss=0.07317]\n",
      "Step 542810  [5.385 sec/step, loss=0.07139, avg_loss=0.07314]\n",
      "Step 542811  [5.380 sec/step, loss=0.07472, avg_loss=0.07314]\n",
      "Step 542812  [5.400 sec/step, loss=0.07321, avg_loss=0.07316]\n",
      "Step 542813  [5.406 sec/step, loss=0.07420, avg_loss=0.07317]\n",
      "Step 542814  [5.409 sec/step, loss=0.07406, avg_loss=0.07319]\n",
      "Step 542815  [5.408 sec/step, loss=0.07279, avg_loss=0.07316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 542816  [5.402 sec/step, loss=0.07564, avg_loss=0.07318]\n",
      "Step 542817  [5.413 sec/step, loss=0.07567, avg_loss=0.07319]\n",
      "Generated 32 batches of size 32 in 2.485 sec\n",
      "Step 542818  [5.426 sec/step, loss=0.07297, avg_loss=0.07318]\n",
      "Step 542819  [5.427 sec/step, loss=0.07446, avg_loss=0.07317]\n",
      "Step 542820  [5.425 sec/step, loss=0.07399, avg_loss=0.07316]\n",
      "Step 542821  [5.402 sec/step, loss=0.07155, avg_loss=0.07312]\n",
      "Step 542822  [5.383 sec/step, loss=0.07305, avg_loss=0.07310]\n",
      "Step 542823  [5.392 sec/step, loss=0.07597, avg_loss=0.07315]\n",
      "Step 542824  [5.398 sec/step, loss=0.07330, avg_loss=0.07316]\n",
      "Step 542825  [5.388 sec/step, loss=0.07369, avg_loss=0.07314]\n",
      "Step 542826  [5.391 sec/step, loss=0.07349, avg_loss=0.07315]\n",
      "Step 542827  [5.392 sec/step, loss=0.06975, avg_loss=0.07314]\n",
      "Step 542828  [5.365 sec/step, loss=0.06293, avg_loss=0.07302]\n",
      "Step 542829  [5.380 sec/step, loss=0.07517, avg_loss=0.07305]\n",
      "Step 542830  [5.376 sec/step, loss=0.07427, avg_loss=0.07304]\n",
      "Step 542831  [5.378 sec/step, loss=0.07509, avg_loss=0.07305]\n",
      "Step 542832  [5.367 sec/step, loss=0.07410, avg_loss=0.07306]\n",
      "Step 542833  [5.354 sec/step, loss=0.07268, avg_loss=0.07304]\n",
      "Step 542834  [5.357 sec/step, loss=0.07174, avg_loss=0.07302]\n",
      "Step 542835  [5.342 sec/step, loss=0.07451, avg_loss=0.07302]\n",
      "Step 542836  [5.364 sec/step, loss=0.07529, avg_loss=0.07306]\n",
      "Step 542837  [5.352 sec/step, loss=0.07289, avg_loss=0.07304]\n",
      "Step 542838  [5.344 sec/step, loss=0.07000, avg_loss=0.07301]\n",
      "Step 542839  [5.369 sec/step, loss=0.07266, avg_loss=0.07303]\n",
      "Step 542840  [5.376 sec/step, loss=0.07438, avg_loss=0.07305]\n",
      "Step 542841  [5.399 sec/step, loss=0.07468, avg_loss=0.07313]\n",
      "Step 542842  [5.412 sec/step, loss=0.07463, avg_loss=0.07315]\n",
      "Step 542843  [5.402 sec/step, loss=0.07480, avg_loss=0.07318]\n",
      "Step 542844  [5.391 sec/step, loss=0.07090, avg_loss=0.07317]\n",
      "Step 542845  [5.394 sec/step, loss=0.07535, avg_loss=0.07320]\n",
      "Step 542846  [5.389 sec/step, loss=0.07264, avg_loss=0.07319]\n",
      "Step 542847  [5.408 sec/step, loss=0.07237, avg_loss=0.07318]\n",
      "Step 542848  [5.403 sec/step, loss=0.07343, avg_loss=0.07317]\n",
      "Step 542849  [5.451 sec/step, loss=0.06466, avg_loss=0.07307]\n",
      "Generated 32 batches of size 32 in 2.480 sec\n",
      "Step 542850  [5.461 sec/step, loss=0.07412, avg_loss=0.07306]\n",
      "Step 542851  [5.446 sec/step, loss=0.07435, avg_loss=0.07306]\n",
      "Step 542852  [5.454 sec/step, loss=0.07425, avg_loss=0.07304]\n",
      "Step 542853  [5.461 sec/step, loss=0.07551, avg_loss=0.07306]\n",
      "Step 542854  [5.470 sec/step, loss=0.07203, avg_loss=0.07304]\n",
      "Step 542855  [5.445 sec/step, loss=0.07417, avg_loss=0.07306]\n",
      "Step 542856  [5.446 sec/step, loss=0.07393, avg_loss=0.07306]\n",
      "Step 542857  [5.422 sec/step, loss=0.07332, avg_loss=0.07304]\n",
      "Step 542858  [5.383 sec/step, loss=0.07513, avg_loss=0.07313]\n",
      "Step 542859  [5.386 sec/step, loss=0.07402, avg_loss=0.07313]\n",
      "Step 542860  [5.395 sec/step, loss=0.07447, avg_loss=0.07316]\n",
      "Step 542861  [5.410 sec/step, loss=0.07528, avg_loss=0.07318]\n",
      "Step 542862  [5.382 sec/step, loss=0.07022, avg_loss=0.07317]\n",
      "Step 542863  [5.380 sec/step, loss=0.07469, avg_loss=0.07316]\n",
      "Step 542864  [5.386 sec/step, loss=0.07305, avg_loss=0.07315]\n",
      "Step 542865  [5.368 sec/step, loss=0.07360, avg_loss=0.07313]\n",
      "Step 542866  [5.380 sec/step, loss=0.07408, avg_loss=0.07317]\n",
      "Step 542867  [5.377 sec/step, loss=0.07276, avg_loss=0.07315]\n",
      "Step 542868  [5.377 sec/step, loss=0.07428, avg_loss=0.07320]\n",
      "Step 542869  [5.383 sec/step, loss=0.07392, avg_loss=0.07319]\n",
      "Step 542870  [5.438 sec/step, loss=0.06745, avg_loss=0.07314]\n",
      "Step 542871  [5.428 sec/step, loss=0.07150, avg_loss=0.07312]\n",
      "Step 542872  [5.432 sec/step, loss=0.07310, avg_loss=0.07312]\n",
      "Step 542873  [5.448 sec/step, loss=0.07508, avg_loss=0.07316]\n",
      "Step 542874  [5.428 sec/step, loss=0.07352, avg_loss=0.07315]\n",
      "Step 542875  [5.429 sec/step, loss=0.07530, avg_loss=0.07315]\n",
      "Step 542876  [5.457 sec/step, loss=0.07323, avg_loss=0.07314]\n",
      "Step 542877  [5.452 sec/step, loss=0.07398, avg_loss=0.07314]\n",
      "Step 542878  [5.388 sec/step, loss=0.07135, avg_loss=0.07321]\n",
      "Step 542879  [5.379 sec/step, loss=0.07527, avg_loss=0.07320]\n",
      "Step 542880  [5.393 sec/step, loss=0.07421, avg_loss=0.07320]\n",
      "Step 542881  [5.408 sec/step, loss=0.07397, avg_loss=0.07320]\n",
      "Generated 32 batches of size 32 in 2.412 sec\n",
      "Step 542882  [5.403 sec/step, loss=0.07574, avg_loss=0.07322]\n",
      "Step 542883  [5.408 sec/step, loss=0.07121, avg_loss=0.07320]\n",
      "Step 542884  [5.400 sec/step, loss=0.07135, avg_loss=0.07316]\n",
      "Step 542885  [5.393 sec/step, loss=0.07358, avg_loss=0.07317]\n",
      "Step 542886  [5.394 sec/step, loss=0.07492, avg_loss=0.07318]\n",
      "Step 542887  [5.409 sec/step, loss=0.07347, avg_loss=0.07317]\n",
      "Step 542888  [5.426 sec/step, loss=0.07488, avg_loss=0.07322]\n",
      "Step 542889  [5.417 sec/step, loss=0.07440, avg_loss=0.07321]\n",
      "Step 542890  [5.404 sec/step, loss=0.06747, avg_loss=0.07314]\n",
      "Step 542891  [5.388 sec/step, loss=0.07624, avg_loss=0.07316]\n",
      "Step 542892  [5.382 sec/step, loss=0.07297, avg_loss=0.07314]\n",
      "Step 542893  [5.385 sec/step, loss=0.07547, avg_loss=0.07317]\n",
      "Step 542894  [5.398 sec/step, loss=0.07408, avg_loss=0.07326]\n",
      "Step 542895  [5.381 sec/step, loss=0.07420, avg_loss=0.07326]\n",
      "Step 542896  [5.388 sec/step, loss=0.07534, avg_loss=0.07327]\n",
      "Step 542897  [5.376 sec/step, loss=0.06968, avg_loss=0.07326]\n",
      "Step 542898  [5.365 sec/step, loss=0.07512, avg_loss=0.07326]\n",
      "Step 542899  [5.357 sec/step, loss=0.06500, avg_loss=0.07322]\n",
      "Step 542900  [5.344 sec/step, loss=0.07559, avg_loss=0.07325]\n",
      "Writing summary at step: 542900\n",
      "Step 542901  [5.353 sec/step, loss=0.07582, avg_loss=0.07325]\n",
      "Step 542902  [5.346 sec/step, loss=0.07337, avg_loss=0.07326]\n",
      "Step 542903  [5.355 sec/step, loss=0.07546, avg_loss=0.07325]\n",
      "Step 542904  [5.357 sec/step, loss=0.07562, avg_loss=0.07326]\n",
      "Step 542905  [5.365 sec/step, loss=0.07711, avg_loss=0.07329]\n",
      "Step 542906  [5.368 sec/step, loss=0.07451, avg_loss=0.07333]\n",
      "Step 542907  [5.380 sec/step, loss=0.07597, avg_loss=0.07336]\n",
      "Step 542908  [5.404 sec/step, loss=0.07667, avg_loss=0.07346]\n",
      "Step 542909  [5.397 sec/step, loss=0.07078, avg_loss=0.07344]\n",
      "Step 542910  [5.398 sec/step, loss=0.07164, avg_loss=0.07344]\n",
      "Step 542911  [5.451 sec/step, loss=0.06616, avg_loss=0.07336]\n",
      "Step 542912  [5.457 sec/step, loss=0.07493, avg_loss=0.07338]\n",
      "Generated 32 batches of size 32 in 2.579 sec\n",
      "Step 542913  [5.453 sec/step, loss=0.07426, avg_loss=0.07338]\n",
      "Step 542914  [5.454 sec/step, loss=0.07556, avg_loss=0.07339]\n",
      "Step 542915  [5.452 sec/step, loss=0.07115, avg_loss=0.07338]\n",
      "Step 542916  [5.447 sec/step, loss=0.07547, avg_loss=0.07337]\n",
      "Step 542917  [5.446 sec/step, loss=0.07641, avg_loss=0.07338]\n",
      "Step 542918  [5.451 sec/step, loss=0.07451, avg_loss=0.07340]\n",
      "Step 542919  [5.443 sec/step, loss=0.07430, avg_loss=0.07340]\n",
      "Step 542920  [5.443 sec/step, loss=0.07525, avg_loss=0.07341]\n",
      "Step 542921  [5.450 sec/step, loss=0.07295, avg_loss=0.07342]\n",
      "Step 542922  [5.466 sec/step, loss=0.07253, avg_loss=0.07342]\n",
      "Step 542923  [5.453 sec/step, loss=0.07510, avg_loss=0.07341]\n",
      "Step 542924  [5.452 sec/step, loss=0.07616, avg_loss=0.07344]\n",
      "Step 542925  [5.452 sec/step, loss=0.07530, avg_loss=0.07345]\n",
      "Step 542926  [5.451 sec/step, loss=0.07442, avg_loss=0.07346]\n",
      "Step 542927  [5.457 sec/step, loss=0.07370, avg_loss=0.07350]\n",
      "Step 542928  [5.467 sec/step, loss=0.07045, avg_loss=0.07358]\n",
      "Step 542929  [5.466 sec/step, loss=0.07404, avg_loss=0.07357]\n",
      "Step 542930  [5.456 sec/step, loss=0.07532, avg_loss=0.07358]\n",
      "Step 542931  [5.432 sec/step, loss=0.07154, avg_loss=0.07354]\n",
      "Step 542932  [5.439 sec/step, loss=0.07605, avg_loss=0.07356]\n",
      "Step 542933  [5.445 sec/step, loss=0.07349, avg_loss=0.07357]\n",
      "Step 542934  [5.445 sec/step, loss=0.07361, avg_loss=0.07359]\n",
      "Step 542935  [5.456 sec/step, loss=0.07548, avg_loss=0.07360]\n",
      "Step 542936  [5.440 sec/step, loss=0.07341, avg_loss=0.07358]\n",
      "Step 542937  [5.461 sec/step, loss=0.07680, avg_loss=0.07362]\n",
      "Step 542938  [5.485 sec/step, loss=0.07320, avg_loss=0.07365]\n",
      "Step 542939  [5.473 sec/step, loss=0.07399, avg_loss=0.07366]\n",
      "Step 542940  [5.456 sec/step, loss=0.06769, avg_loss=0.07359]\n",
      "Step 542941  [5.464 sec/step, loss=0.07447, avg_loss=0.07359]\n",
      "Step 542942  [5.455 sec/step, loss=0.07555, avg_loss=0.07360]\n",
      "Step 542943  [5.480 sec/step, loss=0.07291, avg_loss=0.07358]\n",
      "Step 542944  [5.499 sec/step, loss=0.07200, avg_loss=0.07359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 32 batches of size 32 in 2.461 sec\n",
      "Step 542945  [5.496 sec/step, loss=0.07111, avg_loss=0.07355]\n",
      "Step 542946  [5.483 sec/step, loss=0.07193, avg_loss=0.07354]\n",
      "Step 542947  [5.509 sec/step, loss=0.06564, avg_loss=0.07348]\n",
      "Step 542948  [5.522 sec/step, loss=0.07577, avg_loss=0.07350]\n",
      "Step 542949  [5.486 sec/step, loss=0.07575, avg_loss=0.07361]\n",
      "Step 542950  [5.473 sec/step, loss=0.07314, avg_loss=0.07360]\n",
      "Step 542951  [5.473 sec/step, loss=0.07467, avg_loss=0.07360]\n",
      "Step 542952  [5.463 sec/step, loss=0.07554, avg_loss=0.07362]\n",
      "Step 542953  [5.454 sec/step, loss=0.07274, avg_loss=0.07359]\n",
      "Step 542954  [5.440 sec/step, loss=0.07197, avg_loss=0.07359]\n",
      "Step 542955  [5.435 sec/step, loss=0.07432, avg_loss=0.07359]\n",
      "Step 542956  [5.427 sec/step, loss=0.07337, avg_loss=0.07359]\n",
      "Step 542957  [5.448 sec/step, loss=0.07253, avg_loss=0.07358]\n",
      "Step 542958  [5.449 sec/step, loss=0.07558, avg_loss=0.07358]\n",
      "Step 542959  [5.468 sec/step, loss=0.07493, avg_loss=0.07359]\n",
      "Step 542960  [5.446 sec/step, loss=0.07182, avg_loss=0.07356]\n",
      "Step 542961  [5.435 sec/step, loss=0.07449, avg_loss=0.07356]\n",
      "Step 542962  [5.454 sec/step, loss=0.07537, avg_loss=0.07361]\n",
      "Step 542963  [5.445 sec/step, loss=0.07148, avg_loss=0.07358]\n",
      "Step 542964  [5.417 sec/step, loss=0.06385, avg_loss=0.07348]\n",
      "Step 542965  [5.439 sec/step, loss=0.07514, avg_loss=0.07350]\n",
      "Step 542966  [5.446 sec/step, loss=0.07453, avg_loss=0.07350]\n",
      "Step 542967  [5.441 sec/step, loss=0.07037, avg_loss=0.07348]\n",
      "Step 542968  [5.434 sec/step, loss=0.07379, avg_loss=0.07348]\n",
      "Step 542969  [5.431 sec/step, loss=0.07415, avg_loss=0.07348]\n",
      "Step 542970  [5.382 sec/step, loss=0.07491, avg_loss=0.07355]\n",
      "Step 542971  [5.397 sec/step, loss=0.07354, avg_loss=0.07357]\n",
      "Step 542972  [5.401 sec/step, loss=0.07293, avg_loss=0.07357]\n",
      "Step 542973  [5.391 sec/step, loss=0.07495, avg_loss=0.07357]\n",
      "Step 542974  [5.414 sec/step, loss=0.07303, avg_loss=0.07356]\n",
      "Step 542975  [5.400 sec/step, loss=0.07365, avg_loss=0.07355]\n",
      "Step 542976  [5.380 sec/step, loss=0.07575, avg_loss=0.07357]\n",
      "Generated 32 batches of size 32 in 2.380 sec\n",
      "Step 542977  [5.387 sec/step, loss=0.07399, avg_loss=0.07357]\n",
      "Step 542978  [5.451 sec/step, loss=0.06625, avg_loss=0.07352]\n",
      "Step 542979  [5.446 sec/step, loss=0.07264, avg_loss=0.07350]\n",
      "Step 542980  [5.445 sec/step, loss=0.07457, avg_loss=0.07350]\n",
      "Step 542981  [5.443 sec/step, loss=0.07485, avg_loss=0.07351]\n",
      "Step 542982  [5.426 sec/step, loss=0.07110, avg_loss=0.07346]\n",
      "Step 542983  [5.434 sec/step, loss=0.07455, avg_loss=0.07350]\n",
      "Step 542984  [5.432 sec/step, loss=0.07461, avg_loss=0.07353]\n",
      "Step 542985  [5.437 sec/step, loss=0.07573, avg_loss=0.07355]\n",
      "Step 542986  [5.445 sec/step, loss=0.07542, avg_loss=0.07356]\n",
      "Step 542987  [5.424 sec/step, loss=0.07504, avg_loss=0.07357]\n",
      "Step 542988  [5.416 sec/step, loss=0.07461, avg_loss=0.07357]\n",
      "Step 542989  [5.410 sec/step, loss=0.07447, avg_loss=0.07357]\n",
      "Step 542990  [5.428 sec/step, loss=0.07494, avg_loss=0.07364]\n",
      "Step 542991  [5.423 sec/step, loss=0.07320, avg_loss=0.07361]\n",
      "Step 542992  [5.431 sec/step, loss=0.07477, avg_loss=0.07363]\n",
      "Step 542993  [5.428 sec/step, loss=0.07440, avg_loss=0.07362]\n",
      "Step 542994  [5.427 sec/step, loss=0.07211, avg_loss=0.07360]\n",
      "Step 542995  [5.430 sec/step, loss=0.07340, avg_loss=0.07359]\n",
      "Step 542996  [5.421 sec/step, loss=0.07105, avg_loss=0.07355]\n",
      "Step 542997  [5.433 sec/step, loss=0.07252, avg_loss=0.07358]\n",
      "Step 542998  [5.426 sec/step, loss=0.06999, avg_loss=0.07353]\n",
      "Step 542999  [5.455 sec/step, loss=0.07500, avg_loss=0.07363]\n",
      "Step 543000  [5.443 sec/step, loss=0.07461, avg_loss=0.07362]\n",
      "Writing summary at step: 543000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-543000\n",
      "Saving audio and alignment...\n",
      "Input: halkii phulkii tsaaddar bitsh bitshaa kur sayntsarjaan guzaaroon or ddaygtsii kay bayx miin nihaan muung kay paraathay ddaaggay dzaauun~___________________________________________________________________________________\n",
      "Step 543001  [5.442 sec/step, loss=0.07333, avg_loss=0.07359]\n",
      "Step 543002  [5.428 sec/step, loss=0.07050, avg_loss=0.07356]\n",
      "Step 543003  [5.415 sec/step, loss=0.07413, avg_loss=0.07355]\n",
      "Step 543004  [5.385 sec/step, loss=0.06505, avg_loss=0.07344]\n",
      "Step 543005  [5.378 sec/step, loss=0.07409, avg_loss=0.07341]\n",
      "Step 543006  [5.393 sec/step, loss=0.07563, avg_loss=0.07343]\n",
      "Generated 32 batches of size 32 in 2.464 sec\n",
      "Step 543007  [5.404 sec/step, loss=0.07481, avg_loss=0.07341]\n",
      "Step 543008  [5.424 sec/step, loss=0.07145, avg_loss=0.07336]\n",
      "Step 543009  [5.448 sec/step, loss=0.07536, avg_loss=0.07341]\n",
      "Step 543010  [5.467 sec/step, loss=0.07516, avg_loss=0.07344]\n",
      "Step 543011  [5.410 sec/step, loss=0.07299, avg_loss=0.07351]\n",
      "Step 543012  [5.385 sec/step, loss=0.07147, avg_loss=0.07348]\n",
      "Step 543013  [5.390 sec/step, loss=0.07546, avg_loss=0.07349]\n",
      "Step 543014  [5.380 sec/step, loss=0.07119, avg_loss=0.07344]\n",
      "Step 543015  [5.391 sec/step, loss=0.07521, avg_loss=0.07348]\n",
      "Step 543016  [5.396 sec/step, loss=0.07519, avg_loss=0.07348]\n",
      "Step 543017  [5.381 sec/step, loss=0.07463, avg_loss=0.07346]\n",
      "Step 543018  [5.378 sec/step, loss=0.07532, avg_loss=0.07347]\n",
      "Step 543019  [5.388 sec/step, loss=0.07532, avg_loss=0.07348]\n",
      "Step 543020  [5.396 sec/step, loss=0.07525, avg_loss=0.07348]\n",
      "Step 543021  [5.423 sec/step, loss=0.07218, avg_loss=0.07347]\n",
      "Step 543022  [5.404 sec/step, loss=0.07335, avg_loss=0.07348]\n",
      "Step 543023  [5.416 sec/step, loss=0.07543, avg_loss=0.07349]\n",
      "Step 543024  [5.409 sec/step, loss=0.07410, avg_loss=0.07347]\n",
      "Step 543025  [5.388 sec/step, loss=0.06586, avg_loss=0.07337]\n",
      "Step 543026  [5.381 sec/step, loss=0.07263, avg_loss=0.07335]\n",
      "Step 543027  [5.379 sec/step, loss=0.07345, avg_loss=0.07335]\n",
      "Step 543028  [5.380 sec/step, loss=0.07320, avg_loss=0.07338]\n",
      "Step 543029  [5.365 sec/step, loss=0.07432, avg_loss=0.07338]\n",
      "Step 543030  [5.366 sec/step, loss=0.07442, avg_loss=0.07337]\n",
      "Step 543031  [5.366 sec/step, loss=0.07109, avg_loss=0.07337]\n",
      "Step 543032  [5.358 sec/step, loss=0.07420, avg_loss=0.07335]\n",
      "Step 543033  [5.360 sec/step, loss=0.07433, avg_loss=0.07336]\n",
      "Step 543034  [5.377 sec/step, loss=0.07389, avg_loss=0.07336]\n",
      "Step 543035  [5.379 sec/step, loss=0.07301, avg_loss=0.07334]\n",
      "Step 543036  [5.409 sec/step, loss=0.07289, avg_loss=0.07333]\n",
      "Step 543037  [5.399 sec/step, loss=0.07356, avg_loss=0.07330]\n",
      "Step 543038  [5.393 sec/step, loss=0.07292, avg_loss=0.07330]\n",
      "Generated 32 batches of size 32 in 2.811 sec\n",
      "Step 543039  [5.384 sec/step, loss=0.07150, avg_loss=0.07327]\n",
      "Step 543040  [5.401 sec/step, loss=0.07073, avg_loss=0.07330]\n",
      "Step 543041  [5.391 sec/step, loss=0.07118, avg_loss=0.07327]\n",
      "Step 543042  [5.387 sec/step, loss=0.07134, avg_loss=0.07323]\n",
      "Step 543043  [5.410 sec/step, loss=0.06542, avg_loss=0.07315]\n",
      "Step 543044  [5.404 sec/step, loss=0.07320, avg_loss=0.07316]\n",
      "Step 543045  [5.393 sec/step, loss=0.07221, avg_loss=0.07317]\n",
      "Step 543046  [5.407 sec/step, loss=0.07397, avg_loss=0.07319]\n",
      "Step 543047  [5.357 sec/step, loss=0.07561, avg_loss=0.07329]\n",
      "Step 543048  [5.355 sec/step, loss=0.07522, avg_loss=0.07329]\n",
      "Step 543049  [5.345 sec/step, loss=0.07428, avg_loss=0.07327]\n",
      "Step 543050  [5.362 sec/step, loss=0.07563, avg_loss=0.07330]\n",
      "Step 543051  [5.363 sec/step, loss=0.07396, avg_loss=0.07329]\n",
      "Step 543052  [5.366 sec/step, loss=0.07459, avg_loss=0.07328]\n",
      "Step 543053  [5.355 sec/step, loss=0.07076, avg_loss=0.07326]\n",
      "Step 543054  [5.390 sec/step, loss=0.07327, avg_loss=0.07328]\n",
      "Step 543055  [5.387 sec/step, loss=0.07091, avg_loss=0.07324]\n",
      "Step 543056  [5.400 sec/step, loss=0.07235, avg_loss=0.07323]\n",
      "Step 543057  [5.390 sec/step, loss=0.07447, avg_loss=0.07325]\n",
      "Step 543058  [5.387 sec/step, loss=0.07543, avg_loss=0.07325]\n",
      "Step 543059  [5.381 sec/step, loss=0.07470, avg_loss=0.07325]\n",
      "Step 543060  [5.403 sec/step, loss=0.07579, avg_loss=0.07329]\n",
      "Step 543061  [5.402 sec/step, loss=0.07442, avg_loss=0.07329]\n",
      "Step 543062  [5.392 sec/step, loss=0.07249, avg_loss=0.07326]\n",
      "Step 543063  [5.397 sec/step, loss=0.07401, avg_loss=0.07328]\n",
      "Step 543064  [5.403 sec/step, loss=0.07316, avg_loss=0.07338]\n",
      "Step 543065  [5.385 sec/step, loss=0.07321, avg_loss=0.07336]\n",
      "Step 543066  [5.371 sec/step, loss=0.07208, avg_loss=0.07333]\n",
      "Step 543067  [5.390 sec/step, loss=0.07276, avg_loss=0.07336]\n",
      "Step 543068  [5.395 sec/step, loss=0.07293, avg_loss=0.07335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 543069  [5.367 sec/step, loss=0.06442, avg_loss=0.07325]\n",
      "Step 543070  [5.414 sec/step, loss=0.06633, avg_loss=0.07316]\n",
      "Generated 32 batches of size 32 in 2.579 sec\n",
      "Step 543071  [5.403 sec/step, loss=0.06977, avg_loss=0.07313]\n",
      "Step 543072  [5.408 sec/step, loss=0.07267, avg_loss=0.07312]\n",
      "Step 543073  [5.404 sec/step, loss=0.07419, avg_loss=0.07312]\n",
      "Step 543074  [5.379 sec/step, loss=0.07488, avg_loss=0.07313]\n",
      "Step 543075  [5.378 sec/step, loss=0.07398, avg_loss=0.07314]\n",
      "Step 543076  [5.367 sec/step, loss=0.07117, avg_loss=0.07309]\n",
      "Step 543077  [5.373 sec/step, loss=0.07507, avg_loss=0.07310]\n",
      "Step 543078  [5.341 sec/step, loss=0.07228, avg_loss=0.07316]\n",
      "Step 543079  [5.337 sec/step, loss=0.07404, avg_loss=0.07318]\n",
      "Step 543080  [5.333 sec/step, loss=0.07504, avg_loss=0.07318]\n",
      "Step 543081  [5.320 sec/step, loss=0.07308, avg_loss=0.07316]\n",
      "Step 543082  [5.329 sec/step, loss=0.07148, avg_loss=0.07317]\n",
      "Step 543083  [5.318 sec/step, loss=0.07436, avg_loss=0.07317]\n",
      "Step 543084  [5.343 sec/step, loss=0.07434, avg_loss=0.07316]\n",
      "Step 543085  [5.341 sec/step, loss=0.07118, avg_loss=0.07312]\n",
      "Step 543086  [5.330 sec/step, loss=0.07197, avg_loss=0.07308]\n",
      "Step 543087  [5.331 sec/step, loss=0.07402, avg_loss=0.07307]\n",
      "Step 543088  [5.341 sec/step, loss=0.07424, avg_loss=0.07307]\n",
      "Step 543089  [5.352 sec/step, loss=0.07517, avg_loss=0.07308]\n",
      "Step 543090  [5.343 sec/step, loss=0.07257, avg_loss=0.07305]\n",
      "Step 543091  [5.362 sec/step, loss=0.07514, avg_loss=0.07307]\n",
      "Step 543092  [5.370 sec/step, loss=0.07265, avg_loss=0.07305]\n",
      "Step 543093  [5.379 sec/step, loss=0.07580, avg_loss=0.07306]\n",
      "Step 543094  [5.373 sec/step, loss=0.07054, avg_loss=0.07305]\n",
      "Step 543095  [5.378 sec/step, loss=0.07068, avg_loss=0.07302]\n",
      "Step 543096  [5.390 sec/step, loss=0.07577, avg_loss=0.07307]\n",
      "Step 543097  [5.389 sec/step, loss=0.07327, avg_loss=0.07308]\n",
      "Step 543098  [5.392 sec/step, loss=0.07185, avg_loss=0.07310]\n",
      "Step 543099  [5.373 sec/step, loss=0.07303, avg_loss=0.07308]\n",
      "Step 543100  [5.374 sec/step, loss=0.07473, avg_loss=0.07308]\n",
      "Writing summary at step: 543100\n",
      "Step 543101  [5.368 sec/step, loss=0.07471, avg_loss=0.07309]\n",
      "Generated 32 batches of size 32 in 2.503 sec\n",
      "Step 543102  [5.395 sec/step, loss=0.07386, avg_loss=0.07312]\n",
      "Step 543103  [5.385 sec/step, loss=0.07334, avg_loss=0.07312]\n",
      "Step 543104  [5.451 sec/step, loss=0.06485, avg_loss=0.07311]\n",
      "Step 543105  [5.434 sec/step, loss=0.07116, avg_loss=0.07308]\n",
      "Step 543106  [5.444 sec/step, loss=0.07405, avg_loss=0.07307]\n",
      "Step 543107  [5.406 sec/step, loss=0.06585, avg_loss=0.07298]\n",
      "Step 543108  [5.380 sec/step, loss=0.07386, avg_loss=0.07300]\n",
      "Step 543109  [5.376 sec/step, loss=0.07569, avg_loss=0.07301]\n",
      "Step 543110  [5.357 sec/step, loss=0.07142, avg_loss=0.07297]\n",
      "Step 543111  [5.375 sec/step, loss=0.07473, avg_loss=0.07299]\n",
      "Step 543112  [5.367 sec/step, loss=0.07299, avg_loss=0.07300]\n",
      "Step 543113  [5.361 sec/step, loss=0.07220, avg_loss=0.07297]\n",
      "Step 543114  [5.372 sec/step, loss=0.07380, avg_loss=0.07300]\n",
      "Step 543115  [5.358 sec/step, loss=0.07383, avg_loss=0.07298]\n",
      "Step 543116  [5.347 sec/step, loss=0.07474, avg_loss=0.07298]\n",
      "Step 543117  [5.360 sec/step, loss=0.07545, avg_loss=0.07299]\n",
      "Step 543118  [5.353 sec/step, loss=0.07445, avg_loss=0.07298]\n",
      "Step 543119  [5.333 sec/step, loss=0.07022, avg_loss=0.07293]\n",
      "Step 543120  [5.328 sec/step, loss=0.07581, avg_loss=0.07293]\n",
      "Step 543121  [5.315 sec/step, loss=0.07437, avg_loss=0.07295]\n",
      "Step 543122  [5.329 sec/step, loss=0.07378, avg_loss=0.07296]\n",
      "Step 543123  [5.321 sec/step, loss=0.07474, avg_loss=0.07295]\n",
      "Step 543124  [5.366 sec/step, loss=0.06476, avg_loss=0.07286]\n",
      "Step 543125  [5.369 sec/step, loss=0.07166, avg_loss=0.07292]\n",
      "Step 543126  [5.380 sec/step, loss=0.07203, avg_loss=0.07291]\n",
      "Step 543127  [5.397 sec/step, loss=0.07483, avg_loss=0.07292]\n",
      "Step 543128  [5.400 sec/step, loss=0.07272, avg_loss=0.07292]\n",
      "Step 543129  [5.419 sec/step, loss=0.07514, avg_loss=0.07293]\n",
      "Step 543130  [5.417 sec/step, loss=0.07457, avg_loss=0.07293]\n",
      "Step 543131  [5.444 sec/step, loss=0.07405, avg_loss=0.07296]\n",
      "Step 543132  [5.442 sec/step, loss=0.07305, avg_loss=0.07295]\n",
      "Step 543133  [5.437 sec/step, loss=0.07020, avg_loss=0.07291]\n",
      "Generated 32 batches of size 32 in 2.597 sec\n",
      "Step 543134  [5.429 sec/step, loss=0.07313, avg_loss=0.07290]\n",
      "Step 543135  [5.421 sec/step, loss=0.07479, avg_loss=0.07292]\n",
      "Step 543136  [5.419 sec/step, loss=0.07435, avg_loss=0.07293]\n",
      "Step 543137  [5.419 sec/step, loss=0.07377, avg_loss=0.07293]\n",
      "Step 543138  [5.416 sec/step, loss=0.07514, avg_loss=0.07295]\n",
      "Step 543139  [5.408 sec/step, loss=0.06605, avg_loss=0.07290]\n",
      "Step 543140  [5.398 sec/step, loss=0.07243, avg_loss=0.07292]\n",
      "Step 543141  [5.395 sec/step, loss=0.07346, avg_loss=0.07294]\n",
      "Step 543142  [5.408 sec/step, loss=0.07304, avg_loss=0.07296]\n",
      "Step 543143  [5.359 sec/step, loss=0.07521, avg_loss=0.07305]\n",
      "Step 543144  [5.359 sec/step, loss=0.07368, avg_loss=0.07306]\n",
      "Step 543145  [5.363 sec/step, loss=0.07285, avg_loss=0.07307]\n",
      "Step 543146  [5.348 sec/step, loss=0.07009, avg_loss=0.07303]\n",
      "Step 543147  [5.358 sec/step, loss=0.07559, avg_loss=0.07303]\n",
      "Step 543148  [5.345 sec/step, loss=0.07112, avg_loss=0.07299]\n",
      "Step 543149  [5.391 sec/step, loss=0.06715, avg_loss=0.07291]\n",
      "Step 543150  [5.390 sec/step, loss=0.07285, avg_loss=0.07289]\n",
      "Step 543151  [5.385 sec/step, loss=0.07158, avg_loss=0.07286]\n",
      "Step 543152  [5.386 sec/step, loss=0.07330, avg_loss=0.07285]\n",
      "Step 543153  [5.404 sec/step, loss=0.07438, avg_loss=0.07289]\n",
      "Step 543154  [5.381 sec/step, loss=0.07334, avg_loss=0.07289]\n",
      "Step 543155  [5.380 sec/step, loss=0.07467, avg_loss=0.07292]\n",
      "Step 543156  [5.376 sec/step, loss=0.07457, avg_loss=0.07295]\n",
      "Step 543157  [5.373 sec/step, loss=0.07381, avg_loss=0.07294]\n",
      "Step 543158  [5.387 sec/step, loss=0.07350, avg_loss=0.07292]\n",
      "Step 543159  [5.383 sec/step, loss=0.07428, avg_loss=0.07292]\n",
      "Step 543160  [5.383 sec/step, loss=0.07294, avg_loss=0.07289]\n",
      "Step 543161  [5.376 sec/step, loss=0.07307, avg_loss=0.07287]\n",
      "Step 543162  [5.392 sec/step, loss=0.07398, avg_loss=0.07289]\n",
      "Step 543163  [5.382 sec/step, loss=0.07168, avg_loss=0.07287]\n",
      "Step 543164  [5.394 sec/step, loss=0.07454, avg_loss=0.07288]\n",
      "Step 543165  [5.388 sec/step, loss=0.06986, avg_loss=0.07285]\n",
      "Generated 32 batches of size 32 in 2.424 sec\n",
      "Step 543166  [5.429 sec/step, loss=0.07282, avg_loss=0.07285]\n",
      "Step 543167  [5.435 sec/step, loss=0.07498, avg_loss=0.07288]\n",
      "Step 543168  [5.448 sec/step, loss=0.07396, avg_loss=0.07289]\n",
      "Step 543169  [5.478 sec/step, loss=0.07527, avg_loss=0.07299]\n",
      "Step 543170  [5.428 sec/step, loss=0.07471, avg_loss=0.07308]\n",
      "Step 543171  [5.432 sec/step, loss=0.07334, avg_loss=0.07311]\n",
      "Step 543172  [5.413 sec/step, loss=0.06596, avg_loss=0.07305]\n",
      "Step 543173  [5.410 sec/step, loss=0.07288, avg_loss=0.07303]\n",
      "Step 543174  [5.419 sec/step, loss=0.07588, avg_loss=0.07304]\n",
      "Step 543175  [5.433 sec/step, loss=0.07508, avg_loss=0.07305]\n",
      "Step 543176  [5.489 sec/step, loss=0.06422, avg_loss=0.07299]\n",
      "Step 543177  [5.488 sec/step, loss=0.07289, avg_loss=0.07296]\n",
      "Step 543178  [5.467 sec/step, loss=0.07126, avg_loss=0.07295]\n",
      "Step 543179  [5.478 sec/step, loss=0.07351, avg_loss=0.07295]\n",
      "Step 543180  [5.462 sec/step, loss=0.07308, avg_loss=0.07293]\n",
      "Step 543181  [5.467 sec/step, loss=0.07265, avg_loss=0.07292]\n",
      "Step 543182  [5.471 sec/step, loss=0.07520, avg_loss=0.07296]\n",
      "Step 543183  [5.478 sec/step, loss=0.07450, avg_loss=0.07296]\n",
      "Step 543184  [5.466 sec/step, loss=0.07541, avg_loss=0.07297]\n",
      "Step 543185  [5.460 sec/step, loss=0.07365, avg_loss=0.07300]\n",
      "Step 543186  [5.464 sec/step, loss=0.07474, avg_loss=0.07303]\n",
      "Step 543187  [5.454 sec/step, loss=0.07340, avg_loss=0.07302]\n",
      "Step 543188  [5.456 sec/step, loss=0.07532, avg_loss=0.07303]\n",
      "Step 543189  [5.452 sec/step, loss=0.07372, avg_loss=0.07302]\n",
      "Step 543190  [5.471 sec/step, loss=0.07245, avg_loss=0.07301]\n",
      "Step 543191  [5.463 sec/step, loss=0.07490, avg_loss=0.07301]\n",
      "Step 543192  [5.449 sec/step, loss=0.07222, avg_loss=0.07301]\n",
      "Step 543193  [5.430 sec/step, loss=0.07328, avg_loss=0.07298]\n",
      "Step 543194  [5.439 sec/step, loss=0.07404, avg_loss=0.07302]\n",
      "Step 543195  [5.438 sec/step, loss=0.07427, avg_loss=0.07305]\n",
      "Step 543196  [5.415 sec/step, loss=0.07075, avg_loss=0.07300]\n",
      "Step 543197  [5.432 sec/step, loss=0.07513, avg_loss=0.07302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 32 batches of size 32 in 2.938 sec\n",
      "Step 543198  [5.423 sec/step, loss=0.06533, avg_loss=0.07296]\n",
      "Step 543199  [5.454 sec/step, loss=0.07216, avg_loss=0.07295]\n",
      "Step 543200  [5.442 sec/step, loss=0.07001, avg_loss=0.07290]\n",
      "Writing summary at step: 543200\n",
      "Step 543201  [5.447 sec/step, loss=0.07583, avg_loss=0.07291]\n",
      "Step 543202  [5.448 sec/step, loss=0.07463, avg_loss=0.07292]\n",
      "Step 543203  [5.447 sec/step, loss=0.07293, avg_loss=0.07292]\n",
      "Step 543204  [5.392 sec/step, loss=0.07415, avg_loss=0.07301]\n",
      "Step 543205  [5.407 sec/step, loss=0.07484, avg_loss=0.07305]\n",
      "Step 543206  [5.403 sec/step, loss=0.07567, avg_loss=0.07306]\n",
      "Step 543207  [5.410 sec/step, loss=0.06969, avg_loss=0.07310]\n",
      "Step 543208  [5.413 sec/step, loss=0.07400, avg_loss=0.07310]\n",
      "Step 543209  [5.407 sec/step, loss=0.07428, avg_loss=0.07309]\n",
      "Step 543210  [5.415 sec/step, loss=0.07542, avg_loss=0.07313]\n",
      "Step 543211  [5.386 sec/step, loss=0.06437, avg_loss=0.07302]\n",
      "Step 543212  [5.417 sec/step, loss=0.07249, avg_loss=0.07302]\n",
      "Step 543213  [5.467 sec/step, loss=0.06635, avg_loss=0.07296]\n",
      "Step 543214  [5.480 sec/step, loss=0.07304, avg_loss=0.07295]\n",
      "Step 543215  [5.492 sec/step, loss=0.07386, avg_loss=0.07295]\n",
      "Step 543216  [5.493 sec/step, loss=0.07286, avg_loss=0.07293]\n",
      "Step 543217  [5.486 sec/step, loss=0.07426, avg_loss=0.07292]\n",
      "Step 543218  [5.495 sec/step, loss=0.07510, avg_loss=0.07293]\n",
      "Step 543219  [5.501 sec/step, loss=0.07060, avg_loss=0.07293]\n",
      "Step 543220  [5.484 sec/step, loss=0.07303, avg_loss=0.07290]\n",
      "Step 543221  [5.477 sec/step, loss=0.07357, avg_loss=0.07290]\n",
      "Step 543222  [5.470 sec/step, loss=0.07428, avg_loss=0.07290]\n",
      "Step 543223  [5.464 sec/step, loss=0.07427, avg_loss=0.07290]\n",
      "Step 543224  [5.410 sec/step, loss=0.07222, avg_loss=0.07297]\n",
      "Step 543225  [5.432 sec/step, loss=0.07460, avg_loss=0.07300]\n",
      "Step 543226  [5.440 sec/step, loss=0.07585, avg_loss=0.07304]\n",
      "Step 543227  [5.441 sec/step, loss=0.07536, avg_loss=0.07304]\n",
      "Step 543228  [5.447 sec/step, loss=0.07448, avg_loss=0.07306]\n",
      "Generated 32 batches of size 32 in 2.454 sec\n",
      "Step 543229  [5.444 sec/step, loss=0.07557, avg_loss=0.07307]\n",
      "Step 543230  [5.441 sec/step, loss=0.07341, avg_loss=0.07305]\n",
      "Step 543231  [5.424 sec/step, loss=0.07286, avg_loss=0.07304]\n",
      "Step 543232  [5.420 sec/step, loss=0.07192, avg_loss=0.07303]\n",
      "Step 543233  [5.415 sec/step, loss=0.07334, avg_loss=0.07306]\n",
      "Step 543234  [5.410 sec/step, loss=0.07420, avg_loss=0.07307]\n",
      "Step 543235  [5.433 sec/step, loss=0.07309, avg_loss=0.07306]\n",
      "Step 543236  [5.419 sec/step, loss=0.07545, avg_loss=0.07307]\n",
      "Step 543237  [5.401 sec/step, loss=0.07070, avg_loss=0.07304]\n",
      "Step 543238  [5.393 sec/step, loss=0.07094, avg_loss=0.07299]\n",
      "Step 543239  [5.397 sec/step, loss=0.07121, avg_loss=0.07305]\n",
      "Step 543240  [5.411 sec/step, loss=0.07402, avg_loss=0.07306]\n",
      "Step 543241  [5.419 sec/step, loss=0.07355, avg_loss=0.07306]\n",
      "Step 543242  [5.409 sec/step, loss=0.07404, avg_loss=0.07307]\n",
      "Step 543243  [5.418 sec/step, loss=0.07571, avg_loss=0.07308]\n",
      "Step 543244  [5.423 sec/step, loss=0.07275, avg_loss=0.07307]\n",
      "Step 543245  [5.424 sec/step, loss=0.07349, avg_loss=0.07308]\n",
      "Step 543246  [5.440 sec/step, loss=0.07482, avg_loss=0.07312]\n",
      "Step 543247  [5.437 sec/step, loss=0.07532, avg_loss=0.07312]\n",
      "Step 543248  [5.458 sec/step, loss=0.07482, avg_loss=0.07316]\n",
      "Step 543249  [5.399 sec/step, loss=0.07398, avg_loss=0.07323]\n",
      "Step 543250  [5.386 sec/step, loss=0.07044, avg_loss=0.07320]\n",
      "Step 543251  [5.380 sec/step, loss=0.07173, avg_loss=0.07320]\n",
      "Step 543252  [5.363 sec/step, loss=0.07261, avg_loss=0.07320]\n",
      "Step 543253  [5.384 sec/step, loss=0.07214, avg_loss=0.07317]\n",
      "Step 543254  [5.384 sec/step, loss=0.07447, avg_loss=0.07318]\n",
      "Step 543255  [5.399 sec/step, loss=0.07298, avg_loss=0.07317]\n",
      "Step 543256  [5.409 sec/step, loss=0.07292, avg_loss=0.07315]\n",
      "Step 543257  [5.408 sec/step, loss=0.07419, avg_loss=0.07316]\n",
      "Step 543258  [5.397 sec/step, loss=0.07478, avg_loss=0.07317]\n",
      "Step 543259  [5.382 sec/step, loss=0.07344, avg_loss=0.07316]\n",
      "Step 543260  [5.357 sec/step, loss=0.06705, avg_loss=0.07310]\n",
      "Generated 32 batches of size 32 in 2.633 sec\n",
      "Step 543261  [5.423 sec/step, loss=0.06544, avg_loss=0.07302]\n",
      "Step 543262  [5.420 sec/step, loss=0.07533, avg_loss=0.07304]\n",
      "Step 543263  [5.428 sec/step, loss=0.07509, avg_loss=0.07307]\n",
      "Step 543264  [5.422 sec/step, loss=0.07293, avg_loss=0.07306]\n",
      "Step 543265  [5.433 sec/step, loss=0.07368, avg_loss=0.07309]\n",
      "Step 543266  [5.395 sec/step, loss=0.07298, avg_loss=0.07310]\n",
      "Step 543267  [5.389 sec/step, loss=0.07358, avg_loss=0.07308]\n",
      "Step 543268  [5.379 sec/step, loss=0.07446, avg_loss=0.07309]\n",
      "Step 543269  [5.381 sec/step, loss=0.07500, avg_loss=0.07308]\n",
      "Step 543270  [5.377 sec/step, loss=0.07277, avg_loss=0.07306]\n",
      "Step 543271  [5.384 sec/step, loss=0.07460, avg_loss=0.07308]\n",
      "Step 543272  [5.382 sec/step, loss=0.06434, avg_loss=0.07306]\n",
      "Step 543273  [5.386 sec/step, loss=0.07342, avg_loss=0.07307]\n",
      "Step 543274  [5.373 sec/step, loss=0.07415, avg_loss=0.07305]\n",
      "Step 543275  [5.361 sec/step, loss=0.07460, avg_loss=0.07304]\n",
      "Step 543276  [5.320 sec/step, loss=0.07519, avg_loss=0.07315]\n",
      "Step 543277  [5.300 sec/step, loss=0.06964, avg_loss=0.07312]\n",
      "Step 543278  [5.316 sec/step, loss=0.07494, avg_loss=0.07316]\n",
      "Step 543279  [5.337 sec/step, loss=0.07223, avg_loss=0.07315]\n",
      "Step 543280  [5.327 sec/step, loss=0.07040, avg_loss=0.07312]\n",
      "Step 543281  [5.340 sec/step, loss=0.07582, avg_loss=0.07315]\n",
      "Step 543282  [5.320 sec/step, loss=0.07046, avg_loss=0.07310]\n",
      "Step 543283  [5.336 sec/step, loss=0.07430, avg_loss=0.07310]\n",
      "Step 543284  [5.325 sec/step, loss=0.07480, avg_loss=0.07310]\n",
      "Step 543285  [5.322 sec/step, loss=0.07223, avg_loss=0.07308]\n",
      "Step 543286  [5.321 sec/step, loss=0.07322, avg_loss=0.07307]\n",
      "Step 543287  [5.323 sec/step, loss=0.07301, avg_loss=0.07306]\n",
      "Step 543288  [5.311 sec/step, loss=0.07378, avg_loss=0.07305]\n",
      "Step 543289  [5.356 sec/step, loss=0.06542, avg_loss=0.07296]\n",
      "Step 543290  [5.349 sec/step, loss=0.07400, avg_loss=0.07298]\n",
      "Step 543291  [5.342 sec/step, loss=0.07497, avg_loss=0.07298]\n",
      "Step 543292  [5.353 sec/step, loss=0.07562, avg_loss=0.07301]\n",
      "Generated 32 batches of size 32 in 2.451 sec\n",
      "Step 543293  [5.376 sec/step, loss=0.07345, avg_loss=0.07302]\n",
      "Step 543294  [5.369 sec/step, loss=0.07322, avg_loss=0.07301]\n",
      "Step 543295  [5.386 sec/step, loss=0.07481, avg_loss=0.07301]\n",
      "Step 543296  [5.401 sec/step, loss=0.07057, avg_loss=0.07301]\n",
      "Step 543297  [5.397 sec/step, loss=0.07573, avg_loss=0.07302]\n",
      "Step 543298  [5.420 sec/step, loss=0.07244, avg_loss=0.07309]\n",
      "Step 543299  [5.399 sec/step, loss=0.07422, avg_loss=0.07311]\n",
      "Step 543300  [5.402 sec/step, loss=0.07287, avg_loss=0.07314]\n",
      "Writing summary at step: 543300\n",
      "Step 543301  [5.393 sec/step, loss=0.07351, avg_loss=0.07311]\n",
      "Step 543302  [5.387 sec/step, loss=0.07341, avg_loss=0.07310]\n",
      "Step 543303  [5.393 sec/step, loss=0.07344, avg_loss=0.07311]\n",
      "Step 543304  [5.448 sec/step, loss=0.06559, avg_loss=0.07302]\n",
      "Step 543305  [5.440 sec/step, loss=0.07349, avg_loss=0.07301]\n",
      "Step 543306  [5.449 sec/step, loss=0.07242, avg_loss=0.07298]\n",
      "Step 543307  [5.453 sec/step, loss=0.07276, avg_loss=0.07301]\n",
      "Step 543308  [5.434 sec/step, loss=0.06651, avg_loss=0.07293]\n",
      "Step 543309  [5.440 sec/step, loss=0.07406, avg_loss=0.07293]\n",
      "Step 543310  [5.435 sec/step, loss=0.07416, avg_loss=0.07292]\n",
      "Step 543311  [5.450 sec/step, loss=0.07295, avg_loss=0.07300]\n",
      "Step 543312  [5.452 sec/step, loss=0.07290, avg_loss=0.07301]\n",
      "Step 543313  [5.411 sec/step, loss=0.07570, avg_loss=0.07310]\n",
      "Step 543314  [5.409 sec/step, loss=0.07567, avg_loss=0.07313]\n",
      "Step 543315  [5.401 sec/step, loss=0.07415, avg_loss=0.07313]\n",
      "Step 543316  [5.401 sec/step, loss=0.07196, avg_loss=0.07312]\n",
      "Step 543317  [5.387 sec/step, loss=0.07334, avg_loss=0.07311]\n",
      "Step 543318  [5.388 sec/step, loss=0.07259, avg_loss=0.07309]\n",
      "Step 543319  [5.395 sec/step, loss=0.07427, avg_loss=0.07312]\n",
      "Step 543320  [5.401 sec/step, loss=0.07457, avg_loss=0.07314]\n",
      "Step 543321  [5.400 sec/step, loss=0.07379, avg_loss=0.07314]\n",
      "Step 543322  [5.409 sec/step, loss=0.07390, avg_loss=0.07314]\n",
      "Step 543323  [5.420 sec/step, loss=0.07511, avg_loss=0.07314]\n",
      "Generated 32 batches of size 32 in 2.474 sec\n",
      "Step 543324  [5.430 sec/step, loss=0.07454, avg_loss=0.07317]\n",
      "Step 543325  [5.427 sec/step, loss=0.07537, avg_loss=0.07318]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 543326  [5.431 sec/step, loss=0.07501, avg_loss=0.07317]\n",
      "Step 543327  [5.418 sec/step, loss=0.07178, avg_loss=0.07313]\n",
      "Step 543328  [5.420 sec/step, loss=0.07369, avg_loss=0.07312]\n",
      "Step 543329  [5.422 sec/step, loss=0.07550, avg_loss=0.07312]\n",
      "Step 543330  [5.423 sec/step, loss=0.07512, avg_loss=0.07314]\n",
      "Step 543331  [5.412 sec/step, loss=0.07050, avg_loss=0.07312]\n",
      "Step 543332  [5.404 sec/step, loss=0.07067, avg_loss=0.07310]\n",
      "Step 543333  [5.434 sec/step, loss=0.07248, avg_loss=0.07309]\n",
      "Step 543334  [5.432 sec/step, loss=0.07277, avg_loss=0.07308]\n",
      "Step 543335  [5.419 sec/step, loss=0.07488, avg_loss=0.07310]\n",
      "Step 543336  [5.413 sec/step, loss=0.07390, avg_loss=0.07308]\n",
      "Step 543337  [5.426 sec/step, loss=0.07354, avg_loss=0.07311]\n",
      "Step 543338  [5.423 sec/step, loss=0.07235, avg_loss=0.07313]\n",
      "Step 543339  [5.428 sec/step, loss=0.07380, avg_loss=0.07315]\n",
      "Step 543340  [5.423 sec/step, loss=0.07402, avg_loss=0.07315]\n",
      "Step 543341  [5.414 sec/step, loss=0.07059, avg_loss=0.07312]\n",
      "Step 543342  [5.435 sec/step, loss=0.07233, avg_loss=0.07310]\n",
      "Step 543343  [5.415 sec/step, loss=0.07063, avg_loss=0.07305]\n",
      "Step 543344  [5.404 sec/step, loss=0.07286, avg_loss=0.07305]\n",
      "Step 543345  [5.409 sec/step, loss=0.07526, avg_loss=0.07307]\n",
      "Step 543346  [5.397 sec/step, loss=0.07324, avg_loss=0.07306]\n",
      "Step 543347  [5.397 sec/step, loss=0.07604, avg_loss=0.07306]\n",
      "Step 543348  [5.392 sec/step, loss=0.07534, avg_loss=0.07307]\n",
      "Step 543349  [5.416 sec/step, loss=0.07493, avg_loss=0.07308]\n",
      "Step 543350  [5.426 sec/step, loss=0.07544, avg_loss=0.07313]\n",
      "Step 543351  [5.441 sec/step, loss=0.07463, avg_loss=0.07316]\n",
      "Step 543352  [5.456 sec/step, loss=0.07317, avg_loss=0.07316]\n",
      "Step 543353  [5.441 sec/step, loss=0.07548, avg_loss=0.07320]\n",
      "Step 543354  [5.438 sec/step, loss=0.07240, avg_loss=0.07318]\n",
      "Step 543355  [5.427 sec/step, loss=0.07395, avg_loss=0.07319]\n",
      "Generated 32 batches of size 32 in 2.408 sec\n",
      "Step 543356  [5.422 sec/step, loss=0.07496, avg_loss=0.07321]\n",
      "Step 543357  [5.432 sec/step, loss=0.07420, avg_loss=0.07321]\n",
      "Step 543358  [5.407 sec/step, loss=0.07083, avg_loss=0.07317]\n",
      "Step 543359  [5.418 sec/step, loss=0.07405, avg_loss=0.07317]\n",
      "Step 543360  [5.418 sec/step, loss=0.06553, avg_loss=0.07316]\n",
      "Step 543361  [5.366 sec/step, loss=0.07164, avg_loss=0.07322]\n",
      "Step 543362  [5.346 sec/step, loss=0.07241, avg_loss=0.07319]\n",
      "Step 543363  [5.340 sec/step, loss=0.07451, avg_loss=0.07318]\n",
      "Step 543364  [5.394 sec/step, loss=0.06557, avg_loss=0.07311]\n",
      "Step 543365  [5.377 sec/step, loss=0.06550, avg_loss=0.07303]\n",
      "Step 543366  [5.382 sec/step, loss=0.07403, avg_loss=0.07304]\n",
      "Step 543367  [5.374 sec/step, loss=0.07460, avg_loss=0.07305]\n",
      "Step 543368  [5.372 sec/step, loss=0.07430, avg_loss=0.07305]\n",
      "Step 543369  [5.407 sec/step, loss=0.06613, avg_loss=0.07296]\n",
      "Step 543370  [5.421 sec/step, loss=0.07295, avg_loss=0.07296]\n",
      "Step 543371  [5.421 sec/step, loss=0.07402, avg_loss=0.07296]\n",
      "Step 543372  [5.427 sec/step, loss=0.07092, avg_loss=0.07302]\n",
      "Step 543373  [5.432 sec/step, loss=0.07315, avg_loss=0.07302]\n",
      "Step 543374  [5.434 sec/step, loss=0.07284, avg_loss=0.07301]\n",
      "Step 543375  [5.460 sec/step, loss=0.07243, avg_loss=0.07298]\n",
      "Step 543376  [5.450 sec/step, loss=0.07468, avg_loss=0.07298]\n",
      "Step 543377  [5.450 sec/step, loss=0.07366, avg_loss=0.07302]\n",
      "Step 543378  [5.435 sec/step, loss=0.07330, avg_loss=0.07300]\n",
      "Step 543379  [5.404 sec/step, loss=0.07248, avg_loss=0.07300]\n",
      "Step 543380  [5.403 sec/step, loss=0.07102, avg_loss=0.07301]\n",
      "Step 543381  [5.406 sec/step, loss=0.07562, avg_loss=0.07301]\n",
      "Step 543382  [5.430 sec/step, loss=0.07474, avg_loss=0.07305]\n",
      "Step 543383  [5.420 sec/step, loss=0.07572, avg_loss=0.07307]\n",
      "Step 543384  [5.416 sec/step, loss=0.07459, avg_loss=0.07306]\n",
      "Step 543385  [5.430 sec/step, loss=0.07541, avg_loss=0.07310]\n",
      "Step 543386  [5.427 sec/step, loss=0.07442, avg_loss=0.07311]\n",
      "Step 543387  [5.440 sec/step, loss=0.07511, avg_loss=0.07313]\n",
      "Generated 32 batches of size 32 in 2.446 sec\n",
      "Step 543388  [5.457 sec/step, loss=0.07517, avg_loss=0.07314]\n",
      "Step 543389  [5.425 sec/step, loss=0.07525, avg_loss=0.07324]\n",
      "Step 543390  [5.429 sec/step, loss=0.07518, avg_loss=0.07325]\n",
      "Step 543391  [5.424 sec/step, loss=0.07239, avg_loss=0.07323]\n",
      "Step 543392  [5.419 sec/step, loss=0.07395, avg_loss=0.07321]\n",
      "Step 543393  [5.399 sec/step, loss=0.07241, avg_loss=0.07320]\n",
      "Step 543394  [5.404 sec/step, loss=0.07009, avg_loss=0.07317]\n",
      "Step 543395  [5.398 sec/step, loss=0.07327, avg_loss=0.07315]\n",
      "Step 543396  [5.386 sec/step, loss=0.07332, avg_loss=0.07318]\n",
      "Step 543397  [5.367 sec/step, loss=0.07269, avg_loss=0.07315]\n",
      "Step 543398  [5.370 sec/step, loss=0.07537, avg_loss=0.07318]\n",
      "Step 543399  [5.360 sec/step, loss=0.07284, avg_loss=0.07317]\n",
      "Step 543400  [5.364 sec/step, loss=0.07417, avg_loss=0.07318]\n",
      "Writing summary at step: 543400\n",
      "Step 543401  [5.416 sec/step, loss=0.06576, avg_loss=0.07310]\n",
      "Step 543402  [5.411 sec/step, loss=0.07525, avg_loss=0.07312]\n",
      "Step 543403  [5.415 sec/step, loss=0.07229, avg_loss=0.07311]\n",
      "Step 543404  [5.371 sec/step, loss=0.07551, avg_loss=0.07321]\n",
      "Step 543405  [5.383 sec/step, loss=0.07265, avg_loss=0.07320]\n",
      "Step 543406  [5.374 sec/step, loss=0.07352, avg_loss=0.07321]\n",
      "Step 543407  [5.384 sec/step, loss=0.07481, avg_loss=0.07323]\n",
      "Step 543408  [5.411 sec/step, loss=0.07381, avg_loss=0.07330]\n",
      "Step 543409  [5.404 sec/step, loss=0.07396, avg_loss=0.07330]\n",
      "Step 543410  [5.425 sec/step, loss=0.07225, avg_loss=0.07328]\n",
      "Step 543411  [5.413 sec/step, loss=0.06426, avg_loss=0.07320]\n",
      "Step 543412  [5.392 sec/step, loss=0.07472, avg_loss=0.07322]\n",
      "Step 543413  [5.369 sec/step, loss=0.07072, avg_loss=0.07317]\n",
      "Step 543414  [5.373 sec/step, loss=0.07296, avg_loss=0.07314]\n",
      "Step 543415  [5.366 sec/step, loss=0.07348, avg_loss=0.07313]\n",
      "Step 543416  [5.376 sec/step, loss=0.07435, avg_loss=0.07316]\n",
      "Step 543417  [5.379 sec/step, loss=0.07224, avg_loss=0.07314]\n",
      "Step 543418  [5.366 sec/step, loss=0.07448, avg_loss=0.07316]\n",
      "Generated 32 batches of size 32 in 2.565 sec\n",
      "Step 543419  [5.370 sec/step, loss=0.07452, avg_loss=0.07317]\n",
      "Step 543420  [5.381 sec/step, loss=0.07343, avg_loss=0.07315]\n",
      "Step 543421  [5.380 sec/step, loss=0.07021, avg_loss=0.07312]\n",
      "Step 543422  [5.369 sec/step, loss=0.07109, avg_loss=0.07309]\n",
      "Step 543423  [5.358 sec/step, loss=0.07371, avg_loss=0.07308]\n",
      "Step 543424  [5.379 sec/step, loss=0.07275, avg_loss=0.07306]\n",
      "Step 543425  [5.387 sec/step, loss=0.07551, avg_loss=0.07306]\n",
      "Step 543426  [5.371 sec/step, loss=0.07188, avg_loss=0.07303]\n",
      "Step 543427  [5.383 sec/step, loss=0.07511, avg_loss=0.07306]\n",
      "Step 543428  [5.384 sec/step, loss=0.07433, avg_loss=0.07307]\n",
      "Step 543429  [5.384 sec/step, loss=0.07451, avg_loss=0.07306]\n",
      "Step 543430  [5.393 sec/step, loss=0.07257, avg_loss=0.07303]\n",
      "Step 543431  [5.456 sec/step, loss=0.06370, avg_loss=0.07296]\n",
      "Step 543432  [5.464 sec/step, loss=0.07311, avg_loss=0.07299]\n",
      "Step 543433  [5.442 sec/step, loss=0.07343, avg_loss=0.07300]\n",
      "Step 543434  [5.458 sec/step, loss=0.07572, avg_loss=0.07303]\n",
      "Step 543435  [5.444 sec/step, loss=0.07300, avg_loss=0.07301]\n",
      "Step 543436  [5.442 sec/step, loss=0.07303, avg_loss=0.07300]\n",
      "Step 543437  [5.440 sec/step, loss=0.07170, avg_loss=0.07298]\n",
      "Step 543438  [5.457 sec/step, loss=0.07437, avg_loss=0.07300]\n",
      "Step 543439  [5.475 sec/step, loss=0.07505, avg_loss=0.07302]\n",
      "Step 543440  [5.488 sec/step, loss=0.07477, avg_loss=0.07302]\n",
      "Step 543441  [5.493 sec/step, loss=0.07387, avg_loss=0.07306]\n",
      "Step 543442  [5.464 sec/step, loss=0.07307, avg_loss=0.07306]\n",
      "Step 543443  [5.473 sec/step, loss=0.07157, avg_loss=0.07307]\n",
      "Step 543444  [5.463 sec/step, loss=0.06519, avg_loss=0.07300]\n",
      "Step 543445  [5.446 sec/step, loss=0.07041, avg_loss=0.07295]\n",
      "Step 543446  [5.450 sec/step, loss=0.06969, avg_loss=0.07291]\n",
      "Step 543447  [5.443 sec/step, loss=0.07373, avg_loss=0.07289]\n",
      "Step 543448  [5.439 sec/step, loss=0.07515, avg_loss=0.07289]\n",
      "Step 543449  [5.411 sec/step, loss=0.07137, avg_loss=0.07285]\n",
      "Step 543450  [5.405 sec/step, loss=0.07523, avg_loss=0.07285]\n",
      "Generated 32 batches of size 32 in 2.469 sec\n",
      "Step 543451  [5.413 sec/step, loss=0.07556, avg_loss=0.07286]\n",
      "Step 543452  [5.409 sec/step, loss=0.07392, avg_loss=0.07287]\n",
      "Step 543453  [5.424 sec/step, loss=0.07375, avg_loss=0.07285]\n",
      "Step 543454  [5.423 sec/step, loss=0.07102, avg_loss=0.07283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 543455  [5.425 sec/step, loss=0.07166, avg_loss=0.07281]\n",
      "Step 543456  [5.438 sec/step, loss=0.07452, avg_loss=0.07281]\n",
      "Step 543457  [5.442 sec/step, loss=0.07439, avg_loss=0.07281]\n",
      "Step 543458  [5.461 sec/step, loss=0.07398, avg_loss=0.07284]\n",
      "Step 543459  [5.457 sec/step, loss=0.07406, avg_loss=0.07284]\n",
      "Step 543460  [5.470 sec/step, loss=0.07193, avg_loss=0.07290]\n",
      "Step 543461  [5.483 sec/step, loss=0.07288, avg_loss=0.07292]\n",
      "Step 543462  [5.497 sec/step, loss=0.07458, avg_loss=0.07294]\n",
      "Step 543463  [5.511 sec/step, loss=0.07557, avg_loss=0.07295]\n",
      "Step 543464  [5.459 sec/step, loss=0.07448, avg_loss=0.07304]\n",
      "Step 543465  [5.461 sec/step, loss=0.07147, avg_loss=0.07310]\n",
      "Step 543466  [5.474 sec/step, loss=0.07353, avg_loss=0.07309]\n",
      "Step 543467  [5.477 sec/step, loss=0.07151, avg_loss=0.07306]\n",
      "Step 543468  [5.477 sec/step, loss=0.07166, avg_loss=0.07304]\n",
      "Step 543469  [5.430 sec/step, loss=0.07508, avg_loss=0.07313]\n",
      "Step 543470  [5.438 sec/step, loss=0.07412, avg_loss=0.07314]\n",
      "Step 543471  [5.445 sec/step, loss=0.07534, avg_loss=0.07315]\n",
      "Step 543472  [5.461 sec/step, loss=0.07383, avg_loss=0.07318]\n",
      "Step 543473  [5.462 sec/step, loss=0.07473, avg_loss=0.07320]\n",
      "Step 543474  [5.463 sec/step, loss=0.07330, avg_loss=0.07320]\n",
      "Step 543475  [5.426 sec/step, loss=0.07317, avg_loss=0.07321]\n",
      "Step 543476  [5.434 sec/step, loss=0.07523, avg_loss=0.07321]\n",
      "Step 543477  [5.469 sec/step, loss=0.07301, avg_loss=0.07321]\n",
      "Step 543478  [5.468 sec/step, loss=0.07422, avg_loss=0.07322]\n",
      "Step 543479  [5.463 sec/step, loss=0.07095, avg_loss=0.07320]\n",
      "Step 543480  [5.471 sec/step, loss=0.07263, avg_loss=0.07322]\n",
      "Step 543481  [5.511 sec/step, loss=0.06714, avg_loss=0.07313]\n",
      "Step 543482  [5.500 sec/step, loss=0.07434, avg_loss=0.07313]\n",
      "Generated 32 batches of size 32 in 2.567 sec\n",
      "Step 543483  [5.490 sec/step, loss=0.07332, avg_loss=0.07310]\n",
      "Step 543484  [5.499 sec/step, loss=0.07391, avg_loss=0.07310]\n",
      "Step 543485  [5.488 sec/step, loss=0.07333, avg_loss=0.07308]\n",
      "Step 543486  [5.484 sec/step, loss=0.07372, avg_loss=0.07307]\n",
      "Step 543487  [5.489 sec/step, loss=0.07420, avg_loss=0.07306]\n",
      "Step 543488  [5.494 sec/step, loss=0.07310, avg_loss=0.07304]\n",
      "Step 543489  [5.478 sec/step, loss=0.07128, avg_loss=0.07300]\n",
      "Step 543490  [5.478 sec/step, loss=0.07464, avg_loss=0.07299]\n",
      "Step 543491  [5.466 sec/step, loss=0.06532, avg_loss=0.07292]\n",
      "Step 543492  [5.459 sec/step, loss=0.07156, avg_loss=0.07290]\n",
      "Step 543493  [5.468 sec/step, loss=0.07496, avg_loss=0.07292]\n",
      "Step 543494  [5.457 sec/step, loss=0.07088, avg_loss=0.07293]\n",
      "Step 543495  [5.459 sec/step, loss=0.07488, avg_loss=0.07295]\n",
      "Step 543496  [5.465 sec/step, loss=0.07328, avg_loss=0.07295]\n",
      "Step 543497  [5.500 sec/step, loss=0.07231, avg_loss=0.07294]\n",
      "Step 543498  [5.489 sec/step, loss=0.07436, avg_loss=0.07293]\n",
      "Step 543499  [5.493 sec/step, loss=0.07206, avg_loss=0.07293]\n",
      "Step 543500  [5.503 sec/step, loss=0.07423, avg_loss=0.07293]\n",
      "Writing summary at step: 543500\n",
      "Step 543501  [5.452 sec/step, loss=0.07433, avg_loss=0.07301]\n",
      "Step 543502  [5.457 sec/step, loss=0.07491, avg_loss=0.07301]\n",
      "Step 543503  [5.460 sec/step, loss=0.07431, avg_loss=0.07303]\n",
      "Step 543504  [5.465 sec/step, loss=0.07484, avg_loss=0.07302]\n",
      "Step 543505  [5.469 sec/step, loss=0.07487, avg_loss=0.07304]\n",
      "Step 543506  [5.460 sec/step, loss=0.07186, avg_loss=0.07303]\n",
      "Step 543507  [5.451 sec/step, loss=0.07298, avg_loss=0.07301]\n",
      "Step 543508  [5.423 sec/step, loss=0.06482, avg_loss=0.07292]\n",
      "Step 543509  [5.441 sec/step, loss=0.07483, avg_loss=0.07293]\n",
      "Step 543510  [5.435 sec/step, loss=0.07444, avg_loss=0.07295]\n",
      "Step 543511  [5.440 sec/step, loss=0.07325, avg_loss=0.07304]\n",
      "Step 543512  [5.424 sec/step, loss=0.07006, avg_loss=0.07299]\n",
      "Step 543513  [5.447 sec/step, loss=0.07606, avg_loss=0.07305]\n",
      "Generated 32 batches of size 32 in 2.545 sec\n",
      "Step 543514  [5.432 sec/step, loss=0.07330, avg_loss=0.07305]\n",
      "Step 543515  [5.453 sec/step, loss=0.07454, avg_loss=0.07306]\n",
      "Step 543516  [5.445 sec/step, loss=0.07442, avg_loss=0.07306]\n",
      "Step 543517  [5.450 sec/step, loss=0.07334, avg_loss=0.07307]\n",
      "Step 543518  [5.461 sec/step, loss=0.07543, avg_loss=0.07308]\n",
      "Step 543519  [5.456 sec/step, loss=0.07470, avg_loss=0.07308]\n",
      "Step 543520  [5.447 sec/step, loss=0.07050, avg_loss=0.07306]\n",
      "Step 543521  [5.447 sec/step, loss=0.07464, avg_loss=0.07310]\n",
      "Step 543522  [5.502 sec/step, loss=0.06578, avg_loss=0.07305]\n",
      "Step 543523  [5.515 sec/step, loss=0.07494, avg_loss=0.07306]\n",
      "Step 543524  [5.481 sec/step, loss=0.07192, avg_loss=0.07305]\n",
      "Step 543525  [5.469 sec/step, loss=0.07351, avg_loss=0.07303]\n",
      "Step 543526  [5.481 sec/step, loss=0.07296, avg_loss=0.07304]\n",
      "Step 543527  [5.474 sec/step, loss=0.07414, avg_loss=0.07303]\n",
      "Step 543528  [5.474 sec/step, loss=0.07268, avg_loss=0.07302]\n",
      "Step 543529  [5.445 sec/step, loss=0.06515, avg_loss=0.07292]\n",
      "Step 543530  [5.429 sec/step, loss=0.07130, avg_loss=0.07291]\n",
      "Step 543531  [5.384 sec/step, loss=0.07549, avg_loss=0.07303]\n",
      "Step 543532  [5.401 sec/step, loss=0.07563, avg_loss=0.07305]\n",
      "Step 543533  [5.409 sec/step, loss=0.07525, avg_loss=0.07307]\n",
      "Step 543534  [5.425 sec/step, loss=0.07177, avg_loss=0.07303]\n",
      "Step 543535  [5.441 sec/step, loss=0.07372, avg_loss=0.07304]\n",
      "Step 543536  [5.436 sec/step, loss=0.07445, avg_loss=0.07305]\n",
      "Step 543537  [5.446 sec/step, loss=0.07356, avg_loss=0.07307]\n",
      "Step 543538  [5.432 sec/step, loss=0.07325, avg_loss=0.07306]\n",
      "Step 543539  [5.412 sec/step, loss=0.07033, avg_loss=0.07301]\n",
      "Step 543540  [5.401 sec/step, loss=0.07253, avg_loss=0.07299]\n",
      "Step 543541  [5.417 sec/step, loss=0.07425, avg_loss=0.07299]\n",
      "Step 543542  [5.425 sec/step, loss=0.07360, avg_loss=0.07300]\n",
      "Step 543543  [5.423 sec/step, loss=0.07319, avg_loss=0.07302]\n",
      "Step 543544  [5.431 sec/step, loss=0.07337, avg_loss=0.07310]\n",
      "Step 543545  [5.440 sec/step, loss=0.06945, avg_loss=0.07309]\n",
      "Generated 32 batches of size 32 in 2.931 sec\n",
      "Step 543546  [5.437 sec/step, loss=0.07046, avg_loss=0.07309]\n",
      "Step 543547  [5.489 sec/step, loss=0.06498, avg_loss=0.07301]\n",
      "Step 543548  [5.483 sec/step, loss=0.07444, avg_loss=0.07300]\n",
      "Step 543549  [5.502 sec/step, loss=0.07455, avg_loss=0.07303]\n",
      "Step 543550  [5.503 sec/step, loss=0.07530, avg_loss=0.07303]\n",
      "Step 543551  [5.485 sec/step, loss=0.07362, avg_loss=0.07301]\n",
      "Step 543552  [5.477 sec/step, loss=0.07444, avg_loss=0.07302]\n",
      "Step 543553  [5.463 sec/step, loss=0.07513, avg_loss=0.07303]\n",
      "Step 543554  [5.477 sec/step, loss=0.07544, avg_loss=0.07308]\n",
      "Step 543555  [5.470 sec/step, loss=0.07202, avg_loss=0.07308]\n",
      "Step 543556  [5.453 sec/step, loss=0.07467, avg_loss=0.07308]\n",
      "Step 543557  [5.432 sec/step, loss=0.07306, avg_loss=0.07307]\n",
      "Step 543558  [5.423 sec/step, loss=0.07274, avg_loss=0.07306]\n",
      "Step 543559  [5.416 sec/step, loss=0.07298, avg_loss=0.07304]\n",
      "Step 543560  [5.469 sec/step, loss=0.06462, avg_loss=0.07297]\n",
      "Step 543561  [5.465 sec/step, loss=0.07425, avg_loss=0.07299]\n",
      "Step 543562  [5.475 sec/step, loss=0.07472, avg_loss=0.07299]\n",
      "Step 543563  [5.470 sec/step, loss=0.07405, avg_loss=0.07297]\n",
      "Step 543564  [5.484 sec/step, loss=0.07544, avg_loss=0.07298]\n",
      "Step 543565  [5.514 sec/step, loss=0.07473, avg_loss=0.07301]\n",
      "Step 543566  [5.530 sec/step, loss=0.07346, avg_loss=0.07301]\n",
      "Step 543567  [5.524 sec/step, loss=0.07024, avg_loss=0.07300]\n",
      "Step 543568  [5.533 sec/step, loss=0.07394, avg_loss=0.07302]\n",
      "Step 543569  [5.534 sec/step, loss=0.07493, avg_loss=0.07302]\n",
      "Step 543570  [5.522 sec/step, loss=0.07343, avg_loss=0.07301]\n",
      "Step 543571  [5.520 sec/step, loss=0.07528, avg_loss=0.07301]\n",
      "Step 543572  [5.516 sec/step, loss=0.07336, avg_loss=0.07301]\n",
      "Step 543573  [5.502 sec/step, loss=0.07139, avg_loss=0.07298]\n",
      "Step 543574  [5.495 sec/step, loss=0.07276, avg_loss=0.07297]\n",
      "Step 543575  [5.489 sec/step, loss=0.06544, avg_loss=0.07289]\n",
      "Step 543576  [5.479 sec/step, loss=0.07440, avg_loss=0.07289]\n",
      "Step 543577  [5.459 sec/step, loss=0.07430, avg_loss=0.07290]\n",
      "Generated 32 batches of size 32 in 2.484 sec\n",
      "Step 543578  [5.474 sec/step, loss=0.07558, avg_loss=0.07291]\n",
      "Step 543579  [5.471 sec/step, loss=0.07184, avg_loss=0.07292]\n",
      "Step 543580  [5.476 sec/step, loss=0.07439, avg_loss=0.07294]\n",
      "Step 543581  [5.426 sec/step, loss=0.07424, avg_loss=0.07301]\n",
      "Step 543582  [5.431 sec/step, loss=0.07557, avg_loss=0.07302]\n",
      "Step 543583  [5.436 sec/step, loss=0.07471, avg_loss=0.07304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 543584  [5.431 sec/step, loss=0.07234, avg_loss=0.07302]\n",
      "Step 543585  [5.443 sec/step, loss=0.07507, avg_loss=0.07304]\n",
      "Step 543586  [5.459 sec/step, loss=0.07437, avg_loss=0.07304]\n",
      "Step 543587  [5.448 sec/step, loss=0.07428, avg_loss=0.07304]\n",
      "Step 543588  [5.425 sec/step, loss=0.07305, avg_loss=0.07304]\n",
      "Step 543589  [5.421 sec/step, loss=0.07116, avg_loss=0.07304]\n",
      "Step 543590  [5.409 sec/step, loss=0.07314, avg_loss=0.07303]\n",
      "Step 543591  [5.429 sec/step, loss=0.07401, avg_loss=0.07311]\n",
      "Step 543592  [5.443 sec/step, loss=0.07365, avg_loss=0.07314]\n",
      "Step 543593  [5.449 sec/step, loss=0.07438, avg_loss=0.07313]\n",
      "Step 543594  [5.488 sec/step, loss=0.07339, avg_loss=0.07315]\n",
      "Step 543595  [5.486 sec/step, loss=0.07529, avg_loss=0.07316]\n",
      "Step 543596  [5.497 sec/step, loss=0.07546, avg_loss=0.07318]\n",
      "Step 543597  [5.488 sec/step, loss=0.07379, avg_loss=0.07320]\n",
      "Step 543598  [5.502 sec/step, loss=0.07251, avg_loss=0.07318]\n",
      "Step 543599  [5.514 sec/step, loss=0.07526, avg_loss=0.07321]\n",
      "Step 543600  [5.507 sec/step, loss=0.07405, avg_loss=0.07321]\n",
      "Writing summary at step: 543600\n",
      "Step 543601  [5.497 sec/step, loss=0.07059, avg_loss=0.07317]\n",
      "Step 543602  [5.483 sec/step, loss=0.07348, avg_loss=0.07316]\n",
      "Step 543603  [5.482 sec/step, loss=0.07494, avg_loss=0.07316]\n",
      "Step 543604  [5.492 sec/step, loss=0.07396, avg_loss=0.07315]\n",
      "Step 543605  [5.471 sec/step, loss=0.07059, avg_loss=0.07311]\n",
      "Step 543606  [5.476 sec/step, loss=0.07540, avg_loss=0.07315]\n",
      "Step 543607  [5.496 sec/step, loss=0.07343, avg_loss=0.07315]\n",
      "Step 543608  [5.519 sec/step, loss=0.07251, avg_loss=0.07323]\n",
      "Generated 32 batches of size 32 in 2.409 sec\n",
      "Step 543609  [5.510 sec/step, loss=0.07192, avg_loss=0.07320]\n",
      "Step 543610  [5.484 sec/step, loss=0.07347, avg_loss=0.07319]\n",
      "Step 543611  [5.488 sec/step, loss=0.07340, avg_loss=0.07319]\n",
      "Step 543612  [5.499 sec/step, loss=0.07403, avg_loss=0.07323]\n",
      "Step 543613  [5.541 sec/step, loss=0.06800, avg_loss=0.07315]\n",
      "Step 543614  [5.546 sec/step, loss=0.07204, avg_loss=0.07314]\n",
      "Step 543615  [5.540 sec/step, loss=0.07335, avg_loss=0.07312]\n",
      "Step 543616  [5.527 sec/step, loss=0.06543, avg_loss=0.07303]\n",
      "Step 543617  [5.525 sec/step, loss=0.07271, avg_loss=0.07303]\n",
      "Step 543618  [5.520 sec/step, loss=0.07082, avg_loss=0.07298]\n",
      "Step 543619  [5.513 sec/step, loss=0.07109, avg_loss=0.07295]\n",
      "Step 543620  [5.554 sec/step, loss=0.06999, avg_loss=0.07294]\n",
      "Step 543621  [5.561 sec/step, loss=0.07429, avg_loss=0.07294]\n",
      "Step 543622  [5.520 sec/step, loss=0.07550, avg_loss=0.07303]\n",
      "Step 543623  [5.518 sec/step, loss=0.07499, avg_loss=0.07304]\n",
      "Step 543624  [5.525 sec/step, loss=0.07286, avg_loss=0.07304]\n",
      "Step 543625  [5.517 sec/step, loss=0.07280, avg_loss=0.07304]\n",
      "Step 543626  [5.515 sec/step, loss=0.07579, avg_loss=0.07307]\n",
      "Step 543627  [5.515 sec/step, loss=0.07397, avg_loss=0.07306]\n",
      "Step 543628  [5.522 sec/step, loss=0.07528, avg_loss=0.07309]\n",
      "Step 543629  [5.542 sec/step, loss=0.07341, avg_loss=0.07317]\n",
      "Step 543630  [5.533 sec/step, loss=0.07059, avg_loss=0.07317]\n",
      "Step 543631  [5.510 sec/step, loss=0.06487, avg_loss=0.07306]\n",
      "Step 543632  [5.506 sec/step, loss=0.07511, avg_loss=0.07305]\n",
      "Step 543633  [5.516 sec/step, loss=0.07481, avg_loss=0.07305]\n",
      "Step 543634  [5.490 sec/step, loss=0.07507, avg_loss=0.07308]\n",
      "Step 543635  [5.490 sec/step, loss=0.07510, avg_loss=0.07310]\n",
      "Step 543636  [5.520 sec/step, loss=0.07154, avg_loss=0.07307]\n",
      "Step 543637  [5.513 sec/step, loss=0.07356, avg_loss=0.07307]\n",
      "Step 543638  [5.527 sec/step, loss=0.07526, avg_loss=0.07309]\n",
      "Step 543639  [5.540 sec/step, loss=0.07410, avg_loss=0.07313]\n",
      "Step 543640  [5.543 sec/step, loss=0.07450, avg_loss=0.07315]\n",
      "Generated 32 batches of size 32 in 2.803 sec\n",
      "Step 543641  [5.527 sec/step, loss=0.07480, avg_loss=0.07315]\n",
      "Step 543642  [5.526 sec/step, loss=0.07453, avg_loss=0.07316]\n",
      "Step 543643  [5.524 sec/step, loss=0.07286, avg_loss=0.07316]\n",
      "Step 543644  [5.526 sec/step, loss=0.07047, avg_loss=0.07313]\n",
      "Step 543645  [5.525 sec/step, loss=0.07227, avg_loss=0.07316]\n",
      "Step 543646  [5.545 sec/step, loss=0.07245, avg_loss=0.07318]\n",
      "Step 543647  [5.483 sec/step, loss=0.07339, avg_loss=0.07326]\n",
      "Step 543648  [5.495 sec/step, loss=0.07504, avg_loss=0.07327]\n",
      "Step 543649  [5.487 sec/step, loss=0.07302, avg_loss=0.07325]\n",
      "Step 543650  [5.479 sec/step, loss=0.07267, avg_loss=0.07322]\n",
      "Step 543651  [5.469 sec/step, loss=0.06566, avg_loss=0.07314]\n",
      "Step 543652  [5.471 sec/step, loss=0.07349, avg_loss=0.07314]\n",
      "Step 543653  [5.483 sec/step, loss=0.07464, avg_loss=0.07313]\n",
      "Step 543654  [5.480 sec/step, loss=0.07287, avg_loss=0.07310]\n",
      "Step 543655  [5.476 sec/step, loss=0.07390, avg_loss=0.07312]\n",
      "Step 543656  [5.460 sec/step, loss=0.07121, avg_loss=0.07309]\n",
      "Step 543657  [5.476 sec/step, loss=0.07607, avg_loss=0.07312]\n",
      "Step 543658  [5.491 sec/step, loss=0.07575, avg_loss=0.07315]\n",
      "Step 543659  [5.511 sec/step, loss=0.07567, avg_loss=0.07318]\n",
      "Step 543660  [5.457 sec/step, loss=0.07421, avg_loss=0.07327]\n",
      "Step 543661  [5.464 sec/step, loss=0.07282, avg_loss=0.07326]\n",
      "Step 543662  [5.455 sec/step, loss=0.07324, avg_loss=0.07324]\n",
      "Step 543663  [5.455 sec/step, loss=0.07342, avg_loss=0.07324]\n",
      "Step 543664  [5.439 sec/step, loss=0.07292, avg_loss=0.07321]\n",
      "Step 543665  [5.428 sec/step, loss=0.07263, avg_loss=0.07319]\n",
      "Step 543666  [5.403 sec/step, loss=0.07332, avg_loss=0.07319]\n",
      "Step 543667  [5.407 sec/step, loss=0.07556, avg_loss=0.07324]\n",
      "Step 543668  [5.407 sec/step, loss=0.07380, avg_loss=0.07324]\n",
      "Step 543669  [5.406 sec/step, loss=0.07452, avg_loss=0.07324]\n",
      "Step 543670  [5.399 sec/step, loss=0.07398, avg_loss=0.07324]\n",
      "Step 543671  [5.378 sec/step, loss=0.07047, avg_loss=0.07319]\n",
      "Step 543672  [5.375 sec/step, loss=0.07286, avg_loss=0.07319]\n",
      "Generated 32 batches of size 32 in 2.556 sec\n",
      "Step 543673  [5.379 sec/step, loss=0.07366, avg_loss=0.07321]\n",
      "Step 543674  [5.383 sec/step, loss=0.07020, avg_loss=0.07319]\n",
      "Step 543675  [5.409 sec/step, loss=0.07333, avg_loss=0.07326]\n",
      "Step 543676  [5.405 sec/step, loss=0.07134, avg_loss=0.07323]\n",
      "Step 543677  [5.448 sec/step, loss=0.06488, avg_loss=0.07314]\n",
      "Step 543678  [5.444 sec/step, loss=0.07502, avg_loss=0.07313]\n",
      "Step 543679  [5.473 sec/step, loss=0.07472, avg_loss=0.07316]\n",
      "Step 543680  [5.481 sec/step, loss=0.07467, avg_loss=0.07317]\n",
      "Step 543681  [5.480 sec/step, loss=0.07425, avg_loss=0.07317]\n",
      "Step 543682  [5.475 sec/step, loss=0.07477, avg_loss=0.07316]\n",
      "Step 543683  [5.478 sec/step, loss=0.07436, avg_loss=0.07315]\n",
      "Step 543684  [5.483 sec/step, loss=0.07431, avg_loss=0.07317]\n",
      "Step 543685  [5.476 sec/step, loss=0.07449, avg_loss=0.07317]\n",
      "Step 543686  [5.516 sec/step, loss=0.06701, avg_loss=0.07310]\n",
      "Step 543687  [5.503 sec/step, loss=0.07055, avg_loss=0.07306]\n",
      "Step 543688  [5.517 sec/step, loss=0.07515, avg_loss=0.07308]\n",
      "Step 543689  [5.531 sec/step, loss=0.07513, avg_loss=0.07312]\n",
      "Step 543690  [5.528 sec/step, loss=0.07346, avg_loss=0.07312]\n",
      "Step 543691  [5.534 sec/step, loss=0.07582, avg_loss=0.07314]\n",
      "Step 543692  [5.526 sec/step, loss=0.07519, avg_loss=0.07316]\n",
      "Step 543693  [5.508 sec/step, loss=0.07410, avg_loss=0.07315]\n",
      "Step 543694  [5.497 sec/step, loss=0.07508, avg_loss=0.07317]\n",
      "Step 543695  [5.492 sec/step, loss=0.07419, avg_loss=0.07316]\n",
      "Step 543696  [5.495 sec/step, loss=0.07519, avg_loss=0.07316]\n",
      "Step 543697  [5.505 sec/step, loss=0.07316, avg_loss=0.07315]\n",
      "Step 543698  [5.508 sec/step, loss=0.07531, avg_loss=0.07318]\n",
      "Step 543699  [5.490 sec/step, loss=0.07411, avg_loss=0.07317]\n",
      "Step 543700  [5.504 sec/step, loss=0.07282, avg_loss=0.07315]\n",
      "Writing summary at step: 543700\n",
      "Step 543701  [5.521 sec/step, loss=0.07521, avg_loss=0.07320]\n",
      "Step 543702  [5.518 sec/step, loss=0.07503, avg_loss=0.07322]\n",
      "Step 543703  [5.501 sec/step, loss=0.07083, avg_loss=0.07317]\n",
      "Generated 32 batches of size 32 in 3.206 sec\n",
      "Step 543704  [5.465 sec/step, loss=0.06548, avg_loss=0.07309]\n",
      "Step 543705  [5.491 sec/step, loss=0.07577, avg_loss=0.07314]\n",
      "Step 543706  [5.482 sec/step, loss=0.07299, avg_loss=0.07312]\n",
      "Step 543707  [5.470 sec/step, loss=0.07366, avg_loss=0.07312]\n",
      "Step 543708  [5.458 sec/step, loss=0.07319, avg_loss=0.07313]\n",
      "Step 543709  [5.447 sec/step, loss=0.07121, avg_loss=0.07312]\n",
      "Step 543710  [5.469 sec/step, loss=0.07570, avg_loss=0.07314]\n",
      "Step 543711  [5.474 sec/step, loss=0.07462, avg_loss=0.07315]\n",
      "Step 543712  [5.471 sec/step, loss=0.07262, avg_loss=0.07314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 543713  [5.410 sec/step, loss=0.07352, avg_loss=0.07319]\n",
      "Step 543714  [5.407 sec/step, loss=0.07174, avg_loss=0.07319]\n",
      "Step 543715  [5.411 sec/step, loss=0.07602, avg_loss=0.07322]\n",
      "Step 543716  [5.454 sec/step, loss=0.07508, avg_loss=0.07331]\n",
      "Step 543717  [5.455 sec/step, loss=0.07214, avg_loss=0.07331]\n",
      "Step 543718  [5.458 sec/step, loss=0.07414, avg_loss=0.07334]\n",
      "Step 543719  [5.469 sec/step, loss=0.07557, avg_loss=0.07339]\n",
      "Step 543720  [5.422 sec/step, loss=0.07313, avg_loss=0.07342]\n",
      "Step 543721  [5.430 sec/step, loss=0.07515, avg_loss=0.07343]\n",
      "Step 543722  [5.435 sec/step, loss=0.07405, avg_loss=0.07341]\n",
      "Step 543723  [5.420 sec/step, loss=0.07355, avg_loss=0.07340]\n",
      "Step 543724  [5.422 sec/step, loss=0.07506, avg_loss=0.07342]\n",
      "Step 543725  [5.440 sec/step, loss=0.07381, avg_loss=0.07343]\n",
      "Step 543726  [5.432 sec/step, loss=0.07252, avg_loss=0.07340]\n",
      "Step 543727  [5.437 sec/step, loss=0.07443, avg_loss=0.07340]\n",
      "Step 543728  [5.412 sec/step, loss=0.07151, avg_loss=0.07336]\n",
      "Step 543729  [5.421 sec/step, loss=0.07563, avg_loss=0.07339]\n",
      "Step 543730  [5.425 sec/step, loss=0.07032, avg_loss=0.07338]\n",
      "Step 543731  [5.425 sec/step, loss=0.06509, avg_loss=0.07339]\n",
      "Step 543732  [5.422 sec/step, loss=0.07484, avg_loss=0.07338]\n",
      "Step 543733  [5.409 sec/step, loss=0.07394, avg_loss=0.07337]\n",
      "Step 543734  [5.428 sec/step, loss=0.07228, avg_loss=0.07335]\n",
      "Step 543735  [5.414 sec/step, loss=0.07436, avg_loss=0.07334]\n",
      "Generated 32 batches of size 32 in 2.416 sec\n",
      "Step 543736  [5.397 sec/step, loss=0.07455, avg_loss=0.07337]\n",
      "Step 543737  [5.400 sec/step, loss=0.07179, avg_loss=0.07335]\n",
      "Step 543738  [5.399 sec/step, loss=0.07591, avg_loss=0.07336]\n",
      "Step 543739  [5.399 sec/step, loss=0.07397, avg_loss=0.07336]\n",
      "Step 543740  [5.393 sec/step, loss=0.07457, avg_loss=0.07336]\n",
      "Step 543741  [5.397 sec/step, loss=0.07523, avg_loss=0.07336]\n",
      "Step 543742  [5.392 sec/step, loss=0.07343, avg_loss=0.07335]\n",
      "Step 543743  [5.401 sec/step, loss=0.07453, avg_loss=0.07337]\n",
      "Step 543744  [5.461 sec/step, loss=0.06662, avg_loss=0.07333]\n",
      "Step 543745  [5.470 sec/step, loss=0.07500, avg_loss=0.07336]\n",
      "Step 543746  [5.463 sec/step, loss=0.07580, avg_loss=0.07339]\n",
      "Step 543747  [5.490 sec/step, loss=0.07518, avg_loss=0.07341]\n",
      "Step 543748  [5.484 sec/step, loss=0.07396, avg_loss=0.07340]\n",
      "Step 543749  [5.497 sec/step, loss=0.07605, avg_loss=0.07343]\n",
      "Step 543750  [5.504 sec/step, loss=0.07563, avg_loss=0.07346]\n",
      "Step 543751  [5.520 sec/step, loss=0.07405, avg_loss=0.07354]\n",
      "Step 543752  [5.523 sec/step, loss=0.07484, avg_loss=0.07355]\n",
      "Step 543753  [5.495 sec/step, loss=0.07344, avg_loss=0.07354]\n",
      "Step 543754  [5.470 sec/step, loss=0.07102, avg_loss=0.07352]\n",
      "Step 543755  [5.490 sec/step, loss=0.07411, avg_loss=0.07353]\n",
      "Step 543756  [5.512 sec/step, loss=0.07568, avg_loss=0.07357]\n",
      "Step 543757  [5.496 sec/step, loss=0.07425, avg_loss=0.07355]\n",
      "Step 543758  [5.496 sec/step, loss=0.07590, avg_loss=0.07355]\n",
      "Step 543759  [5.536 sec/step, loss=0.06667, avg_loss=0.07346]\n",
      "Step 543760  [5.553 sec/step, loss=0.07507, avg_loss=0.07347]\n",
      "Step 543761  [5.524 sec/step, loss=0.07148, avg_loss=0.07346]\n",
      "Step 543762  [5.512 sec/step, loss=0.07546, avg_loss=0.07348]\n",
      "Step 543763  [5.514 sec/step, loss=0.07581, avg_loss=0.07351]\n",
      "Step 543764  [5.519 sec/step, loss=0.07408, avg_loss=0.07352]\n",
      "Step 543765  [5.511 sec/step, loss=0.07349, avg_loss=0.07353]\n",
      "Step 543766  [5.523 sec/step, loss=0.07596, avg_loss=0.07355]\n",
      "Step 543767  [5.518 sec/step, loss=0.07016, avg_loss=0.07350]\n",
      "Generated 32 batches of size 32 in 2.544 sec\n",
      "Step 543768  [5.510 sec/step, loss=0.07463, avg_loss=0.07351]\n",
      "Step 543769  [5.520 sec/step, loss=0.07614, avg_loss=0.07352]\n",
      "Step 543770  [5.515 sec/step, loss=0.07363, avg_loss=0.07352]\n",
      "Step 543771  [5.555 sec/step, loss=0.07285, avg_loss=0.07354]\n",
      "Step 543772  [5.555 sec/step, loss=0.07464, avg_loss=0.07356]\n",
      "Step 543773  [5.545 sec/step, loss=0.06579, avg_loss=0.07348]\n",
      "Step 543774  [5.568 sec/step, loss=0.07542, avg_loss=0.07353]\n",
      "Step 543775  [5.570 sec/step, loss=0.07576, avg_loss=0.07356]\n",
      "Step 543776  [5.577 sec/step, loss=0.07412, avg_loss=0.07359]\n",
      "Step 543777  [5.523 sec/step, loss=0.07308, avg_loss=0.07367]\n",
      "Step 543778  [5.519 sec/step, loss=0.07469, avg_loss=0.07367]\n",
      "Step 543779  [5.501 sec/step, loss=0.07286, avg_loss=0.07365]\n",
      "Step 543780  [5.507 sec/step, loss=0.07528, avg_loss=0.07365]\n",
      "Step 543781  [5.512 sec/step, loss=0.07513, avg_loss=0.07366]\n",
      "Step 543782  [5.502 sec/step, loss=0.07305, avg_loss=0.07364]\n",
      "Step 543783  [5.495 sec/step, loss=0.07449, avg_loss=0.07365]\n",
      "Step 543784  [5.502 sec/step, loss=0.07272, avg_loss=0.07363]\n",
      "Step 543785  [5.500 sec/step, loss=0.07168, avg_loss=0.07360]\n",
      "Step 543786  [5.445 sec/step, loss=0.07432, avg_loss=0.07367]\n",
      "Step 543787  [5.458 sec/step, loss=0.07488, avg_loss=0.07372]\n",
      "Step 543788  [5.458 sec/step, loss=0.07605, avg_loss=0.07373]\n",
      "Step 543789  [5.496 sec/step, loss=0.06574, avg_loss=0.07363]\n",
      "Step 543790  [5.513 sec/step, loss=0.07398, avg_loss=0.07364]\n",
      "Step 543791  [5.503 sec/step, loss=0.07413, avg_loss=0.07362]\n",
      "Step 543792  [5.490 sec/step, loss=0.07374, avg_loss=0.07361]\n",
      "Step 543793  [5.506 sec/step, loss=0.07400, avg_loss=0.07361]\n",
      "Step 543794  [5.504 sec/step, loss=0.07526, avg_loss=0.07361]\n",
      "Step 543795  [5.515 sec/step, loss=0.07550, avg_loss=0.07362]\n",
      "Step 543796  [5.505 sec/step, loss=0.07250, avg_loss=0.07359]\n",
      "Step 543797  [5.461 sec/step, loss=0.06610, avg_loss=0.07352]\n",
      "Step 543798  [5.448 sec/step, loss=0.07419, avg_loss=0.07351]\n",
      "Step 543799  [5.477 sec/step, loss=0.07224, avg_loss=0.07349]\n",
      "Generated 32 batches of size 32 in 2.551 sec\n",
      "Step 543800  [5.484 sec/step, loss=0.07540, avg_loss=0.07352]\n",
      "Writing summary at step: 543800\n",
      "Step 543801  [5.471 sec/step, loss=0.07505, avg_loss=0.07352]\n",
      "Step 543802  [5.475 sec/step, loss=0.07304, avg_loss=0.07350]\n",
      "Step 543803  [5.495 sec/step, loss=0.07408, avg_loss=0.07353]\n",
      "Step 543804  [5.515 sec/step, loss=0.07376, avg_loss=0.07361]\n",
      "Step 543805  [5.511 sec/step, loss=0.07557, avg_loss=0.07361]\n",
      "Step 543806  [5.539 sec/step, loss=0.07274, avg_loss=0.07361]\n",
      "Step 543807  [5.525 sec/step, loss=0.07129, avg_loss=0.07358]\n",
      "Step 543808  [5.523 sec/step, loss=0.07359, avg_loss=0.07359]\n",
      "Step 543809  [5.577 sec/step, loss=0.06461, avg_loss=0.07352]\n",
      "Step 543810  [5.568 sec/step, loss=0.07490, avg_loss=0.07351]\n",
      "Step 543811  [5.571 sec/step, loss=0.07359, avg_loss=0.07350]\n",
      "Step 543812  [5.577 sec/step, loss=0.07425, avg_loss=0.07352]\n",
      "Step 543813  [5.591 sec/step, loss=0.07441, avg_loss=0.07353]\n",
      "Step 543814  [5.582 sec/step, loss=0.06971, avg_loss=0.07351]\n",
      "Step 543815  [5.583 sec/step, loss=0.07281, avg_loss=0.07348]\n",
      "Step 543816  [5.572 sec/step, loss=0.07549, avg_loss=0.07348]\n",
      "Step 543817  [5.562 sec/step, loss=0.07112, avg_loss=0.07347]\n",
      "Step 543818  [5.557 sec/step, loss=0.07268, avg_loss=0.07346]\n",
      "Step 543819  [5.564 sec/step, loss=0.07313, avg_loss=0.07343]\n",
      "Step 543820  [5.555 sec/step, loss=0.06468, avg_loss=0.07335]\n",
      "Step 543821  [5.555 sec/step, loss=0.07570, avg_loss=0.07335]\n",
      "Step 543822  [5.540 sec/step, loss=0.07479, avg_loss=0.07336]\n",
      "Step 543823  [5.558 sec/step, loss=0.07585, avg_loss=0.07338]\n",
      "Step 543824  [5.574 sec/step, loss=0.07509, avg_loss=0.07338]\n",
      "Step 543825  [5.571 sec/step, loss=0.07540, avg_loss=0.07340]\n",
      "Step 543826  [5.582 sec/step, loss=0.07353, avg_loss=0.07341]\n",
      "Step 543827  [5.571 sec/step, loss=0.07056, avg_loss=0.07337]\n",
      "Step 543828  [5.593 sec/step, loss=0.07396, avg_loss=0.07340]\n",
      "Step 543829  [5.572 sec/step, loss=0.07413, avg_loss=0.07338]\n",
      "Step 543830  [5.588 sec/step, loss=0.07435, avg_loss=0.07342]\n",
      "Generated 32 batches of size 32 in 2.631 sec\n",
      "Step 543831  [5.606 sec/step, loss=0.07407, avg_loss=0.07351]\n",
      "Step 543832  [5.604 sec/step, loss=0.07438, avg_loss=0.07351]\n",
      "Step 543833  [5.591 sec/step, loss=0.07453, avg_loss=0.07351]\n",
      "Step 543834  [5.561 sec/step, loss=0.07381, avg_loss=0.07353]\n",
      "Step 543835  [5.571 sec/step, loss=0.07601, avg_loss=0.07354]\n",
      "Step 543836  [5.556 sec/step, loss=0.07339, avg_loss=0.07353]\n",
      "Step 543837  [5.577 sec/step, loss=0.07480, avg_loss=0.07356]\n",
      "Step 543838  [5.573 sec/step, loss=0.07398, avg_loss=0.07354]\n",
      "Step 543839  [5.579 sec/step, loss=0.07464, avg_loss=0.07355]\n",
      "Step 543840  [5.574 sec/step, loss=0.07278, avg_loss=0.07353]\n",
      "Step 543841  [5.576 sec/step, loss=0.07577, avg_loss=0.07354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 543842  [5.581 sec/step, loss=0.07273, avg_loss=0.07353]\n",
      "Step 543843  [5.603 sec/step, loss=0.07391, avg_loss=0.07352]\n",
      "Step 543844  [5.578 sec/step, loss=0.07338, avg_loss=0.07359]\n",
      "Step 543845  [5.559 sec/step, loss=0.07207, avg_loss=0.07356]\n",
      "Step 543846  [5.551 sec/step, loss=0.07284, avg_loss=0.07353]\n",
      "Step 543847  [5.519 sec/step, loss=0.06563, avg_loss=0.07344]\n",
      "Step 543848  [5.513 sec/step, loss=0.07118, avg_loss=0.07341]\n",
      "Step 543849  [5.510 sec/step, loss=0.07287, avg_loss=0.07338]\n",
      "Step 543850  [5.511 sec/step, loss=0.07472, avg_loss=0.07337]\n",
      "Step 543851  [5.515 sec/step, loss=0.07343, avg_loss=0.07336]\n",
      "Step 543852  [5.511 sec/step, loss=0.07302, avg_loss=0.07334]\n",
      "Step 543853  [5.567 sec/step, loss=0.06697, avg_loss=0.07328]\n",
      "Step 543854  [5.582 sec/step, loss=0.07353, avg_loss=0.07330]\n",
      "Step 543855  [5.583 sec/step, loss=0.07559, avg_loss=0.07332]\n",
      "Step 543856  [5.589 sec/step, loss=0.07610, avg_loss=0.07332]\n",
      "Step 543857  [5.609 sec/step, loss=0.07440, avg_loss=0.07332]\n",
      "Step 543858  [5.588 sec/step, loss=0.07226, avg_loss=0.07329]\n",
      "Step 543859  [5.537 sec/step, loss=0.07437, avg_loss=0.07337]\n",
      "Step 543860  [5.538 sec/step, loss=0.07378, avg_loss=0.07335]\n",
      "Step 543861  [5.541 sec/step, loss=0.07472, avg_loss=0.07338]\n",
      "Step 543862  [5.549 sec/step, loss=0.07245, avg_loss=0.07335]\n",
      "Generated 32 batches of size 32 in 2.500 sec\n",
      "Step 543863  [5.558 sec/step, loss=0.07347, avg_loss=0.07333]\n",
      "Step 543864  [5.567 sec/step, loss=0.07570, avg_loss=0.07335]\n",
      "Step 543865  [5.568 sec/step, loss=0.07436, avg_loss=0.07336]\n",
      "Step 543866  [5.559 sec/step, loss=0.07266, avg_loss=0.07332]\n",
      "Step 543867  [5.576 sec/step, loss=0.07583, avg_loss=0.07338]\n",
      "Step 543868  [5.571 sec/step, loss=0.07025, avg_loss=0.07334]\n",
      "Step 543869  [5.565 sec/step, loss=0.07417, avg_loss=0.07332]\n",
      "Step 543870  [5.576 sec/step, loss=0.07455, avg_loss=0.07333]\n",
      "Step 543871  [5.556 sec/step, loss=0.07588, avg_loss=0.07336]\n",
      "Step 543872  [5.544 sec/step, loss=0.07095, avg_loss=0.07332]\n",
      "Step 543873  [5.565 sec/step, loss=0.07470, avg_loss=0.07341]\n",
      "Step 543874  [5.555 sec/step, loss=0.07365, avg_loss=0.07339]\n",
      "Step 543875  [5.542 sec/step, loss=0.07481, avg_loss=0.07338]\n",
      "Step 543876  [5.547 sec/step, loss=0.07421, avg_loss=0.07338]\n",
      "Step 543877  [5.561 sec/step, loss=0.07506, avg_loss=0.07340]\n",
      "Step 543878  [5.560 sec/step, loss=0.07460, avg_loss=0.07340]\n",
      "Step 543879  [5.556 sec/step, loss=0.07451, avg_loss=0.07342]\n",
      "Step 543880  [5.540 sec/step, loss=0.07154, avg_loss=0.07338]\n",
      "Step 543881  [5.551 sec/step, loss=0.07420, avg_loss=0.07337]\n",
      "Step 543882  [5.560 sec/step, loss=0.07224, avg_loss=0.07336]\n",
      "Step 543883  [5.551 sec/step, loss=0.06959, avg_loss=0.07331]\n",
      "Step 543884  [5.541 sec/step, loss=0.07401, avg_loss=0.07333]\n",
      "Step 543885  [5.549 sec/step, loss=0.07532, avg_loss=0.07336]\n",
      "Step 543886  [5.549 sec/step, loss=0.07421, avg_loss=0.07336]\n",
      "Step 543887  [5.556 sec/step, loss=0.07616, avg_loss=0.07337]\n",
      "Step 543888  [5.552 sec/step, loss=0.07436, avg_loss=0.07336]\n",
      "Step 543889  [5.485 sec/step, loss=0.06480, avg_loss=0.07335]\n",
      "Step 543890  [5.480 sec/step, loss=0.07247, avg_loss=0.07333]\n",
      "Step 543891  [5.499 sec/step, loss=0.07446, avg_loss=0.07334]\n",
      "Step 543892  [5.561 sec/step, loss=0.06480, avg_loss=0.07325]\n",
      "Step 543893  [5.553 sec/step, loss=0.06999, avg_loss=0.07321]\n",
      "Step 543894  [5.557 sec/step, loss=0.07429, avg_loss=0.07320]\n",
      "Generated 32 batches of size 32 in 2.541 sec\n",
      "Step 543895  [5.555 sec/step, loss=0.07166, avg_loss=0.07316]\n",
      "Step 543896  [5.547 sec/step, loss=0.07308, avg_loss=0.07317]\n",
      "Step 543897  [5.576 sec/step, loss=0.07524, avg_loss=0.07326]\n",
      "Step 543898  [5.570 sec/step, loss=0.07353, avg_loss=0.07325]\n",
      "Step 543899  [5.577 sec/step, loss=0.07215, avg_loss=0.07325]\n",
      "Step 543900  [5.550 sec/step, loss=0.07232, avg_loss=0.07322]\n",
      "Writing summary at step: 543900\n",
      "Step 543901  [5.561 sec/step, loss=0.07264, avg_loss=0.07319]\n",
      "Step 543902  [5.555 sec/step, loss=0.07302, avg_loss=0.07319]\n",
      "Step 543903  [5.555 sec/step, loss=0.07399, avg_loss=0.07319]\n",
      "Step 543904  [5.559 sec/step, loss=0.07511, avg_loss=0.07321]\n",
      "Step 543905  [5.547 sec/step, loss=0.07211, avg_loss=0.07317]\n",
      "Step 543906  [5.508 sec/step, loss=0.07303, avg_loss=0.07317]\n",
      "Step 543907  [5.524 sec/step, loss=0.07414, avg_loss=0.07320]\n",
      "Step 543908  [5.532 sec/step, loss=0.07368, avg_loss=0.07320]\n",
      "Step 543909  [5.490 sec/step, loss=0.07416, avg_loss=0.07330]\n",
      "Step 543910  [5.487 sec/step, loss=0.07214, avg_loss=0.07327]\n",
      "Step 543911  [5.493 sec/step, loss=0.07488, avg_loss=0.07328]\n",
      "Step 543912  [5.516 sec/step, loss=0.07375, avg_loss=0.07328]\n",
      "Step 543913  [5.516 sec/step, loss=0.07499, avg_loss=0.07329]\n",
      "Step 543914  [5.543 sec/step, loss=0.07402, avg_loss=0.07333]\n",
      "Step 543915  [5.544 sec/step, loss=0.07252, avg_loss=0.07333]\n",
      "Step 543916  [5.534 sec/step, loss=0.07408, avg_loss=0.07331]\n",
      "Step 543917  [5.532 sec/step, loss=0.06469, avg_loss=0.07325]\n",
      "Step 543918  [5.545 sec/step, loss=0.07470, avg_loss=0.07327]\n",
      "Step 543919  [5.517 sec/step, loss=0.07033, avg_loss=0.07324]\n",
      "Step 543920  [5.526 sec/step, loss=0.07290, avg_loss=0.07332]\n",
      "Step 543921  [5.513 sec/step, loss=0.06975, avg_loss=0.07326]\n",
      "Step 543922  [5.526 sec/step, loss=0.07519, avg_loss=0.07327]\n",
      "Step 543923  [5.516 sec/step, loss=0.07088, avg_loss=0.07322]\n",
      "Step 543924  [5.500 sec/step, loss=0.07465, avg_loss=0.07321]\n",
      "Step 543925  [5.482 sec/step, loss=0.06946, avg_loss=0.07315]\n",
      "Generated 32 batches of size 32 in 2.543 sec\n",
      "Step 543926  [5.467 sec/step, loss=0.07300, avg_loss=0.07315]\n",
      "Step 543927  [5.520 sec/step, loss=0.06559, avg_loss=0.07310]\n",
      "Step 543928  [5.519 sec/step, loss=0.07454, avg_loss=0.07310]\n",
      "Step 543929  [5.536 sec/step, loss=0.07534, avg_loss=0.07312]\n",
      "Step 543930  [5.527 sec/step, loss=0.07468, avg_loss=0.07312]\n",
      "Step 543931  [5.522 sec/step, loss=0.07270, avg_loss=0.07311]\n",
      "Step 543932  [5.519 sec/step, loss=0.07427, avg_loss=0.07310]\n",
      "Step 543933  [5.536 sec/step, loss=0.07526, avg_loss=0.07311]\n",
      "Step 543934  [5.563 sec/step, loss=0.07496, avg_loss=0.07312]\n",
      "Step 543935  [5.606 sec/step, loss=0.06598, avg_loss=0.07302]\n",
      "Step 543936  [5.622 sec/step, loss=0.07584, avg_loss=0.07305]\n",
      "Step 543937  [5.592 sec/step, loss=0.07287, avg_loss=0.07303]\n",
      "Step 543938  [5.610 sec/step, loss=0.07306, avg_loss=0.07302]\n",
      "Step 543939  [5.606 sec/step, loss=0.07401, avg_loss=0.07301]\n",
      "Step 543940  [5.622 sec/step, loss=0.07549, avg_loss=0.07304]\n",
      "Step 543941  [5.615 sec/step, loss=0.07518, avg_loss=0.07303]\n",
      "Step 543942  [5.632 sec/step, loss=0.07583, avg_loss=0.07306]\n",
      "Step 543943  [5.590 sec/step, loss=0.06715, avg_loss=0.07300]\n",
      "Step 543944  [5.573 sec/step, loss=0.07559, avg_loss=0.07302]\n",
      "Step 543945  [5.592 sec/step, loss=0.07392, avg_loss=0.07304]\n",
      "Step 543946  [5.603 sec/step, loss=0.07433, avg_loss=0.07305]\n",
      "Step 543947  [5.622 sec/step, loss=0.07080, avg_loss=0.07310]\n",
      "Step 543948  [5.624 sec/step, loss=0.07466, avg_loss=0.07314]\n",
      "Step 543949  [5.643 sec/step, loss=0.07324, avg_loss=0.07314]\n",
      "Step 543950  [5.639 sec/step, loss=0.07233, avg_loss=0.07312]\n",
      "Step 543951  [5.640 sec/step, loss=0.07358, avg_loss=0.07312]\n",
      "Step 543952  [5.644 sec/step, loss=0.07398, avg_loss=0.07313]\n",
      "Step 543953  [5.588 sec/step, loss=0.07082, avg_loss=0.07317]\n",
      "Step 543954  [5.598 sec/step, loss=0.07358, avg_loss=0.07317]\n",
      "Step 543955  [5.586 sec/step, loss=0.07324, avg_loss=0.07315]\n",
      "Step 543956  [5.569 sec/step, loss=0.07438, avg_loss=0.07313]\n",
      "Step 543957  [5.574 sec/step, loss=0.07402, avg_loss=0.07312]\n",
      "Generated 32 batches of size 32 in 2.667 sec\n",
      "Step 543958  [5.586 sec/step, loss=0.07421, avg_loss=0.07314]\n",
      "Step 543959  [5.593 sec/step, loss=0.07501, avg_loss=0.07315]\n",
      "Step 543960  [5.594 sec/step, loss=0.07538, avg_loss=0.07317]\n",
      "Step 543961  [5.610 sec/step, loss=0.07555, avg_loss=0.07317]\n",
      "Step 543962  [5.601 sec/step, loss=0.07328, avg_loss=0.07318]\n",
      "Step 543963  [5.575 sec/step, loss=0.07189, avg_loss=0.07317]\n",
      "Step 543964  [5.558 sec/step, loss=0.07292, avg_loss=0.07314]\n",
      "Step 543965  [5.546 sec/step, loss=0.07212, avg_loss=0.07312]\n",
      "Step 543966  [5.533 sec/step, loss=0.07029, avg_loss=0.07309]\n",
      "Step 543967  [5.516 sec/step, loss=0.07206, avg_loss=0.07306]\n",
      "Step 543968  [5.537 sec/step, loss=0.07516, avg_loss=0.07310]\n",
      "Step 543969  [5.542 sec/step, loss=0.07275, avg_loss=0.07309]\n",
      "Step 543970  [5.589 sec/step, loss=0.06521, avg_loss=0.07300]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 543971  [5.584 sec/step, loss=0.07149, avg_loss=0.07295]\n",
      "Step 543972  [5.603 sec/step, loss=0.07405, avg_loss=0.07298]\n",
      "Step 543973  [5.614 sec/step, loss=0.07453, avg_loss=0.07298]\n",
      "Step 543974  [5.616 sec/step, loss=0.07470, avg_loss=0.07299]\n",
      "Step 543975  [5.629 sec/step, loss=0.07538, avg_loss=0.07300]\n",
      "Step 543976  [5.652 sec/step, loss=0.07462, avg_loss=0.07300]\n",
      "Step 543977  [5.648 sec/step, loss=0.07454, avg_loss=0.07300]\n",
      "Step 543978  [5.639 sec/step, loss=0.07366, avg_loss=0.07299]\n",
      "Step 543979  [5.648 sec/step, loss=0.07200, avg_loss=0.07296]\n",
      "Step 543980  [5.661 sec/step, loss=0.07514, avg_loss=0.07300]\n",
      "Step 543981  [5.650 sec/step, loss=0.07476, avg_loss=0.07300]\n",
      "Step 543982  [5.650 sec/step, loss=0.07361, avg_loss=0.07302]\n",
      "Step 543983  [5.664 sec/step, loss=0.07424, avg_loss=0.07306]\n",
      "Step 543984  [5.672 sec/step, loss=0.07532, avg_loss=0.07308]\n",
      "Step 543985  [5.661 sec/step, loss=0.07062, avg_loss=0.07303]\n",
      "Step 543986  [5.662 sec/step, loss=0.07448, avg_loss=0.07303]\n",
      "Step 543987  [5.650 sec/step, loss=0.07285, avg_loss=0.07300]\n",
      "Step 543988  [5.634 sec/step, loss=0.07318, avg_loss=0.07299]\n",
      "Step 543989  [5.643 sec/step, loss=0.07303, avg_loss=0.07307]\n",
      "Generated 32 batches of size 32 in 2.620 sec\n",
      "Step 543990  [5.645 sec/step, loss=0.07389, avg_loss=0.07309]\n",
      "Step 543991  [5.645 sec/step, loss=0.07517, avg_loss=0.07309]\n",
      "Step 543992  [5.580 sec/step, loss=0.07022, avg_loss=0.07315]\n",
      "Step 543993  [5.571 sec/step, loss=0.07069, avg_loss=0.07315]\n",
      "Step 543994  [5.563 sec/step, loss=0.07126, avg_loss=0.07312]\n",
      "Step 543995  [5.534 sec/step, loss=0.06553, avg_loss=0.07306]\n",
      "Step 543996  [5.553 sec/step, loss=0.07621, avg_loss=0.07309]\n",
      "Step 543997  [5.540 sec/step, loss=0.07135, avg_loss=0.07305]\n",
      "Step 543998  [5.549 sec/step, loss=0.07525, avg_loss=0.07307]\n",
      "Step 543999  [5.531 sec/step, loss=0.07557, avg_loss=0.07311]\n",
      "Step 544000  [5.553 sec/step, loss=0.07338, avg_loss=0.07312]\n",
      "Writing summary at step: 544000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-544000\n",
      "Saving audio and alignment...\n",
      "Input: baaasar ddhoobii konslar rijaan kay zayrayihtdimaam jaan malii sakuulz kii tdaymiir kay xaerxaa hiin~_________________________________\n",
      "Step 544001  [5.550 sec/step, loss=0.07260, avg_loss=0.07312]\n",
      "Step 544002  [5.560 sec/step, loss=0.07244, avg_loss=0.07311]\n",
      "Step 544003  [5.544 sec/step, loss=0.07094, avg_loss=0.07308]\n",
      "Step 544004  [5.520 sec/step, loss=0.07160, avg_loss=0.07304]\n",
      "Step 544005  [5.518 sec/step, loss=0.07263, avg_loss=0.07305]\n",
      "Step 544006  [5.512 sec/step, loss=0.06587, avg_loss=0.07298]\n",
      "Step 544007  [5.528 sec/step, loss=0.07483, avg_loss=0.07298]\n",
      "Step 544008  [5.528 sec/step, loss=0.07366, avg_loss=0.07298]\n",
      "Step 544009  [5.520 sec/step, loss=0.07399, avg_loss=0.07298]\n",
      "Step 544010  [5.517 sec/step, loss=0.07439, avg_loss=0.07301]\n",
      "Step 544011  [5.510 sec/step, loss=0.07401, avg_loss=0.07300]\n",
      "Step 544012  [5.476 sec/step, loss=0.07352, avg_loss=0.07299]\n",
      "Step 544013  [5.484 sec/step, loss=0.07513, avg_loss=0.07300]\n",
      "Step 544014  [5.457 sec/step, loss=0.07343, avg_loss=0.07299]\n",
      "Step 544015  [5.473 sec/step, loss=0.07272, avg_loss=0.07299]\n",
      "Step 544016  [5.471 sec/step, loss=0.07391, avg_loss=0.07299]\n",
      "Step 544017  [5.489 sec/step, loss=0.07505, avg_loss=0.07309]\n",
      "Step 544018  [5.476 sec/step, loss=0.07470, avg_loss=0.07309]\n",
      "Step 544019  [5.500 sec/step, loss=0.07336, avg_loss=0.07312]\n",
      "Generated 32 batches of size 32 in 2.401 sec\n",
      "Step 544020  [5.524 sec/step, loss=0.07521, avg_loss=0.07315]\n",
      "Step 544021  [5.524 sec/step, loss=0.07439, avg_loss=0.07319]\n",
      "Step 544022  [5.510 sec/step, loss=0.07175, avg_loss=0.07316]\n",
      "Step 544023  [5.522 sec/step, loss=0.07579, avg_loss=0.07321]\n",
      "Step 544024  [5.511 sec/step, loss=0.07457, avg_loss=0.07321]\n",
      "Step 544025  [5.572 sec/step, loss=0.06629, avg_loss=0.07318]\n",
      "Step 544026  [5.580 sec/step, loss=0.07460, avg_loss=0.07319]\n",
      "Step 544027  [5.538 sec/step, loss=0.07353, avg_loss=0.07327]\n",
      "Step 544028  [5.537 sec/step, loss=0.07314, avg_loss=0.07326]\n",
      "Step 544029  [5.538 sec/step, loss=0.07551, avg_loss=0.07326]\n",
      "Step 544030  [5.544 sec/step, loss=0.07470, avg_loss=0.07326]\n",
      "Step 544031  [5.559 sec/step, loss=0.07346, avg_loss=0.07327]\n",
      "Step 544032  [5.561 sec/step, loss=0.07474, avg_loss=0.07327]\n",
      "Step 544033  [5.551 sec/step, loss=0.07367, avg_loss=0.07325]\n",
      "Step 544034  [5.539 sec/step, loss=0.07452, avg_loss=0.07325]\n",
      "Step 544035  [5.496 sec/step, loss=0.07459, avg_loss=0.07334]\n",
      "Step 544036  [5.482 sec/step, loss=0.07413, avg_loss=0.07332]\n",
      "Step 544037  [5.496 sec/step, loss=0.07594, avg_loss=0.07335]\n",
      "Step 544038  [5.450 sec/step, loss=0.06579, avg_loss=0.07328]\n",
      "Step 544039  [5.460 sec/step, loss=0.07351, avg_loss=0.07327]\n",
      "Step 544040  [5.479 sec/step, loss=0.07315, avg_loss=0.07325]\n",
      "Step 544041  [5.527 sec/step, loss=0.06484, avg_loss=0.07315]\n",
      "Step 544042  [5.507 sec/step, loss=0.07302, avg_loss=0.07312]\n",
      "Step 544043  [5.528 sec/step, loss=0.07407, avg_loss=0.07319]\n",
      "Step 544044  [5.511 sec/step, loss=0.07261, avg_loss=0.07316]\n",
      "Step 544045  [5.501 sec/step, loss=0.07260, avg_loss=0.07314]\n",
      "Step 544046  [5.497 sec/step, loss=0.07205, avg_loss=0.07312]\n",
      "Step 544047  [5.515 sec/step, loss=0.07275, avg_loss=0.07314]\n",
      "Step 544048  [5.497 sec/step, loss=0.07024, avg_loss=0.07310]\n",
      "Step 544049  [5.487 sec/step, loss=0.07552, avg_loss=0.07312]\n",
      "Step 544050  [5.495 sec/step, loss=0.07433, avg_loss=0.07314]\n",
      "Step 544051  [5.504 sec/step, loss=0.07585, avg_loss=0.07316]\n",
      "Generated 32 batches of size 32 in 2.405 sec\n",
      "Step 544052  [5.514 sec/step, loss=0.07131, avg_loss=0.07314]\n",
      "Step 544053  [5.526 sec/step, loss=0.07404, avg_loss=0.07317]\n",
      "Step 544054  [5.523 sec/step, loss=0.07538, avg_loss=0.07319]\n",
      "Step 544055  [5.510 sec/step, loss=0.06889, avg_loss=0.07314]\n",
      "Step 544056  [5.510 sec/step, loss=0.07106, avg_loss=0.07311]\n",
      "Step 544057  [5.498 sec/step, loss=0.07255, avg_loss=0.07309]\n",
      "Step 544058  [5.493 sec/step, loss=0.07475, avg_loss=0.07310]\n",
      "Step 544059  [5.478 sec/step, loss=0.07369, avg_loss=0.07309]\n",
      "Step 544060  [5.453 sec/step, loss=0.07319, avg_loss=0.07306]\n",
      "Step 544061  [5.445 sec/step, loss=0.07409, avg_loss=0.07305]\n",
      "Step 544062  [5.448 sec/step, loss=0.07410, avg_loss=0.07306]\n",
      "Step 544063  [5.466 sec/step, loss=0.07501, avg_loss=0.07309]\n",
      "Step 544064  [5.499 sec/step, loss=0.07214, avg_loss=0.07308]\n",
      "Step 544065  [5.518 sec/step, loss=0.07261, avg_loss=0.07309]\n",
      "Step 544066  [5.516 sec/step, loss=0.07381, avg_loss=0.07312]\n",
      "Step 544067  [5.571 sec/step, loss=0.06646, avg_loss=0.07307]\n",
      "Step 544068  [5.570 sec/step, loss=0.07555, avg_loss=0.07307]\n",
      "Step 544069  [5.555 sec/step, loss=0.07474, avg_loss=0.07309]\n",
      "Step 544070  [5.507 sec/step, loss=0.07460, avg_loss=0.07318]\n",
      "Step 544071  [5.513 sec/step, loss=0.07561, avg_loss=0.07322]\n",
      "Step 544072  [5.513 sec/step, loss=0.07251, avg_loss=0.07321]\n",
      "Step 544073  [5.503 sec/step, loss=0.07482, avg_loss=0.07321]\n",
      "Step 544074  [5.499 sec/step, loss=0.07534, avg_loss=0.07322]\n",
      "Step 544075  [5.475 sec/step, loss=0.07136, avg_loss=0.07318]\n",
      "Step 544076  [5.457 sec/step, loss=0.07518, avg_loss=0.07318]\n",
      "Step 544077  [5.447 sec/step, loss=0.07111, avg_loss=0.07315]\n",
      "Step 544078  [5.446 sec/step, loss=0.07390, avg_loss=0.07315]\n",
      "Step 544079  [5.438 sec/step, loss=0.07347, avg_loss=0.07317]\n",
      "Step 544080  [5.428 sec/step, loss=0.07351, avg_loss=0.07315]\n",
      "Step 544081  [5.440 sec/step, loss=0.07441, avg_loss=0.07315]\n",
      "Step 544082  [5.421 sec/step, loss=0.06631, avg_loss=0.07307]\n",
      "Step 544083  [5.430 sec/step, loss=0.07447, avg_loss=0.07308]\n",
      "Generated 32 batches of size 32 in 2.535 sec\n",
      "Step 544084  [5.423 sec/step, loss=0.07044, avg_loss=0.07303]\n",
      "Step 544085  [5.412 sec/step, loss=0.07102, avg_loss=0.07303]\n",
      "Step 544086  [5.417 sec/step, loss=0.07367, avg_loss=0.07302]\n",
      "Step 544087  [5.429 sec/step, loss=0.07305, avg_loss=0.07302]\n",
      "Step 544088  [5.435 sec/step, loss=0.07290, avg_loss=0.07302]\n",
      "Step 544089  [5.457 sec/step, loss=0.07448, avg_loss=0.07304]\n",
      "Step 544090  [5.451 sec/step, loss=0.07338, avg_loss=0.07303]\n",
      "Step 544091  [5.431 sec/step, loss=0.07368, avg_loss=0.07302]\n",
      "Step 544092  [5.444 sec/step, loss=0.07233, avg_loss=0.07304]\n",
      "Step 544093  [5.455 sec/step, loss=0.07515, avg_loss=0.07308]\n",
      "Step 544094  [5.437 sec/step, loss=0.07400, avg_loss=0.07311]\n",
      "Step 544095  [5.461 sec/step, loss=0.07450, avg_loss=0.07320]\n",
      "Step 544096  [5.440 sec/step, loss=0.06930, avg_loss=0.07313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 544097  [5.445 sec/step, loss=0.07470, avg_loss=0.07316]\n",
      "Step 544098  [5.428 sec/step, loss=0.07224, avg_loss=0.07313]\n",
      "Step 544099  [5.415 sec/step, loss=0.07351, avg_loss=0.07311]\n",
      "Step 544100  [5.408 sec/step, loss=0.07487, avg_loss=0.07313]\n",
      "Writing summary at step: 544100\n",
      "Step 544101  [5.397 sec/step, loss=0.07306, avg_loss=0.07313]\n",
      "Step 544102  [5.388 sec/step, loss=0.07330, avg_loss=0.07314]\n",
      "Step 544103  [5.393 sec/step, loss=0.07456, avg_loss=0.07318]\n",
      "Step 544104  [5.402 sec/step, loss=0.07398, avg_loss=0.07320]\n",
      "Step 544105  [5.457 sec/step, loss=0.06528, avg_loss=0.07313]\n",
      "Step 544106  [5.482 sec/step, loss=0.07563, avg_loss=0.07323]\n",
      "Step 544107  [5.476 sec/step, loss=0.07274, avg_loss=0.07320]\n",
      "Step 544108  [5.458 sec/step, loss=0.06412, avg_loss=0.07311]\n",
      "Step 544109  [5.454 sec/step, loss=0.07132, avg_loss=0.07308]\n",
      "Step 544110  [5.480 sec/step, loss=0.07442, avg_loss=0.07308]\n",
      "Step 544111  [5.481 sec/step, loss=0.07098, avg_loss=0.07305]\n",
      "Step 544112  [5.475 sec/step, loss=0.07085, avg_loss=0.07303]\n",
      "Step 544113  [5.475 sec/step, loss=0.07567, avg_loss=0.07303]\n",
      "Step 544114  [5.486 sec/step, loss=0.07484, avg_loss=0.07305]\n",
      "Generated 32 batches of size 32 in 2.389 sec\n",
      "Step 544115  [5.472 sec/step, loss=0.07538, avg_loss=0.07307]\n",
      "Step 544116  [5.473 sec/step, loss=0.07357, avg_loss=0.07307]\n",
      "Step 544117  [5.485 sec/step, loss=0.07543, avg_loss=0.07307]\n",
      "Step 544118  [5.499 sec/step, loss=0.07498, avg_loss=0.07307]\n",
      "Step 544119  [5.483 sec/step, loss=0.07263, avg_loss=0.07307]\n",
      "Step 544120  [5.468 sec/step, loss=0.07439, avg_loss=0.07306]\n",
      "Step 544121  [5.488 sec/step, loss=0.07504, avg_loss=0.07307]\n",
      "Step 544122  [5.487 sec/step, loss=0.07285, avg_loss=0.07308]\n",
      "Step 544123  [5.477 sec/step, loss=0.07375, avg_loss=0.07306]\n",
      "Step 544124  [5.492 sec/step, loss=0.07606, avg_loss=0.07307]\n",
      "Step 544125  [5.441 sec/step, loss=0.07324, avg_loss=0.07314]\n",
      "Step 544126  [5.420 sec/step, loss=0.07062, avg_loss=0.07310]\n",
      "Step 544127  [5.412 sec/step, loss=0.07485, avg_loss=0.07311]\n",
      "Step 544128  [5.421 sec/step, loss=0.07527, avg_loss=0.07314]\n",
      "Step 544129  [5.405 sec/step, loss=0.07068, avg_loss=0.07309]\n",
      "Step 544130  [5.392 sec/step, loss=0.07306, avg_loss=0.07307]\n",
      "Step 544131  [5.379 sec/step, loss=0.07416, avg_loss=0.07308]\n",
      "Step 544132  [5.380 sec/step, loss=0.07485, avg_loss=0.07308]\n",
      "Step 544133  [5.386 sec/step, loss=0.07604, avg_loss=0.07310]\n",
      "Step 544134  [5.386 sec/step, loss=0.07405, avg_loss=0.07310]\n",
      "Step 544135  [5.370 sec/step, loss=0.07215, avg_loss=0.07307]\n",
      "Step 544136  [5.364 sec/step, loss=0.07351, avg_loss=0.07307]\n",
      "Step 544137  [5.356 sec/step, loss=0.07425, avg_loss=0.07305]\n",
      "Step 544138  [5.380 sec/step, loss=0.07547, avg_loss=0.07315]\n",
      "Step 544139  [5.376 sec/step, loss=0.07567, avg_loss=0.07317]\n",
      "Step 544140  [5.348 sec/step, loss=0.07239, avg_loss=0.07316]\n",
      "Step 544141  [5.285 sec/step, loss=0.06928, avg_loss=0.07321]\n",
      "Step 544142  [5.339 sec/step, loss=0.06612, avg_loss=0.07314]\n",
      "Step 544143  [5.338 sec/step, loss=0.07445, avg_loss=0.07314]\n",
      "Step 544144  [5.343 sec/step, loss=0.07393, avg_loss=0.07315]\n",
      "Step 544145  [5.357 sec/step, loss=0.07276, avg_loss=0.07316]\n",
      "Step 544146  [5.351 sec/step, loss=0.07310, avg_loss=0.07317]\n",
      "Generated 32 batches of size 32 in 2.407 sec\n",
      "Step 544147  [5.349 sec/step, loss=0.07601, avg_loss=0.07320]\n",
      "Step 544148  [5.372 sec/step, loss=0.07437, avg_loss=0.07324]\n",
      "Step 544149  [5.377 sec/step, loss=0.07210, avg_loss=0.07321]\n",
      "Step 544150  [5.396 sec/step, loss=0.07288, avg_loss=0.07319]\n",
      "Step 544151  [5.378 sec/step, loss=0.07300, avg_loss=0.07316]\n",
      "Step 544152  [5.364 sec/step, loss=0.07485, avg_loss=0.07320]\n",
      "Step 544153  [5.371 sec/step, loss=0.07317, avg_loss=0.07319]\n",
      "Step 544154  [5.370 sec/step, loss=0.07423, avg_loss=0.07318]\n",
      "Step 544155  [5.365 sec/step, loss=0.06580, avg_loss=0.07315]\n",
      "Step 544156  [5.357 sec/step, loss=0.07204, avg_loss=0.07316]\n",
      "Step 544157  [5.339 sec/step, loss=0.06515, avg_loss=0.07308]\n",
      "Step 544158  [5.352 sec/step, loss=0.07261, avg_loss=0.07306]\n",
      "Step 544159  [5.355 sec/step, loss=0.07277, avg_loss=0.07305]\n",
      "Step 544160  [5.373 sec/step, loss=0.07446, avg_loss=0.07306]\n",
      "Step 544161  [5.379 sec/step, loss=0.07543, avg_loss=0.07308]\n",
      "Step 544162  [5.393 sec/step, loss=0.07558, avg_loss=0.07309]\n",
      "Step 544163  [5.396 sec/step, loss=0.07518, avg_loss=0.07309]\n",
      "Step 544164  [5.373 sec/step, loss=0.07187, avg_loss=0.07309]\n",
      "Step 544165  [5.360 sec/step, loss=0.07300, avg_loss=0.07310]\n",
      "Step 544166  [5.356 sec/step, loss=0.07180, avg_loss=0.07308]\n",
      "Step 544167  [5.319 sec/step, loss=0.07520, avg_loss=0.07316]\n",
      "Step 544168  [5.324 sec/step, loss=0.07512, avg_loss=0.07316]\n",
      "Step 544169  [5.331 sec/step, loss=0.07360, avg_loss=0.07315]\n",
      "Step 544170  [5.337 sec/step, loss=0.07462, avg_loss=0.07315]\n",
      "Step 544171  [5.353 sec/step, loss=0.07206, avg_loss=0.07311]\n",
      "Step 544172  [5.342 sec/step, loss=0.07013, avg_loss=0.07309]\n",
      "Step 544173  [5.388 sec/step, loss=0.06577, avg_loss=0.07300]\n",
      "Step 544174  [5.381 sec/step, loss=0.07146, avg_loss=0.07296]\n",
      "Step 544175  [5.391 sec/step, loss=0.07471, avg_loss=0.07299]\n",
      "Step 544176  [5.371 sec/step, loss=0.07137, avg_loss=0.07295]\n",
      "Step 544177  [5.379 sec/step, loss=0.07456, avg_loss=0.07299]\n",
      "Step 544178  [5.387 sec/step, loss=0.07408, avg_loss=0.07299]\n",
      "Generated 32 batches of size 32 in 2.357 sec\n",
      "Step 544179  [5.403 sec/step, loss=0.07441, avg_loss=0.07300]\n",
      "Step 544180  [5.403 sec/step, loss=0.07300, avg_loss=0.07299]\n",
      "Step 544181  [5.386 sec/step, loss=0.07404, avg_loss=0.07299]\n",
      "Step 544182  [5.405 sec/step, loss=0.07109, avg_loss=0.07304]\n",
      "Step 544183  [5.403 sec/step, loss=0.07542, avg_loss=0.07305]\n",
      "Step 544184  [5.410 sec/step, loss=0.07600, avg_loss=0.07310]\n",
      "Step 544185  [5.424 sec/step, loss=0.07457, avg_loss=0.07314]\n",
      "Step 544186  [5.415 sec/step, loss=0.07330, avg_loss=0.07314]\n",
      "Step 544187  [5.407 sec/step, loss=0.07413, avg_loss=0.07315]\n",
      "Step 544188  [5.423 sec/step, loss=0.07481, avg_loss=0.07317]\n",
      "Step 544189  [5.409 sec/step, loss=0.07398, avg_loss=0.07316]\n",
      "Step 544190  [5.425 sec/step, loss=0.07427, avg_loss=0.07317]\n",
      "Step 544191  [5.415 sec/step, loss=0.06888, avg_loss=0.07312]\n",
      "Step 544192  [5.416 sec/step, loss=0.07373, avg_loss=0.07314]\n",
      "Step 544193  [5.415 sec/step, loss=0.07254, avg_loss=0.07311]\n",
      "Step 544194  [5.432 sec/step, loss=0.07551, avg_loss=0.07312]\n",
      "Step 544195  [5.429 sec/step, loss=0.07315, avg_loss=0.07311]\n",
      "Step 544196  [5.438 sec/step, loss=0.07447, avg_loss=0.07316]\n",
      "Step 544197  [5.424 sec/step, loss=0.07325, avg_loss=0.07315]\n",
      "Step 544198  [5.442 sec/step, loss=0.07562, avg_loss=0.07318]\n",
      "Step 544199  [5.441 sec/step, loss=0.07427, avg_loss=0.07319]\n",
      "Step 544200  [5.434 sec/step, loss=0.07480, avg_loss=0.07319]\n",
      "Writing summary at step: 544200\n",
      "Step 544201  [5.458 sec/step, loss=0.07465, avg_loss=0.07320]\n",
      "Step 544202  [5.465 sec/step, loss=0.07431, avg_loss=0.07321]\n",
      "Step 544203  [5.466 sec/step, loss=0.07184, avg_loss=0.07319]\n",
      "Step 544204  [5.479 sec/step, loss=0.07526, avg_loss=0.07320]\n",
      "Step 544205  [5.425 sec/step, loss=0.07019, avg_loss=0.07325]\n",
      "Step 544206  [5.441 sec/step, loss=0.07368, avg_loss=0.07323]\n",
      "Step 544207  [5.416 sec/step, loss=0.07016, avg_loss=0.07320]\n",
      "Step 544208  [5.428 sec/step, loss=0.07280, avg_loss=0.07329]\n",
      "Step 544209  [5.449 sec/step, loss=0.07426, avg_loss=0.07332]\n",
      "Generated 32 batches of size 32 in 2.399 sec\n",
      "Step 544210  [5.438 sec/step, loss=0.07530, avg_loss=0.07333]\n",
      "Step 544211  [5.442 sec/step, loss=0.07312, avg_loss=0.07335]\n",
      "Step 544212  [5.456 sec/step, loss=0.07385, avg_loss=0.07338]\n",
      "Step 544213  [5.451 sec/step, loss=0.07343, avg_loss=0.07336]\n",
      "Step 544214  [5.442 sec/step, loss=0.07386, avg_loss=0.07335]\n",
      "Step 544215  [5.410 sec/step, loss=0.06510, avg_loss=0.07325]\n",
      "Step 544216  [5.457 sec/step, loss=0.06496, avg_loss=0.07316]\n",
      "Step 544217  [5.435 sec/step, loss=0.07253, avg_loss=0.07313]\n",
      "Step 544218  [5.428 sec/step, loss=0.07370, avg_loss=0.07312]\n",
      "Step 544219  [5.436 sec/step, loss=0.07445, avg_loss=0.07314]\n",
      "Step 544220  [5.428 sec/step, loss=0.07218, avg_loss=0.07311]\n",
      "Step 544221  [5.452 sec/step, loss=0.06873, avg_loss=0.07305]\n",
      "Step 544222  [5.461 sec/step, loss=0.07566, avg_loss=0.07308]\n",
      "Step 544223  [5.454 sec/step, loss=0.07228, avg_loss=0.07306]\n",
      "Step 544224  [5.465 sec/step, loss=0.07276, avg_loss=0.07303]\n",
      "Step 544225  [5.462 sec/step, loss=0.07298, avg_loss=0.07303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 544226  [5.490 sec/step, loss=0.07295, avg_loss=0.07305]\n",
      "Step 544227  [5.502 sec/step, loss=0.07525, avg_loss=0.07306]\n",
      "Step 544228  [5.503 sec/step, loss=0.07516, avg_loss=0.07306]\n",
      "Step 544229  [5.513 sec/step, loss=0.07427, avg_loss=0.07309]\n",
      "Step 544230  [5.536 sec/step, loss=0.07378, avg_loss=0.07310]\n",
      "Step 544231  [5.530 sec/step, loss=0.06975, avg_loss=0.07305]\n",
      "Step 544232  [5.526 sec/step, loss=0.07479, avg_loss=0.07305]\n",
      "Step 544233  [5.513 sec/step, loss=0.07364, avg_loss=0.07303]\n",
      "Step 544234  [5.509 sec/step, loss=0.07431, avg_loss=0.07303]\n",
      "Step 544235  [5.528 sec/step, loss=0.07505, avg_loss=0.07306]\n",
      "Step 544236  [5.529 sec/step, loss=0.07297, avg_loss=0.07306]\n",
      "Step 544237  [5.554 sec/step, loss=0.07274, avg_loss=0.07304]\n",
      "Step 544238  [5.546 sec/step, loss=0.07419, avg_loss=0.07303]\n",
      "Step 544239  [5.539 sec/step, loss=0.07404, avg_loss=0.07301]\n",
      "Step 544240  [5.546 sec/step, loss=0.07453, avg_loss=0.07303]\n",
      "Step 544241  [5.562 sec/step, loss=0.07148, avg_loss=0.07305]\n",
      "Generated 32 batches of size 32 in 2.772 sec\n",
      "Step 544242  [5.502 sec/step, loss=0.07165, avg_loss=0.07311]\n",
      "Step 544243  [5.499 sec/step, loss=0.07486, avg_loss=0.07311]\n",
      "Step 544244  [5.512 sec/step, loss=0.07420, avg_loss=0.07312]\n",
      "Step 544245  [5.492 sec/step, loss=0.07164, avg_loss=0.07311]\n",
      "Step 544246  [5.504 sec/step, loss=0.07536, avg_loss=0.07313]\n",
      "Step 544247  [5.482 sec/step, loss=0.07026, avg_loss=0.07307]\n",
      "Step 544248  [5.474 sec/step, loss=0.07321, avg_loss=0.07306]\n",
      "Step 544249  [5.436 sec/step, loss=0.06534, avg_loss=0.07299]\n",
      "Step 544250  [5.418 sec/step, loss=0.07555, avg_loss=0.07302]\n",
      "Step 544251  [5.419 sec/step, loss=0.07058, avg_loss=0.07299]\n",
      "Step 544252  [5.434 sec/step, loss=0.07193, avg_loss=0.07297]\n",
      "Step 544253  [5.416 sec/step, loss=0.07290, avg_loss=0.07296]\n",
      "Step 544254  [5.419 sec/step, loss=0.07560, avg_loss=0.07298]\n",
      "Step 544255  [5.440 sec/step, loss=0.07349, avg_loss=0.07305]\n",
      "Step 544256  [5.452 sec/step, loss=0.07498, avg_loss=0.07308]\n",
      "Step 544257  [5.451 sec/step, loss=0.06451, avg_loss=0.07308]\n",
      "Step 544258  [5.449 sec/step, loss=0.07599, avg_loss=0.07311]\n",
      "Step 544259  [5.457 sec/step, loss=0.07426, avg_loss=0.07312]\n",
      "Step 544260  [5.452 sec/step, loss=0.07458, avg_loss=0.07313]\n",
      "Step 544261  [5.455 sec/step, loss=0.07573, avg_loss=0.07313]\n",
      "Step 544262  [5.444 sec/step, loss=0.07449, avg_loss=0.07312]\n",
      "Step 544263  [5.433 sec/step, loss=0.07299, avg_loss=0.07310]\n",
      "Step 544264  [5.441 sec/step, loss=0.07366, avg_loss=0.07311]\n",
      "Step 544265  [5.451 sec/step, loss=0.07085, avg_loss=0.07309]\n",
      "Step 544266  [5.471 sec/step, loss=0.07370, avg_loss=0.07311]\n",
      "Step 544267  [5.455 sec/step, loss=0.07434, avg_loss=0.07310]\n",
      "Step 544268  [5.465 sec/step, loss=0.07192, avg_loss=0.07307]\n",
      "Step 544269  [5.453 sec/step, loss=0.07313, avg_loss=0.07307]\n",
      "Step 544270  [5.437 sec/step, loss=0.07339, avg_loss=0.07305]\n",
      "Step 544271  [5.408 sec/step, loss=0.07430, avg_loss=0.07308]\n",
      "Step 544272  [5.417 sec/step, loss=0.07369, avg_loss=0.07311]\n",
      "Step 544273  [5.353 sec/step, loss=0.07129, avg_loss=0.07317]\n",
      "Generated 32 batches of size 32 in 2.477 sec\n",
      "Step 544274  [5.369 sec/step, loss=0.07548, avg_loss=0.07321]\n",
      "Step 544275  [5.388 sec/step, loss=0.07451, avg_loss=0.07321]\n",
      "Step 544276  [5.386 sec/step, loss=0.07042, avg_loss=0.07320]\n",
      "Step 544277  [5.391 sec/step, loss=0.07570, avg_loss=0.07321]\n",
      "Step 544278  [5.441 sec/step, loss=0.06659, avg_loss=0.07313]\n",
      "Step 544279  [5.431 sec/step, loss=0.07348, avg_loss=0.07312]\n",
      "Step 544280  [5.424 sec/step, loss=0.06978, avg_loss=0.07309]\n",
      "Step 544281  [5.443 sec/step, loss=0.07424, avg_loss=0.07309]\n",
      "Step 544282  [5.440 sec/step, loss=0.07296, avg_loss=0.07311]\n",
      "Step 544283  [5.422 sec/step, loss=0.07378, avg_loss=0.07310]\n",
      "Step 544284  [5.402 sec/step, loss=0.07333, avg_loss=0.07307]\n",
      "Step 544285  [5.407 sec/step, loss=0.07411, avg_loss=0.07306]\n",
      "Step 544286  [5.423 sec/step, loss=0.07581, avg_loss=0.07309]\n",
      "Step 544287  [5.427 sec/step, loss=0.07425, avg_loss=0.07309]\n",
      "Step 544288  [5.414 sec/step, loss=0.07268, avg_loss=0.07307]\n",
      "Step 544289  [5.402 sec/step, loss=0.06983, avg_loss=0.07303]\n",
      "Step 544290  [5.397 sec/step, loss=0.07513, avg_loss=0.07304]\n",
      "Step 544291  [5.402 sec/step, loss=0.07334, avg_loss=0.07308]\n",
      "Step 544292  [5.399 sec/step, loss=0.07426, avg_loss=0.07309]\n",
      "Step 544293  [5.402 sec/step, loss=0.07145, avg_loss=0.07307]\n",
      "Step 544294  [5.444 sec/step, loss=0.06491, avg_loss=0.07297]\n",
      "Step 544295  [5.440 sec/step, loss=0.07396, avg_loss=0.07298]\n",
      "Step 544296  [5.444 sec/step, loss=0.07478, avg_loss=0.07298]\n",
      "Step 544297  [5.454 sec/step, loss=0.07368, avg_loss=0.07298]\n",
      "Step 544298  [5.464 sec/step, loss=0.07277, avg_loss=0.07296]\n",
      "Step 544299  [5.454 sec/step, loss=0.07004, avg_loss=0.07291]\n",
      "Step 544300  [5.449 sec/step, loss=0.07087, avg_loss=0.07287]\n",
      "Writing summary at step: 544300\n",
      "Step 544301  [5.446 sec/step, loss=0.07516, avg_loss=0.07288]\n",
      "Step 544302  [5.447 sec/step, loss=0.07466, avg_loss=0.07288]\n",
      "Step 544303  [5.457 sec/step, loss=0.07529, avg_loss=0.07292]\n",
      "Step 544304  [5.447 sec/step, loss=0.07077, avg_loss=0.07287]\n",
      "Generated 32 batches of size 32 in 2.560 sec\n",
      "Step 544305  [5.448 sec/step, loss=0.07240, avg_loss=0.07289]\n",
      "Step 544306  [5.431 sec/step, loss=0.07492, avg_loss=0.07291]\n",
      "Step 544307  [5.450 sec/step, loss=0.07365, avg_loss=0.07294]\n",
      "Step 544308  [5.451 sec/step, loss=0.07446, avg_loss=0.07296]\n",
      "Step 544309  [5.441 sec/step, loss=0.07522, avg_loss=0.07297]\n",
      "Step 544310  [5.428 sec/step, loss=0.07439, avg_loss=0.07296]\n",
      "Step 544311  [5.405 sec/step, loss=0.06554, avg_loss=0.07288]\n",
      "Step 544312  [5.431 sec/step, loss=0.07426, avg_loss=0.07289]\n",
      "Step 544313  [5.439 sec/step, loss=0.07467, avg_loss=0.07290]\n",
      "Step 544314  [5.465 sec/step, loss=0.07211, avg_loss=0.07288]\n",
      "Step 544315  [5.497 sec/step, loss=0.07541, avg_loss=0.07299]\n",
      "Step 544316  [5.451 sec/step, loss=0.07539, avg_loss=0.07309]\n",
      "Step 544317  [5.474 sec/step, loss=0.07620, avg_loss=0.07313]\n",
      "Step 544318  [5.468 sec/step, loss=0.07392, avg_loss=0.07313]\n",
      "Step 544319  [5.474 sec/step, loss=0.07571, avg_loss=0.07314]\n",
      "Step 544320  [5.469 sec/step, loss=0.07132, avg_loss=0.07313]\n",
      "Step 544321  [5.424 sec/step, loss=0.07163, avg_loss=0.07316]\n",
      "Step 544322  [5.415 sec/step, loss=0.07362, avg_loss=0.07314]\n",
      "Step 544323  [5.466 sec/step, loss=0.06624, avg_loss=0.07308]\n",
      "Step 544324  [5.459 sec/step, loss=0.07348, avg_loss=0.07309]\n",
      "Step 544325  [5.460 sec/step, loss=0.07470, avg_loss=0.07311]\n",
      "Step 544326  [5.443 sec/step, loss=0.07285, avg_loss=0.07310]\n",
      "Step 544327  [5.439 sec/step, loss=0.07541, avg_loss=0.07311]\n",
      "Step 544328  [5.451 sec/step, loss=0.07274, avg_loss=0.07308]\n",
      "Step 544329  [5.452 sec/step, loss=0.07154, avg_loss=0.07305]\n",
      "Step 544330  [5.449 sec/step, loss=0.07509, avg_loss=0.07307]\n",
      "Step 544331  [5.471 sec/step, loss=0.07350, avg_loss=0.07310]\n",
      "Step 544332  [5.467 sec/step, loss=0.07339, avg_loss=0.07309]\n",
      "Step 544333  [5.487 sec/step, loss=0.07528, avg_loss=0.07311]\n",
      "Step 544334  [5.488 sec/step, loss=0.07402, avg_loss=0.07310]\n",
      "Step 544335  [5.463 sec/step, loss=0.07267, avg_loss=0.07308]\n",
      "Step 544336  [5.453 sec/step, loss=0.06623, avg_loss=0.07301]\n",
      "Generated 32 batches of size 32 in 2.537 sec\n",
      "Step 544337  [5.435 sec/step, loss=0.07477, avg_loss=0.07303]\n",
      "Step 544338  [5.429 sec/step, loss=0.07375, avg_loss=0.07303]\n",
      "Step 544339  [5.415 sec/step, loss=0.07349, avg_loss=0.07302]\n",
      "Step 544340  [5.413 sec/step, loss=0.07137, avg_loss=0.07299]\n",
      "Step 544341  [5.410 sec/step, loss=0.07071, avg_loss=0.07298]\n",
      "Step 544342  [5.425 sec/step, loss=0.07461, avg_loss=0.07301]\n",
      "Step 544343  [5.419 sec/step, loss=0.07289, avg_loss=0.07299]\n",
      "Step 544344  [5.414 sec/step, loss=0.07367, avg_loss=0.07299]\n",
      "Step 544345  [5.433 sec/step, loss=0.07267, avg_loss=0.07300]\n",
      "Step 544346  [5.409 sec/step, loss=0.06437, avg_loss=0.07289]\n",
      "Step 544347  [5.405 sec/step, loss=0.07357, avg_loss=0.07292]\n",
      "Step 544348  [5.408 sec/step, loss=0.07369, avg_loss=0.07293]\n",
      "Step 544349  [5.426 sec/step, loss=0.07055, avg_loss=0.07298]\n",
      "Step 544350  [5.421 sec/step, loss=0.07374, avg_loss=0.07296]\n",
      "Step 544351  [5.473 sec/step, loss=0.06492, avg_loss=0.07290]\n",
      "Step 544352  [5.465 sec/step, loss=0.07350, avg_loss=0.07292]\n",
      "Step 544353  [5.496 sec/step, loss=0.07151, avg_loss=0.07291]\n",
      "Step 544354  [5.492 sec/step, loss=0.07532, avg_loss=0.07290]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 544355  [5.481 sec/step, loss=0.07299, avg_loss=0.07290]\n",
      "Step 544356  [5.494 sec/step, loss=0.07387, avg_loss=0.07289]\n",
      "Step 544357  [5.529 sec/step, loss=0.07464, avg_loss=0.07299]\n",
      "Step 544358  [5.533 sec/step, loss=0.07496, avg_loss=0.07298]\n",
      "Step 544359  [5.526 sec/step, loss=0.07427, avg_loss=0.07298]\n",
      "Step 544360  [5.537 sec/step, loss=0.07479, avg_loss=0.07298]\n",
      "Step 544361  [5.537 sec/step, loss=0.07296, avg_loss=0.07295]\n",
      "Step 544362  [5.537 sec/step, loss=0.07301, avg_loss=0.07294]\n",
      "Step 544363  [5.522 sec/step, loss=0.07029, avg_loss=0.07291]\n",
      "Step 544364  [5.518 sec/step, loss=0.07274, avg_loss=0.07290]\n",
      "Step 544365  [5.525 sec/step, loss=0.07528, avg_loss=0.07295]\n",
      "Step 544366  [5.522 sec/step, loss=0.07430, avg_loss=0.07295]\n",
      "Step 544367  [5.521 sec/step, loss=0.07323, avg_loss=0.07294]\n",
      "Step 544368  [5.493 sec/step, loss=0.07116, avg_loss=0.07293]\n",
      "Generated 32 batches of size 32 in 2.515 sec\n",
      "Step 544369  [5.509 sec/step, loss=0.07468, avg_loss=0.07295]\n",
      "Step 544370  [5.526 sec/step, loss=0.07518, avg_loss=0.07297]\n",
      "Step 544371  [5.519 sec/step, loss=0.07054, avg_loss=0.07293]\n",
      "Step 544372  [5.506 sec/step, loss=0.07356, avg_loss=0.07293]\n",
      "Step 544373  [5.519 sec/step, loss=0.07372, avg_loss=0.07295]\n",
      "Step 544374  [5.504 sec/step, loss=0.07487, avg_loss=0.07295]\n",
      "Step 544375  [5.500 sec/step, loss=0.07514, avg_loss=0.07295]\n",
      "Step 544376  [5.506 sec/step, loss=0.07443, avg_loss=0.07299]\n",
      "Step 544377  [5.492 sec/step, loss=0.07240, avg_loss=0.07296]\n",
      "Step 544378  [5.442 sec/step, loss=0.07238, avg_loss=0.07302]\n",
      "Step 544379  [5.438 sec/step, loss=0.07415, avg_loss=0.07302]\n",
      "Step 544380  [5.448 sec/step, loss=0.07439, avg_loss=0.07307]\n",
      "Step 544381  [5.429 sec/step, loss=0.07365, avg_loss=0.07306]\n",
      "Step 544382  [5.443 sec/step, loss=0.07219, avg_loss=0.07306]\n",
      "Step 544383  [5.463 sec/step, loss=0.07507, avg_loss=0.07307]\n",
      "Step 544384  [5.463 sec/step, loss=0.07342, avg_loss=0.07307]\n",
      "Step 544385  [5.453 sec/step, loss=0.07285, avg_loss=0.07306]\n",
      "Step 544386  [5.434 sec/step, loss=0.07332, avg_loss=0.07303]\n",
      "Step 544387  [5.437 sec/step, loss=0.07555, avg_loss=0.07305]\n",
      "Step 544388  [5.452 sec/step, loss=0.07524, avg_loss=0.07307]\n",
      "Step 544389  [5.474 sec/step, loss=0.07502, avg_loss=0.07312]\n",
      "Step 544390  [5.455 sec/step, loss=0.07289, avg_loss=0.07310]\n",
      "Step 544391  [5.472 sec/step, loss=0.07495, avg_loss=0.07312]\n",
      "Step 544392  [5.495 sec/step, loss=0.07252, avg_loss=0.07310]\n",
      "Step 544393  [5.495 sec/step, loss=0.07363, avg_loss=0.07312]\n",
      "Step 544394  [5.451 sec/step, loss=0.07544, avg_loss=0.07323]\n",
      "Step 544395  [5.455 sec/step, loss=0.07401, avg_loss=0.07323]\n",
      "Step 544396  [5.465 sec/step, loss=0.07417, avg_loss=0.07322]\n",
      "Step 544397  [5.450 sec/step, loss=0.07092, avg_loss=0.07319]\n",
      "Step 544398  [5.432 sec/step, loss=0.07480, avg_loss=0.07321]\n",
      "Step 544399  [5.443 sec/step, loss=0.06962, avg_loss=0.07321]\n",
      "Step 544400  [5.452 sec/step, loss=0.07466, avg_loss=0.07325]\n",
      "Writing summary at step: 544400\n",
      "Generated 32 batches of size 32 in 2.447 sec\n",
      "Step 544401  [5.446 sec/step, loss=0.07271, avg_loss=0.07322]\n",
      "Step 544402  [5.490 sec/step, loss=0.06548, avg_loss=0.07313]\n",
      "Step 544403  [5.489 sec/step, loss=0.07363, avg_loss=0.07312]\n",
      "Step 544404  [5.485 sec/step, loss=0.07113, avg_loss=0.07312]\n",
      "Step 544405  [5.486 sec/step, loss=0.07493, avg_loss=0.07314]\n",
      "Step 544406  [5.505 sec/step, loss=0.07268, avg_loss=0.07312]\n",
      "Step 544407  [5.500 sec/step, loss=0.07335, avg_loss=0.07312]\n",
      "Step 544408  [5.485 sec/step, loss=0.06573, avg_loss=0.07303]\n",
      "Step 544409  [5.479 sec/step, loss=0.07189, avg_loss=0.07300]\n",
      "Step 544410  [5.481 sec/step, loss=0.07427, avg_loss=0.07300]\n",
      "Step 544411  [5.552 sec/step, loss=0.06428, avg_loss=0.07298]\n",
      "Step 544412  [5.527 sec/step, loss=0.07381, avg_loss=0.07298]\n",
      "Step 544413  [5.505 sec/step, loss=0.07067, avg_loss=0.07294]\n",
      "Step 544414  [5.489 sec/step, loss=0.07446, avg_loss=0.07296]\n",
      "Step 544415  [5.481 sec/step, loss=0.07180, avg_loss=0.07293]\n",
      "Step 544416  [5.487 sec/step, loss=0.07552, avg_loss=0.07293]\n",
      "Step 544417  [5.468 sec/step, loss=0.07315, avg_loss=0.07290]\n",
      "Step 544418  [5.465 sec/step, loss=0.07136, avg_loss=0.07287]\n",
      "Step 544419  [5.450 sec/step, loss=0.07372, avg_loss=0.07285]\n",
      "Step 544420  [5.444 sec/step, loss=0.06573, avg_loss=0.07280]\n",
      "Step 544421  [5.473 sec/step, loss=0.07387, avg_loss=0.07282]\n",
      "Step 544422  [5.485 sec/step, loss=0.07290, avg_loss=0.07281]\n",
      "Step 544423  [5.434 sec/step, loss=0.07456, avg_loss=0.07289]\n",
      "Step 544424  [5.431 sec/step, loss=0.07589, avg_loss=0.07292]\n",
      "Step 544425  [5.443 sec/step, loss=0.07433, avg_loss=0.07292]\n",
      "Step 544426  [5.451 sec/step, loss=0.07453, avg_loss=0.07293]\n",
      "Step 544427  [5.447 sec/step, loss=0.07396, avg_loss=0.07292]\n",
      "Step 544428  [5.437 sec/step, loss=0.07509, avg_loss=0.07294]\n",
      "Step 544429  [5.447 sec/step, loss=0.07571, avg_loss=0.07298]\n",
      "Step 544430  [5.451 sec/step, loss=0.07518, avg_loss=0.07298]\n",
      "Step 544431  [5.432 sec/step, loss=0.07002, avg_loss=0.07295]\n",
      "Generated 32 batches of size 32 in 2.425 sec\n",
      "Step 544432  [5.457 sec/step, loss=0.07340, avg_loss=0.07295]\n",
      "Step 544433  [5.441 sec/step, loss=0.07263, avg_loss=0.07292]\n",
      "Step 544434  [5.430 sec/step, loss=0.07173, avg_loss=0.07290]\n",
      "Step 544435  [5.453 sec/step, loss=0.07526, avg_loss=0.07293]\n",
      "Step 544436  [5.477 sec/step, loss=0.07140, avg_loss=0.07298]\n",
      "Step 544437  [5.455 sec/step, loss=0.07173, avg_loss=0.07295]\n",
      "Step 544438  [5.472 sec/step, loss=0.07502, avg_loss=0.07296]\n",
      "Step 544439  [5.480 sec/step, loss=0.07437, avg_loss=0.07297]\n",
      "Step 544440  [5.479 sec/step, loss=0.07304, avg_loss=0.07299]\n",
      "Step 544441  [5.473 sec/step, loss=0.07294, avg_loss=0.07301]\n",
      "Step 544442  [5.458 sec/step, loss=0.07304, avg_loss=0.07299]\n",
      "Step 544443  [5.511 sec/step, loss=0.06580, avg_loss=0.07292]\n",
      "Step 544444  [5.509 sec/step, loss=0.07487, avg_loss=0.07293]\n",
      "Step 544445  [5.490 sec/step, loss=0.06898, avg_loss=0.07290]\n",
      "Step 544446  [5.508 sec/step, loss=0.07400, avg_loss=0.07299]\n",
      "Step 544447  [5.513 sec/step, loss=0.07463, avg_loss=0.07300]\n",
      "Step 544448  [5.524 sec/step, loss=0.07546, avg_loss=0.07302]\n",
      "Step 544449  [5.553 sec/step, loss=0.07278, avg_loss=0.07304]\n",
      "Step 544450  [5.536 sec/step, loss=0.06938, avg_loss=0.07300]\n",
      "Step 544451  [5.507 sec/step, loss=0.07231, avg_loss=0.07307]\n",
      "Step 544452  [5.502 sec/step, loss=0.07407, avg_loss=0.07308]\n",
      "Step 544453  [5.468 sec/step, loss=0.07179, avg_loss=0.07308]\n",
      "Step 544454  [5.460 sec/step, loss=0.07253, avg_loss=0.07305]\n",
      "Step 544455  [5.476 sec/step, loss=0.07479, avg_loss=0.07307]\n",
      "Step 544456  [5.471 sec/step, loss=0.07514, avg_loss=0.07308]\n",
      "Step 544457  [5.458 sec/step, loss=0.07202, avg_loss=0.07306]\n",
      "Step 544458  [5.442 sec/step, loss=0.06984, avg_loss=0.07301]\n",
      "Step 544459  [5.455 sec/step, loss=0.07536, avg_loss=0.07302]\n",
      "Step 544460  [5.435 sec/step, loss=0.07377, avg_loss=0.07301]\n",
      "Step 544461  [5.434 sec/step, loss=0.07319, avg_loss=0.07301]\n",
      "Step 544462  [5.438 sec/step, loss=0.07443, avg_loss=0.07302]\n",
      "Step 544463  [5.455 sec/step, loss=0.07527, avg_loss=0.07307]\n",
      "Generated 32 batches of size 32 in 2.469 sec\n",
      "Step 544464  [5.455 sec/step, loss=0.07341, avg_loss=0.07308]\n",
      "Step 544465  [5.455 sec/step, loss=0.07570, avg_loss=0.07308]\n",
      "Step 544466  [5.449 sec/step, loss=0.07383, avg_loss=0.07308]\n",
      "Step 544467  [5.461 sec/step, loss=0.07305, avg_loss=0.07308]\n",
      "Step 544468  [5.477 sec/step, loss=0.07478, avg_loss=0.07311]\n",
      "Step 544469  [5.481 sec/step, loss=0.07492, avg_loss=0.07312]\n",
      "Step 544470  [5.476 sec/step, loss=0.07409, avg_loss=0.07311]\n",
      "Step 544471  [5.468 sec/step, loss=0.06367, avg_loss=0.07304]\n",
      "Step 544472  [5.477 sec/step, loss=0.06951, avg_loss=0.07300]\n",
      "Step 544473  [5.483 sec/step, loss=0.07399, avg_loss=0.07300]\n",
      "Step 544474  [5.470 sec/step, loss=0.07322, avg_loss=0.07298]\n",
      "Step 544475  [5.464 sec/step, loss=0.07444, avg_loss=0.07298]\n",
      "Step 544476  [5.469 sec/step, loss=0.07114, avg_loss=0.07294]\n",
      "Step 544477  [5.480 sec/step, loss=0.07158, avg_loss=0.07293]\n",
      "Step 544478  [5.490 sec/step, loss=0.07518, avg_loss=0.07296]\n",
      "Step 544479  [5.495 sec/step, loss=0.07440, avg_loss=0.07297]\n",
      "Step 544480  [5.477 sec/step, loss=0.06604, avg_loss=0.07288]\n",
      "Step 544481  [5.481 sec/step, loss=0.07431, avg_loss=0.07289]\n",
      "Step 544482  [5.471 sec/step, loss=0.07319, avg_loss=0.07290]\n",
      "Step 544483  [5.457 sec/step, loss=0.07407, avg_loss=0.07289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 544484  [5.460 sec/step, loss=0.07376, avg_loss=0.07289]\n",
      "Step 544485  [5.481 sec/step, loss=0.07235, avg_loss=0.07289]\n",
      "Step 544486  [5.497 sec/step, loss=0.07468, avg_loss=0.07290]\n",
      "Step 544487  [5.540 sec/step, loss=0.06534, avg_loss=0.07280]\n",
      "Step 544488  [5.538 sec/step, loss=0.07408, avg_loss=0.07279]\n",
      "Step 544489  [5.524 sec/step, loss=0.07159, avg_loss=0.07275]\n",
      "Step 544490  [5.533 sec/step, loss=0.07334, avg_loss=0.07276]\n",
      "Step 544491  [5.518 sec/step, loss=0.07455, avg_loss=0.07275]\n",
      "Step 544492  [5.496 sec/step, loss=0.07299, avg_loss=0.07276]\n",
      "Step 544493  [5.487 sec/step, loss=0.07367, avg_loss=0.07276]\n",
      "Step 544494  [5.495 sec/step, loss=0.07494, avg_loss=0.07275]\n",
      "Step 544495  [5.505 sec/step, loss=0.07550, avg_loss=0.07277]\n",
      "Generated 32 batches of size 32 in 2.478 sec\n",
      "Step 544496  [5.502 sec/step, loss=0.07468, avg_loss=0.07277]\n",
      "Step 544497  [5.523 sec/step, loss=0.07552, avg_loss=0.07282]\n",
      "Step 544498  [5.531 sec/step, loss=0.07506, avg_loss=0.07282]\n",
      "Step 544499  [5.530 sec/step, loss=0.07231, avg_loss=0.07285]\n",
      "Step 544500  [5.511 sec/step, loss=0.07064, avg_loss=0.07281]\n",
      "Writing summary at step: 544500\n",
      "Step 544501  [5.535 sec/step, loss=0.07231, avg_loss=0.07280]\n",
      "Step 544502  [5.474 sec/step, loss=0.07034, avg_loss=0.07285]\n",
      "Step 544503  [5.478 sec/step, loss=0.07459, avg_loss=0.07286]\n",
      "Step 544504  [5.489 sec/step, loss=0.07260, avg_loss=0.07288]\n",
      "Step 544505  [5.488 sec/step, loss=0.07309, avg_loss=0.07286]\n",
      "Step 544506  [5.466 sec/step, loss=0.07412, avg_loss=0.07287]\n",
      "Step 544507  [5.475 sec/step, loss=0.07493, avg_loss=0.07289]\n",
      "Step 544508  [5.480 sec/step, loss=0.07155, avg_loss=0.07295]\n",
      "Step 544509  [5.481 sec/step, loss=0.07457, avg_loss=0.07297]\n",
      "Step 544510  [5.474 sec/step, loss=0.07220, avg_loss=0.07295]\n",
      "Step 544511  [5.427 sec/step, loss=0.07305, avg_loss=0.07304]\n",
      "Step 544512  [5.426 sec/step, loss=0.07291, avg_loss=0.07303]\n",
      "Step 544513  [5.425 sec/step, loss=0.07336, avg_loss=0.07306]\n",
      "Step 544514  [5.434 sec/step, loss=0.07177, avg_loss=0.07303]\n",
      "Step 544515  [5.431 sec/step, loss=0.07439, avg_loss=0.07306]\n",
      "Step 544516  [5.413 sec/step, loss=0.07261, avg_loss=0.07303]\n",
      "Step 544517  [5.426 sec/step, loss=0.07614, avg_loss=0.07306]\n",
      "Step 544518  [5.411 sec/step, loss=0.06517, avg_loss=0.07300]\n",
      "Step 544519  [5.418 sec/step, loss=0.07395, avg_loss=0.07300]\n",
      "Step 544520  [5.436 sec/step, loss=0.07529, avg_loss=0.07309]\n",
      "Step 544521  [5.423 sec/step, loss=0.07490, avg_loss=0.07310]\n",
      "Step 544522  [5.423 sec/step, loss=0.07506, avg_loss=0.07313]\n",
      "Step 544523  [5.425 sec/step, loss=0.07332, avg_loss=0.07311]\n",
      "Step 544524  [5.429 sec/step, loss=0.07516, avg_loss=0.07311]\n",
      "Step 544525  [5.428 sec/step, loss=0.07367, avg_loss=0.07310]\n",
      "Step 544526  [5.432 sec/step, loss=0.07424, avg_loss=0.07310]\n",
      "Generated 32 batches of size 32 in 2.389 sec\n",
      "Step 544527  [5.449 sec/step, loss=0.07486, avg_loss=0.07311]\n",
      "Step 544528  [5.420 sec/step, loss=0.07318, avg_loss=0.07309]\n",
      "Step 544529  [5.403 sec/step, loss=0.07400, avg_loss=0.07307]\n",
      "Step 544530  [5.440 sec/step, loss=0.06618, avg_loss=0.07298]\n",
      "Step 544531  [5.456 sec/step, loss=0.07627, avg_loss=0.07304]\n",
      "Step 544532  [5.436 sec/step, loss=0.07455, avg_loss=0.07305]\n",
      "Step 544533  [5.436 sec/step, loss=0.07047, avg_loss=0.07303]\n",
      "Step 544534  [5.474 sec/step, loss=0.07275, avg_loss=0.07304]\n",
      "Step 544535  [5.455 sec/step, loss=0.07097, avg_loss=0.07300]\n",
      "Step 544536  [5.437 sec/step, loss=0.07290, avg_loss=0.07301]\n",
      "Step 544537  [5.452 sec/step, loss=0.07398, avg_loss=0.07304]\n",
      "Step 544538  [5.453 sec/step, loss=0.07498, avg_loss=0.07304]\n",
      "Step 544539  [5.456 sec/step, loss=0.07340, avg_loss=0.07303]\n",
      "Step 544540  [5.464 sec/step, loss=0.07547, avg_loss=0.07305]\n",
      "Step 544541  [5.451 sec/step, loss=0.06491, avg_loss=0.07297]\n",
      "Step 544542  [5.473 sec/step, loss=0.07398, avg_loss=0.07298]\n",
      "Step 544543  [5.434 sec/step, loss=0.07299, avg_loss=0.07305]\n",
      "Step 544544  [5.433 sec/step, loss=0.07447, avg_loss=0.07305]\n",
      "Step 544545  [5.442 sec/step, loss=0.06973, avg_loss=0.07306]\n",
      "Step 544546  [5.436 sec/step, loss=0.07133, avg_loss=0.07303]\n",
      "Step 544547  [5.434 sec/step, loss=0.07443, avg_loss=0.07303]\n",
      "Step 544548  [5.422 sec/step, loss=0.07452, avg_loss=0.07302]\n",
      "Step 544549  [5.442 sec/step, loss=0.06553, avg_loss=0.07295]\n",
      "Step 544550  [5.457 sec/step, loss=0.07502, avg_loss=0.07300]\n",
      "Step 544551  [5.449 sec/step, loss=0.07316, avg_loss=0.07301]\n",
      "Step 544552  [5.435 sec/step, loss=0.07095, avg_loss=0.07298]\n",
      "Step 544553  [5.441 sec/step, loss=0.07435, avg_loss=0.07300]\n",
      "Step 544554  [5.446 sec/step, loss=0.07446, avg_loss=0.07302]\n",
      "Step 544555  [5.426 sec/step, loss=0.06990, avg_loss=0.07298]\n",
      "Step 544556  [5.421 sec/step, loss=0.07302, avg_loss=0.07295]\n",
      "Step 544557  [5.444 sec/step, loss=0.07215, avg_loss=0.07296]\n",
      "Step 544558  [5.466 sec/step, loss=0.07490, avg_loss=0.07301]\n",
      "Generated 32 batches of size 32 in 2.510 sec\n",
      "Step 544559  [5.462 sec/step, loss=0.07201, avg_loss=0.07297]\n",
      "Step 544560  [5.483 sec/step, loss=0.07485, avg_loss=0.07298]\n",
      "Step 544561  [5.465 sec/step, loss=0.07246, avg_loss=0.07298]\n",
      "Step 544562  [5.455 sec/step, loss=0.07334, avg_loss=0.07297]\n",
      "Step 544563  [5.467 sec/step, loss=0.07531, avg_loss=0.07297]\n",
      "Step 544564  [5.468 sec/step, loss=0.07543, avg_loss=0.07299]\n",
      "Step 544565  [5.453 sec/step, loss=0.07337, avg_loss=0.07296]\n",
      "Step 544566  [5.465 sec/step, loss=0.07391, avg_loss=0.07296]\n",
      "Step 544567  [5.464 sec/step, loss=0.07467, avg_loss=0.07298]\n",
      "Step 544568  [5.461 sec/step, loss=0.07239, avg_loss=0.07296]\n",
      "Step 544569  [5.449 sec/step, loss=0.07046, avg_loss=0.07291]\n",
      "Step 544570  [5.433 sec/step, loss=0.07060, avg_loss=0.07288]\n",
      "Step 544571  [5.437 sec/step, loss=0.07038, avg_loss=0.07294]\n",
      "Step 544572  [5.438 sec/step, loss=0.07370, avg_loss=0.07299]\n",
      "Step 544573  [5.430 sec/step, loss=0.07239, avg_loss=0.07297]\n",
      "Step 544574  [5.444 sec/step, loss=0.07197, avg_loss=0.07296]\n",
      "Step 544575  [5.437 sec/step, loss=0.07254, avg_loss=0.07294]\n",
      "Step 544576  [5.439 sec/step, loss=0.07452, avg_loss=0.07297]\n",
      "Step 544577  [5.446 sec/step, loss=0.07549, avg_loss=0.07301]\n",
      "Step 544578  [5.427 sec/step, loss=0.07349, avg_loss=0.07299]\n",
      "Step 544579  [5.435 sec/step, loss=0.07366, avg_loss=0.07299]\n",
      "Step 544580  [5.460 sec/step, loss=0.07599, avg_loss=0.07309]\n",
      "Step 544581  [5.446 sec/step, loss=0.07395, avg_loss=0.07308]\n",
      "Step 544582  [5.446 sec/step, loss=0.07080, avg_loss=0.07306]\n",
      "Step 544583  [5.450 sec/step, loss=0.07471, avg_loss=0.07306]\n",
      "Step 544584  [5.469 sec/step, loss=0.07499, avg_loss=0.07308]\n",
      "Step 544585  [5.449 sec/step, loss=0.07393, avg_loss=0.07309]\n",
      "Step 544586  [5.460 sec/step, loss=0.07222, avg_loss=0.07307]\n",
      "Step 544587  [5.423 sec/step, loss=0.07456, avg_loss=0.07316]\n",
      "Step 544588  [5.409 sec/step, loss=0.07421, avg_loss=0.07316]\n",
      "Step 544589  [5.425 sec/step, loss=0.07597, avg_loss=0.07321]\n",
      "Step 544590  [5.452 sec/step, loss=0.07242, avg_loss=0.07320]\n",
      "Generated 32 batches of size 32 in 2.538 sec\n",
      "Step 544591  [5.451 sec/step, loss=0.07328, avg_loss=0.07318]\n",
      "Step 544592  [5.459 sec/step, loss=0.07531, avg_loss=0.07321]\n",
      "Step 544593  [5.450 sec/step, loss=0.06532, avg_loss=0.07312]\n",
      "Step 544594  [5.487 sec/step, loss=0.06512, avg_loss=0.07303]\n",
      "Step 544595  [5.477 sec/step, loss=0.07409, avg_loss=0.07301]\n",
      "Step 544596  [5.471 sec/step, loss=0.07472, avg_loss=0.07301]\n",
      "Step 544597  [5.460 sec/step, loss=0.07302, avg_loss=0.07299]\n",
      "Step 544598  [5.459 sec/step, loss=0.07541, avg_loss=0.07299]\n",
      "Step 544599  [5.469 sec/step, loss=0.07397, avg_loss=0.07301]\n",
      "Step 544600  [5.481 sec/step, loss=0.06998, avg_loss=0.07300]\n",
      "Writing summary at step: 544600\n",
      "Step 544601  [5.468 sec/step, loss=0.07464, avg_loss=0.07302]\n",
      "Step 544602  [5.478 sec/step, loss=0.07419, avg_loss=0.07306]\n",
      "Step 544603  [5.472 sec/step, loss=0.07436, avg_loss=0.07306]\n",
      "Step 544604  [5.475 sec/step, loss=0.07371, avg_loss=0.07307]\n",
      "Step 544605  [5.474 sec/step, loss=0.07235, avg_loss=0.07306]\n",
      "Step 544606  [5.458 sec/step, loss=0.07314, avg_loss=0.07305]\n",
      "Step 544607  [5.445 sec/step, loss=0.07470, avg_loss=0.07305]\n",
      "Step 544608  [5.455 sec/step, loss=0.07167, avg_loss=0.07305]\n",
      "Step 544609  [5.442 sec/step, loss=0.07061, avg_loss=0.07301]\n",
      "Step 544610  [5.458 sec/step, loss=0.07514, avg_loss=0.07304]\n",
      "Step 544611  [5.457 sec/step, loss=0.07362, avg_loss=0.07305]\n",
      "Step 544612  [5.476 sec/step, loss=0.07438, avg_loss=0.07306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 544613  [5.466 sec/step, loss=0.06692, avg_loss=0.07300]\n",
      "Step 544614  [5.459 sec/step, loss=0.07311, avg_loss=0.07301]\n",
      "Step 544615  [5.449 sec/step, loss=0.07407, avg_loss=0.07301]\n",
      "Step 544616  [5.443 sec/step, loss=0.07139, avg_loss=0.07300]\n",
      "Step 544617  [5.439 sec/step, loss=0.07378, avg_loss=0.07297]\n",
      "Step 544618  [5.454 sec/step, loss=0.07303, avg_loss=0.07305]\n",
      "Step 544619  [5.504 sec/step, loss=0.06611, avg_loss=0.07297]\n",
      "Step 544620  [5.531 sec/step, loss=0.07228, avg_loss=0.07294]\n",
      "Step 544621  [5.531 sec/step, loss=0.07517, avg_loss=0.07295]\n",
      "Generated 32 batches of size 32 in 2.516 sec\n",
      "Step 544622  [5.525 sec/step, loss=0.07357, avg_loss=0.07293]\n",
      "Step 544623  [5.525 sec/step, loss=0.07419, avg_loss=0.07294]\n",
      "Step 544624  [5.513 sec/step, loss=0.07446, avg_loss=0.07293]\n",
      "Step 544625  [5.511 sec/step, loss=0.07450, avg_loss=0.07294]\n",
      "Step 544626  [5.516 sec/step, loss=0.07529, avg_loss=0.07295]\n",
      "Step 544627  [5.485 sec/step, loss=0.07209, avg_loss=0.07292]\n",
      "Step 544628  [5.502 sec/step, loss=0.07433, avg_loss=0.07293]\n",
      "Step 544629  [5.513 sec/step, loss=0.07520, avg_loss=0.07295]\n",
      "Step 544630  [5.471 sec/step, loss=0.07574, avg_loss=0.07304]\n",
      "Step 544631  [5.468 sec/step, loss=0.07571, avg_loss=0.07304]\n",
      "Step 544632  [5.481 sec/step, loss=0.07422, avg_loss=0.07303]\n",
      "Step 544633  [5.496 sec/step, loss=0.07372, avg_loss=0.07307]\n",
      "Step 544634  [5.469 sec/step, loss=0.07190, avg_loss=0.07306]\n",
      "Step 544635  [5.499 sec/step, loss=0.07310, avg_loss=0.07308]\n",
      "Step 544636  [5.510 sec/step, loss=0.07339, avg_loss=0.07308]\n",
      "Step 544637  [5.519 sec/step, loss=0.07547, avg_loss=0.07310]\n",
      "Step 544638  [5.516 sec/step, loss=0.07500, avg_loss=0.07310]\n",
      "Step 544639  [5.515 sec/step, loss=0.07114, avg_loss=0.07308]\n",
      "Step 544640  [5.489 sec/step, loss=0.06412, avg_loss=0.07296]\n",
      "Step 544641  [5.512 sec/step, loss=0.07399, avg_loss=0.07305]\n",
      "Step 544642  [5.496 sec/step, loss=0.07291, avg_loss=0.07304]\n",
      "Step 544643  [5.514 sec/step, loss=0.07249, avg_loss=0.07304]\n",
      "Step 544644  [5.502 sec/step, loss=0.07112, avg_loss=0.07300]\n",
      "Step 544645  [5.508 sec/step, loss=0.07222, avg_loss=0.07303]\n",
      "Step 544646  [5.524 sec/step, loss=0.07522, avg_loss=0.07307]\n",
      "Step 544647  [5.530 sec/step, loss=0.07430, avg_loss=0.07307]\n",
      "Step 544648  [5.525 sec/step, loss=0.07249, avg_loss=0.07305]\n",
      "Step 544649  [5.461 sec/step, loss=0.07084, avg_loss=0.07310]\n",
      "Step 544650  [5.454 sec/step, loss=0.07360, avg_loss=0.07308]\n",
      "Step 544651  [5.446 sec/step, loss=0.07464, avg_loss=0.07310]\n",
      "Step 544652  [5.451 sec/step, loss=0.06966, avg_loss=0.07309]\n",
      "Step 544653  [5.450 sec/step, loss=0.07242, avg_loss=0.07307]\n",
      "Generated 32 batches of size 32 in 2.665 sec\n",
      "Step 544654  [5.500 sec/step, loss=0.06636, avg_loss=0.07299]\n",
      "Step 544655  [5.520 sec/step, loss=0.07561, avg_loss=0.07304]\n",
      "Step 544656  [5.530 sec/step, loss=0.07272, avg_loss=0.07304]\n",
      "Step 544657  [5.504 sec/step, loss=0.07343, avg_loss=0.07305]\n",
      "Step 544658  [5.489 sec/step, loss=0.07399, avg_loss=0.07304]\n",
      "Step 544659  [5.489 sec/step, loss=0.07224, avg_loss=0.07305]\n",
      "Step 544660  [5.493 sec/step, loss=0.07370, avg_loss=0.07304]\n",
      "Step 544661  [5.498 sec/step, loss=0.06969, avg_loss=0.07301]\n",
      "Step 544662  [5.501 sec/step, loss=0.07402, avg_loss=0.07301]\n",
      "Step 544663  [5.482 sec/step, loss=0.07117, avg_loss=0.07297]\n",
      "Step 544664  [5.464 sec/step, loss=0.07328, avg_loss=0.07295]\n",
      "Step 544665  [5.455 sec/step, loss=0.06475, avg_loss=0.07286]\n",
      "Step 544666  [5.445 sec/step, loss=0.07344, avg_loss=0.07286]\n",
      "Step 544667  [5.432 sec/step, loss=0.07319, avg_loss=0.07285]\n",
      "Step 544668  [5.422 sec/step, loss=0.07459, avg_loss=0.07287]\n",
      "Step 544669  [5.437 sec/step, loss=0.07432, avg_loss=0.07291]\n",
      "Step 544670  [5.448 sec/step, loss=0.07412, avg_loss=0.07294]\n",
      "Step 544671  [5.474 sec/step, loss=0.07494, avg_loss=0.07299]\n",
      "Step 544672  [5.483 sec/step, loss=0.07501, avg_loss=0.07300]\n",
      "Step 544673  [5.503 sec/step, loss=0.07451, avg_loss=0.07302]\n",
      "Step 544674  [5.527 sec/step, loss=0.07200, avg_loss=0.07302]\n",
      "Step 544675  [5.533 sec/step, loss=0.07466, avg_loss=0.07304]\n",
      "Step 544676  [5.540 sec/step, loss=0.07558, avg_loss=0.07305]\n",
      "Step 544677  [5.531 sec/step, loss=0.07508, avg_loss=0.07305]\n",
      "Step 544678  [5.524 sec/step, loss=0.06993, avg_loss=0.07301]\n",
      "Step 544679  [5.521 sec/step, loss=0.07314, avg_loss=0.07301]\n",
      "Step 544680  [5.512 sec/step, loss=0.07323, avg_loss=0.07298]\n",
      "Step 544681  [5.526 sec/step, loss=0.07355, avg_loss=0.07298]\n",
      "Step 544682  [5.532 sec/step, loss=0.07357, avg_loss=0.07300]\n",
      "Step 544683  [5.536 sec/step, loss=0.07239, avg_loss=0.07298]\n",
      "Step 544684  [5.525 sec/step, loss=0.07410, avg_loss=0.07297]\n",
      "Step 544685  [5.535 sec/step, loss=0.07534, avg_loss=0.07299]\n",
      "Generated 32 batches of size 32 in 2.584 sec\n",
      "Step 544686  [5.518 sec/step, loss=0.07453, avg_loss=0.07301]\n",
      "Step 544687  [5.512 sec/step, loss=0.07551, avg_loss=0.07302]\n",
      "Step 544688  [5.506 sec/step, loss=0.07329, avg_loss=0.07301]\n",
      "Step 544689  [5.506 sec/step, loss=0.07526, avg_loss=0.07300]\n",
      "Step 544690  [5.529 sec/step, loss=0.06501, avg_loss=0.07293]\n",
      "Step 544691  [5.531 sec/step, loss=0.07416, avg_loss=0.07294]\n",
      "Step 544692  [5.523 sec/step, loss=0.07063, avg_loss=0.07289]\n",
      "Step 544693  [5.528 sec/step, loss=0.06993, avg_loss=0.07294]\n",
      "Step 544694  [5.474 sec/step, loss=0.07284, avg_loss=0.07301]\n",
      "Step 544695  [5.496 sec/step, loss=0.07218, avg_loss=0.07299]\n",
      "Step 544696  [5.511 sec/step, loss=0.07476, avg_loss=0.07300]\n",
      "Step 544697  [5.529 sec/step, loss=0.07499, avg_loss=0.07301]\n",
      "Step 544698  [5.524 sec/step, loss=0.07307, avg_loss=0.07299]\n",
      "Step 544699  [5.523 sec/step, loss=0.07404, avg_loss=0.07299]\n",
      "Step 544700  [5.524 sec/step, loss=0.07230, avg_loss=0.07302]\n",
      "Writing summary at step: 544700\n",
      "Step 544701  [5.515 sec/step, loss=0.07450, avg_loss=0.07301]\n",
      "Step 544702  [5.532 sec/step, loss=0.07463, avg_loss=0.07302]\n",
      "Step 544703  [5.523 sec/step, loss=0.07254, avg_loss=0.07300]\n",
      "Step 544704  [5.496 sec/step, loss=0.06487, avg_loss=0.07291]\n",
      "Step 544705  [5.497 sec/step, loss=0.07457, avg_loss=0.07293]\n",
      "Step 544706  [5.493 sec/step, loss=0.07066, avg_loss=0.07291]\n",
      "Step 544707  [5.492 sec/step, loss=0.07034, avg_loss=0.07287]\n",
      "Step 544708  [5.496 sec/step, loss=0.07383, avg_loss=0.07289]\n",
      "Step 544709  [5.517 sec/step, loss=0.07327, avg_loss=0.07291]\n",
      "Step 544710  [5.512 sec/step, loss=0.07420, avg_loss=0.07290]\n",
      "Step 544711  [5.558 sec/step, loss=0.06580, avg_loss=0.07283]\n",
      "Step 544712  [5.550 sec/step, loss=0.07240, avg_loss=0.07281]\n",
      "Step 544713  [5.554 sec/step, loss=0.06929, avg_loss=0.07283]\n",
      "Step 544714  [5.543 sec/step, loss=0.07479, avg_loss=0.07285]\n",
      "Step 544715  [5.552 sec/step, loss=0.07390, avg_loss=0.07285]\n",
      "Step 544716  [5.555 sec/step, loss=0.07397, avg_loss=0.07287]\n",
      "Generated 32 batches of size 32 in 2.485 sec\n",
      "Step 544717  [5.557 sec/step, loss=0.07467, avg_loss=0.07288]\n",
      "Step 544718  [5.570 sec/step, loss=0.07507, avg_loss=0.07290]\n",
      "Step 544719  [5.522 sec/step, loss=0.07378, avg_loss=0.07298]\n",
      "Step 544720  [5.503 sec/step, loss=0.07574, avg_loss=0.07301]\n",
      "Step 544721  [5.499 sec/step, loss=0.07641, avg_loss=0.07302]\n",
      "Step 544722  [5.496 sec/step, loss=0.07450, avg_loss=0.07303]\n",
      "Step 544723  [5.495 sec/step, loss=0.07424, avg_loss=0.07303]\n",
      "Step 544724  [5.489 sec/step, loss=0.07229, avg_loss=0.07301]\n",
      "Step 544725  [5.495 sec/step, loss=0.07542, avg_loss=0.07302]\n",
      "Step 544726  [5.479 sec/step, loss=0.07440, avg_loss=0.07301]\n",
      "Step 544727  [5.497 sec/step, loss=0.07568, avg_loss=0.07305]\n",
      "Step 544728  [5.484 sec/step, loss=0.07330, avg_loss=0.07304]\n",
      "Step 544729  [5.480 sec/step, loss=0.07386, avg_loss=0.07302]\n",
      "Step 544730  [5.474 sec/step, loss=0.07398, avg_loss=0.07301]\n",
      "Step 544731  [5.466 sec/step, loss=0.07280, avg_loss=0.07298]\n",
      "Step 544732  [5.467 sec/step, loss=0.07313, avg_loss=0.07297]\n",
      "Step 544733  [5.448 sec/step, loss=0.07285, avg_loss=0.07296]\n",
      "Step 544734  [5.437 sec/step, loss=0.07329, avg_loss=0.07297]\n",
      "Step 544735  [5.401 sec/step, loss=0.06705, avg_loss=0.07291]\n",
      "Step 544736  [5.412 sec/step, loss=0.07397, avg_loss=0.07292]\n",
      "Step 544737  [5.402 sec/step, loss=0.07087, avg_loss=0.07287]\n",
      "Step 544738  [5.402 sec/step, loss=0.07518, avg_loss=0.07287]\n",
      "Step 544739  [5.427 sec/step, loss=0.07508, avg_loss=0.07291]\n",
      "Step 544740  [5.440 sec/step, loss=0.07439, avg_loss=0.07302]\n",
      "Step 544741  [5.425 sec/step, loss=0.07083, avg_loss=0.07298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 544742  [5.479 sec/step, loss=0.06512, avg_loss=0.07291]\n",
      "Step 544743  [5.453 sec/step, loss=0.07452, avg_loss=0.07293]\n",
      "Step 544744  [5.474 sec/step, loss=0.07483, avg_loss=0.07296]\n",
      "Step 544745  [5.471 sec/step, loss=0.07478, avg_loss=0.07299]\n",
      "Step 544746  [5.456 sec/step, loss=0.07283, avg_loss=0.07296]\n",
      "Step 544747  [5.462 sec/step, loss=0.07457, avg_loss=0.07297]\n",
      "Step 544748  [5.463 sec/step, loss=0.07145, avg_loss=0.07296]\n",
      "Generated 32 batches of size 32 in 2.351 sec\n",
      "Step 544749  [5.488 sec/step, loss=0.07515, avg_loss=0.07300]\n",
      "Step 544750  [5.510 sec/step, loss=0.07544, avg_loss=0.07302]\n",
      "Step 544751  [5.497 sec/step, loss=0.07282, avg_loss=0.07300]\n",
      "Step 544752  [5.512 sec/step, loss=0.07202, avg_loss=0.07302]\n",
      "Step 544753  [5.502 sec/step, loss=0.07153, avg_loss=0.07302]\n",
      "Step 544754  [5.451 sec/step, loss=0.07339, avg_loss=0.07309]\n",
      "Step 544755  [5.455 sec/step, loss=0.07540, avg_loss=0.07308]\n",
      "Step 544756  [5.441 sec/step, loss=0.07383, avg_loss=0.07309]\n",
      "Step 544757  [5.460 sec/step, loss=0.07204, avg_loss=0.07308]\n",
      "Step 544758  [5.460 sec/step, loss=0.07436, avg_loss=0.07308]\n",
      "Step 544759  [5.440 sec/step, loss=0.07108, avg_loss=0.07307]\n",
      "Step 544760  [5.417 sec/step, loss=0.07189, avg_loss=0.07305]\n",
      "Step 544761  [5.428 sec/step, loss=0.07380, avg_loss=0.07310]\n",
      "Step 544762  [5.432 sec/step, loss=0.07399, avg_loss=0.07310]\n",
      "Step 544763  [5.450 sec/step, loss=0.07324, avg_loss=0.07312]\n",
      "Step 544764  [5.475 sec/step, loss=0.07509, avg_loss=0.07313]\n",
      "Step 544765  [5.492 sec/step, loss=0.07337, avg_loss=0.07322]\n",
      "Step 544766  [5.497 sec/step, loss=0.07396, avg_loss=0.07323]\n",
      "Step 544767  [5.493 sec/step, loss=0.07118, avg_loss=0.07321]\n",
      "Step 544768  [5.497 sec/step, loss=0.07452, avg_loss=0.07321]\n",
      "Step 544769  [5.474 sec/step, loss=0.07316, avg_loss=0.07319]\n",
      "Step 544770  [5.471 sec/step, loss=0.07332, avg_loss=0.07319]\n",
      "Step 544771  [5.450 sec/step, loss=0.07314, avg_loss=0.07317]\n",
      "Step 544772  [5.449 sec/step, loss=0.07447, avg_loss=0.07316]\n",
      "Step 544773  [5.459 sec/step, loss=0.07217, avg_loss=0.07314]\n",
      "Step 544774  [5.435 sec/step, loss=0.07169, avg_loss=0.07314]\n",
      "Step 544775  [5.427 sec/step, loss=0.07264, avg_loss=0.07312]\n",
      "Step 544776  [5.414 sec/step, loss=0.07416, avg_loss=0.07310]\n",
      "Step 544777  [5.416 sec/step, loss=0.07351, avg_loss=0.07309]\n",
      "Step 544778  [5.430 sec/step, loss=0.07234, avg_loss=0.07311]\n",
      "Step 544779  [5.476 sec/step, loss=0.06536, avg_loss=0.07303]\n",
      "Step 544780  [5.473 sec/step, loss=0.07468, avg_loss=0.07305]\n",
      "Generated 32 batches of size 32 in 2.399 sec\n",
      "Step 544781  [5.485 sec/step, loss=0.07511, avg_loss=0.07306]\n",
      "Step 544782  [5.485 sec/step, loss=0.07506, avg_loss=0.07308]\n",
      "Step 544783  [5.498 sec/step, loss=0.07422, avg_loss=0.07309]\n",
      "Step 544784  [5.482 sec/step, loss=0.06632, avg_loss=0.07302]\n",
      "Step 544785  [5.466 sec/step, loss=0.07336, avg_loss=0.07300]\n",
      "Step 544786  [5.461 sec/step, loss=0.07126, avg_loss=0.07296]\n",
      "Step 544787  [5.467 sec/step, loss=0.07567, avg_loss=0.07297]\n",
      "Step 544788  [5.482 sec/step, loss=0.07573, avg_loss=0.07299]\n",
      "Step 544789  [5.479 sec/step, loss=0.07325, avg_loss=0.07297]\n",
      "Step 544790  [5.479 sec/step, loss=0.06545, avg_loss=0.07297]\n",
      "Step 544791  [5.480 sec/step, loss=0.07099, avg_loss=0.07294]\n",
      "Step 544792  [5.479 sec/step, loss=0.07461, avg_loss=0.07298]\n",
      "Step 544793  [5.505 sec/step, loss=0.07535, avg_loss=0.07304]\n",
      "Step 544794  [5.500 sec/step, loss=0.07410, avg_loss=0.07305]\n",
      "Step 544795  [5.490 sec/step, loss=0.07501, avg_loss=0.07308]\n",
      "Step 544796  [5.455 sec/step, loss=0.06437, avg_loss=0.07297]\n",
      "Step 544797  [5.446 sec/step, loss=0.07313, avg_loss=0.07296]\n",
      "Step 544798  [5.452 sec/step, loss=0.07516, avg_loss=0.07298]\n",
      "Step 544799  [5.451 sec/step, loss=0.07371, avg_loss=0.07297]\n",
      "Step 544800  [5.455 sec/step, loss=0.07499, avg_loss=0.07300]\n",
      "Writing summary at step: 544800\n",
      "Step 544801  [5.461 sec/step, loss=0.07431, avg_loss=0.07300]\n",
      "Step 544802  [5.446 sec/step, loss=0.07401, avg_loss=0.07299]\n",
      "Step 544803  [5.466 sec/step, loss=0.07552, avg_loss=0.07302]\n",
      "Step 544804  [5.479 sec/step, loss=0.07150, avg_loss=0.07309]\n",
      "Step 544805  [5.484 sec/step, loss=0.07467, avg_loss=0.07309]\n",
      "Step 544806  [5.500 sec/step, loss=0.07414, avg_loss=0.07312]\n",
      "Step 544807  [5.512 sec/step, loss=0.07554, avg_loss=0.07318]\n",
      "Step 544808  [5.523 sec/step, loss=0.07307, avg_loss=0.07317]\n",
      "Step 544809  [5.514 sec/step, loss=0.07525, avg_loss=0.07319]\n",
      "Step 544810  [5.497 sec/step, loss=0.07361, avg_loss=0.07318]\n",
      "Step 544811  [5.454 sec/step, loss=0.07431, avg_loss=0.07327]\n",
      "Generated 32 batches of size 32 in 2.527 sec\n",
      "Step 544812  [5.437 sec/step, loss=0.07021, avg_loss=0.07325]\n",
      "Step 544813  [5.458 sec/step, loss=0.07563, avg_loss=0.07331]\n",
      "Step 544814  [5.461 sec/step, loss=0.07347, avg_loss=0.07330]\n",
      "Step 544815  [5.460 sec/step, loss=0.07309, avg_loss=0.07329]\n",
      "Step 544816  [5.482 sec/step, loss=0.07524, avg_loss=0.07330]\n",
      "Step 544817  [5.475 sec/step, loss=0.07305, avg_loss=0.07328]\n",
      "Step 544818  [5.456 sec/step, loss=0.07273, avg_loss=0.07326]\n",
      "Step 544819  [5.439 sec/step, loss=0.07095, avg_loss=0.07323]\n",
      "Step 544820  [5.429 sec/step, loss=0.07452, avg_loss=0.07322]\n",
      "Step 544821  [5.410 sec/step, loss=0.07079, avg_loss=0.07316]\n",
      "Step 544822  [5.391 sec/step, loss=0.06522, avg_loss=0.07307]\n",
      "Step 544823  [5.419 sec/step, loss=0.07267, avg_loss=0.07306]\n",
      "Step 544824  [5.443 sec/step, loss=0.07281, avg_loss=0.07306]\n",
      "Step 544825  [5.444 sec/step, loss=0.07203, avg_loss=0.07303]\n",
      "Step 544826  [5.454 sec/step, loss=0.07519, avg_loss=0.07303]\n",
      "Step 544827  [5.446 sec/step, loss=0.07461, avg_loss=0.07302]\n",
      "Step 544828  [5.454 sec/step, loss=0.07483, avg_loss=0.07304]\n",
      "Step 544829  [5.457 sec/step, loss=0.07368, avg_loss=0.07304]\n",
      "Step 544830  [5.464 sec/step, loss=0.07608, avg_loss=0.07306]\n",
      "Step 544831  [5.513 sec/step, loss=0.06617, avg_loss=0.07299]\n",
      "Step 544832  [5.491 sec/step, loss=0.07329, avg_loss=0.07299]\n",
      "Step 544833  [5.509 sec/step, loss=0.07308, avg_loss=0.07300]\n",
      "Step 544834  [5.524 sec/step, loss=0.07393, avg_loss=0.07300]\n",
      "Step 544835  [5.543 sec/step, loss=0.07468, avg_loss=0.07308]\n",
      "Step 544836  [5.531 sec/step, loss=0.07307, avg_loss=0.07307]\n",
      "Step 544837  [5.536 sec/step, loss=0.07530, avg_loss=0.07311]\n",
      "Step 544838  [5.529 sec/step, loss=0.07266, avg_loss=0.07309]\n",
      "Step 544839  [5.510 sec/step, loss=0.07338, avg_loss=0.07307]\n",
      "Step 544840  [5.500 sec/step, loss=0.07168, avg_loss=0.07304]\n",
      "Step 544841  [5.507 sec/step, loss=0.07458, avg_loss=0.07308]\n",
      "Step 544842  [5.464 sec/step, loss=0.07556, avg_loss=0.07319]\n",
      "Step 544843  [5.459 sec/step, loss=0.07147, avg_loss=0.07316]\n",
      "Generated 32 batches of size 32 in 2.516 sec\n",
      "Step 544844  [5.445 sec/step, loss=0.07366, avg_loss=0.07314]\n",
      "Step 544845  [5.443 sec/step, loss=0.07466, avg_loss=0.07314]\n",
      "Step 544846  [5.461 sec/step, loss=0.07569, avg_loss=0.07317]\n",
      "Step 544847  [5.449 sec/step, loss=0.07261, avg_loss=0.07315]\n",
      "Step 544848  [5.444 sec/step, loss=0.07290, avg_loss=0.07317]\n",
      "Step 544849  [5.446 sec/step, loss=0.07612, avg_loss=0.07318]\n",
      "Step 544850  [5.430 sec/step, loss=0.07500, avg_loss=0.07317]\n",
      "Step 544851  [5.435 sec/step, loss=0.07334, avg_loss=0.07318]\n",
      "Step 544852  [5.439 sec/step, loss=0.07589, avg_loss=0.07322]\n",
      "Step 544853  [5.457 sec/step, loss=0.07449, avg_loss=0.07325]\n",
      "Step 544854  [5.467 sec/step, loss=0.07412, avg_loss=0.07325]\n",
      "Step 544855  [5.443 sec/step, loss=0.07328, avg_loss=0.07323]\n",
      "Step 544856  [5.492 sec/step, loss=0.06525, avg_loss=0.07315]\n",
      "Step 544857  [5.477 sec/step, loss=0.07552, avg_loss=0.07318]\n",
      "Step 544858  [5.476 sec/step, loss=0.07540, avg_loss=0.07319]\n",
      "Step 544859  [5.504 sec/step, loss=0.07521, avg_loss=0.07323]\n",
      "Step 544860  [5.517 sec/step, loss=0.07539, avg_loss=0.07327]\n",
      "Step 544861  [5.507 sec/step, loss=0.07456, avg_loss=0.07327]\n",
      "Step 544862  [5.513 sec/step, loss=0.07554, avg_loss=0.07329]\n",
      "Step 544863  [5.495 sec/step, loss=0.07290, avg_loss=0.07329]\n",
      "Step 544864  [5.462 sec/step, loss=0.06638, avg_loss=0.07320]\n",
      "Step 544865  [5.485 sec/step, loss=0.07414, avg_loss=0.07321]\n",
      "Step 544866  [5.473 sec/step, loss=0.07312, avg_loss=0.07320]\n",
      "Step 544867  [5.485 sec/step, loss=0.07466, avg_loss=0.07323]\n",
      "Step 544868  [5.491 sec/step, loss=0.07615, avg_loss=0.07325]\n",
      "Step 544869  [5.512 sec/step, loss=0.07530, avg_loss=0.07327]\n",
      "Step 544870  [5.524 sec/step, loss=0.07299, avg_loss=0.07327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 544871  [5.531 sec/step, loss=0.07224, avg_loss=0.07326]\n",
      "Step 544872  [5.523 sec/step, loss=0.07408, avg_loss=0.07326]\n",
      "Step 544873  [5.496 sec/step, loss=0.07427, avg_loss=0.07328]\n",
      "Step 544874  [5.479 sec/step, loss=0.07072, avg_loss=0.07327]\n",
      "Step 544875  [5.475 sec/step, loss=0.07292, avg_loss=0.07327]\n",
      "Generated 32 batches of size 32 in 2.388 sec\n",
      "Step 544876  [5.486 sec/step, loss=0.07389, avg_loss=0.07327]\n",
      "Step 544877  [5.480 sec/step, loss=0.07107, avg_loss=0.07324]\n",
      "Step 544878  [5.482 sec/step, loss=0.07024, avg_loss=0.07322]\n",
      "Step 544879  [5.431 sec/step, loss=0.07296, avg_loss=0.07330]\n",
      "Step 544880  [5.443 sec/step, loss=0.07436, avg_loss=0.07329]\n",
      "Step 544881  [5.416 sec/step, loss=0.07026, avg_loss=0.07325]\n",
      "Step 544882  [5.407 sec/step, loss=0.07407, avg_loss=0.07324]\n",
      "Step 544883  [5.394 sec/step, loss=0.07459, avg_loss=0.07324]\n",
      "Step 544884  [5.429 sec/step, loss=0.07417, avg_loss=0.07332]\n",
      "Step 544885  [5.442 sec/step, loss=0.07382, avg_loss=0.07332]\n",
      "Step 544886  [5.446 sec/step, loss=0.07385, avg_loss=0.07335]\n",
      "Step 544887  [5.443 sec/step, loss=0.07303, avg_loss=0.07332]\n",
      "Step 544888  [5.431 sec/step, loss=0.07319, avg_loss=0.07330]\n",
      "Step 544889  [5.430 sec/step, loss=0.07570, avg_loss=0.07332]\n",
      "Step 544890  [5.384 sec/step, loss=0.07440, avg_loss=0.07341]\n",
      "Step 544891  [5.384 sec/step, loss=0.07278, avg_loss=0.07343]\n",
      "Step 544892  [5.377 sec/step, loss=0.07308, avg_loss=0.07341]\n",
      "Step 544893  [5.383 sec/step, loss=0.07282, avg_loss=0.07339]\n",
      "Step 544894  [5.391 sec/step, loss=0.07314, avg_loss=0.07338]\n",
      "Step 544895  [5.371 sec/step, loss=0.07468, avg_loss=0.07338]\n",
      "Step 544896  [5.387 sec/step, loss=0.07378, avg_loss=0.07347]\n",
      "Step 544897  [5.392 sec/step, loss=0.07449, avg_loss=0.07348]\n",
      "Step 544898  [5.375 sec/step, loss=0.07333, avg_loss=0.07346]\n",
      "Step 544899  [5.372 sec/step, loss=0.07454, avg_loss=0.07347]\n",
      "Step 544900  [5.376 sec/step, loss=0.07489, avg_loss=0.07347]\n",
      "Writing summary at step: 544900\n",
      "Step 544901  [5.347 sec/step, loss=0.06532, avg_loss=0.07338]\n",
      "Step 544902  [5.374 sec/step, loss=0.07267, avg_loss=0.07337]\n",
      "Step 544903  [5.350 sec/step, loss=0.07069, avg_loss=0.07332]\n",
      "Step 544904  [5.341 sec/step, loss=0.07098, avg_loss=0.07332]\n",
      "Step 544905  [5.354 sec/step, loss=0.07526, avg_loss=0.07332]\n",
      "Step 544906  [5.355 sec/step, loss=0.07569, avg_loss=0.07334]\n",
      "Generated 32 batches of size 32 in 2.353 sec\n",
      "Step 544907  [5.361 sec/step, loss=0.07542, avg_loss=0.07334]\n",
      "Step 544908  [5.343 sec/step, loss=0.07375, avg_loss=0.07334]\n",
      "Step 544909  [5.350 sec/step, loss=0.07482, avg_loss=0.07334]\n",
      "Step 544910  [5.411 sec/step, loss=0.06623, avg_loss=0.07326]\n",
      "Step 544911  [5.400 sec/step, loss=0.07255, avg_loss=0.07325]\n",
      "Step 544912  [5.421 sec/step, loss=0.07546, avg_loss=0.07330]\n",
      "Step 544913  [5.413 sec/step, loss=0.07418, avg_loss=0.07328]\n",
      "Step 544914  [5.429 sec/step, loss=0.07493, avg_loss=0.07330]\n",
      "Step 544915  [5.435 sec/step, loss=0.07445, avg_loss=0.07331]\n",
      "Step 544916  [5.416 sec/step, loss=0.07234, avg_loss=0.07328]\n",
      "Step 544917  [5.421 sec/step, loss=0.07458, avg_loss=0.07330]\n",
      "Step 544918  [5.431 sec/step, loss=0.07373, avg_loss=0.07331]\n",
      "Step 544919  [5.454 sec/step, loss=0.07525, avg_loss=0.07335]\n",
      "Step 544920  [5.458 sec/step, loss=0.07436, avg_loss=0.07335]\n",
      "Step 544921  [5.481 sec/step, loss=0.07505, avg_loss=0.07339]\n",
      "Step 544922  [5.497 sec/step, loss=0.07335, avg_loss=0.07347]\n",
      "Step 544923  [5.467 sec/step, loss=0.07378, avg_loss=0.07349]\n",
      "Step 544924  [5.446 sec/step, loss=0.07293, avg_loss=0.07349]\n",
      "Step 544925  [5.429 sec/step, loss=0.07127, avg_loss=0.07348]\n",
      "Step 544926  [5.410 sec/step, loss=0.07347, avg_loss=0.07346]\n",
      "Step 544927  [5.409 sec/step, loss=0.07381, avg_loss=0.07345]\n",
      "Step 544928  [5.390 sec/step, loss=0.06415, avg_loss=0.07335]\n",
      "Step 544929  [5.383 sec/step, loss=0.07501, avg_loss=0.07336]\n",
      "Step 544930  [5.383 sec/step, loss=0.07592, avg_loss=0.07336]\n",
      "Step 544931  [5.346 sec/step, loss=0.07540, avg_loss=0.07345]\n",
      "Step 544932  [5.356 sec/step, loss=0.07241, avg_loss=0.07344]\n",
      "Step 544933  [5.348 sec/step, loss=0.07314, avg_loss=0.07344]\n",
      "Step 544934  [5.349 sec/step, loss=0.07328, avg_loss=0.07344]\n",
      "Step 544935  [5.343 sec/step, loss=0.07301, avg_loss=0.07342]\n",
      "Step 544936  [5.369 sec/step, loss=0.07209, avg_loss=0.07341]\n",
      "Step 544937  [5.373 sec/step, loss=0.07572, avg_loss=0.07341]\n",
      "Step 544938  [5.422 sec/step, loss=0.06477, avg_loss=0.07334]\n",
      "Generated 32 batches of size 32 in 2.672 sec\n",
      "Step 544939  [5.411 sec/step, loss=0.07004, avg_loss=0.07330]\n",
      "Step 544940  [5.436 sec/step, loss=0.07505, avg_loss=0.07334]\n",
      "Step 544941  [5.432 sec/step, loss=0.07288, avg_loss=0.07332]\n",
      "Step 544942  [5.412 sec/step, loss=0.07063, avg_loss=0.07327]\n",
      "Step 544943  [5.422 sec/step, loss=0.07375, avg_loss=0.07329]\n",
      "Step 544944  [5.439 sec/step, loss=0.07465, avg_loss=0.07330]\n",
      "Step 544945  [5.444 sec/step, loss=0.07455, avg_loss=0.07330]\n",
      "Step 544946  [5.426 sec/step, loss=0.07512, avg_loss=0.07330]\n",
      "Step 544947  [5.447 sec/step, loss=0.07489, avg_loss=0.07332]\n",
      "Step 544948  [5.468 sec/step, loss=0.07516, avg_loss=0.07334]\n",
      "Step 544949  [5.453 sec/step, loss=0.07408, avg_loss=0.07332]\n",
      "Step 544950  [5.463 sec/step, loss=0.07554, avg_loss=0.07333]\n",
      "Step 544951  [5.517 sec/step, loss=0.06721, avg_loss=0.07326]\n",
      "Step 544952  [5.514 sec/step, loss=0.07570, avg_loss=0.07326]\n",
      "Step 544953  [5.503 sec/step, loss=0.07303, avg_loss=0.07325]\n",
      "Step 544954  [5.480 sec/step, loss=0.07383, avg_loss=0.07325]\n",
      "Step 544955  [5.503 sec/step, loss=0.07599, avg_loss=0.07327]\n",
      "Step 544956  [5.466 sec/step, loss=0.07387, avg_loss=0.07336]\n",
      "Step 544957  [5.474 sec/step, loss=0.07613, avg_loss=0.07336]\n",
      "Step 544958  [5.473 sec/step, loss=0.07273, avg_loss=0.07334]\n",
      "Step 544959  [5.487 sec/step, loss=0.07316, avg_loss=0.07332]\n",
      "Step 544960  [5.484 sec/step, loss=0.07475, avg_loss=0.07331]\n",
      "Step 544961  [5.492 sec/step, loss=0.07382, avg_loss=0.07330]\n",
      "Step 544962  [5.484 sec/step, loss=0.07184, avg_loss=0.07327]\n",
      "Step 544963  [5.498 sec/step, loss=0.07486, avg_loss=0.07329]\n",
      "Step 544964  [5.500 sec/step, loss=0.06732, avg_loss=0.07330]\n",
      "Step 544965  [5.462 sec/step, loss=0.07140, avg_loss=0.07327]\n",
      "Step 544966  [5.470 sec/step, loss=0.07365, avg_loss=0.07327]\n",
      "Step 544967  [5.459 sec/step, loss=0.07137, avg_loss=0.07324]\n",
      "Step 544968  [5.454 sec/step, loss=0.07286, avg_loss=0.07321]\n",
      "Step 544969  [5.451 sec/step, loss=0.07542, avg_loss=0.07321]\n",
      "Step 544970  [5.445 sec/step, loss=0.07237, avg_loss=0.07320]\n",
      "Generated 32 batches of size 32 in 2.528 sec\n",
      "Step 544971  [5.445 sec/step, loss=0.06973, avg_loss=0.07318]\n",
      "Step 544972  [5.455 sec/step, loss=0.07384, avg_loss=0.07318]\n",
      "Step 544973  [5.451 sec/step, loss=0.07363, avg_loss=0.07317]\n",
      "Step 544974  [5.463 sec/step, loss=0.07250, avg_loss=0.07319]\n",
      "Step 544975  [5.472 sec/step, loss=0.07471, avg_loss=0.07320]\n",
      "Step 544976  [5.483 sec/step, loss=0.07297, avg_loss=0.07320]\n",
      "Step 544977  [5.481 sec/step, loss=0.07460, avg_loss=0.07323]\n",
      "Step 544978  [5.482 sec/step, loss=0.07476, avg_loss=0.07328]\n",
      "Step 544979  [5.478 sec/step, loss=0.07296, avg_loss=0.07328]\n",
      "Step 544980  [5.461 sec/step, loss=0.07360, avg_loss=0.07327]\n",
      "Step 544981  [5.471 sec/step, loss=0.07458, avg_loss=0.07331]\n",
      "Step 544982  [5.484 sec/step, loss=0.07520, avg_loss=0.07332]\n",
      "Step 544983  [5.493 sec/step, loss=0.07578, avg_loss=0.07333]\n",
      "Step 544984  [5.474 sec/step, loss=0.07390, avg_loss=0.07333]\n",
      "Step 544985  [5.459 sec/step, loss=0.07344, avg_loss=0.07333]\n",
      "Step 544986  [5.442 sec/step, loss=0.07068, avg_loss=0.07330]\n",
      "Step 544987  [5.436 sec/step, loss=0.07384, avg_loss=0.07330]\n",
      "Step 544988  [5.441 sec/step, loss=0.07239, avg_loss=0.07330]\n",
      "Step 544989  [5.415 sec/step, loss=0.06395, avg_loss=0.07318]\n",
      "Step 544990  [5.400 sec/step, loss=0.07055, avg_loss=0.07314]\n",
      "Step 544991  [5.412 sec/step, loss=0.07291, avg_loss=0.07314]\n",
      "Step 544992  [5.437 sec/step, loss=0.07439, avg_loss=0.07315]\n",
      "Step 544993  [5.409 sec/step, loss=0.07307, avg_loss=0.07316]\n",
      "Step 544994  [5.452 sec/step, loss=0.06769, avg_loss=0.07310]\n",
      "Step 544995  [5.453 sec/step, loss=0.07281, avg_loss=0.07308]\n",
      "Step 544996  [5.453 sec/step, loss=0.07429, avg_loss=0.07309]\n",
      "Step 544997  [5.439 sec/step, loss=0.07265, avg_loss=0.07307]\n",
      "Step 544998  [5.457 sec/step, loss=0.07517, avg_loss=0.07309]\n",
      "Step 544999  [5.460 sec/step, loss=0.07270, avg_loss=0.07307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 545000  [5.452 sec/step, loss=0.07085, avg_loss=0.07303]\n",
      "Writing summary at step: 545000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-545000\n",
      "Saving audio and alignment...\n",
      "Input: taauun miin tshaapaa kay liiay intaeldzans bhaydzoon tdoo athaavan fahsh masnuuaatd aap koo dabooayn gii~___________________\n",
      "Generated 32 batches of size 32 in 2.349 sec\n",
      "Step 545001  [5.485 sec/step, loss=0.07382, avg_loss=0.07312]\n",
      "Step 545002  [5.460 sec/step, loss=0.07445, avg_loss=0.07313]\n",
      "Step 545003  [5.471 sec/step, loss=0.07483, avg_loss=0.07317]\n",
      "Step 545004  [5.483 sec/step, loss=0.07117, avg_loss=0.07318]\n",
      "Step 545005  [5.486 sec/step, loss=0.07523, avg_loss=0.07318]\n",
      "Step 545006  [5.494 sec/step, loss=0.07478, avg_loss=0.07317]\n",
      "Step 545007  [5.504 sec/step, loss=0.07454, avg_loss=0.07316]\n",
      "Step 545008  [5.521 sec/step, loss=0.07498, avg_loss=0.07317]\n",
      "Step 545009  [5.518 sec/step, loss=0.07525, avg_loss=0.07318]\n",
      "Step 545010  [5.460 sec/step, loss=0.07350, avg_loss=0.07325]\n",
      "Step 545011  [5.458 sec/step, loss=0.07240, avg_loss=0.07325]\n",
      "Step 545012  [5.435 sec/step, loss=0.07272, avg_loss=0.07322]\n",
      "Step 545013  [5.431 sec/step, loss=0.07430, avg_loss=0.07322]\n",
      "Step 545014  [5.428 sec/step, loss=0.07361, avg_loss=0.07321]\n",
      "Step 545015  [5.406 sec/step, loss=0.06521, avg_loss=0.07311]\n",
      "Step 545016  [5.421 sec/step, loss=0.07520, avg_loss=0.07314]\n",
      "Step 545017  [5.415 sec/step, loss=0.07366, avg_loss=0.07313]\n",
      "Step 545018  [5.423 sec/step, loss=0.07296, avg_loss=0.07313]\n",
      "Step 545019  [5.419 sec/step, loss=0.07370, avg_loss=0.07311]\n",
      "Step 545020  [5.417 sec/step, loss=0.07555, avg_loss=0.07312]\n",
      "Step 545021  [5.407 sec/step, loss=0.07324, avg_loss=0.07310]\n",
      "Step 545022  [5.420 sec/step, loss=0.07481, avg_loss=0.07312]\n",
      "Step 545023  [5.419 sec/step, loss=0.07190, avg_loss=0.07310]\n",
      "Step 545024  [5.420 sec/step, loss=0.07256, avg_loss=0.07310]\n",
      "Step 545025  [5.423 sec/step, loss=0.07378, avg_loss=0.07312]\n",
      "Step 545026  [5.431 sec/step, loss=0.07123, avg_loss=0.07310]\n",
      "Step 545027  [5.447 sec/step, loss=0.07429, avg_loss=0.07310]\n",
      "Step 545028  [5.476 sec/step, loss=0.07527, avg_loss=0.07322]\n",
      "Step 545029  [5.464 sec/step, loss=0.07335, avg_loss=0.07320]\n",
      "Step 545030  [5.481 sec/step, loss=0.07262, avg_loss=0.07317]\n",
      "Step 545031  [5.517 sec/step, loss=0.06618, avg_loss=0.07307]\n",
      "Step 545032  [5.531 sec/step, loss=0.07315, avg_loss=0.07308]\n",
      "Generated 32 batches of size 32 in 2.875 sec\n",
      "Step 545033  [5.517 sec/step, loss=0.07116, avg_loss=0.07306]\n",
      "Step 545034  [5.517 sec/step, loss=0.07476, avg_loss=0.07308]\n",
      "Step 545035  [5.510 sec/step, loss=0.07025, avg_loss=0.07305]\n",
      "Step 545036  [5.487 sec/step, loss=0.07424, avg_loss=0.07307]\n",
      "Step 545037  [5.479 sec/step, loss=0.07437, avg_loss=0.07306]\n",
      "Step 545038  [5.432 sec/step, loss=0.07390, avg_loss=0.07315]\n",
      "Step 545039  [5.447 sec/step, loss=0.07540, avg_loss=0.07320]\n",
      "Step 545040  [5.447 sec/step, loss=0.07546, avg_loss=0.07320]\n",
      "Step 545041  [5.450 sec/step, loss=0.07453, avg_loss=0.07322]\n",
      "Step 545042  [5.514 sec/step, loss=0.06632, avg_loss=0.07318]\n",
      "Step 545043  [5.508 sec/step, loss=0.07510, avg_loss=0.07319]\n",
      "Step 545044  [5.487 sec/step, loss=0.07273, avg_loss=0.07317]\n",
      "Step 545045  [5.486 sec/step, loss=0.07392, avg_loss=0.07317]\n",
      "Step 545046  [5.486 sec/step, loss=0.07451, avg_loss=0.07316]\n",
      "Step 545047  [5.469 sec/step, loss=0.07104, avg_loss=0.07312]\n",
      "Step 545048  [5.462 sec/step, loss=0.07156, avg_loss=0.07309]\n",
      "Step 545049  [5.474 sec/step, loss=0.07370, avg_loss=0.07308]\n",
      "Step 545050  [5.461 sec/step, loss=0.07235, avg_loss=0.07305]\n",
      "Step 545051  [5.396 sec/step, loss=0.07147, avg_loss=0.07309]\n",
      "Step 545052  [5.411 sec/step, loss=0.07320, avg_loss=0.07307]\n",
      "Step 545053  [5.417 sec/step, loss=0.07087, avg_loss=0.07305]\n",
      "Step 545054  [5.440 sec/step, loss=0.07284, avg_loss=0.07304]\n",
      "Step 545055  [5.429 sec/step, loss=0.07330, avg_loss=0.07301]\n",
      "Step 545056  [5.410 sec/step, loss=0.07289, avg_loss=0.07300]\n",
      "Step 545057  [5.405 sec/step, loss=0.07532, avg_loss=0.07299]\n",
      "Step 545058  [5.404 sec/step, loss=0.07406, avg_loss=0.07300]\n",
      "Step 545059  [5.377 sec/step, loss=0.07520, avg_loss=0.07302]\n",
      "Step 545060  [5.361 sec/step, loss=0.07327, avg_loss=0.07301]\n",
      "Step 545061  [5.345 sec/step, loss=0.06989, avg_loss=0.07297]\n",
      "Step 545062  [5.339 sec/step, loss=0.07227, avg_loss=0.07298]\n",
      "Step 545063  [5.338 sec/step, loss=0.07521, avg_loss=0.07298]\n",
      "Step 545064  [5.363 sec/step, loss=0.07568, avg_loss=0.07306]\n",
      "Generated 32 batches of size 32 in 2.504 sec\n",
      "Step 545065  [5.381 sec/step, loss=0.07345, avg_loss=0.07308]\n",
      "Step 545066  [5.394 sec/step, loss=0.07607, avg_loss=0.07311]\n",
      "Step 545067  [5.417 sec/step, loss=0.07340, avg_loss=0.07313]\n",
      "Step 545068  [5.420 sec/step, loss=0.07509, avg_loss=0.07315]\n",
      "Step 545069  [5.395 sec/step, loss=0.06560, avg_loss=0.07305]\n",
      "Step 545070  [5.416 sec/step, loss=0.07297, avg_loss=0.07306]\n",
      "Step 545071  [5.419 sec/step, loss=0.07420, avg_loss=0.07310]\n",
      "Step 545072  [5.413 sec/step, loss=0.07442, avg_loss=0.07311]\n",
      "Step 545073  [5.423 sec/step, loss=0.07515, avg_loss=0.07312]\n",
      "Step 545074  [5.426 sec/step, loss=0.07404, avg_loss=0.07314]\n",
      "Step 545075  [5.413 sec/step, loss=0.07099, avg_loss=0.07310]\n",
      "Step 545076  [5.411 sec/step, loss=0.07398, avg_loss=0.07311]\n",
      "Step 545077  [5.411 sec/step, loss=0.07433, avg_loss=0.07311]\n",
      "Step 545078  [5.400 sec/step, loss=0.07363, avg_loss=0.07310]\n",
      "Step 545079  [5.389 sec/step, loss=0.06510, avg_loss=0.07302]\n",
      "Step 545080  [5.391 sec/step, loss=0.07206, avg_loss=0.07300]\n",
      "Step 545081  [5.404 sec/step, loss=0.07498, avg_loss=0.07301]\n",
      "Step 545082  [5.402 sec/step, loss=0.07617, avg_loss=0.07302]\n",
      "Step 545083  [5.374 sec/step, loss=0.07034, avg_loss=0.07296]\n",
      "Step 545084  [5.369 sec/step, loss=0.07032, avg_loss=0.07293]\n",
      "Step 545085  [5.384 sec/step, loss=0.07337, avg_loss=0.07293]\n",
      "Step 545086  [5.393 sec/step, loss=0.07326, avg_loss=0.07295]\n",
      "Step 545087  [5.392 sec/step, loss=0.07421, avg_loss=0.07296]\n",
      "Step 545088  [5.392 sec/step, loss=0.07306, avg_loss=0.07296]\n",
      "Step 545089  [5.421 sec/step, loss=0.07375, avg_loss=0.07306]\n",
      "Step 545090  [5.444 sec/step, loss=0.07361, avg_loss=0.07309]\n",
      "Step 545091  [5.434 sec/step, loss=0.07419, avg_loss=0.07310]\n",
      "Step 545092  [5.408 sec/step, loss=0.07353, avg_loss=0.07309]\n",
      "Step 545093  [5.423 sec/step, loss=0.07518, avg_loss=0.07312]\n",
      "Step 545094  [5.429 sec/step, loss=0.06503, avg_loss=0.07309]\n",
      "Step 545095  [5.435 sec/step, loss=0.07331, avg_loss=0.07309]\n",
      "Step 545096  [5.434 sec/step, loss=0.07415, avg_loss=0.07309]\n",
      "Generated 32 batches of size 32 in 2.407 sec\n",
      "Step 545097  [5.447 sec/step, loss=0.07298, avg_loss=0.07310]\n",
      "Step 545098  [5.439 sec/step, loss=0.07493, avg_loss=0.07309]\n",
      "Step 545099  [5.441 sec/step, loss=0.07326, avg_loss=0.07310]\n",
      "Step 545100  [5.465 sec/step, loss=0.07449, avg_loss=0.07314]\n",
      "Writing summary at step: 545100\n",
      "Step 545101  [5.444 sec/step, loss=0.07334, avg_loss=0.07313]\n",
      "Step 545102  [5.442 sec/step, loss=0.07340, avg_loss=0.07312]\n",
      "Step 545103  [5.450 sec/step, loss=0.07544, avg_loss=0.07313]\n",
      "Step 545104  [5.465 sec/step, loss=0.07492, avg_loss=0.07316]\n",
      "Step 545105  [5.449 sec/step, loss=0.07421, avg_loss=0.07315]\n",
      "Step 545106  [5.442 sec/step, loss=0.07191, avg_loss=0.07313]\n",
      "Step 545107  [5.418 sec/step, loss=0.07485, avg_loss=0.07313]\n",
      "Step 545108  [5.416 sec/step, loss=0.07552, avg_loss=0.07313]\n",
      "Step 545109  [5.422 sec/step, loss=0.07414, avg_loss=0.07312]\n",
      "Step 545110  [5.434 sec/step, loss=0.07252, avg_loss=0.07311]\n",
      "Step 545111  [5.439 sec/step, loss=0.07340, avg_loss=0.07312]\n",
      "Step 545112  [5.446 sec/step, loss=0.07312, avg_loss=0.07313]\n",
      "Step 545113  [5.457 sec/step, loss=0.07584, avg_loss=0.07314]\n",
      "Step 545114  [5.441 sec/step, loss=0.07321, avg_loss=0.07314]\n",
      "Step 545115  [5.452 sec/step, loss=0.07152, avg_loss=0.07320]\n",
      "Step 545116  [5.444 sec/step, loss=0.07405, avg_loss=0.07319]\n",
      "Step 545117  [5.495 sec/step, loss=0.06505, avg_loss=0.07310]\n",
      "Step 545118  [5.489 sec/step, loss=0.07410, avg_loss=0.07312]\n",
      "Step 545119  [5.482 sec/step, loss=0.07268, avg_loss=0.07311]\n",
      "Step 545120  [5.505 sec/step, loss=0.07426, avg_loss=0.07309]\n",
      "Step 545121  [5.500 sec/step, loss=0.07440, avg_loss=0.07310]\n",
      "Step 545122  [5.475 sec/step, loss=0.07032, avg_loss=0.07306]\n",
      "Step 545123  [5.491 sec/step, loss=0.07493, avg_loss=0.07309]\n",
      "Step 545124  [5.501 sec/step, loss=0.07495, avg_loss=0.07311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 545125  [5.507 sec/step, loss=0.07430, avg_loss=0.07312]\n",
      "Step 545126  [5.508 sec/step, loss=0.07292, avg_loss=0.07314]\n",
      "Step 545127  [5.477 sec/step, loss=0.07158, avg_loss=0.07311]\n",
      "Generated 32 batches of size 32 in 2.510 sec\n",
      "Step 545128  [5.464 sec/step, loss=0.07265, avg_loss=0.07308]\n",
      "Step 545129  [5.458 sec/step, loss=0.06596, avg_loss=0.07301]\n",
      "Step 545130  [5.428 sec/step, loss=0.07048, avg_loss=0.07299]\n",
      "Step 545131  [5.395 sec/step, loss=0.07416, avg_loss=0.07307]\n",
      "Step 545132  [5.380 sec/step, loss=0.07455, avg_loss=0.07308]\n",
      "Step 545133  [5.404 sec/step, loss=0.07387, avg_loss=0.07311]\n",
      "Step 545134  [5.407 sec/step, loss=0.07279, avg_loss=0.07309]\n",
      "Step 545135  [5.428 sec/step, loss=0.07488, avg_loss=0.07313]\n",
      "Step 545136  [5.413 sec/step, loss=0.07353, avg_loss=0.07313]\n",
      "Step 545137  [5.426 sec/step, loss=0.07363, avg_loss=0.07312]\n",
      "Step 545138  [5.425 sec/step, loss=0.07134, avg_loss=0.07309]\n",
      "Step 545139  [5.429 sec/step, loss=0.07562, avg_loss=0.07310]\n",
      "Step 545140  [5.419 sec/step, loss=0.07133, avg_loss=0.07305]\n",
      "Step 545141  [5.422 sec/step, loss=0.07322, avg_loss=0.07304]\n",
      "Step 545142  [5.365 sec/step, loss=0.07329, avg_loss=0.07311]\n",
      "Step 545143  [5.362 sec/step, loss=0.07367, avg_loss=0.07310]\n",
      "Step 545144  [5.378 sec/step, loss=0.07461, avg_loss=0.07312]\n",
      "Step 545145  [5.369 sec/step, loss=0.07237, avg_loss=0.07310]\n",
      "Step 545146  [5.362 sec/step, loss=0.06981, avg_loss=0.07305]\n",
      "Step 545147  [5.375 sec/step, loss=0.07610, avg_loss=0.07310]\n",
      "Step 545148  [5.377 sec/step, loss=0.07497, avg_loss=0.07314]\n",
      "Step 545149  [5.361 sec/step, loss=0.07309, avg_loss=0.07313]\n",
      "Step 545150  [5.369 sec/step, loss=0.07425, avg_loss=0.07315]\n",
      "Step 545151  [5.369 sec/step, loss=0.07052, avg_loss=0.07314]\n",
      "Step 545152  [5.353 sec/step, loss=0.07566, avg_loss=0.07317]\n",
      "Step 545153  [5.396 sec/step, loss=0.06796, avg_loss=0.07314]\n",
      "Step 545154  [5.376 sec/step, loss=0.07454, avg_loss=0.07315]\n",
      "Step 545155  [5.387 sec/step, loss=0.07278, avg_loss=0.07315]\n",
      "Step 545156  [5.399 sec/step, loss=0.07107, avg_loss=0.07313]\n",
      "Step 545157  [5.399 sec/step, loss=0.07538, avg_loss=0.07313]\n",
      "Step 545158  [5.394 sec/step, loss=0.07383, avg_loss=0.07313]\n",
      "Step 545159  [5.413 sec/step, loss=0.07195, avg_loss=0.07310]\n",
      "Generated 32 batches of size 32 in 2.575 sec\n",
      "Step 545160  [5.421 sec/step, loss=0.07308, avg_loss=0.07309]\n",
      "Step 545161  [5.441 sec/step, loss=0.07517, avg_loss=0.07315]\n",
      "Step 545162  [5.448 sec/step, loss=0.07514, avg_loss=0.07318]\n",
      "Step 545163  [5.441 sec/step, loss=0.07414, avg_loss=0.07317]\n",
      "Step 545164  [5.458 sec/step, loss=0.07259, avg_loss=0.07313]\n",
      "Step 545165  [5.436 sec/step, loss=0.06476, avg_loss=0.07305]\n",
      "Step 545166  [5.419 sec/step, loss=0.07123, avg_loss=0.07300]\n",
      "Step 545167  [5.396 sec/step, loss=0.07319, avg_loss=0.07300]\n",
      "Step 545168  [5.391 sec/step, loss=0.07395, avg_loss=0.07299]\n",
      "Step 545169  [5.419 sec/step, loss=0.07514, avg_loss=0.07308]\n",
      "Step 545170  [5.411 sec/step, loss=0.07226, avg_loss=0.07307]\n",
      "Step 545171  [5.410 sec/step, loss=0.07035, avg_loss=0.07304]\n",
      "Step 545172  [5.398 sec/step, loss=0.07306, avg_loss=0.07302]\n",
      "Step 545173  [5.379 sec/step, loss=0.06934, avg_loss=0.07296]\n",
      "Step 545174  [5.386 sec/step, loss=0.07555, avg_loss=0.07298]\n",
      "Step 545175  [5.423 sec/step, loss=0.07245, avg_loss=0.07299]\n",
      "Step 545176  [5.407 sec/step, loss=0.07436, avg_loss=0.07300]\n",
      "Step 545177  [5.407 sec/step, loss=0.07494, avg_loss=0.07300]\n",
      "Step 545178  [5.416 sec/step, loss=0.07472, avg_loss=0.07301]\n",
      "Step 545179  [5.443 sec/step, loss=0.07307, avg_loss=0.07309]\n",
      "Step 545180  [5.432 sec/step, loss=0.06389, avg_loss=0.07301]\n",
      "Step 545181  [5.432 sec/step, loss=0.07528, avg_loss=0.07302]\n",
      "Step 545182  [5.424 sec/step, loss=0.07443, avg_loss=0.07300]\n",
      "Step 545183  [5.435 sec/step, loss=0.07388, avg_loss=0.07303]\n",
      "Step 545184  [5.430 sec/step, loss=0.07142, avg_loss=0.07304]\n",
      "Step 545185  [5.429 sec/step, loss=0.07356, avg_loss=0.07305]\n",
      "Step 545186  [5.446 sec/step, loss=0.07406, avg_loss=0.07305]\n",
      "Step 545187  [5.455 sec/step, loss=0.07530, avg_loss=0.07307]\n",
      "Step 545188  [5.446 sec/step, loss=0.07310, avg_loss=0.07307]\n",
      "Step 545189  [5.444 sec/step, loss=0.07326, avg_loss=0.07306]\n",
      "Step 545190  [5.439 sec/step, loss=0.07484, avg_loss=0.07307]\n",
      "Step 545191  [5.435 sec/step, loss=0.07040, avg_loss=0.07304]\n",
      "Generated 32 batches of size 32 in 2.387 sec\n",
      "Step 545192  [5.454 sec/step, loss=0.07453, avg_loss=0.07305]\n",
      "Step 545193  [5.445 sec/step, loss=0.07344, avg_loss=0.07303]\n",
      "Step 545194  [5.381 sec/step, loss=0.07076, avg_loss=0.07308]\n",
      "Step 545195  [5.372 sec/step, loss=0.07289, avg_loss=0.07308]\n",
      "Step 545196  [5.376 sec/step, loss=0.07424, avg_loss=0.07308]\n",
      "Step 545197  [5.365 sec/step, loss=0.07417, avg_loss=0.07309]\n",
      "Step 545198  [5.412 sec/step, loss=0.06625, avg_loss=0.07301]\n",
      "Step 545199  [5.413 sec/step, loss=0.07562, avg_loss=0.07303]\n",
      "Step 545200  [5.387 sec/step, loss=0.07239, avg_loss=0.07301]\n",
      "Writing summary at step: 545200\n",
      "Step 545201  [5.389 sec/step, loss=0.07077, avg_loss=0.07298]\n",
      "Step 545202  [5.395 sec/step, loss=0.07482, avg_loss=0.07300]\n",
      "Step 545203  [5.400 sec/step, loss=0.07519, avg_loss=0.07300]\n",
      "Step 545204  [5.386 sec/step, loss=0.07275, avg_loss=0.07297]\n",
      "Step 545205  [5.370 sec/step, loss=0.07178, avg_loss=0.07295]\n",
      "Step 545206  [5.370 sec/step, loss=0.07153, avg_loss=0.07295]\n",
      "Step 545207  [5.368 sec/step, loss=0.07378, avg_loss=0.07293]\n",
      "Step 545208  [5.361 sec/step, loss=0.07272, avg_loss=0.07291]\n",
      "Step 545209  [5.359 sec/step, loss=0.07520, avg_loss=0.07292]\n",
      "Step 545210  [5.352 sec/step, loss=0.07064, avg_loss=0.07290]\n",
      "Step 545211  [5.346 sec/step, loss=0.07154, avg_loss=0.07288]\n",
      "Step 545212  [5.338 sec/step, loss=0.07188, avg_loss=0.07287]\n",
      "Step 545213  [5.331 sec/step, loss=0.07415, avg_loss=0.07285]\n",
      "Step 545214  [5.346 sec/step, loss=0.07578, avg_loss=0.07288]\n",
      "Step 545215  [5.365 sec/step, loss=0.07515, avg_loss=0.07291]\n",
      "Step 545216  [5.368 sec/step, loss=0.07354, avg_loss=0.07291]\n",
      "Step 545217  [5.319 sec/step, loss=0.07407, avg_loss=0.07300]\n",
      "Step 545218  [5.296 sec/step, loss=0.06566, avg_loss=0.07291]\n",
      "Step 545219  [5.298 sec/step, loss=0.07432, avg_loss=0.07293]\n",
      "Step 545220  [5.284 sec/step, loss=0.07544, avg_loss=0.07294]\n",
      "Step 545221  [5.279 sec/step, loss=0.07322, avg_loss=0.07293]\n",
      "Step 545222  [5.309 sec/step, loss=0.07206, avg_loss=0.07295]\n",
      "Generated 32 batches of size 32 in 2.479 sec\n",
      "Step 545223  [5.314 sec/step, loss=0.07291, avg_loss=0.07293]\n",
      "Step 545224  [5.302 sec/step, loss=0.07301, avg_loss=0.07291]\n",
      "Step 545225  [5.302 sec/step, loss=0.07372, avg_loss=0.07290]\n",
      "Step 545226  [5.299 sec/step, loss=0.07252, avg_loss=0.07290]\n",
      "Step 545227  [5.310 sec/step, loss=0.07451, avg_loss=0.07293]\n",
      "Step 545228  [5.363 sec/step, loss=0.06604, avg_loss=0.07286]\n",
      "Step 545229  [5.384 sec/step, loss=0.07446, avg_loss=0.07295]\n",
      "Step 545230  [5.400 sec/step, loss=0.07273, avg_loss=0.07297]\n",
      "Step 545231  [5.409 sec/step, loss=0.07270, avg_loss=0.07295]\n",
      "Step 545232  [5.418 sec/step, loss=0.07442, avg_loss=0.07295]\n",
      "Step 545233  [5.389 sec/step, loss=0.07139, avg_loss=0.07293]\n",
      "Step 545234  [5.382 sec/step, loss=0.07424, avg_loss=0.07294]\n",
      "Step 545235  [5.353 sec/step, loss=0.06416, avg_loss=0.07283]\n",
      "Step 545236  [5.359 sec/step, loss=0.07409, avg_loss=0.07284]\n",
      "Step 545237  [5.347 sec/step, loss=0.07418, avg_loss=0.07285]\n",
      "Step 545238  [5.370 sec/step, loss=0.07188, avg_loss=0.07285]\n",
      "Step 545239  [5.367 sec/step, loss=0.07312, avg_loss=0.07283]\n",
      "Step 545240  [5.359 sec/step, loss=0.07277, avg_loss=0.07284]\n",
      "Step 545241  [5.369 sec/step, loss=0.07410, avg_loss=0.07285]\n",
      "Step 545242  [5.364 sec/step, loss=0.07081, avg_loss=0.07283]\n",
      "Step 545243  [5.379 sec/step, loss=0.07507, avg_loss=0.07284]\n",
      "Step 545244  [5.369 sec/step, loss=0.07379, avg_loss=0.07283]\n",
      "Step 545245  [5.390 sec/step, loss=0.07425, avg_loss=0.07285]\n",
      "Step 545246  [5.401 sec/step, loss=0.07122, avg_loss=0.07286]\n",
      "Step 545247  [5.400 sec/step, loss=0.07466, avg_loss=0.07285]\n",
      "Step 545248  [5.401 sec/step, loss=0.07495, avg_loss=0.07285]\n",
      "Step 545249  [5.416 sec/step, loss=0.07525, avg_loss=0.07287]\n",
      "Step 545250  [5.463 sec/step, loss=0.06568, avg_loss=0.07279]\n",
      "Step 545251  [5.484 sec/step, loss=0.07500, avg_loss=0.07283]\n",
      "Step 545252  [5.471 sec/step, loss=0.07245, avg_loss=0.07280]\n",
      "Step 545253  [5.430 sec/step, loss=0.07314, avg_loss=0.07285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 545254  [5.444 sec/step, loss=0.07490, avg_loss=0.07285]\n",
      "Generated 32 batches of size 32 in 2.427 sec\n",
      "Step 545255  [5.440 sec/step, loss=0.07515, avg_loss=0.07288]\n",
      "Step 545256  [5.428 sec/step, loss=0.07329, avg_loss=0.07290]\n",
      "Step 545257  [5.423 sec/step, loss=0.07381, avg_loss=0.07288]\n",
      "Step 545258  [5.425 sec/step, loss=0.07058, avg_loss=0.07285]\n",
      "Step 545259  [5.402 sec/step, loss=0.07403, avg_loss=0.07287]\n",
      "Step 545260  [5.402 sec/step, loss=0.07279, avg_loss=0.07287]\n",
      "Step 545261  [5.398 sec/step, loss=0.07343, avg_loss=0.07285]\n",
      "Step 545262  [5.386 sec/step, loss=0.07309, avg_loss=0.07283]\n",
      "Step 545263  [5.398 sec/step, loss=0.07554, avg_loss=0.07285]\n",
      "Step 545264  [5.383 sec/step, loss=0.07475, avg_loss=0.07287]\n",
      "Step 545265  [5.384 sec/step, loss=0.06484, avg_loss=0.07287]\n",
      "Step 545266  [5.395 sec/step, loss=0.07481, avg_loss=0.07290]\n",
      "Step 545267  [5.408 sec/step, loss=0.07414, avg_loss=0.07291]\n",
      "Step 545268  [5.389 sec/step, loss=0.07033, avg_loss=0.07288]\n",
      "Step 545269  [5.374 sec/step, loss=0.07401, avg_loss=0.07287]\n",
      "Step 545270  [5.348 sec/step, loss=0.07337, avg_loss=0.07288]\n",
      "Step 545271  [5.368 sec/step, loss=0.07471, avg_loss=0.07292]\n",
      "Step 545272  [5.376 sec/step, loss=0.07424, avg_loss=0.07293]\n",
      "Step 545273  [5.385 sec/step, loss=0.07282, avg_loss=0.07297]\n",
      "Step 545274  [5.372 sec/step, loss=0.07071, avg_loss=0.07292]\n",
      "Step 545275  [5.351 sec/step, loss=0.07398, avg_loss=0.07293]\n",
      "Step 545276  [5.345 sec/step, loss=0.07434, avg_loss=0.07293]\n",
      "Step 545277  [5.346 sec/step, loss=0.07166, avg_loss=0.07290]\n",
      "Step 545278  [5.346 sec/step, loss=0.07374, avg_loss=0.07289]\n",
      "Step 545279  [5.322 sec/step, loss=0.06945, avg_loss=0.07285]\n",
      "Step 545280  [5.327 sec/step, loss=0.07344, avg_loss=0.07295]\n",
      "Step 545281  [5.310 sec/step, loss=0.07239, avg_loss=0.07292]\n",
      "Step 545282  [5.318 sec/step, loss=0.07491, avg_loss=0.07293]\n",
      "Step 545283  [5.321 sec/step, loss=0.07438, avg_loss=0.07293]\n",
      "Step 545284  [5.382 sec/step, loss=0.06486, avg_loss=0.07286]\n",
      "Step 545285  [5.392 sec/step, loss=0.07507, avg_loss=0.07288]\n",
      "Step 545286  [5.390 sec/step, loss=0.07566, avg_loss=0.07290]\n",
      "Generated 32 batches of size 32 in 2.534 sec\n",
      "Step 545287  [5.381 sec/step, loss=0.07289, avg_loss=0.07287]\n",
      "Step 545288  [5.397 sec/step, loss=0.07581, avg_loss=0.07290]\n",
      "Step 545289  [5.391 sec/step, loss=0.07487, avg_loss=0.07292]\n",
      "Step 545290  [5.395 sec/step, loss=0.07339, avg_loss=0.07290]\n",
      "Step 545291  [5.425 sec/step, loss=0.07511, avg_loss=0.07295]\n",
      "Step 545292  [5.421 sec/step, loss=0.07255, avg_loss=0.07293]\n",
      "Step 545293  [5.415 sec/step, loss=0.07128, avg_loss=0.07291]\n",
      "Step 545294  [5.436 sec/step, loss=0.07334, avg_loss=0.07293]\n",
      "Step 545295  [5.457 sec/step, loss=0.07482, avg_loss=0.07295]\n",
      "Step 545296  [5.470 sec/step, loss=0.07511, avg_loss=0.07296]\n",
      "Step 545297  [5.492 sec/step, loss=0.07267, avg_loss=0.07295]\n",
      "Step 545298  [5.453 sec/step, loss=0.07437, avg_loss=0.07303]\n",
      "Step 545299  [5.442 sec/step, loss=0.07204, avg_loss=0.07299]\n",
      "Step 545300  [5.447 sec/step, loss=0.07388, avg_loss=0.07301]\n",
      "Writing summary at step: 545300\n",
      "Step 545301  [5.444 sec/step, loss=0.06968, avg_loss=0.07299]\n",
      "Step 545302  [5.442 sec/step, loss=0.07398, avg_loss=0.07299]\n",
      "Step 545303  [5.427 sec/step, loss=0.07366, avg_loss=0.07297]\n",
      "Step 545304  [5.441 sec/step, loss=0.07511, avg_loss=0.07299]\n",
      "Step 545305  [5.439 sec/step, loss=0.06709, avg_loss=0.07295]\n",
      "Step 545306  [5.440 sec/step, loss=0.07477, avg_loss=0.07298]\n",
      "Step 545307  [5.436 sec/step, loss=0.07321, avg_loss=0.07297]\n",
      "Step 545308  [5.450 sec/step, loss=0.07587, avg_loss=0.07301]\n",
      "Step 545309  [5.441 sec/step, loss=0.07340, avg_loss=0.07299]\n",
      "Step 545310  [5.443 sec/step, loss=0.07393, avg_loss=0.07302]\n",
      "Step 545311  [5.447 sec/step, loss=0.07446, avg_loss=0.07305]\n",
      "Step 545312  [5.450 sec/step, loss=0.07355, avg_loss=0.07307]\n",
      "Step 545313  [5.446 sec/step, loss=0.07407, avg_loss=0.07307]\n",
      "Step 545314  [5.445 sec/step, loss=0.07542, avg_loss=0.07306]\n",
      "Step 545315  [5.428 sec/step, loss=0.07205, avg_loss=0.07303]\n",
      "Step 545316  [5.410 sec/step, loss=0.07133, avg_loss=0.07301]\n",
      "Step 545317  [5.417 sec/step, loss=0.07520, avg_loss=0.07302]\n",
      "Generated 32 batches of size 32 in 2.678 sec\n",
      "Step 545318  [5.490 sec/step, loss=0.06598, avg_loss=0.07302]\n",
      "Step 545319  [5.494 sec/step, loss=0.07431, avg_loss=0.07302]\n",
      "Step 545320  [5.492 sec/step, loss=0.07449, avg_loss=0.07301]\n",
      "Step 545321  [5.527 sec/step, loss=0.07220, avg_loss=0.07300]\n",
      "Step 545322  [5.512 sec/step, loss=0.07439, avg_loss=0.07303]\n",
      "Step 545323  [5.507 sec/step, loss=0.07343, avg_loss=0.07303]\n",
      "Step 545324  [5.503 sec/step, loss=0.07336, avg_loss=0.07304]\n",
      "Step 545325  [5.513 sec/step, loss=0.07392, avg_loss=0.07304]\n",
      "Step 545326  [5.507 sec/step, loss=0.07106, avg_loss=0.07302]\n",
      "Step 545327  [5.519 sec/step, loss=0.07521, avg_loss=0.07303]\n",
      "Step 545328  [5.469 sec/step, loss=0.07467, avg_loss=0.07312]\n",
      "Step 545329  [5.478 sec/step, loss=0.07490, avg_loss=0.07312]\n",
      "Step 545330  [5.478 sec/step, loss=0.07515, avg_loss=0.07314]\n",
      "Step 545331  [5.453 sec/step, loss=0.07165, avg_loss=0.07313]\n",
      "Step 545332  [5.448 sec/step, loss=0.07327, avg_loss=0.07312]\n",
      "Step 545333  [5.472 sec/step, loss=0.07515, avg_loss=0.07316]\n",
      "Step 545334  [5.522 sec/step, loss=0.06567, avg_loss=0.07307]\n",
      "Step 545335  [5.554 sec/step, loss=0.07534, avg_loss=0.07319]\n",
      "Step 545336  [5.567 sec/step, loss=0.07461, avg_loss=0.07319]\n",
      "Step 545337  [5.564 sec/step, loss=0.07418, avg_loss=0.07319]\n",
      "Step 545338  [5.528 sec/step, loss=0.07003, avg_loss=0.07317]\n",
      "Step 545339  [5.516 sec/step, loss=0.07157, avg_loss=0.07316]\n",
      "Step 545340  [5.509 sec/step, loss=0.06960, avg_loss=0.07313]\n",
      "Step 545341  [5.500 sec/step, loss=0.07337, avg_loss=0.07312]\n",
      "Step 545342  [5.509 sec/step, loss=0.07266, avg_loss=0.07314]\n",
      "Step 545343  [5.503 sec/step, loss=0.07144, avg_loss=0.07310]\n",
      "Step 545344  [5.513 sec/step, loss=0.07555, avg_loss=0.07312]\n",
      "Step 545345  [5.503 sec/step, loss=0.07554, avg_loss=0.07313]\n",
      "Step 545346  [5.493 sec/step, loss=0.07298, avg_loss=0.07315]\n",
      "Step 545347  [5.481 sec/step, loss=0.07537, avg_loss=0.07316]\n",
      "Step 545348  [5.456 sec/step, loss=0.06528, avg_loss=0.07306]\n",
      "Step 545349  [5.438 sec/step, loss=0.07391, avg_loss=0.07305]\n",
      "Generated 32 batches of size 32 in 2.529 sec\n",
      "Step 545350  [5.383 sec/step, loss=0.07343, avg_loss=0.07312]\n",
      "Step 545351  [5.388 sec/step, loss=0.07332, avg_loss=0.07311]\n",
      "Step 545352  [5.398 sec/step, loss=0.07425, avg_loss=0.07312]\n",
      "Step 545353  [5.395 sec/step, loss=0.07422, avg_loss=0.07314]\n",
      "Step 545354  [5.412 sec/step, loss=0.07218, avg_loss=0.07311]\n",
      "Step 545355  [5.409 sec/step, loss=0.07403, avg_loss=0.07310]\n",
      "Step 545356  [5.419 sec/step, loss=0.07189, avg_loss=0.07308]\n",
      "Step 545357  [5.411 sec/step, loss=0.07149, avg_loss=0.07306]\n",
      "Step 545358  [5.431 sec/step, loss=0.07452, avg_loss=0.07310]\n",
      "Step 545359  [5.462 sec/step, loss=0.07184, avg_loss=0.07308]\n",
      "Step 545360  [5.465 sec/step, loss=0.07352, avg_loss=0.07308]\n",
      "Step 545361  [5.471 sec/step, loss=0.07491, avg_loss=0.07310]\n",
      "Step 545362  [5.500 sec/step, loss=0.07487, avg_loss=0.07312]\n",
      "Step 545363  [5.488 sec/step, loss=0.07378, avg_loss=0.07310]\n",
      "Step 545364  [5.475 sec/step, loss=0.06997, avg_loss=0.07305]\n",
      "Step 545365  [5.493 sec/step, loss=0.07433, avg_loss=0.07315]\n",
      "Step 545366  [5.497 sec/step, loss=0.07526, avg_loss=0.07315]\n",
      "Step 545367  [5.499 sec/step, loss=0.07403, avg_loss=0.07315]\n",
      "Step 545368  [5.511 sec/step, loss=0.07424, avg_loss=0.07319]\n",
      "Step 545369  [5.507 sec/step, loss=0.07280, avg_loss=0.07318]\n",
      "Step 545370  [5.514 sec/step, loss=0.07180, avg_loss=0.07316]\n",
      "Step 545371  [5.498 sec/step, loss=0.07463, avg_loss=0.07316]\n",
      "Step 545372  [5.494 sec/step, loss=0.07214, avg_loss=0.07314]\n",
      "Step 545373  [5.506 sec/step, loss=0.07569, avg_loss=0.07317]\n",
      "Step 545374  [5.526 sec/step, loss=0.07524, avg_loss=0.07321]\n",
      "Step 545375  [5.514 sec/step, loss=0.07261, avg_loss=0.07320]\n",
      "Step 545376  [5.513 sec/step, loss=0.07321, avg_loss=0.07319]\n",
      "Step 545377  [5.522 sec/step, loss=0.07250, avg_loss=0.07320]\n",
      "Step 545378  [5.506 sec/step, loss=0.07104, avg_loss=0.07317]\n",
      "Step 545379  [5.515 sec/step, loss=0.07433, avg_loss=0.07322]\n",
      "Step 545380  [5.534 sec/step, loss=0.07464, avg_loss=0.07323]\n",
      "Step 545381  [5.544 sec/step, loss=0.07349, avg_loss=0.07324]\n",
      "Generated 32 batches of size 32 in 2.924 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 545382  [5.523 sec/step, loss=0.06545, avg_loss=0.07315]\n",
      "Step 545383  [5.529 sec/step, loss=0.07548, avg_loss=0.07316]\n",
      "Step 545384  [5.527 sec/step, loss=0.06540, avg_loss=0.07316]\n",
      "Step 545385  [5.525 sec/step, loss=0.07271, avg_loss=0.07314]\n",
      "Step 545386  [5.519 sec/step, loss=0.07353, avg_loss=0.07312]\n",
      "Step 545387  [5.504 sec/step, loss=0.07336, avg_loss=0.07312]\n",
      "Step 545388  [5.488 sec/step, loss=0.07067, avg_loss=0.07307]\n",
      "Step 545389  [5.486 sec/step, loss=0.07311, avg_loss=0.07305]\n",
      "Step 545390  [5.486 sec/step, loss=0.07521, avg_loss=0.07307]\n",
      "Step 545391  [5.450 sec/step, loss=0.07040, avg_loss=0.07303]\n",
      "Step 545392  [5.426 sec/step, loss=0.06527, avg_loss=0.07295]\n",
      "Step 545393  [5.448 sec/step, loss=0.07518, avg_loss=0.07299]\n",
      "Step 545394  [5.467 sec/step, loss=0.07430, avg_loss=0.07300]\n",
      "Step 545395  [5.443 sec/step, loss=0.06954, avg_loss=0.07295]\n",
      "Step 545396  [5.428 sec/step, loss=0.07414, avg_loss=0.07294]\n",
      "Step 545397  [5.416 sec/step, loss=0.07436, avg_loss=0.07296]\n",
      "Step 545398  [5.409 sec/step, loss=0.07409, avg_loss=0.07295]\n",
      "Step 545399  [5.409 sec/step, loss=0.07216, avg_loss=0.07295]\n",
      "Step 545400  [5.418 sec/step, loss=0.07546, avg_loss=0.07297]\n",
      "Writing summary at step: 545400\n",
      "Step 545401  [5.419 sec/step, loss=0.07279, avg_loss=0.07300]\n",
      "Step 545402  [5.427 sec/step, loss=0.07356, avg_loss=0.07300]\n",
      "Step 545403  [5.413 sec/step, loss=0.07128, avg_loss=0.07297]\n",
      "Step 545404  [5.411 sec/step, loss=0.07519, avg_loss=0.07297]\n",
      "Step 545405  [5.436 sec/step, loss=0.07529, avg_loss=0.07306]\n",
      "Step 545406  [5.425 sec/step, loss=0.07351, avg_loss=0.07304]\n",
      "Step 545407  [5.442 sec/step, loss=0.07512, avg_loss=0.07306]\n",
      "Step 545408  [5.428 sec/step, loss=0.07343, avg_loss=0.07304]\n",
      "Step 545409  [5.432 sec/step, loss=0.07400, avg_loss=0.07304]\n",
      "Step 545410  [5.433 sec/step, loss=0.07452, avg_loss=0.07305]\n",
      "Step 545411  [5.435 sec/step, loss=0.07372, avg_loss=0.07304]\n",
      "Step 545412  [5.444 sec/step, loss=0.07512, avg_loss=0.07306]\n",
      "Generated 32 batches of size 32 in 2.665 sec\n",
      "Step 545413  [5.449 sec/step, loss=0.07385, avg_loss=0.07306]\n",
      "Step 545414  [5.439 sec/step, loss=0.07186, avg_loss=0.07302]\n",
      "Step 545415  [5.453 sec/step, loss=0.07258, avg_loss=0.07303]\n",
      "Step 545416  [5.485 sec/step, loss=0.07488, avg_loss=0.07306]\n",
      "Step 545417  [5.480 sec/step, loss=0.07446, avg_loss=0.07305]\n",
      "Step 545418  [5.431 sec/step, loss=0.07423, avg_loss=0.07314]\n",
      "Step 545419  [5.477 sec/step, loss=0.06502, avg_loss=0.07304]\n",
      "Step 545420  [5.472 sec/step, loss=0.07108, avg_loss=0.07301]\n",
      "Step 545421  [5.455 sec/step, loss=0.07399, avg_loss=0.07303]\n",
      "Step 545422  [5.455 sec/step, loss=0.07387, avg_loss=0.07302]\n",
      "Step 545423  [5.450 sec/step, loss=0.07352, avg_loss=0.07302]\n",
      "Step 545424  [5.468 sec/step, loss=0.07540, avg_loss=0.07304]\n",
      "Step 545425  [5.438 sec/step, loss=0.06500, avg_loss=0.07295]\n",
      "Step 545426  [5.453 sec/step, loss=0.07342, avg_loss=0.07298]\n",
      "Step 545427  [5.451 sec/step, loss=0.07494, avg_loss=0.07298]\n",
      "Step 545428  [5.449 sec/step, loss=0.07307, avg_loss=0.07296]\n",
      "Step 545429  [5.451 sec/step, loss=0.07542, avg_loss=0.07296]\n",
      "Step 545430  [5.442 sec/step, loss=0.07324, avg_loss=0.07295]\n",
      "Step 545431  [5.438 sec/step, loss=0.07409, avg_loss=0.07297]\n",
      "Step 545432  [5.428 sec/step, loss=0.07129, avg_loss=0.07295]\n",
      "Step 545433  [5.415 sec/step, loss=0.07055, avg_loss=0.07290]\n",
      "Step 545434  [5.350 sec/step, loss=0.07118, avg_loss=0.07296]\n",
      "Step 545435  [5.340 sec/step, loss=0.07442, avg_loss=0.07295]\n",
      "Step 545436  [5.344 sec/step, loss=0.07307, avg_loss=0.07293]\n",
      "Step 545437  [5.343 sec/step, loss=0.07204, avg_loss=0.07291]\n",
      "Step 545438  [5.379 sec/step, loss=0.07279, avg_loss=0.07294]\n",
      "Step 545439  [5.372 sec/step, loss=0.07335, avg_loss=0.07296]\n",
      "Step 545440  [5.437 sec/step, loss=0.06654, avg_loss=0.07293]\n",
      "Step 545441  [5.447 sec/step, loss=0.07326, avg_loss=0.07293]\n",
      "Step 545442  [5.449 sec/step, loss=0.07307, avg_loss=0.07293]\n",
      "Step 545443  [5.435 sec/step, loss=0.07368, avg_loss=0.07295]\n",
      "Step 545444  [5.446 sec/step, loss=0.07304, avg_loss=0.07293]\n",
      "Generated 32 batches of size 32 in 2.773 sec\n",
      "Step 545445  [5.434 sec/step, loss=0.07309, avg_loss=0.07290]\n",
      "Step 545446  [5.453 sec/step, loss=0.07542, avg_loss=0.07293]\n",
      "Step 545447  [5.465 sec/step, loss=0.07479, avg_loss=0.07292]\n",
      "Step 545448  [5.482 sec/step, loss=0.07378, avg_loss=0.07301]\n",
      "Step 545449  [5.485 sec/step, loss=0.07295, avg_loss=0.07300]\n",
      "Step 545450  [5.486 sec/step, loss=0.07487, avg_loss=0.07301]\n",
      "Step 545451  [5.479 sec/step, loss=0.07466, avg_loss=0.07303]\n",
      "Step 545452  [5.476 sec/step, loss=0.07513, avg_loss=0.07303]\n",
      "Step 545453  [5.489 sec/step, loss=0.07408, avg_loss=0.07303]\n",
      "Step 545454  [5.471 sec/step, loss=0.07557, avg_loss=0.07307]\n",
      "Step 545455  [5.452 sec/step, loss=0.07129, avg_loss=0.07304]\n",
      "Step 545456  [5.461 sec/step, loss=0.07512, avg_loss=0.07307]\n",
      "Step 545457  [5.483 sec/step, loss=0.07488, avg_loss=0.07311]\n",
      "Step 545458  [5.460 sec/step, loss=0.07277, avg_loss=0.07309]\n",
      "Step 545459  [5.432 sec/step, loss=0.07042, avg_loss=0.07307]\n",
      "Step 545460  [5.422 sec/step, loss=0.07292, avg_loss=0.07307]\n",
      "Step 545461  [5.416 sec/step, loss=0.07343, avg_loss=0.07305]\n",
      "Step 545462  [5.398 sec/step, loss=0.07361, avg_loss=0.07304]\n",
      "Step 545463  [5.402 sec/step, loss=0.07178, avg_loss=0.07302]\n",
      "Step 545464  [5.406 sec/step, loss=0.07277, avg_loss=0.07305]\n",
      "Step 545465  [5.409 sec/step, loss=0.07350, avg_loss=0.07304]\n",
      "Step 545466  [5.399 sec/step, loss=0.07434, avg_loss=0.07303]\n",
      "Step 545467  [5.409 sec/step, loss=0.07416, avg_loss=0.07303]\n",
      "Step 545468  [5.421 sec/step, loss=0.07287, avg_loss=0.07302]\n",
      "Step 545469  [5.425 sec/step, loss=0.07320, avg_loss=0.07302]\n",
      "Step 545470  [5.418 sec/step, loss=0.07298, avg_loss=0.07303]\n",
      "Step 545471  [5.397 sec/step, loss=0.06636, avg_loss=0.07295]\n",
      "Step 545472  [5.413 sec/step, loss=0.07296, avg_loss=0.07296]\n",
      "Step 545473  [5.402 sec/step, loss=0.07216, avg_loss=0.07292]\n",
      "Step 545474  [5.395 sec/step, loss=0.07493, avg_loss=0.07292]\n",
      "Step 545475  [5.402 sec/step, loss=0.07369, avg_loss=0.07293]\n",
      "Step 545476  [5.419 sec/step, loss=0.07533, avg_loss=0.07295]\n",
      "Generated 32 batches of size 32 in 2.644 sec\n",
      "Step 545477  [5.468 sec/step, loss=0.06489, avg_loss=0.07288]\n",
      "Step 545478  [5.476 sec/step, loss=0.07324, avg_loss=0.07290]\n",
      "Step 545479  [5.484 sec/step, loss=0.07459, avg_loss=0.07290]\n",
      "Step 545480  [5.465 sec/step, loss=0.07016, avg_loss=0.07286]\n",
      "Step 545481  [5.462 sec/step, loss=0.07482, avg_loss=0.07287]\n",
      "Step 545482  [5.500 sec/step, loss=0.07454, avg_loss=0.07296]\n",
      "Step 545483  [5.503 sec/step, loss=0.07586, avg_loss=0.07297]\n",
      "Step 545484  [5.451 sec/step, loss=0.07403, avg_loss=0.07305]\n",
      "Step 545485  [5.449 sec/step, loss=0.07507, avg_loss=0.07308]\n",
      "Step 545486  [5.463 sec/step, loss=0.07473, avg_loss=0.07309]\n",
      "Step 545487  [5.482 sec/step, loss=0.07545, avg_loss=0.07311]\n",
      "Step 545488  [5.493 sec/step, loss=0.07332, avg_loss=0.07313]\n",
      "Step 545489  [5.492 sec/step, loss=0.07451, avg_loss=0.07315]\n",
      "Step 545490  [5.486 sec/step, loss=0.07202, avg_loss=0.07312]\n",
      "Step 545491  [5.486 sec/step, loss=0.07313, avg_loss=0.07314]\n",
      "Step 545492  [5.502 sec/step, loss=0.07385, avg_loss=0.07323]\n",
      "Step 545493  [5.473 sec/step, loss=0.06970, avg_loss=0.07317]\n",
      "Step 545494  [5.449 sec/step, loss=0.07305, avg_loss=0.07316]\n",
      "Step 545495  [5.465 sec/step, loss=0.07147, avg_loss=0.07318]\n",
      "Step 545496  [5.479 sec/step, loss=0.07486, avg_loss=0.07319]\n",
      "Step 545497  [5.469 sec/step, loss=0.07279, avg_loss=0.07317]\n",
      "Step 545498  [5.453 sec/step, loss=0.06918, avg_loss=0.07312]\n",
      "Step 545499  [5.461 sec/step, loss=0.07385, avg_loss=0.07314]\n",
      "Step 545500  [5.457 sec/step, loss=0.07563, avg_loss=0.07314]\n",
      "Writing summary at step: 545500\n",
      "Step 545501  [5.479 sec/step, loss=0.07420, avg_loss=0.07316]\n",
      "Step 545502  [5.461 sec/step, loss=0.07258, avg_loss=0.07315]\n",
      "Step 545503  [5.481 sec/step, loss=0.07443, avg_loss=0.07318]\n",
      "Step 545504  [5.474 sec/step, loss=0.07490, avg_loss=0.07318]\n",
      "Step 545505  [5.464 sec/step, loss=0.07099, avg_loss=0.07313]\n",
      "Step 545506  [5.489 sec/step, loss=0.07421, avg_loss=0.07314]\n",
      "Step 545507  [5.474 sec/step, loss=0.07410, avg_loss=0.07313]\n",
      "Generated 32 batches of size 32 in 2.547 sec\n",
      "Step 545508  [5.470 sec/step, loss=0.07298, avg_loss=0.07312]\n",
      "Step 545509  [5.460 sec/step, loss=0.07061, avg_loss=0.07309]\n",
      "Step 545510  [5.467 sec/step, loss=0.07529, avg_loss=0.07310]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 545511  [5.450 sec/step, loss=0.06401, avg_loss=0.07300]\n",
      "Step 545512  [5.474 sec/step, loss=0.07279, avg_loss=0.07298]\n",
      "Step 545513  [5.463 sec/step, loss=0.07230, avg_loss=0.07296]\n",
      "Step 545514  [5.517 sec/step, loss=0.06600, avg_loss=0.07290]\n",
      "Step 545515  [5.518 sec/step, loss=0.07514, avg_loss=0.07293]\n",
      "Step 545516  [5.506 sec/step, loss=0.07430, avg_loss=0.07292]\n",
      "Step 545517  [5.508 sec/step, loss=0.07334, avg_loss=0.07291]\n",
      "Step 545518  [5.554 sec/step, loss=0.06501, avg_loss=0.07282]\n",
      "Step 545519  [5.486 sec/step, loss=0.06470, avg_loss=0.07282]\n",
      "Step 545520  [5.477 sec/step, loss=0.07269, avg_loss=0.07283]\n",
      "Step 545521  [5.481 sec/step, loss=0.07500, avg_loss=0.07284]\n",
      "Step 545522  [5.489 sec/step, loss=0.07384, avg_loss=0.07284]\n",
      "Step 545523  [5.479 sec/step, loss=0.07418, avg_loss=0.07285]\n",
      "Step 545524  [5.496 sec/step, loss=0.07194, avg_loss=0.07282]\n",
      "Step 545525  [5.524 sec/step, loss=0.07353, avg_loss=0.07290]\n",
      "Step 545526  [5.508 sec/step, loss=0.07013, avg_loss=0.07287]\n",
      "Step 545527  [5.494 sec/step, loss=0.07329, avg_loss=0.07285]\n",
      "Step 545528  [5.506 sec/step, loss=0.07517, avg_loss=0.07287]\n",
      "Step 545529  [5.493 sec/step, loss=0.07430, avg_loss=0.07286]\n",
      "Step 545530  [5.499 sec/step, loss=0.07522, avg_loss=0.07288]\n",
      "Step 545531  [5.487 sec/step, loss=0.07101, avg_loss=0.07285]\n",
      "Step 545532  [5.500 sec/step, loss=0.07093, avg_loss=0.07285]\n",
      "Step 545533  [5.498 sec/step, loss=0.07067, avg_loss=0.07285]\n",
      "Step 545534  [5.510 sec/step, loss=0.06971, avg_loss=0.07283]\n",
      "Step 545535  [5.495 sec/step, loss=0.07351, avg_loss=0.07282]\n",
      "Step 545536  [5.488 sec/step, loss=0.07460, avg_loss=0.07284]\n",
      "Step 545537  [5.505 sec/step, loss=0.07524, avg_loss=0.07287]\n",
      "Step 545538  [5.491 sec/step, loss=0.07538, avg_loss=0.07290]\n",
      "Step 545539  [5.503 sec/step, loss=0.07516, avg_loss=0.07292]\n",
      "Generated 32 batches of size 32 in 2.552 sec\n",
      "Step 545540  [5.459 sec/step, loss=0.07345, avg_loss=0.07298]\n",
      "Step 545541  [5.450 sec/step, loss=0.07429, avg_loss=0.07299]\n",
      "Step 545542  [5.447 sec/step, loss=0.07331, avg_loss=0.07300]\n",
      "Step 545543  [5.453 sec/step, loss=0.07452, avg_loss=0.07301]\n",
      "Step 545544  [5.442 sec/step, loss=0.07427, avg_loss=0.07302]\n",
      "Step 545545  [5.456 sec/step, loss=0.07556, avg_loss=0.07304]\n",
      "Step 545546  [5.466 sec/step, loss=0.07478, avg_loss=0.07304]\n",
      "Step 545547  [5.458 sec/step, loss=0.07405, avg_loss=0.07303]\n",
      "Step 545548  [5.458 sec/step, loss=0.07358, avg_loss=0.07303]\n",
      "Step 545549  [5.462 sec/step, loss=0.07163, avg_loss=0.07301]\n",
      "Step 545550  [5.460 sec/step, loss=0.07321, avg_loss=0.07300]\n",
      "Step 545551  [5.481 sec/step, loss=0.07465, avg_loss=0.07300]\n",
      "Step 545552  [5.482 sec/step, loss=0.07399, avg_loss=0.07299]\n",
      "Step 545553  [5.478 sec/step, loss=0.07447, avg_loss=0.07299]\n",
      "Step 545554  [5.467 sec/step, loss=0.07401, avg_loss=0.07297]\n",
      "Step 545555  [5.490 sec/step, loss=0.07532, avg_loss=0.07301]\n",
      "Step 545556  [5.468 sec/step, loss=0.07142, avg_loss=0.07298]\n",
      "Step 545557  [5.436 sec/step, loss=0.07120, avg_loss=0.07294]\n",
      "Step 545558  [5.454 sec/step, loss=0.07500, avg_loss=0.07296]\n",
      "Step 545559  [5.446 sec/step, loss=0.07326, avg_loss=0.07299]\n",
      "Step 545560  [5.467 sec/step, loss=0.07562, avg_loss=0.07302]\n",
      "Step 545561  [5.462 sec/step, loss=0.07307, avg_loss=0.07301]\n",
      "Step 545562  [5.457 sec/step, loss=0.06992, avg_loss=0.07298]\n",
      "Step 545563  [5.455 sec/step, loss=0.07471, avg_loss=0.07301]\n",
      "Step 545564  [5.455 sec/step, loss=0.07147, avg_loss=0.07299]\n",
      "Step 545565  [5.449 sec/step, loss=0.07431, avg_loss=0.07300]\n",
      "Step 545566  [5.460 sec/step, loss=0.07299, avg_loss=0.07299]\n",
      "Step 545567  [5.427 sec/step, loss=0.06515, avg_loss=0.07290]\n",
      "Step 545568  [5.411 sec/step, loss=0.07012, avg_loss=0.07287]\n",
      "Step 545569  [5.410 sec/step, loss=0.07288, avg_loss=0.07287]\n",
      "Step 545570  [5.421 sec/step, loss=0.07391, avg_loss=0.07288]\n",
      "Step 545571  [5.448 sec/step, loss=0.07537, avg_loss=0.07297]\n",
      "Generated 32 batches of size 32 in 2.390 sec\n",
      "Step 545572  [5.447 sec/step, loss=0.07387, avg_loss=0.07298]\n",
      "Step 545573  [5.453 sec/step, loss=0.07369, avg_loss=0.07299]\n",
      "Step 545574  [5.445 sec/step, loss=0.07289, avg_loss=0.07297]\n",
      "Step 545575  [5.496 sec/step, loss=0.06543, avg_loss=0.07289]\n",
      "Step 545576  [5.486 sec/step, loss=0.07446, avg_loss=0.07288]\n",
      "Step 545577  [5.448 sec/step, loss=0.07475, avg_loss=0.07298]\n",
      "Step 545578  [5.470 sec/step, loss=0.07517, avg_loss=0.07300]\n",
      "Step 545579  [5.476 sec/step, loss=0.07507, avg_loss=0.07300]\n",
      "Step 545580  [5.496 sec/step, loss=0.07453, avg_loss=0.07305]\n",
      "Step 545581  [5.502 sec/step, loss=0.07408, avg_loss=0.07304]\n",
      "Step 545582  [5.503 sec/step, loss=0.07302, avg_loss=0.07302]\n",
      "Step 545583  [5.475 sec/step, loss=0.06519, avg_loss=0.07292]\n",
      "Step 545584  [5.467 sec/step, loss=0.07352, avg_loss=0.07291]\n",
      "Step 545585  [5.462 sec/step, loss=0.07416, avg_loss=0.07290]\n",
      "Step 545586  [5.436 sec/step, loss=0.06924, avg_loss=0.07285]\n",
      "Step 545587  [5.438 sec/step, loss=0.07546, avg_loss=0.07285]\n",
      "Step 545588  [5.439 sec/step, loss=0.07433, avg_loss=0.07286]\n",
      "Step 545589  [5.443 sec/step, loss=0.07554, avg_loss=0.07287]\n",
      "Step 545590  [5.437 sec/step, loss=0.07396, avg_loss=0.07289]\n",
      "Step 545591  [5.497 sec/step, loss=0.06661, avg_loss=0.07282]\n",
      "Step 545592  [5.509 sec/step, loss=0.07517, avg_loss=0.07284]\n",
      "Step 545593  [5.538 sec/step, loss=0.07610, avg_loss=0.07290]\n",
      "Step 545594  [5.535 sec/step, loss=0.07208, avg_loss=0.07289]\n",
      "Step 545595  [5.540 sec/step, loss=0.07451, avg_loss=0.07292]\n",
      "Step 545596  [5.530 sec/step, loss=0.07512, avg_loss=0.07292]\n",
      "Step 545597  [5.536 sec/step, loss=0.07499, avg_loss=0.07294]\n",
      "Step 545598  [5.568 sec/step, loss=0.07330, avg_loss=0.07299]\n",
      "Step 545599  [5.564 sec/step, loss=0.07325, avg_loss=0.07298]\n",
      "Step 545600  [5.548 sec/step, loss=0.07228, avg_loss=0.07295]\n",
      "Writing summary at step: 545600\n",
      "Step 545601  [5.521 sec/step, loss=0.07070, avg_loss=0.07291]\n",
      "Step 545602  [5.538 sec/step, loss=0.07503, avg_loss=0.07294]\n",
      "Generated 32 batches of size 32 in 2.563 sec\n",
      "Step 545603  [5.532 sec/step, loss=0.07145, avg_loss=0.07291]\n",
      "Step 545604  [5.543 sec/step, loss=0.07378, avg_loss=0.07290]\n",
      "Step 545605  [5.549 sec/step, loss=0.07443, avg_loss=0.07293]\n",
      "Step 545606  [5.530 sec/step, loss=0.07475, avg_loss=0.07293]\n",
      "Step 545607  [5.525 sec/step, loss=0.07382, avg_loss=0.07293]\n",
      "Step 545608  [5.525 sec/step, loss=0.07310, avg_loss=0.07293]\n",
      "Step 545609  [5.537 sec/step, loss=0.07431, avg_loss=0.07297]\n",
      "Step 545610  [5.543 sec/step, loss=0.07545, avg_loss=0.07297]\n",
      "Step 545611  [5.562 sec/step, loss=0.07434, avg_loss=0.07308]\n",
      "Step 545612  [5.522 sec/step, loss=0.07192, avg_loss=0.07307]\n",
      "Step 545613  [5.540 sec/step, loss=0.07588, avg_loss=0.07310]\n",
      "Step 545614  [5.485 sec/step, loss=0.07212, avg_loss=0.07316]\n",
      "Step 545615  [5.486 sec/step, loss=0.07337, avg_loss=0.07315]\n",
      "Step 545616  [5.479 sec/step, loss=0.07041, avg_loss=0.07311]\n",
      "Step 545617  [5.480 sec/step, loss=0.07309, avg_loss=0.07310]\n",
      "Step 545618  [5.417 sec/step, loss=0.07177, avg_loss=0.07317]\n",
      "Step 545619  [5.433 sec/step, loss=0.07461, avg_loss=0.07327]\n",
      "Step 545620  [5.433 sec/step, loss=0.07343, avg_loss=0.07328]\n",
      "Step 545621  [5.429 sec/step, loss=0.07532, avg_loss=0.07328]\n",
      "Step 545622  [5.419 sec/step, loss=0.07377, avg_loss=0.07328]\n",
      "Step 545623  [5.435 sec/step, loss=0.07532, avg_loss=0.07329]\n",
      "Step 545624  [5.433 sec/step, loss=0.07427, avg_loss=0.07332]\n",
      "Step 545625  [5.430 sec/step, loss=0.07524, avg_loss=0.07333]\n",
      "Step 545626  [5.443 sec/step, loss=0.07408, avg_loss=0.07337]\n",
      "Step 545627  [5.456 sec/step, loss=0.07391, avg_loss=0.07338]\n",
      "Step 545628  [5.440 sec/step, loss=0.07291, avg_loss=0.07336]\n",
      "Step 545629  [5.441 sec/step, loss=0.07485, avg_loss=0.07336]\n",
      "Step 545630  [5.416 sec/step, loss=0.06522, avg_loss=0.07326]\n",
      "Step 545631  [5.435 sec/step, loss=0.07416, avg_loss=0.07329]\n",
      "Step 545632  [5.435 sec/step, loss=0.07480, avg_loss=0.07333]\n",
      "Step 545633  [5.455 sec/step, loss=0.07513, avg_loss=0.07338]\n",
      "Step 545634  [5.466 sec/step, loss=0.07343, avg_loss=0.07341]\n",
      "Generated 32 batches of size 32 in 2.542 sec\n",
      "Step 545635  [5.471 sec/step, loss=0.07361, avg_loss=0.07341]\n",
      "Step 545636  [5.463 sec/step, loss=0.07446, avg_loss=0.07341]\n",
      "Step 545637  [5.453 sec/step, loss=0.07303, avg_loss=0.07339]\n",
      "Step 545638  [5.442 sec/step, loss=0.07285, avg_loss=0.07337]\n",
      "Step 545639  [5.441 sec/step, loss=0.07445, avg_loss=0.07336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 545640  [5.453 sec/step, loss=0.07469, avg_loss=0.07337]\n",
      "Step 545641  [5.443 sec/step, loss=0.07359, avg_loss=0.07336]\n",
      "Step 545642  [5.497 sec/step, loss=0.06520, avg_loss=0.07328]\n",
      "Step 545643  [5.510 sec/step, loss=0.07285, avg_loss=0.07327]\n",
      "Step 545644  [5.501 sec/step, loss=0.07371, avg_loss=0.07326]\n",
      "Step 545645  [5.506 sec/step, loss=0.07536, avg_loss=0.07326]\n",
      "Step 545646  [5.480 sec/step, loss=0.07341, avg_loss=0.07324]\n",
      "Step 545647  [5.477 sec/step, loss=0.07174, avg_loss=0.07322]\n",
      "Step 545648  [5.486 sec/step, loss=0.07278, avg_loss=0.07321]\n",
      "Step 545649  [5.496 sec/step, loss=0.07524, avg_loss=0.07325]\n",
      "Step 545650  [5.485 sec/step, loss=0.06666, avg_loss=0.07318]\n",
      "Step 545651  [5.464 sec/step, loss=0.07504, avg_loss=0.07319]\n",
      "Step 545652  [5.473 sec/step, loss=0.07552, avg_loss=0.07320]\n",
      "Step 545653  [5.468 sec/step, loss=0.07377, avg_loss=0.07320]\n",
      "Step 545654  [5.496 sec/step, loss=0.07253, avg_loss=0.07318]\n",
      "Step 545655  [5.483 sec/step, loss=0.07155, avg_loss=0.07314]\n",
      "Step 545656  [5.490 sec/step, loss=0.07282, avg_loss=0.07316]\n",
      "Step 545657  [5.501 sec/step, loss=0.07480, avg_loss=0.07319]\n",
      "Step 545658  [5.476 sec/step, loss=0.07017, avg_loss=0.07315]\n",
      "Step 545659  [5.536 sec/step, loss=0.06672, avg_loss=0.07308]\n",
      "Step 545660  [5.540 sec/step, loss=0.07483, avg_loss=0.07307]\n",
      "Step 545661  [5.544 sec/step, loss=0.07458, avg_loss=0.07309]\n",
      "Step 545662  [5.557 sec/step, loss=0.07564, avg_loss=0.07314]\n",
      "Step 545663  [5.562 sec/step, loss=0.07428, avg_loss=0.07314]\n",
      "Step 545664  [5.562 sec/step, loss=0.07508, avg_loss=0.07318]\n",
      "Step 545665  [5.561 sec/step, loss=0.07274, avg_loss=0.07316]\n",
      "Step 545666  [5.560 sec/step, loss=0.07535, avg_loss=0.07318]\n",
      "Generated 32 batches of size 32 in 2.725 sec\n",
      "Step 545667  [5.572 sec/step, loss=0.07315, avg_loss=0.07326]\n",
      "Step 545668  [5.583 sec/step, loss=0.07226, avg_loss=0.07329]\n",
      "Step 545669  [5.597 sec/step, loss=0.07627, avg_loss=0.07332]\n",
      "Step 545670  [5.586 sec/step, loss=0.07357, avg_loss=0.07332]\n",
      "Step 545671  [5.584 sec/step, loss=0.07047, avg_loss=0.07327]\n",
      "Step 545672  [5.591 sec/step, loss=0.07558, avg_loss=0.07328]\n",
      "Step 545673  [5.590 sec/step, loss=0.07403, avg_loss=0.07329]\n",
      "Step 545674  [5.576 sec/step, loss=0.07141, avg_loss=0.07327]\n",
      "Step 545675  [5.524 sec/step, loss=0.07450, avg_loss=0.07336]\n",
      "Step 545676  [5.514 sec/step, loss=0.07345, avg_loss=0.07335]\n",
      "Step 545677  [5.504 sec/step, loss=0.07435, avg_loss=0.07335]\n",
      "Step 545678  [5.475 sec/step, loss=0.07126, avg_loss=0.07331]\n",
      "Step 545679  [5.468 sec/step, loss=0.07078, avg_loss=0.07327]\n",
      "Step 545680  [5.455 sec/step, loss=0.07259, avg_loss=0.07325]\n",
      "Step 545681  [5.448 sec/step, loss=0.07453, avg_loss=0.07325]\n",
      "Step 545682  [5.441 sec/step, loss=0.07269, avg_loss=0.07325]\n",
      "Step 545683  [5.465 sec/step, loss=0.07442, avg_loss=0.07334]\n",
      "Step 545684  [5.487 sec/step, loss=0.07503, avg_loss=0.07336]\n",
      "Step 545685  [5.466 sec/step, loss=0.06511, avg_loss=0.07327]\n",
      "Step 545686  [5.465 sec/step, loss=0.07323, avg_loss=0.07331]\n",
      "Step 545687  [5.481 sec/step, loss=0.07296, avg_loss=0.07328]\n",
      "Step 545688  [5.488 sec/step, loss=0.07559, avg_loss=0.07329]\n",
      "Step 545689  [5.480 sec/step, loss=0.07343, avg_loss=0.07327]\n",
      "Step 545690  [5.480 sec/step, loss=0.07454, avg_loss=0.07328]\n",
      "Step 545691  [5.422 sec/step, loss=0.07267, avg_loss=0.07334]\n",
      "Step 545692  [5.410 sec/step, loss=0.07129, avg_loss=0.07330]\n",
      "Step 545693  [5.409 sec/step, loss=0.07603, avg_loss=0.07330]\n",
      "Step 545694  [5.461 sec/step, loss=0.06680, avg_loss=0.07325]\n",
      "Step 545695  [5.438 sec/step, loss=0.07062, avg_loss=0.07321]\n",
      "Step 545696  [5.433 sec/step, loss=0.07255, avg_loss=0.07318]\n",
      "Step 545697  [5.443 sec/step, loss=0.07319, avg_loss=0.07316]\n",
      "Step 545698  [5.433 sec/step, loss=0.07376, avg_loss=0.07317]\n",
      "Generated 32 batches of size 32 in 2.602 sec\n",
      "Step 545699  [5.438 sec/step, loss=0.07380, avg_loss=0.07317]\n",
      "Step 545700  [5.458 sec/step, loss=0.07547, avg_loss=0.07321]\n",
      "Writing summary at step: 545700\n",
      "Step 545701  [5.477 sec/step, loss=0.07361, avg_loss=0.07323]\n",
      "Step 545702  [5.471 sec/step, loss=0.07424, avg_loss=0.07323]\n",
      "Step 545703  [5.481 sec/step, loss=0.07534, avg_loss=0.07327]\n",
      "Step 545704  [5.475 sec/step, loss=0.07507, avg_loss=0.07328]\n",
      "Step 545705  [5.474 sec/step, loss=0.07206, avg_loss=0.07325]\n",
      "Step 545706  [5.478 sec/step, loss=0.07455, avg_loss=0.07325]\n",
      "Step 545707  [5.479 sec/step, loss=0.07289, avg_loss=0.07324]\n",
      "Step 545708  [5.483 sec/step, loss=0.07346, avg_loss=0.07325]\n",
      "Step 545709  [5.486 sec/step, loss=0.07497, avg_loss=0.07325]\n",
      "Step 545710  [5.473 sec/step, loss=0.07215, avg_loss=0.07322]\n",
      "Step 545711  [5.479 sec/step, loss=0.07481, avg_loss=0.07323]\n",
      "Step 545712  [5.496 sec/step, loss=0.07453, avg_loss=0.07325]\n",
      "Step 545713  [5.475 sec/step, loss=0.07327, avg_loss=0.07323]\n",
      "Step 545714  [5.478 sec/step, loss=0.07017, avg_loss=0.07321]\n",
      "Step 545715  [5.473 sec/step, loss=0.07525, avg_loss=0.07322]\n",
      "Step 545716  [5.476 sec/step, loss=0.07378, avg_loss=0.07326]\n",
      "Step 545717  [5.468 sec/step, loss=0.07471, avg_loss=0.07327]\n",
      "Step 545718  [5.481 sec/step, loss=0.07364, avg_loss=0.07329]\n",
      "Step 545719  [5.473 sec/step, loss=0.07016, avg_loss=0.07325]\n",
      "Step 545720  [5.481 sec/step, loss=0.07489, avg_loss=0.07326]\n",
      "Step 545721  [5.477 sec/step, loss=0.07293, avg_loss=0.07324]\n",
      "Step 545722  [5.484 sec/step, loss=0.07514, avg_loss=0.07325]\n",
      "Step 545723  [5.463 sec/step, loss=0.07338, avg_loss=0.07323]\n",
      "Step 545724  [5.449 sec/step, loss=0.07304, avg_loss=0.07322]\n",
      "Step 545725  [5.438 sec/step, loss=0.07459, avg_loss=0.07322]\n",
      "Step 545726  [5.448 sec/step, loss=0.07524, avg_loss=0.07323]\n",
      "Step 545727  [5.455 sec/step, loss=0.07487, avg_loss=0.07324]\n",
      "Step 545728  [5.510 sec/step, loss=0.06372, avg_loss=0.07314]\n",
      "Step 545729  [5.504 sec/step, loss=0.07200, avg_loss=0.07312]\n",
      "Generated 32 batches of size 32 in 2.551 sec\n",
      "Step 545730  [5.520 sec/step, loss=0.07301, avg_loss=0.07319]\n",
      "Step 545731  [5.501 sec/step, loss=0.07007, avg_loss=0.07315]\n",
      "Step 545732  [5.482 sec/step, loss=0.06545, avg_loss=0.07306]\n",
      "Step 545733  [5.471 sec/step, loss=0.07451, avg_loss=0.07305]\n",
      "Step 545734  [5.467 sec/step, loss=0.07276, avg_loss=0.07305]\n",
      "Step 545735  [5.487 sec/step, loss=0.07448, avg_loss=0.07306]\n",
      "Step 545736  [5.513 sec/step, loss=0.07442, avg_loss=0.07305]\n",
      "Step 545737  [5.519 sec/step, loss=0.07551, avg_loss=0.07308]\n",
      "Step 545738  [5.535 sec/step, loss=0.07471, avg_loss=0.07310]\n",
      "Step 545739  [5.530 sec/step, loss=0.07020, avg_loss=0.07306]\n",
      "Step 545740  [5.526 sec/step, loss=0.07530, avg_loss=0.07306]\n",
      "Step 545741  [5.549 sec/step, loss=0.07475, avg_loss=0.07307]\n",
      "Step 545742  [5.548 sec/step, loss=0.06589, avg_loss=0.07308]\n",
      "Step 545743  [5.544 sec/step, loss=0.07530, avg_loss=0.07311]\n",
      "Step 545744  [5.547 sec/step, loss=0.07422, avg_loss=0.07311]\n",
      "Step 545745  [5.535 sec/step, loss=0.07422, avg_loss=0.07310]\n",
      "Step 545746  [5.542 sec/step, loss=0.07286, avg_loss=0.07309]\n",
      "Step 545747  [5.546 sec/step, loss=0.07433, avg_loss=0.07312]\n",
      "Step 545748  [5.519 sec/step, loss=0.06411, avg_loss=0.07303]\n",
      "Step 545749  [5.508 sec/step, loss=0.07420, avg_loss=0.07302]\n",
      "Step 545750  [5.528 sec/step, loss=0.07369, avg_loss=0.07309]\n",
      "Step 545751  [5.531 sec/step, loss=0.07504, avg_loss=0.07309]\n",
      "Step 545752  [5.521 sec/step, loss=0.07462, avg_loss=0.07308]\n",
      "Step 545753  [5.549 sec/step, loss=0.07219, avg_loss=0.07307]\n",
      "Step 545754  [5.523 sec/step, loss=0.07378, avg_loss=0.07308]\n",
      "Step 545755  [5.516 sec/step, loss=0.07311, avg_loss=0.07310]\n",
      "Step 545756  [5.521 sec/step, loss=0.07208, avg_loss=0.07309]\n",
      "Step 545757  [5.534 sec/step, loss=0.07516, avg_loss=0.07309]\n",
      "Step 545758  [5.563 sec/step, loss=0.07437, avg_loss=0.07313]\n",
      "Step 545759  [5.524 sec/step, loss=0.07545, avg_loss=0.07322]\n",
      "Step 545760  [5.520 sec/step, loss=0.07308, avg_loss=0.07320]\n",
      "Step 545761  [5.508 sec/step, loss=0.07345, avg_loss=0.07319]\n",
      "Generated 32 batches of size 32 in 2.783 sec\n",
      "Step 545762  [5.490 sec/step, loss=0.07150, avg_loss=0.07315]\n",
      "Step 545763  [5.493 sec/step, loss=0.07507, avg_loss=0.07316]\n",
      "Step 545764  [5.487 sec/step, loss=0.07268, avg_loss=0.07313]\n",
      "Step 545765  [5.486 sec/step, loss=0.07267, avg_loss=0.07313]\n",
      "Step 545766  [5.465 sec/step, loss=0.07061, avg_loss=0.07309]\n",
      "Step 545767  [5.463 sec/step, loss=0.07342, avg_loss=0.07309]\n",
      "Step 545768  [5.457 sec/step, loss=0.07448, avg_loss=0.07311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 545769  [5.455 sec/step, loss=0.07403, avg_loss=0.07309]\n",
      "Step 545770  [5.465 sec/step, loss=0.07124, avg_loss=0.07307]\n",
      "Step 545771  [5.467 sec/step, loss=0.07531, avg_loss=0.07311]\n",
      "Step 545772  [5.476 sec/step, loss=0.07250, avg_loss=0.07308]\n",
      "Step 545773  [5.484 sec/step, loss=0.07340, avg_loss=0.07308]\n",
      "Step 545774  [5.507 sec/step, loss=0.07483, avg_loss=0.07311]\n",
      "Step 545775  [5.517 sec/step, loss=0.07551, avg_loss=0.07312]\n",
      "Step 545776  [5.514 sec/step, loss=0.07345, avg_loss=0.07312]\n",
      "Step 545777  [5.528 sec/step, loss=0.07260, avg_loss=0.07310]\n",
      "Step 545778  [5.535 sec/step, loss=0.07230, avg_loss=0.07311]\n",
      "Step 545779  [5.536 sec/step, loss=0.07292, avg_loss=0.07314]\n",
      "Step 545780  [5.555 sec/step, loss=0.07528, avg_loss=0.07316]\n",
      "Step 545781  [5.552 sec/step, loss=0.07458, avg_loss=0.07316]\n",
      "Step 545782  [5.529 sec/step, loss=0.07198, avg_loss=0.07316]\n",
      "Step 545783  [5.520 sec/step, loss=0.07309, avg_loss=0.07314]\n",
      "Step 545784  [5.511 sec/step, loss=0.07522, avg_loss=0.07315]\n",
      "Step 545785  [5.529 sec/step, loss=0.07382, avg_loss=0.07323]\n",
      "Step 545786  [5.590 sec/step, loss=0.06620, avg_loss=0.07316]\n",
      "Step 545787  [5.553 sec/step, loss=0.07042, avg_loss=0.07314]\n",
      "Step 545788  [5.550 sec/step, loss=0.07376, avg_loss=0.07312]\n",
      "Step 545789  [5.559 sec/step, loss=0.07512, avg_loss=0.07314]\n",
      "Step 545790  [5.557 sec/step, loss=0.07150, avg_loss=0.07310]\n",
      "Step 545791  [5.567 sec/step, loss=0.07459, avg_loss=0.07312]\n",
      "Step 545792  [5.582 sec/step, loss=0.07519, avg_loss=0.07316]\n",
      "Step 545793  [5.569 sec/step, loss=0.07365, avg_loss=0.07314]\n",
      "Generated 32 batches of size 32 in 2.603 sec\n",
      "Step 545794  [5.524 sec/step, loss=0.07334, avg_loss=0.07320]\n",
      "Step 545795  [5.531 sec/step, loss=0.07409, avg_loss=0.07324]\n",
      "Step 545796  [5.535 sec/step, loss=0.07303, avg_loss=0.07324]\n",
      "Step 545797  [5.527 sec/step, loss=0.07403, avg_loss=0.07325]\n",
      "Step 545798  [5.531 sec/step, loss=0.07278, avg_loss=0.07324]\n",
      "Step 545799  [5.511 sec/step, loss=0.07058, avg_loss=0.07321]\n",
      "Step 545800  [5.481 sec/step, loss=0.06570, avg_loss=0.07311]\n",
      "Writing summary at step: 545800\n",
      "Step 545801  [5.488 sec/step, loss=0.07524, avg_loss=0.07313]\n",
      "Step 545802  [5.493 sec/step, loss=0.07306, avg_loss=0.07312]\n",
      "Step 545803  [5.493 sec/step, loss=0.07481, avg_loss=0.07311]\n",
      "Step 545804  [5.490 sec/step, loss=0.07461, avg_loss=0.07311]\n",
      "Step 545805  [5.484 sec/step, loss=0.07101, avg_loss=0.07310]\n",
      "Step 545806  [5.486 sec/step, loss=0.07414, avg_loss=0.07309]\n",
      "Step 545807  [5.504 sec/step, loss=0.07493, avg_loss=0.07311]\n",
      "Step 545808  [5.487 sec/step, loss=0.06441, avg_loss=0.07302]\n",
      "Step 545809  [5.496 sec/step, loss=0.07453, avg_loss=0.07302]\n",
      "Step 545810  [5.504 sec/step, loss=0.07232, avg_loss=0.07302]\n",
      "Step 545811  [5.549 sec/step, loss=0.06439, avg_loss=0.07292]\n",
      "Step 545812  [5.535 sec/step, loss=0.07007, avg_loss=0.07287]\n",
      "Step 545813  [5.547 sec/step, loss=0.07371, avg_loss=0.07288]\n",
      "Step 545814  [5.550 sec/step, loss=0.07293, avg_loss=0.07290]\n",
      "Step 545815  [5.534 sec/step, loss=0.07324, avg_loss=0.07288]\n",
      "Step 545816  [5.535 sec/step, loss=0.07385, avg_loss=0.07288]\n",
      "Step 545817  [5.538 sec/step, loss=0.07468, avg_loss=0.07288]\n",
      "Step 545818  [5.529 sec/step, loss=0.07303, avg_loss=0.07288]\n",
      "Step 545819  [5.544 sec/step, loss=0.07378, avg_loss=0.07291]\n",
      "Step 545820  [5.540 sec/step, loss=0.07323, avg_loss=0.07290]\n",
      "Step 545821  [5.540 sec/step, loss=0.07471, avg_loss=0.07291]\n",
      "Step 545822  [5.529 sec/step, loss=0.07061, avg_loss=0.07287]\n",
      "Step 545823  [5.561 sec/step, loss=0.07457, avg_loss=0.07288]\n",
      "Step 545824  [5.539 sec/step, loss=0.07345, avg_loss=0.07289]\n",
      "Generated 32 batches of size 32 in 2.614 sec\n",
      "Step 545825  [5.547 sec/step, loss=0.07372, avg_loss=0.07288]\n",
      "Step 545826  [5.549 sec/step, loss=0.07466, avg_loss=0.07287]\n",
      "Step 545827  [5.523 sec/step, loss=0.07040, avg_loss=0.07283]\n",
      "Step 545828  [5.479 sec/step, loss=0.07557, avg_loss=0.07294]\n",
      "Step 545829  [5.479 sec/step, loss=0.07454, avg_loss=0.07297]\n",
      "Step 545830  [5.495 sec/step, loss=0.07399, avg_loss=0.07298]\n",
      "Step 545831  [5.518 sec/step, loss=0.07508, avg_loss=0.07303]\n",
      "Step 545832  [5.537 sec/step, loss=0.07441, avg_loss=0.07312]\n",
      "Step 545833  [5.530 sec/step, loss=0.07452, avg_loss=0.07312]\n",
      "Step 545834  [5.533 sec/step, loss=0.07570, avg_loss=0.07315]\n",
      "Step 545835  [5.533 sec/step, loss=0.07347, avg_loss=0.07314]\n",
      "Step 545836  [5.498 sec/step, loss=0.07336, avg_loss=0.07313]\n",
      "Step 545837  [5.496 sec/step, loss=0.07501, avg_loss=0.07312]\n",
      "Step 545838  [5.477 sec/step, loss=0.06952, avg_loss=0.07307]\n",
      "Step 545839  [5.475 sec/step, loss=0.07290, avg_loss=0.07310]\n",
      "Step 545840  [5.474 sec/step, loss=0.07403, avg_loss=0.07309]\n",
      "Step 545841  [5.448 sec/step, loss=0.07054, avg_loss=0.07304]\n",
      "Step 545842  [5.395 sec/step, loss=0.07274, avg_loss=0.07311]\n",
      "Step 545843  [5.392 sec/step, loss=0.07364, avg_loss=0.07310]\n",
      "Step 545844  [5.382 sec/step, loss=0.07368, avg_loss=0.07309]\n",
      "Step 545845  [5.378 sec/step, loss=0.07459, avg_loss=0.07309]\n",
      "Step 545846  [5.383 sec/step, loss=0.07375, avg_loss=0.07310]\n",
      "Step 545847  [5.361 sec/step, loss=0.06731, avg_loss=0.07303]\n",
      "Step 545848  [5.376 sec/step, loss=0.07237, avg_loss=0.07311]\n",
      "Step 545849  [5.378 sec/step, loss=0.07477, avg_loss=0.07312]\n",
      "Step 545850  [5.378 sec/step, loss=0.07347, avg_loss=0.07312]\n",
      "Step 545851  [5.420 sec/step, loss=0.06685, avg_loss=0.07304]\n",
      "Step 545852  [5.435 sec/step, loss=0.07406, avg_loss=0.07303]\n",
      "Step 545853  [5.400 sec/step, loss=0.07339, avg_loss=0.07304]\n",
      "Step 545854  [5.407 sec/step, loss=0.07509, avg_loss=0.07306]\n",
      "Step 545855  [5.416 sec/step, loss=0.07394, avg_loss=0.07306]\n",
      "Step 545856  [5.438 sec/step, loss=0.07350, avg_loss=0.07308]\n",
      "Generated 32 batches of size 32 in 2.623 sec\n",
      "Step 545857  [5.430 sec/step, loss=0.07413, avg_loss=0.07307]\n",
      "Step 545858  [5.438 sec/step, loss=0.07257, avg_loss=0.07305]\n",
      "Step 545859  [5.428 sec/step, loss=0.07445, avg_loss=0.07304]\n",
      "Step 545860  [5.431 sec/step, loss=0.07297, avg_loss=0.07304]\n",
      "Step 545861  [5.438 sec/step, loss=0.07141, avg_loss=0.07302]\n",
      "Step 545862  [5.453 sec/step, loss=0.07444, avg_loss=0.07305]\n",
      "Step 545863  [5.454 sec/step, loss=0.07571, avg_loss=0.07305]\n",
      "Step 545864  [5.460 sec/step, loss=0.07513, avg_loss=0.07308]\n",
      "Step 545865  [5.449 sec/step, loss=0.07224, avg_loss=0.07307]\n",
      "Step 545866  [5.456 sec/step, loss=0.07430, avg_loss=0.07311]\n",
      "Step 545867  [5.462 sec/step, loss=0.07256, avg_loss=0.07310]\n",
      "Step 545868  [5.516 sec/step, loss=0.06526, avg_loss=0.07301]\n",
      "Step 545869  [5.495 sec/step, loss=0.06991, avg_loss=0.07297]\n",
      "Step 545870  [5.501 sec/step, loss=0.07469, avg_loss=0.07300]\n",
      "Step 545871  [5.502 sec/step, loss=0.07596, avg_loss=0.07301]\n",
      "Step 545872  [5.491 sec/step, loss=0.07455, avg_loss=0.07303]\n",
      "Step 545873  [5.468 sec/step, loss=0.07319, avg_loss=0.07303]\n",
      "Step 545874  [5.485 sec/step, loss=0.07460, avg_loss=0.07303]\n",
      "Step 545875  [5.467 sec/step, loss=0.07280, avg_loss=0.07300]\n",
      "Step 545876  [5.482 sec/step, loss=0.07390, avg_loss=0.07300]\n",
      "Step 545877  [5.462 sec/step, loss=0.07469, avg_loss=0.07302]\n",
      "Step 545878  [5.471 sec/step, loss=0.07521, avg_loss=0.07305]\n",
      "Step 545879  [5.484 sec/step, loss=0.07463, avg_loss=0.07307]\n",
      "Step 545880  [5.470 sec/step, loss=0.07407, avg_loss=0.07306]\n",
      "Step 545881  [5.469 sec/step, loss=0.07033, avg_loss=0.07302]\n",
      "Step 545882  [5.485 sec/step, loss=0.07572, avg_loss=0.07305]\n",
      "Step 545883  [5.501 sec/step, loss=0.07530, avg_loss=0.07308]\n",
      "Step 545884  [5.496 sec/step, loss=0.07107, avg_loss=0.07303]\n",
      "Step 545885  [5.502 sec/step, loss=0.07570, avg_loss=0.07305]\n",
      "Step 545886  [5.452 sec/step, loss=0.07374, avg_loss=0.07313]\n",
      "Step 545887  [5.460 sec/step, loss=0.07287, avg_loss=0.07315]\n",
      "Step 545888  [5.457 sec/step, loss=0.07464, avg_loss=0.07316]\n",
      "Generated 32 batches of size 32 in 2.563 sec\n",
      "Step 545889  [5.445 sec/step, loss=0.07358, avg_loss=0.07315]\n",
      "Step 545890  [5.459 sec/step, loss=0.07456, avg_loss=0.07318]\n",
      "Step 545891  [5.455 sec/step, loss=0.07429, avg_loss=0.07317]\n",
      "Step 545892  [5.449 sec/step, loss=0.07494, avg_loss=0.07317]\n",
      "Step 545893  [5.434 sec/step, loss=0.06992, avg_loss=0.07313]\n",
      "Step 545894  [5.431 sec/step, loss=0.07457, avg_loss=0.07315]\n",
      "Step 545895  [5.418 sec/step, loss=0.06464, avg_loss=0.07305]\n",
      "Step 545896  [5.426 sec/step, loss=0.07520, avg_loss=0.07307]\n",
      "Step 545897  [5.431 sec/step, loss=0.07254, avg_loss=0.07306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 545898  [5.417 sec/step, loss=0.07418, avg_loss=0.07307]\n",
      "Step 545899  [5.415 sec/step, loss=0.06493, avg_loss=0.07302]\n",
      "Step 545900  [5.438 sec/step, loss=0.07405, avg_loss=0.07310]\n",
      "Writing summary at step: 545900\n",
      "Step 545901  [5.439 sec/step, loss=0.07564, avg_loss=0.07310]\n",
      "Step 545902  [5.431 sec/step, loss=0.07452, avg_loss=0.07312]\n",
      "Step 545903  [5.422 sec/step, loss=0.07460, avg_loss=0.07312]\n",
      "Step 545904  [5.413 sec/step, loss=0.06908, avg_loss=0.07306]\n",
      "Step 545905  [5.413 sec/step, loss=0.07466, avg_loss=0.07310]\n",
      "Step 545906  [5.398 sec/step, loss=0.07363, avg_loss=0.07309]\n",
      "Step 545907  [5.392 sec/step, loss=0.07377, avg_loss=0.07308]\n",
      "Step 545908  [5.404 sec/step, loss=0.07329, avg_loss=0.07317]\n",
      "Step 545909  [5.394 sec/step, loss=0.07619, avg_loss=0.07319]\n",
      "Step 545910  [5.401 sec/step, loss=0.07491, avg_loss=0.07321]\n",
      "Step 545911  [5.351 sec/step, loss=0.07087, avg_loss=0.07328]\n",
      "Step 545912  [5.373 sec/step, loss=0.07489, avg_loss=0.07333]\n",
      "Step 545913  [5.382 sec/step, loss=0.07533, avg_loss=0.07334]\n",
      "Step 545914  [5.371 sec/step, loss=0.07119, avg_loss=0.07332]\n",
      "Step 545915  [5.429 sec/step, loss=0.06676, avg_loss=0.07326]\n",
      "Step 545916  [5.438 sec/step, loss=0.07552, avg_loss=0.07328]\n",
      "Step 545917  [5.433 sec/step, loss=0.07324, avg_loss=0.07326]\n",
      "Step 545918  [5.445 sec/step, loss=0.07380, avg_loss=0.07327]\n",
      "Step 545919  [5.466 sec/step, loss=0.07227, avg_loss=0.07325]\n",
      "Generated 32 batches of size 32 in 2.508 sec\n",
      "Step 545920  [5.487 sec/step, loss=0.07491, avg_loss=0.07327]\n",
      "Step 545921  [5.473 sec/step, loss=0.07346, avg_loss=0.07326]\n",
      "Step 545922  [5.475 sec/step, loss=0.07490, avg_loss=0.07330]\n",
      "Step 545923  [5.455 sec/step, loss=0.07367, avg_loss=0.07329]\n",
      "Step 545924  [5.477 sec/step, loss=0.07265, avg_loss=0.07328]\n",
      "Step 545925  [5.458 sec/step, loss=0.07115, avg_loss=0.07326]\n",
      "Step 545926  [5.445 sec/step, loss=0.07314, avg_loss=0.07324]\n",
      "Step 545927  [5.470 sec/step, loss=0.07335, avg_loss=0.07327]\n",
      "Step 545928  [5.461 sec/step, loss=0.07150, avg_loss=0.07323]\n",
      "Step 545929  [5.479 sec/step, loss=0.07513, avg_loss=0.07324]\n",
      "Step 545930  [5.466 sec/step, loss=0.07471, avg_loss=0.07325]\n",
      "Step 545931  [5.457 sec/step, loss=0.07324, avg_loss=0.07323]\n",
      "Step 545932  [5.471 sec/step, loss=0.07494, avg_loss=0.07323]\n",
      "Step 545933  [5.482 sec/step, loss=0.07648, avg_loss=0.07325]\n",
      "Step 545934  [5.472 sec/step, loss=0.07446, avg_loss=0.07324]\n",
      "Step 545935  [5.470 sec/step, loss=0.07541, avg_loss=0.07326]\n",
      "Step 545936  [5.482 sec/step, loss=0.07391, avg_loss=0.07326]\n",
      "Step 545937  [5.464 sec/step, loss=0.07332, avg_loss=0.07325]\n",
      "Step 545938  [5.459 sec/step, loss=0.07356, avg_loss=0.07329]\n",
      "Step 545939  [5.457 sec/step, loss=0.07308, avg_loss=0.07329]\n",
      "Step 545940  [5.442 sec/step, loss=0.07437, avg_loss=0.07329]\n",
      "Step 545941  [5.457 sec/step, loss=0.07485, avg_loss=0.07334]\n",
      "Step 545942  [5.470 sec/step, loss=0.07343, avg_loss=0.07334]\n",
      "Step 545943  [5.474 sec/step, loss=0.07593, avg_loss=0.07337]\n",
      "Step 545944  [5.478 sec/step, loss=0.07095, avg_loss=0.07334]\n",
      "Step 545945  [5.487 sec/step, loss=0.07477, avg_loss=0.07334]\n",
      "Step 545946  [5.478 sec/step, loss=0.07231, avg_loss=0.07333]\n",
      "Step 545947  [5.499 sec/step, loss=0.07657, avg_loss=0.07342]\n",
      "Step 545948  [5.512 sec/step, loss=0.07568, avg_loss=0.07345]\n",
      "Step 545949  [5.537 sec/step, loss=0.07509, avg_loss=0.07345]\n",
      "Step 545950  [5.542 sec/step, loss=0.07544, avg_loss=0.07347]\n",
      "Step 545951  [5.489 sec/step, loss=0.07326, avg_loss=0.07354]\n",
      "Generated 32 batches of size 32 in 2.634 sec\n",
      "Step 545952  [5.529 sec/step, loss=0.06501, avg_loss=0.07345]\n",
      "Step 545953  [5.526 sec/step, loss=0.06943, avg_loss=0.07341]\n",
      "Step 545954  [5.520 sec/step, loss=0.07456, avg_loss=0.07340]\n",
      "Step 545955  [5.505 sec/step, loss=0.06436, avg_loss=0.07331]\n",
      "Step 545956  [5.488 sec/step, loss=0.07477, avg_loss=0.07332]\n",
      "Step 545957  [5.487 sec/step, loss=0.07414, avg_loss=0.07332]\n",
      "Step 545958  [5.446 sec/step, loss=0.07175, avg_loss=0.07331]\n",
      "Step 545959  [5.462 sec/step, loss=0.07473, avg_loss=0.07332]\n",
      "Step 545960  [5.450 sec/step, loss=0.07430, avg_loss=0.07333]\n",
      "Step 545961  [5.451 sec/step, loss=0.07476, avg_loss=0.07336]\n",
      "Step 545962  [5.474 sec/step, loss=0.07286, avg_loss=0.07335]\n",
      "Step 545963  [5.474 sec/step, loss=0.07258, avg_loss=0.07331]\n",
      "Step 545964  [5.463 sec/step, loss=0.07052, avg_loss=0.07327]\n",
      "Step 545965  [5.482 sec/step, loss=0.07428, avg_loss=0.07329]\n",
      "Step 545966  [5.495 sec/step, loss=0.07551, avg_loss=0.07330]\n",
      "Step 545967  [5.500 sec/step, loss=0.07591, avg_loss=0.07333]\n",
      "Step 545968  [5.445 sec/step, loss=0.07270, avg_loss=0.07341]\n",
      "Step 545969  [5.453 sec/step, loss=0.07311, avg_loss=0.07344]\n",
      "Step 545970  [5.434 sec/step, loss=0.07178, avg_loss=0.07341]\n",
      "Step 545971  [5.428 sec/step, loss=0.07634, avg_loss=0.07342]\n",
      "Step 545972  [5.410 sec/step, loss=0.07484, avg_loss=0.07342]\n",
      "Step 545973  [5.430 sec/step, loss=0.07542, avg_loss=0.07344]\n",
      "Step 545974  [5.420 sec/step, loss=0.07586, avg_loss=0.07345]\n",
      "Step 545975  [5.439 sec/step, loss=0.07319, avg_loss=0.07346]\n",
      "Step 545976  [5.486 sec/step, loss=0.06555, avg_loss=0.07337]\n",
      "Step 545977  [5.486 sec/step, loss=0.07161, avg_loss=0.07334]\n",
      "Step 545978  [5.491 sec/step, loss=0.07165, avg_loss=0.07331]\n",
      "Step 545979  [5.457 sec/step, loss=0.06689, avg_loss=0.07323]\n",
      "Step 545980  [5.468 sec/step, loss=0.07507, avg_loss=0.07324]\n",
      "Step 545981  [5.470 sec/step, loss=0.07238, avg_loss=0.07326]\n",
      "Step 545982  [5.457 sec/step, loss=0.07173, avg_loss=0.07322]\n",
      "Step 545983  [5.438 sec/step, loss=0.07430, avg_loss=0.07321]\n",
      "Generated 32 batches of size 32 in 2.431 sec\n",
      "Step 545984  [5.447 sec/step, loss=0.07497, avg_loss=0.07325]\n",
      "Step 545985  [5.459 sec/step, loss=0.07313, avg_loss=0.07322]\n",
      "Step 545986  [5.445 sec/step, loss=0.07168, avg_loss=0.07320]\n",
      "Step 545987  [5.447 sec/step, loss=0.07382, avg_loss=0.07321]\n",
      "Step 545988  [5.459 sec/step, loss=0.07639, avg_loss=0.07323]\n",
      "Step 545989  [5.461 sec/step, loss=0.07435, avg_loss=0.07324]\n",
      "Step 545990  [5.446 sec/step, loss=0.07349, avg_loss=0.07323]\n",
      "Step 545991  [5.456 sec/step, loss=0.07508, avg_loss=0.07324]\n",
      "Step 545992  [5.452 sec/step, loss=0.07345, avg_loss=0.07322]\n",
      "Step 545993  [5.472 sec/step, loss=0.07323, avg_loss=0.07325]\n",
      "Step 545994  [5.472 sec/step, loss=0.07463, avg_loss=0.07325]\n",
      "Step 545995  [5.485 sec/step, loss=0.07417, avg_loss=0.07335]\n",
      "Step 545996  [5.484 sec/step, loss=0.07585, avg_loss=0.07336]\n",
      "Step 545997  [5.464 sec/step, loss=0.07316, avg_loss=0.07336]\n",
      "Step 545998  [5.461 sec/step, loss=0.07434, avg_loss=0.07336]\n",
      "Step 545999  [5.527 sec/step, loss=0.06579, avg_loss=0.07337]\n",
      "Step 546000  [5.530 sec/step, loss=0.07489, avg_loss=0.07338]\n",
      "Writing summary at step: 546000\n",
      "Saving checkpoint to: ./logs-aneesa/model.ckpt-546000\n",
      "Saving audio and alignment...\n",
      "Input: tiitsar kii aankhoon miin aansuu ddaykh kur tdahsiil naazim pighal gijaa or barrhaapay miin asay tdhaam lijaa~________\n",
      "Step 546001  [5.514 sec/step, loss=0.07412, avg_loss=0.07337]\n",
      "Step 546002  [5.524 sec/step, loss=0.07895, avg_loss=0.07341]\n",
      "Step 546003  [5.547 sec/step, loss=0.08336, avg_loss=0.07350]\n",
      "Step 546004  [5.570 sec/step, loss=0.07801, avg_loss=0.07359]\n",
      "Step 546005  [5.576 sec/step, loss=0.07879, avg_loss=0.07363]\n",
      "Step 546006  [5.591 sec/step, loss=0.08146, avg_loss=0.07371]\n",
      "Step 546007  [5.578 sec/step, loss=0.07729, avg_loss=0.07374]\n",
      "Step 546008  [5.589 sec/step, loss=0.08200, avg_loss=0.07383]\n",
      "Step 546009  [5.583 sec/step, loss=0.08011, avg_loss=0.07387]\n",
      "Step 546010  [5.581 sec/step, loss=0.08249, avg_loss=0.07394]\n",
      "Step 546011  [5.565 sec/step, loss=0.06844, avg_loss=0.07392]\n",
      "Step 546012  [5.549 sec/step, loss=0.07592, avg_loss=0.07393]\n",
      "Step 546013  [5.525 sec/step, loss=0.07462, avg_loss=0.07392]\n",
      "Generated 32 batches of size 32 in 2.366 sec\n",
      "Step 546014  [5.550 sec/step, loss=0.08244, avg_loss=0.07404]\n",
      "Step 546015  [5.495 sec/step, loss=0.07717, avg_loss=0.07414]\n",
      "Step 546016  [5.485 sec/step, loss=0.07879, avg_loss=0.07417]\n",
      "Step 546017  [5.497 sec/step, loss=0.08555, avg_loss=0.07430]\n",
      "Step 546018  [5.493 sec/step, loss=0.09393, avg_loss=0.07450]\n",
      "Step 546019  [5.479 sec/step, loss=0.12264, avg_loss=0.07500]\n",
      "Step 546020  [5.454 sec/step, loss=0.09795, avg_loss=0.07523]\n",
      "Step 546021  [5.472 sec/step, loss=0.10207, avg_loss=0.07552]\n",
      "Step 546022  [5.473 sec/step, loss=0.09046, avg_loss=0.07567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 546023  [5.460 sec/step, loss=0.08316, avg_loss=0.07577]\n",
      "Step 546024  [5.475 sec/step, loss=0.08690, avg_loss=0.07591]\n",
      "Step 546025  [5.496 sec/step, loss=0.08889, avg_loss=0.07609]\n",
      "Step 546026  [5.503 sec/step, loss=0.09545, avg_loss=0.07631]\n",
      "Step 546027  [5.487 sec/step, loss=0.08849, avg_loss=0.07646]\n",
      "Step 546028  [5.499 sec/step, loss=0.10037, avg_loss=0.07675]\n",
      "Step 546029  [5.484 sec/step, loss=0.10377, avg_loss=0.07704]\n",
      "Step 546030  [5.468 sec/step, loss=0.09117, avg_loss=0.07720]\n",
      "Step 546031  [5.472 sec/step, loss=0.10998, avg_loss=0.07757]\n",
      "Step 546032  [5.464 sec/step, loss=0.11450, avg_loss=0.07796]\n",
      "Step 546033  [5.454 sec/step, loss=0.09697, avg_loss=0.07817]\n",
      "Step 546034  [5.456 sec/step, loss=0.09921, avg_loss=0.07842]\n",
      "Step 546035  [5.447 sec/step, loss=0.09663, avg_loss=0.07863]\n",
      "Step 546036  [5.443 sec/step, loss=0.09303, avg_loss=0.07882]\n",
      "Step 546037  [5.467 sec/step, loss=0.09953, avg_loss=0.07908]\n",
      "Step 546038  [5.475 sec/step, loss=0.09545, avg_loss=0.07930]\n",
      "Step 546039  [5.472 sec/step, loss=0.08835, avg_loss=0.07945]\n",
      "Step 546040  [5.466 sec/step, loss=0.08665, avg_loss=0.07958]\n",
      "Step 546041  [5.511 sec/step, loss=0.08662, avg_loss=0.07969]\n",
      "Step 546042  [5.493 sec/step, loss=0.09020, avg_loss=0.07986]\n",
      "Step 546043  [5.491 sec/step, loss=0.09329, avg_loss=0.08004]\n",
      "Step 546044  [5.508 sec/step, loss=0.09516, avg_loss=0.08028]\n",
      "Step 546045  [5.484 sec/step, loss=0.08142, avg_loss=0.08034]\n",
      "Generated 32 batches of size 32 in 3.980 sec\n",
      "Step 546046  [5.491 sec/step, loss=0.09732, avg_loss=0.08059]\n",
      "Step 546047  [5.504 sec/step, loss=0.09741, avg_loss=0.08080]\n",
      "Step 546048  [5.493 sec/step, loss=0.09366, avg_loss=0.08098]\n",
      "Step 546049  [5.471 sec/step, loss=0.09292, avg_loss=0.08116]\n",
      "Step 546050  [5.472 sec/step, loss=0.09419, avg_loss=0.08135]\n",
      "Step 546051  [5.476 sec/step, loss=0.09104, avg_loss=0.08153]\n",
      "Step 546052  [5.422 sec/step, loss=0.08819, avg_loss=0.08176]\n",
      "Step 546053  [5.430 sec/step, loss=0.09160, avg_loss=0.08198]\n",
      "Step 546054  [5.440 sec/step, loss=0.09506, avg_loss=0.08218]\n",
      "Step 546055  [5.440 sec/step, loss=0.07732, avg_loss=0.08231]\n",
      "Step 546056  [5.422 sec/step, loss=0.08437, avg_loss=0.08241]\n",
      "Step 546057  [5.426 sec/step, loss=0.08958, avg_loss=0.08256]\n",
      "Step 546058  [5.453 sec/step, loss=0.09144, avg_loss=0.08276]\n",
      "Step 546059  [5.433 sec/step, loss=0.08889, avg_loss=0.08290]\n",
      "Step 546060  [5.427 sec/step, loss=0.08718, avg_loss=0.08303]\n",
      "Step 546061  [5.414 sec/step, loss=0.08759, avg_loss=0.08316]\n",
      "Step 546062  [5.378 sec/step, loss=0.08645, avg_loss=0.08330]\n",
      "Step 546063  [5.373 sec/step, loss=0.09419, avg_loss=0.08351]\n",
      "Step 546064  [5.432 sec/step, loss=0.08015, avg_loss=0.08361]\n",
      "Step 546065  [5.438 sec/step, loss=0.08877, avg_loss=0.08375]\n",
      "Step 546066  [5.434 sec/step, loss=0.08436, avg_loss=0.08384]\n",
      "Step 546067  [5.438 sec/step, loss=0.08783, avg_loss=0.08396]\n",
      "Step 546068  [5.443 sec/step, loss=0.08659, avg_loss=0.08410]\n",
      "Step 546069  [5.450 sec/step, loss=0.08362, avg_loss=0.08420]\n",
      "Step 546070  [5.453 sec/step, loss=0.08403, avg_loss=0.08433]\n",
      "Step 546071  [5.443 sec/step, loss=0.08880, avg_loss=0.08445]\n",
      "Step 546072  [5.444 sec/step, loss=0.08548, avg_loss=0.08456]\n",
      "Step 546073  [5.448 sec/step, loss=0.08645, avg_loss=0.08467]\n",
      "Step 546074  [5.437 sec/step, loss=0.08486, avg_loss=0.08476]\n",
      "Step 546075  [5.424 sec/step, loss=0.08816, avg_loss=0.08491]\n",
      "Step 546076  [5.374 sec/step, loss=0.08208, avg_loss=0.08507]\n",
      "Step 546077  [5.379 sec/step, loss=0.08767, avg_loss=0.08523]\n",
      "Generated 32 batches of size 32 in 2.409 sec\n",
      "Step 546078  [5.403 sec/step, loss=0.08417, avg_loss=0.08536]\n",
      "Step 546079  [5.430 sec/step, loss=0.08633, avg_loss=0.08555]\n",
      "Step 546080  [5.438 sec/step, loss=0.08260, avg_loss=0.08563]\n",
      "Step 546081  [5.448 sec/step, loss=0.08772, avg_loss=0.08578]\n",
      "Step 546082  [5.441 sec/step, loss=0.08126, avg_loss=0.08588]\n",
      "Step 546083  [5.435 sec/step, loss=0.07965, avg_loss=0.08593]\n",
      "Step 546084  [5.429 sec/step, loss=0.08323, avg_loss=0.08601]\n",
      "Step 546085  [5.404 sec/step, loss=0.08361, avg_loss=0.08612]\n",
      "Step 546086  [5.430 sec/step, loss=0.08284, avg_loss=0.08623]\n",
      "Step 546087  [5.428 sec/step, loss=0.08428, avg_loss=0.08634]\n",
      "Step 546088  [5.463 sec/step, loss=0.07284, avg_loss=0.08630]\n",
      "Step 546089  [5.477 sec/step, loss=0.08326, avg_loss=0.08639]\n",
      "Step 546090  [5.501 sec/step, loss=0.08300, avg_loss=0.08648]\n",
      "Step 546091  [5.500 sec/step, loss=0.08701, avg_loss=0.08660]\n",
      "Step 546092  [5.521 sec/step, loss=0.08231, avg_loss=0.08669]\n",
      "Step 546093  [5.508 sec/step, loss=0.07966, avg_loss=0.08676]\n",
      "Step 546094  [5.504 sec/step, loss=0.07752, avg_loss=0.08678]\n",
      "Step 546095  [5.518 sec/step, loss=0.08345, avg_loss=0.08688]\n",
      "Step 546096  [5.521 sec/step, loss=0.08037, avg_loss=0.08692]\n",
      "Step 546097  [5.533 sec/step, loss=0.08254, avg_loss=0.08702]\n",
      "Step 546098  [5.533 sec/step, loss=0.08235, avg_loss=0.08710]\n",
      "Step 546099  [5.488 sec/step, loss=0.08131, avg_loss=0.08725]\n",
      "Step 546100  [5.472 sec/step, loss=0.07962, avg_loss=0.08730]\n",
      "Writing summary at step: 546100\n",
      "Step 546101  [5.477 sec/step, loss=0.08393, avg_loss=0.08740]\n",
      "Step 546102  [5.479 sec/step, loss=0.08224, avg_loss=0.08743]\n",
      "Step 546103  [5.451 sec/step, loss=0.07740, avg_loss=0.08737]\n",
      "Step 546104  [5.439 sec/step, loss=0.08404, avg_loss=0.08743]\n",
      "Step 546105  [5.432 sec/step, loss=0.08112, avg_loss=0.08745]\n",
      "Step 546106  [5.411 sec/step, loss=0.07062, avg_loss=0.08735]\n",
      "Step 546107  [5.419 sec/step, loss=0.08069, avg_loss=0.08738]\n",
      "Step 546108  [5.414 sec/step, loss=0.08114, avg_loss=0.08737]\n",
      "Generated 32 batches of size 32 in 2.585 sec\n",
      "Step 546109  [5.406 sec/step, loss=0.07619, avg_loss=0.08733]\n",
      "Step 546110  [5.387 sec/step, loss=0.08139, avg_loss=0.08732]\n",
      "Step 546111  [5.414 sec/step, loss=0.08180, avg_loss=0.08745]\n",
      "Step 546112  [5.408 sec/step, loss=0.07953, avg_loss=0.08749]\n",
      "Step 546113  [5.425 sec/step, loss=0.07955, avg_loss=0.08754]\n",
      "Step 546114  [5.421 sec/step, loss=0.07986, avg_loss=0.08751]\n",
      "Step 546115  [5.424 sec/step, loss=0.08159, avg_loss=0.08756]\n",
      "Step 546116  [5.425 sec/step, loss=0.08024, avg_loss=0.08757]\n",
      "Step 546117  [5.402 sec/step, loss=0.07557, avg_loss=0.08747]\n",
      "Step 546118  [5.406 sec/step, loss=0.08141, avg_loss=0.08735]\n",
      "Step 546119  [5.394 sec/step, loss=0.07978, avg_loss=0.08692]\n",
      "Step 546120  [5.406 sec/step, loss=0.08106, avg_loss=0.08675]\n",
      "Step 546121  [5.392 sec/step, loss=0.07595, avg_loss=0.08649]\n",
      "Step 546122  [5.385 sec/step, loss=0.07754, avg_loss=0.08636]\n",
      "Step 546123  [5.401 sec/step, loss=0.08127, avg_loss=0.08634]\n",
      "Step 546124  [5.400 sec/step, loss=0.07737, avg_loss=0.08625]\n",
      "Step 546125  [5.388 sec/step, loss=0.07647, avg_loss=0.08612]\n",
      "Step 546126  [5.369 sec/step, loss=0.07360, avg_loss=0.08590]\n",
      "Step 546127  [5.391 sec/step, loss=0.08009, avg_loss=0.08582]\n",
      "Step 546128  [5.385 sec/step, loss=0.07874, avg_loss=0.08560]\n",
      "Step 546129  [5.381 sec/step, loss=0.07989, avg_loss=0.08536]\n",
      "Step 546130  [5.446 sec/step, loss=0.07065, avg_loss=0.08516]\n",
      "Step 546131  [5.447 sec/step, loss=0.07875, avg_loss=0.08485]\n",
      "Step 546132  [5.436 sec/step, loss=0.08040, avg_loss=0.08451]\n",
      "Step 546133  [5.439 sec/step, loss=0.07962, avg_loss=0.08433]\n",
      "Step 546134  [5.452 sec/step, loss=0.08048, avg_loss=0.08414]\n",
      "Step 546135  [5.449 sec/step, loss=0.07817, avg_loss=0.08396]\n",
      "Step 546136  [5.436 sec/step, loss=0.06997, avg_loss=0.08373]\n",
      "Step 546137  [5.436 sec/step, loss=0.08029, avg_loss=0.08354]\n",
      "Step 546138  [5.425 sec/step, loss=0.07758, avg_loss=0.08336]\n",
      "Step 546139  [5.435 sec/step, loss=0.07769, avg_loss=0.08325]\n",
      "Step 546140  [5.455 sec/step, loss=0.07864, avg_loss=0.08317]\n",
      "Generated 32 batches of size 32 in 2.396 sec\n",
      "Step 546141  [5.422 sec/step, loss=0.07984, avg_loss=0.08310]\n",
      "Step 546142  [5.425 sec/step, loss=0.07922, avg_loss=0.08299]\n",
      "Step 546143  [5.413 sec/step, loss=0.07710, avg_loss=0.08283]\n",
      "Step 546144  [5.410 sec/step, loss=0.07937, avg_loss=0.08267]\n",
      "Step 546145  [5.427 sec/step, loss=0.08198, avg_loss=0.08268]\n",
      "Step 546146  [5.411 sec/step, loss=0.07470, avg_loss=0.08245]\n",
      "Step 546147  [5.394 sec/step, loss=0.07752, avg_loss=0.08225]\n",
      "Step 546148  [5.397 sec/step, loss=0.07752, avg_loss=0.08209]\n",
      "Step 546149  [5.405 sec/step, loss=0.07997, avg_loss=0.08196]\n",
      "Step 546150  [5.395 sec/step, loss=0.07502, avg_loss=0.08177]\n",
      "Step 546151  [5.393 sec/step, loss=0.07856, avg_loss=0.08165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 546152  [5.375 sec/step, loss=0.06789, avg_loss=0.08144]\n",
      "Step 546153  [5.366 sec/step, loss=0.07563, avg_loss=0.08128]\n",
      "Step 546154  [5.366 sec/step, loss=0.07978, avg_loss=0.08113]\n",
      "Step 546155  [5.388 sec/step, loss=0.07939, avg_loss=0.08115]\n",
      "Step 546156  [5.412 sec/step, loss=0.07910, avg_loss=0.08110]\n",
      "Step 546157  [5.415 sec/step, loss=0.07894, avg_loss=0.08099]\n",
      "Step 546158  [5.406 sec/step, loss=0.07959, avg_loss=0.08088]\n",
      "Step 546159  [5.434 sec/step, loss=0.07652, avg_loss=0.08075]\n",
      "Step 546160  [5.434 sec/step, loss=0.07790, avg_loss=0.08066]\n",
      "Step 546161  [5.454 sec/step, loss=0.07702, avg_loss=0.08055]\n",
      "Step 546162  [5.448 sec/step, loss=0.07397, avg_loss=0.08043]\n",
      "Step 546163  [5.432 sec/step, loss=0.07671, avg_loss=0.08025]\n",
      "Step 546164  [5.381 sec/step, loss=0.07788, avg_loss=0.08023]\n",
      "Step 546165  [5.384 sec/step, loss=0.07884, avg_loss=0.08013]\n",
      "Step 546166  [5.374 sec/step, loss=0.07911, avg_loss=0.08008]\n",
      "Step 546167  [5.378 sec/step, loss=0.07657, avg_loss=0.07997]\n",
      "Step 546168  [5.428 sec/step, loss=0.07033, avg_loss=0.07980]\n",
      "Step 546169  [5.426 sec/step, loss=0.07867, avg_loss=0.07975]\n",
      "Step 546170  [5.443 sec/step, loss=0.07904, avg_loss=0.07970]\n",
      "Step 546171  [5.442 sec/step, loss=0.07759, avg_loss=0.07959]\n",
      "Step 546172  [5.438 sec/step, loss=0.07672, avg_loss=0.07950]\n",
      "Generated 32 batches of size 32 in 2.398 sec\n",
      "Step 546173  [5.435 sec/step, loss=0.07662, avg_loss=0.07941]\n",
      "Step 546174  [5.436 sec/step, loss=0.07765, avg_loss=0.07933]\n",
      "Step 546175  [5.436 sec/step, loss=0.07600, avg_loss=0.07921]\n",
      "Step 546176  [5.433 sec/step, loss=0.07767, avg_loss=0.07917]\n",
      "Step 546177  [5.426 sec/step, loss=0.07857, avg_loss=0.07908]\n",
      "Step 546178  [5.406 sec/step, loss=0.07856, avg_loss=0.07902]\n",
      "Step 546179  [5.386 sec/step, loss=0.07230, avg_loss=0.07888]\n",
      "Step 546180  [5.370 sec/step, loss=0.07406, avg_loss=0.07880]\n",
      "Step 546181  [5.380 sec/step, loss=0.07868, avg_loss=0.07871]\n",
      "Step 546182  [5.382 sec/step, loss=0.07392, avg_loss=0.07863]\n",
      "Step 546183  [5.388 sec/step, loss=0.07575, avg_loss=0.07859]\n",
      "Step 546184  [5.393 sec/step, loss=0.07609, avg_loss=0.07852]\n",
      "Step 546185  [5.408 sec/step, loss=0.07704, avg_loss=0.07846]\n",
      "Step 546186  [5.398 sec/step, loss=0.07738, avg_loss=0.07840]\n",
      "Step 546187  [5.426 sec/step, loss=0.07566, avg_loss=0.07831]\n",
      "Step 546188  [5.390 sec/step, loss=0.07887, avg_loss=0.07838]\n",
      "Step 546189  [5.363 sec/step, loss=0.07369, avg_loss=0.07828]\n",
      "Step 546190  [5.341 sec/step, loss=0.07726, avg_loss=0.07822]\n",
      "Step 546191  [5.342 sec/step, loss=0.07937, avg_loss=0.07815]\n",
      "Step 546192  [5.341 sec/step, loss=0.07581, avg_loss=0.07808]\n",
      "Step 546193  [5.348 sec/step, loss=0.07867, avg_loss=0.07807]\n",
      "Step 546194  [5.347 sec/step, loss=0.07861, avg_loss=0.07808]\n",
      "Step 546195  [5.345 sec/step, loss=0.07859, avg_loss=0.07803]\n",
      "Step 546196  [5.317 sec/step, loss=0.06812, avg_loss=0.07791]\n",
      "Step 546197  [5.316 sec/step, loss=0.07739, avg_loss=0.07786]\n",
      "Step 546198  [5.313 sec/step, loss=0.07180, avg_loss=0.07775]\n",
      "Step 546199  [5.316 sec/step, loss=0.07867, avg_loss=0.07773]\n",
      "Step 546200  [5.321 sec/step, loss=0.07726, avg_loss=0.07770]\n",
      "Writing summary at step: 546200\n",
      "Step 546201  [5.329 sec/step, loss=0.07639, avg_loss=0.07763]\n",
      "Step 546202  [5.306 sec/step, loss=0.07610, avg_loss=0.07757]\n",
      "Step 546203  [5.305 sec/step, loss=0.07717, avg_loss=0.07756]\n",
      "Generated 32 batches of size 32 in 2.370 sec\n",
      "Step 546204  [5.313 sec/step, loss=0.07911, avg_loss=0.07752]\n",
      "Step 546205  [5.318 sec/step, loss=0.07524, avg_loss=0.07746]\n",
      "Step 546206  [5.385 sec/step, loss=0.06936, avg_loss=0.07744]\n",
      "Step 546207  [5.389 sec/step, loss=0.07715, avg_loss=0.07741]\n",
      "Step 546208  [5.384 sec/step, loss=0.07763, avg_loss=0.07737]\n",
      "Step 546209  [5.399 sec/step, loss=0.07775, avg_loss=0.07739]\n",
      "Step 546210  [5.408 sec/step, loss=0.07778, avg_loss=0.07735]\n",
      "Step 546211  [5.402 sec/step, loss=0.07739, avg_loss=0.07731]\n",
      "Step 546212  [5.402 sec/step, loss=0.07428, avg_loss=0.07726]\n",
      "Step 546213  [5.387 sec/step, loss=0.07151, avg_loss=0.07718]\n",
      "Step 546214  [5.378 sec/step, loss=0.07581, avg_loss=0.07714]\n",
      "Step 546215  [5.389 sec/step, loss=0.07683, avg_loss=0.07709]\n",
      "Step 546216  [5.383 sec/step, loss=0.07582, avg_loss=0.07704]\n",
      "Step 546217  [5.423 sec/step, loss=0.07514, avg_loss=0.07704]\n",
      "Step 546218  [5.425 sec/step, loss=0.07506, avg_loss=0.07698]\n",
      "Step 546219  [5.418 sec/step, loss=0.07649, avg_loss=0.07694]\n",
      "Step 546220  [5.424 sec/step, loss=0.07884, avg_loss=0.07692]\n",
      "Step 546221  [5.426 sec/step, loss=0.07647, avg_loss=0.07693]\n",
      "Step 546222  [5.428 sec/step, loss=0.07213, avg_loss=0.07687]\n",
      "Step 546223  [5.422 sec/step, loss=0.07517, avg_loss=0.07681]\n",
      "Step 546224  [5.446 sec/step, loss=0.06737, avg_loss=0.07671]\n",
      "Step 546225  [5.468 sec/step, loss=0.07773, avg_loss=0.07672]\n",
      "Step 546226  [5.476 sec/step, loss=0.07640, avg_loss=0.07675]\n",
      "Step 546227  [5.464 sec/step, loss=0.07816, avg_loss=0.07673]\n",
      "Step 546228  [5.468 sec/step, loss=0.07914, avg_loss=0.07674]\n",
      "Step 546229  [5.483 sec/step, loss=0.07778, avg_loss=0.07671]\n",
      "Step 546230  [5.434 sec/step, loss=0.07690, avg_loss=0.07678]\n",
      "Step 546231  [5.442 sec/step, loss=0.07765, avg_loss=0.07677]\n",
      "Step 546232  [5.452 sec/step, loss=0.07761, avg_loss=0.07674]\n",
      "Step 546233  [5.459 sec/step, loss=0.07804, avg_loss=0.07672]\n",
      "Step 546234  [5.434 sec/step, loss=0.07306, avg_loss=0.07665]\n",
      "Step 546235  [5.413 sec/step, loss=0.06808, avg_loss=0.07655]\n",
      "Generated 32 batches of size 32 in 2.425 sec\n",
      "Step 546236  [5.448 sec/step, loss=0.07798, avg_loss=0.07663]\n",
      "Step 546237  [5.436 sec/step, loss=0.07715, avg_loss=0.07660]\n",
      "Step 546238  [5.444 sec/step, loss=0.07813, avg_loss=0.07660]\n",
      "Step 546239  [5.433 sec/step, loss=0.07516, avg_loss=0.07658]\n",
      "Step 546240  [5.423 sec/step, loss=0.07790, avg_loss=0.07657]\n",
      "Step 546241  [5.403 sec/step, loss=0.07555, avg_loss=0.07653]\n",
      "Step 546242  [5.412 sec/step, loss=0.07629, avg_loss=0.07650]\n",
      "Step 546243  [5.420 sec/step, loss=0.07405, avg_loss=0.07647]\n",
      "Step 546244  [5.401 sec/step, loss=0.07469, avg_loss=0.07642]\n",
      "Step 546245  [5.397 sec/step, loss=0.07973, avg_loss=0.07640]\n",
      "Step 546246  [5.430 sec/step, loss=0.07425, avg_loss=0.07639]\n",
      "Step 546247  [5.432 sec/step, loss=0.07332, avg_loss=0.07635]\n",
      "Step 546248  [5.435 sec/step, loss=0.07669, avg_loss=0.07634]\n",
      "Step 546249  [5.439 sec/step, loss=0.07867, avg_loss=0.07633]\n",
      "Step 546250  [5.437 sec/step, loss=0.07750, avg_loss=0.07635]\n",
      "Step 546251  [5.447 sec/step, loss=0.07735, avg_loss=0.07634]\n",
      "Step 546252  [5.461 sec/step, loss=0.07650, avg_loss=0.07643]\n",
      "Step 546253  [5.480 sec/step, loss=0.07852, avg_loss=0.07646]\n",
      "Step 546254  [5.469 sec/step, loss=0.07646, avg_loss=0.07642]\n",
      "Step 546255  [5.467 sec/step, loss=0.07283, avg_loss=0.07636]\n",
      "Step 546256  [5.463 sec/step, loss=0.07871, avg_loss=0.07635]\n",
      "Step 546257  [5.457 sec/step, loss=0.07682, avg_loss=0.07633]\n",
      "Step 546258  [5.463 sec/step, loss=0.07813, avg_loss=0.07632]\n",
      "Step 546259  [5.450 sec/step, loss=0.07538, avg_loss=0.07631]\n",
      "Step 546260  [5.452 sec/step, loss=0.07414, avg_loss=0.07627]\n",
      "Step 546261  [5.447 sec/step, loss=0.07828, avg_loss=0.07628]\n",
      "Step 546262  [5.511 sec/step, loss=0.06804, avg_loss=0.07622]\n",
      "Step 546263  [5.501 sec/step, loss=0.06758, avg_loss=0.07613]\n",
      "Step 546264  [5.508 sec/step, loss=0.07402, avg_loss=0.07609]\n",
      "Step 546265  [5.488 sec/step, loss=0.07501, avg_loss=0.07605]\n",
      "Step 546266  [5.484 sec/step, loss=0.07187, avg_loss=0.07598]\n",
      "Step 546267  [5.485 sec/step, loss=0.07785, avg_loss=0.07599]\n",
      "Generated 32 batches of size 32 in 2.421 sec\n",
      "Step 546268  [5.450 sec/step, loss=0.07527, avg_loss=0.07604]\n",
      "Step 546269  [5.442 sec/step, loss=0.07473, avg_loss=0.07600]\n",
      "Step 546270  [5.429 sec/step, loss=0.07505, avg_loss=0.07596]\n",
      "Step 546271  [5.422 sec/step, loss=0.07275, avg_loss=0.07592]\n",
      "Step 546272  [5.416 sec/step, loss=0.07235, avg_loss=0.07587]\n",
      "Step 546273  [5.434 sec/step, loss=0.07493, avg_loss=0.07586]\n",
      "Step 546274  [5.426 sec/step, loss=0.07497, avg_loss=0.07583]\n",
      "Step 546275  [5.430 sec/step, loss=0.07654, avg_loss=0.07583]\n",
      "Step 546276  [5.438 sec/step, loss=0.07557, avg_loss=0.07581]\n",
      "Step 546277  [5.434 sec/step, loss=0.07528, avg_loss=0.07578]\n",
      "Step 546278  [5.448 sec/step, loss=0.07428, avg_loss=0.07574]\n",
      "Step 546279  [5.470 sec/step, loss=0.07676, avg_loss=0.07578]\n",
      "Step 546280  [5.470 sec/step, loss=0.07673, avg_loss=0.07581]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 546281  [5.438 sec/step, loss=0.07191, avg_loss=0.07574]\n",
      "Step 546282  [5.452 sec/step, loss=0.07613, avg_loss=0.07576]\n",
      "Step 546283  [5.470 sec/step, loss=0.07780, avg_loss=0.07578]\n",
      "Step 546284  [5.463 sec/step, loss=0.07298, avg_loss=0.07575]\n",
      "Step 546285  [5.502 sec/step, loss=0.06745, avg_loss=0.07566]\n",
      "Step 546286  [5.508 sec/step, loss=0.07779, avg_loss=0.07566]\n",
      "Step 546287  [5.474 sec/step, loss=0.07478, avg_loss=0.07565]\n",
      "Step 546288  [5.455 sec/step, loss=0.07419, avg_loss=0.07561]\n",
      "Step 546289  [5.462 sec/step, loss=0.07449, avg_loss=0.07561]\n",
      "Step 546290  [5.467 sec/step, loss=0.07660, avg_loss=0.07561]\n",
      "Step 546291  [5.465 sec/step, loss=0.07541, avg_loss=0.07557]\n",
      "Step 546292  [5.450 sec/step, loss=0.07689, avg_loss=0.07558]\n",
      "Step 546293  [5.441 sec/step, loss=0.07147, avg_loss=0.07551]\n",
      "Step 546294  [5.445 sec/step, loss=0.07637, avg_loss=0.07548]\n",
      "Step 546295  [5.446 sec/step, loss=0.07492, avg_loss=0.07545]\n",
      "Step 546296  [5.452 sec/step, loss=0.07513, avg_loss=0.07552]\n",
      "Step 546297  [5.455 sec/step, loss=0.07692, avg_loss=0.07551]\n",
      "Step 546298  [5.457 sec/step, loss=0.07719, avg_loss=0.07557]\n",
      "Step 546299  [5.446 sec/step, loss=0.07452, avg_loss=0.07552]\n",
      "Generated 32 batches of size 32 in 2.536 sec\n",
      "Step 546300  [5.469 sec/step, loss=0.07696, avg_loss=0.07552]\n",
      "Writing summary at step: 546300\n",
      "Step 546301  [5.471 sec/step, loss=0.07618, avg_loss=0.07552]\n",
      "Step 546302  [5.482 sec/step, loss=0.07786, avg_loss=0.07554]\n",
      "Step 546303  [5.487 sec/step, loss=0.07696, avg_loss=0.07554]\n",
      "Step 546304  [5.475 sec/step, loss=0.07598, avg_loss=0.07550]\n",
      "Step 546305  [5.471 sec/step, loss=0.07614, avg_loss=0.07551]\n",
      "Step 546306  [5.432 sec/step, loss=0.07718, avg_loss=0.07559]\n",
      "Step 546307  [5.409 sec/step, loss=0.06542, avg_loss=0.07547]\n",
      "Step 546308  [5.407 sec/step, loss=0.07450, avg_loss=0.07544]\n",
      "Step 546309  [5.403 sec/step, loss=0.07557, avg_loss=0.07542]\n",
      "Step 546310  [5.395 sec/step, loss=0.07230, avg_loss=0.07537]\n",
      "Step 546311  [5.396 sec/step, loss=0.07704, avg_loss=0.07536]\n",
      "Step 546312  [5.396 sec/step, loss=0.07192, avg_loss=0.07534]\n",
      "Step 546313  [5.416 sec/step, loss=0.07654, avg_loss=0.07539]\n",
      "Step 546314  [5.412 sec/step, loss=0.07710, avg_loss=0.07540]\n",
      "Step 546315  [5.411 sec/step, loss=0.07710, avg_loss=0.07540]\n",
      "Step 546316  [5.436 sec/step, loss=0.07664, avg_loss=0.07541]\n",
      "Step 546317  [5.393 sec/step, loss=0.06626, avg_loss=0.07532]\n",
      "Step 546318  [5.399 sec/step, loss=0.07717, avg_loss=0.07535]\n",
      "Step 546319  [5.405 sec/step, loss=0.07636, avg_loss=0.07534]\n",
      "Step 546320  [5.385 sec/step, loss=0.07490, avg_loss=0.07530]\n",
      "Step 546321  [5.386 sec/step, loss=0.07576, avg_loss=0.07530]\n",
      "Step 546322  [5.393 sec/step, loss=0.07629, avg_loss=0.07534]\n",
      "Step 546323  [5.401 sec/step, loss=0.07457, avg_loss=0.07533]\n",
      "Step 546324  [5.348 sec/step, loss=0.07368, avg_loss=0.07540]\n",
      "Step 546325  [5.341 sec/step, loss=0.07725, avg_loss=0.07539]\n",
      "Step 546326  [5.345 sec/step, loss=0.07643, avg_loss=0.07539]\n",
      "Step 546327  [5.339 sec/step, loss=0.07418, avg_loss=0.07535]\n",
      "Step 546328  [5.343 sec/step, loss=0.07709, avg_loss=0.07533]\n",
      "Step 546329  [5.336 sec/step, loss=0.07536, avg_loss=0.07531]\n",
      "Step 546330  [5.349 sec/step, loss=0.07634, avg_loss=0.07530]\n",
      "Generated 32 batches of size 32 in 2.472 sec\n",
      "Step 546331  [5.342 sec/step, loss=0.07568, avg_loss=0.07528]\n",
      "Step 546332  [5.328 sec/step, loss=0.07460, avg_loss=0.07525]\n",
      "Step 546333  [5.312 sec/step, loss=0.07479, avg_loss=0.07522]\n",
      "Step 546334  [5.353 sec/step, loss=0.07316, avg_loss=0.07522]\n",
      "Step 546335  [5.383 sec/step, loss=0.07632, avg_loss=0.07530]\n",
      "Step 546336  [5.415 sec/step, loss=0.06678, avg_loss=0.07519]\n",
      "Step 546337  [5.398 sec/step, loss=0.07291, avg_loss=0.07515]\n",
      "Step 546338  [5.406 sec/step, loss=0.07719, avg_loss=0.07514]\n",
      "Step 546339  [5.415 sec/step, loss=0.07639, avg_loss=0.07515]\n",
      "Step 546340  [5.428 sec/step, loss=0.07713, avg_loss=0.07514]\n",
      "Step 546341  [5.442 sec/step, loss=0.07561, avg_loss=0.07514]\n",
      "Step 546342  [5.438 sec/step, loss=0.07426, avg_loss=0.07512]\n",
      "Step 546343  [5.424 sec/step, loss=0.07138, avg_loss=0.07510]\n",
      "Step 546344  [5.453 sec/step, loss=0.07413, avg_loss=0.07509]\n",
      "Step 546345  [5.456 sec/step, loss=0.07550, avg_loss=0.07505]\n",
      "Step 546346  [5.427 sec/step, loss=0.07038, avg_loss=0.07501]\n",
      "Step 546347  [5.434 sec/step, loss=0.07696, avg_loss=0.07505]\n",
      "Step 546348  [5.423 sec/step, loss=0.07296, avg_loss=0.07501]\n",
      "Step 546349  [5.410 sec/step, loss=0.07543, avg_loss=0.07498]\n",
      "Step 546350  [5.401 sec/step, loss=0.07479, avg_loss=0.07495]\n",
      "Step 546351  [5.397 sec/step, loss=0.07615, avg_loss=0.07494]\n",
      "Step 546352  [5.395 sec/step, loss=0.07442, avg_loss=0.07492]\n",
      "Step 546353  [5.392 sec/step, loss=0.07694, avg_loss=0.07490]\n",
      "Step 546354  [5.419 sec/step, loss=0.07423, avg_loss=0.07488]\n",
      "Step 546355  [5.416 sec/step, loss=0.07596, avg_loss=0.07491]\n",
      "Step 546356  [5.421 sec/step, loss=0.07725, avg_loss=0.07490]\n",
      "Step 546357  [5.413 sec/step, loss=0.07359, avg_loss=0.07486]\n",
      "Step 546358  [5.398 sec/step, loss=0.07667, avg_loss=0.07485]\n",
      "Step 546359  [5.395 sec/step, loss=0.07589, avg_loss=0.07485]\n",
      "Step 546360  [5.401 sec/step, loss=0.07731, avg_loss=0.07489]\n",
      "Step 546361  [5.409 sec/step, loss=0.07526, avg_loss=0.07486]\n",
      "Step 546362  [5.342 sec/step, loss=0.06541, avg_loss=0.07483]\n",
      "Generated 32 batches of size 32 in 2.529 sec\n",
      "Step 546363  [5.360 sec/step, loss=0.07359, avg_loss=0.07489]\n",
      "Step 546364  [5.405 sec/step, loss=0.06658, avg_loss=0.07482]\n",
      "Step 546365  [5.410 sec/step, loss=0.07699, avg_loss=0.07484]\n",
      "Step 546366  [5.423 sec/step, loss=0.07542, avg_loss=0.07487]\n",
      "Step 546367  [5.420 sec/step, loss=0.07639, avg_loss=0.07486]\n",
      "Step 546368  [5.405 sec/step, loss=0.07486, avg_loss=0.07485]\n",
      "Step 546369  [5.407 sec/step, loss=0.07562, avg_loss=0.07486]\n",
      "Step 546370  [5.417 sec/step, loss=0.07577, avg_loss=0.07487]\n",
      "Step 546371  [5.423 sec/step, loss=0.07428, avg_loss=0.07488]\n",
      "Step 546372  [5.428 sec/step, loss=0.07467, avg_loss=0.07491]\n",
      "Step 546373  [5.410 sec/step, loss=0.07684, avg_loss=0.07493]\n",
      "Step 546374  [5.463 sec/step, loss=0.06671, avg_loss=0.07484]\n",
      "Step 546375  [5.476 sec/step, loss=0.07564, avg_loss=0.07483]\n",
      "Step 546376  [5.469 sec/step, loss=0.07047, avg_loss=0.07478]\n",
      "Step 546377  [5.471 sec/step, loss=0.07765, avg_loss=0.07481]\n",
      "Step 546378  [5.459 sec/step, loss=0.07629, avg_loss=0.07483]\n",
      "Step 546379  [5.444 sec/step, loss=0.07487, avg_loss=0.07481]\n",
      "Step 546380  [5.441 sec/step, loss=0.07599, avg_loss=0.07480]\n",
      "Step 546381  [5.456 sec/step, loss=0.07532, avg_loss=0.07483]\n",
      "Step 546382  [5.463 sec/step, loss=0.07628, avg_loss=0.07484]\n",
      "Step 546383  [5.436 sec/step, loss=0.06675, avg_loss=0.07473]\n",
      "Step 546384  [5.435 sec/step, loss=0.07849, avg_loss=0.07478]\n",
      "Step 546385  [5.377 sec/step, loss=0.07431, avg_loss=0.07485]\n",
      "Step 546386  [5.372 sec/step, loss=0.07561, avg_loss=0.07483]\n",
      "Step 546387  [5.405 sec/step, loss=0.07316, avg_loss=0.07481]\n",
      "Step 546388  [5.415 sec/step, loss=0.07712, avg_loss=0.07484]\n",
      "Step 546389  [5.432 sec/step, loss=0.07472, avg_loss=0.07484]\n",
      "Step 546390  [5.430 sec/step, loss=0.07539, avg_loss=0.07483]\n",
      "Step 546391  [5.423 sec/step, loss=0.07412, avg_loss=0.07482]\n",
      "Step 546392  [5.405 sec/step, loss=0.07432, avg_loss=0.07479]\n",
      "Step 546393  [5.419 sec/step, loss=0.07499, avg_loss=0.07483]\n",
      "Step 546394  [5.425 sec/step, loss=0.07643, avg_loss=0.07483]\n",
      "Generated 32 batches of size 32 in 2.422 sec\n",
      "Step 546395  [5.423 sec/step, loss=0.07475, avg_loss=0.07483]\n",
      "Step 546396  [5.441 sec/step, loss=0.07723, avg_loss=0.07485]\n",
      "Step 546397  [5.432 sec/step, loss=0.07268, avg_loss=0.07480]\n",
      "Step 546398  [5.445 sec/step, loss=0.07709, avg_loss=0.07480]\n",
      "Step 546399  [5.442 sec/step, loss=0.07380, avg_loss=0.07480]\n",
      "Step 546400  [5.405 sec/step, loss=0.07267, avg_loss=0.07475]\n",
      "Writing summary at step: 546400\n",
      "Step 546401  [5.397 sec/step, loss=0.07559, avg_loss=0.07475]\n",
      "Step 546402  [5.386 sec/step, loss=0.07193, avg_loss=0.07469]\n",
      "Step 546403  [5.386 sec/step, loss=0.07270, avg_loss=0.07465]\n",
      "Step 546404  [5.434 sec/step, loss=0.06802, avg_loss=0.07457]\n",
      "Step 546405  [5.435 sec/step, loss=0.07450, avg_loss=0.07455]\n",
      "Step 546406  [5.409 sec/step, loss=0.07201, avg_loss=0.07450]\n",
      "Step 546407  [5.411 sec/step, loss=0.06576, avg_loss=0.07450]\n",
      "Step 546408  [5.423 sec/step, loss=0.07621, avg_loss=0.07452]\n",
      "Step 546409  [5.410 sec/step, loss=0.07053, avg_loss=0.07447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 546410  [5.417 sec/step, loss=0.07522, avg_loss=0.07450]\n",
      "Step 546411  [5.400 sec/step, loss=0.07171, avg_loss=0.07444]\n",
      "Step 546412  [5.421 sec/step, loss=0.07470, avg_loss=0.07447]\n",
      "Step 546413  [5.434 sec/step, loss=0.07453, avg_loss=0.07445]\n",
      "Step 546414  [5.433 sec/step, loss=0.07580, avg_loss=0.07444]\n",
      "Step 546415  [5.418 sec/step, loss=0.07438, avg_loss=0.07441]\n",
      "Step 546416  [5.410 sec/step, loss=0.07495, avg_loss=0.07439]\n",
      "Step 546417  [5.435 sec/step, loss=0.07617, avg_loss=0.07449]\n",
      "Step 546418  [5.432 sec/step, loss=0.07688, avg_loss=0.07449]\n",
      "Step 546419  [5.425 sec/step, loss=0.07424, avg_loss=0.07447]\n",
      "Step 546420  [5.443 sec/step, loss=0.07708, avg_loss=0.07449]\n",
      "Step 546421  [5.440 sec/step, loss=0.07297, avg_loss=0.07446]\n",
      "Step 546422  [5.437 sec/step, loss=0.07143, avg_loss=0.07441]\n",
      "Step 546423  [5.428 sec/step, loss=0.07521, avg_loss=0.07442]\n",
      "Step 546424  [5.443 sec/step, loss=0.07641, avg_loss=0.07445]\n",
      "Step 546425  [5.441 sec/step, loss=0.07271, avg_loss=0.07440]\n",
      "Generated 32 batches of size 32 in 2.445 sec\n",
      "Step 546426  [5.444 sec/step, loss=0.07463, avg_loss=0.07439]\n",
      "Step 546427  [5.456 sec/step, loss=0.07615, avg_loss=0.07441]\n",
      "Step 546428  [5.471 sec/step, loss=0.07398, avg_loss=0.07437]\n",
      "Step 546429  [5.471 sec/step, loss=0.07734, avg_loss=0.07439]\n",
      "Step 546430  [5.470 sec/step, loss=0.07762, avg_loss=0.07441]\n",
      "Step 546431  [5.465 sec/step, loss=0.07492, avg_loss=0.07440]\n",
      "Step 546432  [5.464 sec/step, loss=0.07429, avg_loss=0.07440]\n",
      "Step 546433  [5.473 sec/step, loss=0.07646, avg_loss=0.07441]\n",
      "Step 546434  [5.440 sec/step, loss=0.06951, avg_loss=0.07438]\n",
      "Step 546435  [5.433 sec/step, loss=0.07447, avg_loss=0.07436]\n",
      "Step 546436  [5.407 sec/step, loss=0.07529, avg_loss=0.07444]\n",
      "Step 546437  [5.410 sec/step, loss=0.07155, avg_loss=0.07443]\n",
      "Step 546438  [5.407 sec/step, loss=0.07276, avg_loss=0.07438]\n",
      "Step 546439  [5.416 sec/step, loss=0.07517, avg_loss=0.07437]\n",
      "Step 546440  [5.403 sec/step, loss=0.07551, avg_loss=0.07436]\n",
      "Step 546441  [5.441 sec/step, loss=0.06508, avg_loss=0.07425]\n",
      "Step 546442  [5.449 sec/step, loss=0.07632, avg_loss=0.07427]\n",
      "Step 546443  [5.453 sec/step, loss=0.07438, avg_loss=0.07430]\n",
      "Step 546444  [5.422 sec/step, loss=0.07394, avg_loss=0.07430]\n",
      "Step 546445  [5.420 sec/step, loss=0.07642, avg_loss=0.07431]\n",
      "Step 546446  [5.432 sec/step, loss=0.07553, avg_loss=0.07436]\n",
      "Step 546447  [5.416 sec/step, loss=0.07373, avg_loss=0.07433]\n",
      "Step 546448  [5.420 sec/step, loss=0.07609, avg_loss=0.07436]\n",
      "Step 546449  [5.428 sec/step, loss=0.07626, avg_loss=0.07437]\n",
      "Step 546450  [5.443 sec/step, loss=0.07655, avg_loss=0.07439]\n",
      "Step 546451  [5.424 sec/step, loss=0.07103, avg_loss=0.07433]\n",
      "Step 546452  [5.424 sec/step, loss=0.07334, avg_loss=0.07432]\n",
      "Step 546453  [5.428 sec/step, loss=0.07405, avg_loss=0.07429]\n",
      "Step 546454  [5.410 sec/step, loss=0.07540, avg_loss=0.07431]\n",
      "Step 546455  [5.426 sec/step, loss=0.07577, avg_loss=0.07430]\n",
      "Step 546456  [5.409 sec/step, loss=0.07677, avg_loss=0.07430]\n",
      "Step 546457  [5.420 sec/step, loss=0.07501, avg_loss=0.07431]\n",
      "Generated 32 batches of size 32 in 2.438 sec\n",
      "Step 546458  [5.429 sec/step, loss=0.07563, avg_loss=0.07430]\n",
      "Step 546459  [5.417 sec/step, loss=0.07374, avg_loss=0.07428]\n",
      "Step 546460  [5.397 sec/step, loss=0.06545, avg_loss=0.07416]\n",
      "Step 546461  [5.379 sec/step, loss=0.07411, avg_loss=0.07415]\n",
      "Step 546462  [5.406 sec/step, loss=0.07658, avg_loss=0.07426]\n",
      "Step 546463  [5.402 sec/step, loss=0.07537, avg_loss=0.07428]\n",
      "Step 546464  [5.364 sec/step, loss=0.07655, avg_loss=0.07438]\n",
      "Step 546465  [5.370 sec/step, loss=0.07661, avg_loss=0.07438]\n",
      "Step 546466  [5.380 sec/step, loss=0.07626, avg_loss=0.07439]\n",
      "Step 546467  [5.373 sec/step, loss=0.07648, avg_loss=0.07439]\n",
      "Step 546468  [5.423 sec/step, loss=0.06729, avg_loss=0.07431]\n",
      "Step 546469  [5.454 sec/step, loss=0.07290, avg_loss=0.07428]\n",
      "Step 546470  [5.444 sec/step, loss=0.07197, avg_loss=0.07425]\n",
      "Step 546471  [5.448 sec/step, loss=0.07467, avg_loss=0.07425]\n",
      "Step 546472  [5.456 sec/step, loss=0.07472, avg_loss=0.07425]\n",
      "Step 546473  [5.448 sec/step, loss=0.07459, avg_loss=0.07423]\n",
      "Step 546474  [5.398 sec/step, loss=0.07555, avg_loss=0.07432]\n",
      "Step 546475  [5.380 sec/step, loss=0.07507, avg_loss=0.07431]\n",
      "Step 546476  [5.386 sec/step, loss=0.07418, avg_loss=0.07435]\n",
      "Step 546477  [5.379 sec/step, loss=0.07367, avg_loss=0.07431]\n",
      "Step 546478  [5.353 sec/step, loss=0.07249, avg_loss=0.07427]\n",
      "Step 546479  [5.368 sec/step, loss=0.07607, avg_loss=0.07428]\n",
      "Step 546480  [5.357 sec/step, loss=0.07384, avg_loss=0.07426]\n",
      "Step 546481  [5.367 sec/step, loss=0.07365, avg_loss=0.07424]\n",
      "Step 546482  [5.353 sec/step, loss=0.07624, avg_loss=0.07424]\n",
      "Step 546483  [5.362 sec/step, loss=0.07317, avg_loss=0.07431]\n",
      "Step 546484  [5.365 sec/step, loss=0.07536, avg_loss=0.07428]\n",
      "Step 546485  [5.378 sec/step, loss=0.07542, avg_loss=0.07429]\n",
      "Step 546486  [5.384 sec/step, loss=0.07731, avg_loss=0.07430]\n",
      "Step 546487  [5.357 sec/step, loss=0.07275, avg_loss=0.07430]\n",
      "Step 546488  [5.336 sec/step, loss=0.06727, avg_loss=0.07420]\n",
      "Step 546489  [5.321 sec/step, loss=0.07591, avg_loss=0.07421]\n",
      "Generated 32 batches of size 32 in 2.374 sec\n",
      "Step 546490  [5.337 sec/step, loss=0.07631, avg_loss=0.07422]\n",
      "Step 546491  [5.345 sec/step, loss=0.07428, avg_loss=0.07422]\n",
      "Step 546492  [5.374 sec/step, loss=0.07596, avg_loss=0.07424]\n",
      "Step 546493  [5.372 sec/step, loss=0.07537, avg_loss=0.07424]\n",
      "Step 546494  [5.377 sec/step, loss=0.07598, avg_loss=0.07424]\n",
      "Step 546495  [5.378 sec/step, loss=0.07623, avg_loss=0.07425]\n",
      "Step 546496  [5.359 sec/step, loss=0.07109, avg_loss=0.07419]\n",
      "Step 546497  [5.362 sec/step, loss=0.07173, avg_loss=0.07418]\n",
      "Step 546498  [5.366 sec/step, loss=0.07576, avg_loss=0.07417]\n",
      "Step 546499  [5.370 sec/step, loss=0.07258, avg_loss=0.07416]\n",
      "Step 546500  [5.393 sec/step, loss=0.07634, avg_loss=0.07419]\n",
      "Writing summary at step: 546500\n",
      "Step 546501  [5.373 sec/step, loss=0.07029, avg_loss=0.07414]\n",
      "Step 546502  [5.373 sec/step, loss=0.07390, avg_loss=0.07416]\n",
      "Step 546503  [5.377 sec/step, loss=0.07582, avg_loss=0.07419]\n",
      "Step 546504  [5.322 sec/step, loss=0.07808, avg_loss=0.07429]\n",
      "Step 546505  [5.335 sec/step, loss=0.07617, avg_loss=0.07431]\n",
      "Step 546506  [5.352 sec/step, loss=0.07660, avg_loss=0.07436]\n",
      "Step 546507  [5.380 sec/step, loss=0.07422, avg_loss=0.07444]\n",
      "Step 546508  [5.423 sec/step, loss=0.06679, avg_loss=0.07435]\n",
      "Step 546509  [5.434 sec/step, loss=0.07564, avg_loss=0.07440]\n",
      "Step 546510  [5.433 sec/step, loss=0.07466, avg_loss=0.07439]\n",
      "Step 546511  [5.446 sec/step, loss=0.07242, avg_loss=0.07440]\n",
      "Step 546512  [5.433 sec/step, loss=0.07062, avg_loss=0.07436]\n",
      "Step 546513  [5.413 sec/step, loss=0.07467, avg_loss=0.07436]\n",
      "Step 546514  [5.430 sec/step, loss=0.07372, avg_loss=0.07434]\n",
      "Step 546515  [5.432 sec/step, loss=0.07490, avg_loss=0.07434]\n",
      "Step 546516  [5.406 sec/step, loss=0.06493, avg_loss=0.07424]\n",
      "Step 546517  [5.391 sec/step, loss=0.07425, avg_loss=0.07422]\n",
      "Step 546518  [5.372 sec/step, loss=0.07102, avg_loss=0.07417]\n",
      "Step 546519  [5.376 sec/step, loss=0.07564, avg_loss=0.07418]\n",
      "Step 546520  [5.376 sec/step, loss=0.07655, avg_loss=0.07417]\n",
      "Generated 32 batches of size 32 in 2.392 sec\n",
      "Step 546521  [5.391 sec/step, loss=0.07616, avg_loss=0.07421]\n",
      "Step 546522  [5.394 sec/step, loss=0.07615, avg_loss=0.07425]\n",
      "Step 546523  [5.399 sec/step, loss=0.07490, avg_loss=0.07425]\n",
      "Step 546524  [5.382 sec/step, loss=0.07418, avg_loss=0.07423]\n",
      "Step 546525  [5.374 sec/step, loss=0.07403, avg_loss=0.07424]\n",
      "Step 546526  [5.390 sec/step, loss=0.07330, avg_loss=0.07423]\n",
      "Step 546527  [5.372 sec/step, loss=0.07354, avg_loss=0.07420]\n",
      "Step 546528  [5.358 sec/step, loss=0.07637, avg_loss=0.07423]\n",
      "Step 546529  [5.362 sec/step, loss=0.07594, avg_loss=0.07421]\n",
      "Step 546530  [5.372 sec/step, loss=0.07570, avg_loss=0.07419]\n",
      "Step 546531  [5.379 sec/step, loss=0.07604, avg_loss=0.07420]\n",
      "Step 546532  [5.397 sec/step, loss=0.07598, avg_loss=0.07422]\n",
      "Step 546533  [5.393 sec/step, loss=0.07150, avg_loss=0.07417]\n",
      "Step 546534  [5.388 sec/step, loss=0.07353, avg_loss=0.07421]\n",
      "Step 546535  [5.388 sec/step, loss=0.07495, avg_loss=0.07422]\n",
      "Step 546536  [5.379 sec/step, loss=0.07559, avg_loss=0.07422]\n",
      "Step 546537  [5.398 sec/step, loss=0.07670, avg_loss=0.07427]\n",
      "Step 546538  [5.383 sec/step, loss=0.07154, avg_loss=0.07426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 546539  [5.380 sec/step, loss=0.07299, avg_loss=0.07424]\n",
      "Step 546540  [5.377 sec/step, loss=0.07620, avg_loss=0.07424]\n",
      "Step 546541  [5.328 sec/step, loss=0.07596, avg_loss=0.07435]\n",
      "Step 546542  [5.329 sec/step, loss=0.07619, avg_loss=0.07435]\n",
      "Step 546543  [5.323 sec/step, loss=0.06597, avg_loss=0.07427]\n",
      "Step 546544  [5.327 sec/step, loss=0.07319, avg_loss=0.07426]\n",
      "Step 546545  [5.328 sec/step, loss=0.07313, avg_loss=0.07423]\n",
      "Step 546546  [5.325 sec/step, loss=0.07561, avg_loss=0.07423]\n",
      "Step 546547  [5.384 sec/step, loss=0.06642, avg_loss=0.07415]\n",
      "Step 546548  [5.377 sec/step, loss=0.07366, avg_loss=0.07413]\n",
      "Step 546549  [5.368 sec/step, loss=0.07494, avg_loss=0.07412]\n",
      "Step 546550  [5.357 sec/step, loss=0.07328, avg_loss=0.07408]\n",
      "Step 546551  [5.379 sec/step, loss=0.07353, avg_loss=0.07411]\n",
      "Step 546552  [5.387 sec/step, loss=0.07629, avg_loss=0.07414]\n",
      "Generated 32 batches of size 32 in 2.567 sec\n",
      "Step 546553  [5.379 sec/step, loss=0.07633, avg_loss=0.07416]\n",
      "Step 546554  [5.361 sec/step, loss=0.07114, avg_loss=0.07412]\n",
      "Step 546555  [5.358 sec/step, loss=0.07541, avg_loss=0.07412]\n",
      "Step 546556  [5.366 sec/step, loss=0.07372, avg_loss=0.07408]\n",
      "Step 546557  [5.363 sec/step, loss=0.07538, avg_loss=0.07409]\n",
      "Step 546558  [5.376 sec/step, loss=0.07495, avg_loss=0.07408]\n",
      "Step 546559  [5.372 sec/step, loss=0.07312, avg_loss=0.07408]\n",
      "Step 546560  [5.389 sec/step, loss=0.07521, avg_loss=0.07417]\n",
      "Step 546561  [5.407 sec/step, loss=0.07567, avg_loss=0.07419]\n",
      "Step 546562  [5.406 sec/step, loss=0.07561, avg_loss=0.07418]\n",
      "Step 546563  [5.421 sec/step, loss=0.07613, avg_loss=0.07419]\n",
      "Step 546564  [5.426 sec/step, loss=0.07577, avg_loss=0.07418]\n",
      "Step 546565  [5.418 sec/step, loss=0.07375, avg_loss=0.07415]\n",
      "Step 546566  [5.411 sec/step, loss=0.07181, avg_loss=0.07411]\n",
      "Step 546567  [5.421 sec/step, loss=0.07408, avg_loss=0.07408]\n",
      "Step 546568  [5.364 sec/step, loss=0.07119, avg_loss=0.07412]\n",
      "Step 546569  [5.339 sec/step, loss=0.07505, avg_loss=0.07414]\n",
      "Step 546570  [5.347 sec/step, loss=0.07487, avg_loss=0.07417]\n",
      "Step 546571  [5.366 sec/step, loss=0.07380, avg_loss=0.07416]\n",
      "Step 546572  [5.399 sec/step, loss=0.07289, avg_loss=0.07414]\n",
      "Step 546573  [5.397 sec/step, loss=0.07263, avg_loss=0.07412]\n",
      "Step 546574  [5.451 sec/step, loss=0.06552, avg_loss=0.07402]\n",
      "Step 546575  [5.460 sec/step, loss=0.07242, avg_loss=0.07400]\n",
      "Step 546576  [5.463 sec/step, loss=0.07606, avg_loss=0.07402]\n",
      "Step 546577  [5.470 sec/step, loss=0.07427, avg_loss=0.07402]\n",
      "Step 546578  [5.493 sec/step, loss=0.07644, avg_loss=0.07406]\n",
      "Step 546579  [5.478 sec/step, loss=0.07333, avg_loss=0.07403]\n",
      "Step 546580  [5.488 sec/step, loss=0.07337, avg_loss=0.07403]\n",
      "Step 546581  [5.491 sec/step, loss=0.07630, avg_loss=0.07406]\n",
      "Step 546582  [5.501 sec/step, loss=0.07646, avg_loss=0.07406]\n",
      "Step 546583  [5.530 sec/step, loss=0.07603, avg_loss=0.07409]\n",
      "Step 546584  [5.535 sec/step, loss=0.07126, avg_loss=0.07405]\n",
      "Generated 32 batches of size 32 in 2.813 sec\n",
      "Step 546585  [5.539 sec/step, loss=0.07517, avg_loss=0.07404]\n",
      "Step 546586  [5.535 sec/step, loss=0.07558, avg_loss=0.07403]\n",
      "Step 546587  [5.522 sec/step, loss=0.06616, avg_loss=0.07396]\n",
      "Step 546588  [5.526 sec/step, loss=0.07171, avg_loss=0.07400]\n",
      "Step 546589  [5.546 sec/step, loss=0.07632, avg_loss=0.07401]\n",
      "Step 546590  [5.538 sec/step, loss=0.07612, avg_loss=0.07401]\n",
      "Step 546591  [5.529 sec/step, loss=0.07613, avg_loss=0.07403]\n",
      "Step 546592  [5.502 sec/step, loss=0.07360, avg_loss=0.07400]\n",
      "Step 546593  [5.502 sec/step, loss=0.07381, avg_loss=0.07399]\n",
      "Step 546594  [5.483 sec/step, loss=0.07023, avg_loss=0.07393]\n",
      "Step 546595  [5.490 sec/step, loss=0.07599, avg_loss=0.07393]\n",
      "Step 546596  [5.502 sec/step, loss=0.07453, avg_loss=0.07396]\n",
      "Step 546597  [5.521 sec/step, loss=0.07609, avg_loss=0.07400]\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python train.py --base_dir . --input training-aneesa/train.txt --name aneesa --restore_step 500000 --hparams \"max_iters=300\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mukhtar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --base_dir . --input training-mukhtar/train.txt --name mukhtar --restore_step 441000 --hparams \"max_iters=400\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note run pronouncer before evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python eval.py --checkpoint logs-aleena/model.ckpt-541000 --hparams \"max_iters=400\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Hyperparameters:\n",
      "  adam_beta1: 0.9\n",
      "  adam_beta2: 0.999\n",
      "  attention_depth: 256\n",
      "  batch_size: 32\n",
      "  cleaners: basic_cleaners\n",
      "  decay_learning_rate: True\n",
      "  decoder_depth: 256\n",
      "  embed_depth: 256\n",
      "  encoder_depth: 256\n",
      "  frame_length_ms: 50\n",
      "  frame_shift_ms: 12.5\n",
      "  griffin_lim_iters: 60\n",
      "  initial_learning_rate: 0.002\n",
      "  max_iters: 300\n",
      "  min_level_db: -100\n",
      "  num_freq: 1025\n",
      "  num_mels: 80\n",
      "  outputs_per_step: 5\n",
      "  postnet_depth: 256\n",
      "  power: 1.5\n",
      "  preemphasis: 0.97\n",
      "  prenet_depths: [256, 128]\n",
      "  ref_level_db: 20\n",
      "  sample_rate: 20000\n",
      "  use_cmudict: False\n",
      "Constructing model: tacotron\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tvenv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tacotron/models/modules.py:10: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tacotron/models/modules.py:11: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tacotron/models/modules.py:106: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv1d instead.\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tacotron/models/modules.py:107: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tacotron/models/modules.py:52: max_pooling1d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling1d instead.\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tacotron/models/modules.py:75: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tacotron/models/modules.py:79: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tvenv/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tvenv/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tacotron/models/tacotron.py:68: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "Initialized Tacotron model. Dimensions: \n",
      "  embedding:               256\n",
      "  prenet out:              128\n",
      "  encoder out:             256\n",
      "  attention out:           256\n",
      "  concat attn & out:       512\n",
      "  decoder cell out:        256\n",
      "  decoder out (5 frames):  400\n",
      "  decoder out (1 frame):   80\n",
      "  postnet out:             256\n",
      "  linear out:              1025\n",
      "Loading checkpoint: logs-aneesa/model.ckpt-546000\n",
      "WARNING:tensorflow:From /home/itu/Desktop/UrduTTSNew/UrduTTS/tvenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "Synthesizing: logs-aneesa/eval-546000-000.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-001.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-002.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-003.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-004.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-005.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-006.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-007.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-008.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-009.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-010.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-011.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-012.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-013.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-014.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-015.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-016.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-017.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-018.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-019.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-020.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-021.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-022.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-023.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-024.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-025.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-026.wav\n",
      "Synthesizing: logs-aneesa/eval-546000-027.wav\n"
     ]
    }
   ],
   "source": [
    "!python eval.py --checkpoint logs-aneesa/model.ckpt-546000 --hparams \"max_iters=300\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
